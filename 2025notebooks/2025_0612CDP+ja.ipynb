{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2025notebooks/2025_0612CDP%2Bja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "filename: 2026_0612CDP+ja.ipynb\n",
        "author: 浅川伸一\n",
        "---\n",
        "\n",
        "# 2025_0623 の議論\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_CDP_fig1.svg\">\n",
        "Zorzi+(1998) Fig.1 Architecture of the model. The arrow means full connectivity between layers. Each box stand for a group of letters (26) or phonemes (44).<br/>\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_CDP_fig8.svg\">\n",
        "<p>Zorzi+(1998) Fig.8. Architecture of the model with the hidden layer pathway. In both the direct pathway and the mediated pathway the layers are fully connected (arrows).</p>\n",
        "\n",
        "\n",
        "orth1, orth2 --> embeddings --> phonology ($p_1,p_2,\\ldots,p_n$) という流れと<br/>\n",
        "\n",
        "```\n",
        "orth1 --> emb1 ---+\n",
        "                  |---> phonology\n",
        "orth2 --> emb2 ---+\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "orth1 ---+\n",
        "         |--> emb ---> phonology\n",
        "orth2 ---+\n",
        "```\n",
        "\n",
        "など考えたら色々と考える必要がありそうだ。"
      ],
      "metadata": {
        "id": "gtTVHJK-oysf"
      },
      "id": "gtTVHJK-oysf"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fce496ec-2d3c-48b2-9e88-ce66421100a2",
      "metadata": {
        "id": "fce496ec-2d3c-48b2-9e88-ce66421100a2",
        "outputId": "7fd196ef-0ce9-4df3-efb8-69a23074ac3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:cuda:0\n"
          ]
        }
      ],
      "source": [
        "import IPython\n",
        "isColab = 'colab' in str(IPython.get_ipython())\n",
        "\n",
        "import torch\n",
        "device=torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "#device='cpu'\n",
        "print(f'device:{device}')\n",
        "\n",
        "# 必要なライブラリの輸入\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import operator\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "class chars_joyo():\n",
        "    \"\"\"\n",
        "    https://ja.wikipedia.org/wiki/%E5%B8%B8%E7%94%A8%E6%BC%A2%E5%AD%97%E4%B8%80%E8%A6%A7\n",
        "    常用漢字一覧（じょうようかんじいちらん）\n",
        "\n",
        "    常用漢字は 2136 字。\n",
        "    下表の配列は常用漢字表（平成22年内閣告示第2号）に準じる。\n",
        "    学年の数字は、小学校学習指導要領（2017年3月告示）の学年別漢字配当表において配当されている学年を示す。\n",
        "    Sは中学校以降で習うことを意味する。\n",
        "    音訓は、常用漢字表に掲げられた音訓を示す。\n",
        "    片仮名は音読み、平仮名は訓読みである。\n",
        "    括弧でくくられた音訓は「特別なものか、又は用法のごく狭いもの」として、1字下げで示されたものである。\n",
        "    ハイフンは送り仮名の付け方（昭和48年内閣告示第2号[1]）による送り仮名の区切りである。\n",
        "    音訓および付表の語の学校段階（小学校・中学校・高等学校）ごとの割り振りについては、音訓の小・中・高等学校段階別割り振り表（2017年3月）を参照。\n",
        "    通用字体は、常用漢字表に掲げられた「印刷文字における現代の通用字体」を示した[2]。\n",
        "    手書き文字（筆写の楷書）の字形と印刷文字の字形に関しては、常用漢字表の字体・字形に関する指針 (PDF) （文化審議会国語分科会報告）を参照。\n",
        "    旧字体は、『新潮日本語漢字辞典』（新潮社、2007年）の「旧字」を示した[3]。\n",
        "    常用漢字表に掲げられた「いわゆる康熙字典体」とは必ずしも一致しない[4]。\n",
        "    なお、表外漢字字体表の簡易慣用字体が通用字体として採用されたものについては、印刷標準字体を旧字体として示した。\n",
        "    部首は康熙字典（214部）に従った。\n",
        "    康熙字典にない字についても、康熙字典に倣って部首を示した。\n",
        "    その際、当用漢字表を参考にした。\n",
        "    画数の数え方が問題となるものは、以下の通りとした。\n",
        "    「牙」……4画[5]\n",
        "    「捗」……10画[6]\n",
        "    「衷」……10画[7]\n",
        "    「葛」……12画[6]\n",
        "    「僅」……13画[6]\n",
        "    「嗅」……13画[6]\n",
        "    「塡」……13画[6]\n",
        "    「箋」……14画[6]\n",
        "    「遜」……14画[8]\n",
        "    「遡」……14画[8]\n",
        "    「稽」……15画[6]\n",
        "    「箸」……15画[6]\n",
        "    「餅」……15画[6]\n",
        "    「餌」……15画[6]\n",
        "    「賭」……16画[6]\n",
        "    「頰」……16画[6]\n",
        "    「謎」……17画[8]\n",
        "    「韓」……18画[5]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        #url = 'https://raw.githubusercontent.com/cjkvi/cjkvi-tables/master/joyo2010.txt'\n",
        "        #url = 'https://raw.githubusercontent.com/cjkvi/cjkvi-tables/master/jinmei2010.txt'\n",
        "        url = 'https://raw.githubusercontent.com/cjkvi/cjkvi-tables/15569eaae99daef9f99f0383e9d8efbec64a7c5a/joyo2010.txt'\n",
        "        joyo_fname = url.split('/')[-1]\n",
        "        joyo_fname = os.path.join(os.getcwd(), 'RAM', joyo_fname)\n",
        "        if os.path.exists(joyo_fname):\n",
        "            joyo_df = pd.read_csv(joyo_fname, header=None, skiprows=1, delimiter='\\t')\n",
        "        else:\n",
        "             joyo_df = pd.read_csv(url, header=None, skiprows=1, delimiter='\\t')\n",
        "\n",
        "        # カラム名の設定\n",
        "        joyo_df.columns = ['通用字体', '旧字体', '総画', '学年', '追加年, 削除年', '音訓']\n",
        "        #print(joyo_df.shape) # (2136, 6)\n",
        "\n",
        "        self.char_list = joyo_df['通用字体'].to_list()\n",
        "        self.df = joyo_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9026f18c-c1b7-44d3-85cf-c91aecacd9eb",
      "metadata": {
        "id": "9026f18c-c1b7-44d3-85cf-c91aecacd9eb"
      },
      "outputs": [],
      "source": [
        "# 書記素の定義，書記素のうちカタカナを音韻表現としても利用\n",
        "\n",
        "seed = 42\n",
        "special_tokens = ['<PAD>', '<EOW>', '<SOW>', '<UNK>']\n",
        "alphabet_upper_chars='ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ'\n",
        "alphabet_lower_chars='ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ'\n",
        "num_chars='０１２３４５６７８９'\n",
        "hira_chars='ぁあぃいぅうぇえぉおかがきぎくぐけげこごさざしじすずせぜそぞただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもゃやゅゆょよらりるれろゎわゐゑをん'\n",
        "kata_chars='ァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモャヤュユョヨラリルレロヮワヰヱヲンヴヵヶ'\n",
        "#kata_chars=kata_chars+'一'  # カタカナ文字に伸ばし記号を加える\n",
        "phon_list = list(kata_chars+'一')\n",
        "\n",
        "# 常用漢字\n",
        "joyo_chars = \"\".join([ch for ch in chars_joyo().char_list])\n",
        "\n",
        "# 学習漢字 学年別\n",
        "_gakushu_list = ['一右雨円王音下火花貝学気休玉金九空月犬見五口校左三山四子糸字耳七車手十出女小上森人水正生青石赤先千川早草足村大男竹中虫町天田土二日入年白八百文本名木目夕立力林六',\n",
        "'引羽雲園遠黄何夏家科歌画会回海絵外角楽活間丸岩顔帰汽記弓牛魚京強教近兄形計元原言古戸午後語交光公工広考行高合国黒今才細作算姉市思止紙寺時自室社弱首秋週春書少場色食心新親図数星晴声西切雪線船前組走多太体台谷知地池茶昼朝長鳥直通弟店点電冬刀東当答頭同道読内南肉馬買売麦半番父風分聞米歩母方北妹毎万明鳴毛門夜野矢友曜用来理里話',\n",
        "'悪安暗委意医育員飲院運泳駅横屋温化荷界開階寒感漢館岸期起客宮急球究級去橋業局曲銀区苦具君係軽決血研県庫湖向幸港号根祭坂皿仕使始指死詩歯事持次式実写者主取守酒受州拾終習集住重宿所暑助勝商昭消章乗植深申真神身進世整昔全想相送息速族他打対待代第題炭短談着柱注丁帳調追定庭笛鉄転登都度島投湯等豆動童農波配倍箱畑発反板悲皮美鼻筆氷表病秒品夫負部服福物平返勉放味命面問役薬油有由遊予様洋羊葉陽落流旅両緑礼列練路和',\n",
        "'愛案以位囲胃衣印栄英塩央億加果課貨芽改械害街各覚完官管観関願喜器希旗機季紀議救求泣給挙漁競共協鏡極訓軍郡型径景芸欠結健建験固候功好康航告差最菜材昨刷察札殺参散産残司史士氏試児治辞失借種周祝順初唱松焼照省笑象賞信臣成清静席積折節説戦浅選然倉巣争側束続卒孫帯隊達単置仲貯兆腸低停底的典伝徒努灯働堂得特毒熱念敗梅博飯費飛必標票不付府副粉兵別変辺便包法望牧末満未脈民無約勇要養浴利陸料良量輪類令例冷歴連労老録',\n",
        "'圧易移因営永衛液益演往応恩仮価可河過賀解快格確額刊幹慣眼基寄規技義逆久旧居許境興均禁句群経潔件券検険減現限個故護効厚構耕講鉱混査再妻採災際在罪財桜雑賛酸師志支枝資飼似示識質舎謝授修術述準序承招証常情条状織職制勢性政精製税績責接設絶舌銭祖素総像増造則測属損態貸退団断築張提程敵適統導銅徳独任燃能破判版犯比肥非備俵評貧婦富布武復複仏編弁保墓報豊暴貿防務夢迷綿輸余預容率略留領',\n",
        "'異遺域宇映延沿我灰拡閣革割株巻干看簡危揮机貴疑吸供胸郷勤筋敬系警劇激穴憲権絹厳源呼己誤后孝皇紅鋼降刻穀骨困砂座済裁策冊蚕姿私至視詞誌磁射捨尺若樹収宗就衆従縦縮熟純処署諸除傷将障城蒸針仁垂推寸盛聖誠宣専泉洗染善創奏層操窓装臓蔵存尊宅担探誕暖段値宙忠著庁潮頂賃痛展党糖討届難乳認納脳派俳拝背肺班晩否批秘腹奮並閉陛片補暮宝訪亡忘棒枚幕密盟模訳優郵幼欲翌乱卵覧裏律臨朗論']\n",
        "\n",
        "_l = []\n",
        "for g in _gakushu_list:\n",
        "    for ch in g:\n",
        "        _l += ch\n",
        "gakushu_chars = \"\".join(ch for ch in _l)\n",
        "\n",
        "grph_list = []\n",
        "for x in [hira_chars, gakushu_chars]:             # 数字は入力文字としない場合\n",
        "#for x in [hira_chars, num_chars, gakushu_chars]: # 数字も入力文字とする場合\n",
        "    for ch in x:\n",
        "        grph_list.append(ch)\n",
        "print(f'len(grph_list):{len(grph_list)}')\n",
        "print(f'全書記素 grph_list:{\"\".join([ch for ch in grph_list])}')\n",
        "\n",
        "print(f'len(phon_list):{len(phon_list)}')\n",
        "print(f'全音素 phon_list:{phon_list}')\n",
        "\n",
        "print(f'入力層の素子数 len(grph_list) + len(special_tokens)={len(grph_list) + len(special_tokens)}')\n",
        "print(f'出力層の素子数 len(phon_list) + len(special_tokens)={len(phon_list) + len(special_tokens)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NTT 日本語の語彙特性単語頻度データ psylex71.txt の読み込み\n",
        "\n",
        "if isColab:\n",
        "    ntt_base = '/content'\n",
        "else:\n",
        "    HOME = os.environ['HOME']\n",
        "    ntt_base = os.path.join(HOME, 'study/2017_2009AmanoKondo_NTTKanjiData')\n",
        "psy71_fname = os.path.join(ntt_base, 'psylex71utf8_.txt')  # ファイル名\n",
        "psylex71raw = open(psy71_fname, 'r').readlines()\n",
        "psylex71raw = [lin.strip().split(' ')[:6] for lin in psylex71raw]   # 空白 ' ' で分離し，年度ごとの頻度を削除\n",
        "\n",
        "# Psylex71 一行のデータは 0:共通ID, 1:独自ID, 2:表記, 3:ヨミ, 4:品詞, 5:頻度 を取り出す。\n",
        "n_idx=0; n_wrd=2; n_yomi=3; n_pos=4; n_frq=5\n",
        "idxes = {'n_idx':0, 'n_idx2':1, 'n_wrd':2, 'n_yomi':3, 'n_pos':4, 'n_frq':5}\n",
        "\n",
        "kata_chars='ァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモャヤュユョヨラリルレロヮワヰヱヲンヴヵヶ'\n",
        "kata_chars=kata_chars+'一'\n",
        "\n",
        "maxlen_grph = 2 # 書記素最大文字数 + 2 しているのは, 単語の前後に特殊トークン <SOW> <EOW> をつけるため\n",
        "_psylex71_ = []\n",
        "ng_yomi_words = []\n",
        "dups_idx = []\n",
        "\n",
        "valid_chars=grph_list\n",
        "\n",
        "grph_cands=valid_chars\n",
        "#phon_cands=kata_chars\n",
        "phon_cands = phon_list\n",
        "\n",
        "Psylex71 = OrderedDict()\n",
        "for lin in psylex71raw:\n",
        "    wrd = lin[n_wrd]\n",
        "    idx = lin[n_idx]\n",
        "    yomi = lin[n_yomi]\n",
        "    pos = lin[n_pos]\n",
        "    frq = lin[n_frq]\n",
        "\n",
        "    if len(wrd) == maxlen_grph:  # 長さが maxlen_grph 文字である語に対して処理を行う\n",
        "\n",
        "        # ヨミの中にカタカナ以外の文字が入っていれば NG_flag を True にする\n",
        "        is_kata_yomi = True\n",
        "        for p in yomi:\n",
        "            if not p in kata_chars:\n",
        "                is_kata_yomi = False\n",
        "\n",
        "        # ヨミにカタカナ以外の文字が含まれていれば ng_yomi_words に加える\n",
        "        if is_kata_yomi == False:\n",
        "            ng_yomi_words.append((wrd,yomi))\n",
        "        else:\n",
        "\n",
        "            # valid_chars (学習漢字+)で構成されているか否かを判断\n",
        "            is_valid_grph = True\n",
        "            for i in range(maxlen_grph):\n",
        "                if not wrd[i] in valid_chars:\n",
        "                    is_valid_grph = False\n",
        "\n",
        "            if is_valid_grph == True:\n",
        "                _psylex71_.append(lin)\n",
        "\n",
        "                if idx in Psylex71:   # すでに ID 番号が登録されていれば dups_idx リストに加える\n",
        "                    dups_idx.append((idx,lin, (Psylex71[idx]['単語'],Psylex71[idx]['ヨミ'])))\n",
        "\n",
        "                Psylex71[idx] = {'単語': wrd, 'ヨミ': yomi, '品詞': pos,'頻度': frq}\n",
        "\n",
        "\n",
        "# 読み (音韻表現) の最大長値を探す\n",
        "maxlen_phon = 0\n",
        "for a in _psylex71_:\n",
        "    if len(a[n_yomi]) > maxlen_phon:\n",
        "         maxlen_phon = len(a[n_yomi])\n",
        "\n",
        "# 結果の表示\n",
        "print(f'読み込んだ psylex71.txt の単語数 len(psylex71raw):{len(psylex71raw)}')\n",
        "print(f'Psylex71 の総単語数 len(_psylex71_):{len(_psylex71_)}')\n",
        "print(f'作成したデータベース辞書の項目数 len(Psylex71):{len(Psylex71)}')\n",
        "print(f'ヨミの最長文字数 maxlen_phon:{maxlen_phon}')\n",
        "print(f'音素 (読みのカタカナ文字)数 len(phon_cands):{len(phon_cands)}')\n",
        "print(f'Psylex71 におけるカタカナ以外のヨミのある単語数 len(ng_yomi_words):{len(ng_yomi_words)}')\n",
        "print(f'Psylex71 における ID 番号の重複数 len(dups_idx):{len(dups_idx)}')"
      ],
      "metadata": {
        "id": "iHll7Cpvg7fH"
      },
      "id": "iHll7Cpvg7fH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a58b357-946d-411c-8353-1ed90e00f713",
      "metadata": {
        "id": "1a58b357-946d-411c-8353-1ed90e00f713"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class Psylex71_Dataset(torch.utils.data.Dataset):\n",
        "    '''ニューラルネットワークモデルに Psylex71 を学習させるための PyTorch 用データセットのクラス'''\n",
        "\n",
        "    def __init__(self,\n",
        "                 dic=Psylex71,\n",
        "                 grph_list=grph_list,\n",
        "                 phon_list=phon_list,\n",
        "                 special_tokens=special_tokens,\n",
        "                 maxlen_phon=maxlen_phon +2, # ＋2 しているのは <SOW>,<EOW> という 2 つのスペシャルトークンを付加するため\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.dic = dic\n",
        "        self.special_tokens = special_tokens\n",
        "        self.maxlen_phon = maxlen_phon\n",
        "        self.grph_list = grph_list\n",
        "        self.phon_list = phon_list\n",
        "        self.input_cands = special_tokens + grph_list\n",
        "        self.target_cands = special_tokens + phon_list\n",
        "        self.inputs = [v['単語'] for v in dic.values()]\n",
        "        self.targets = [v['ヨミ'] for v in dic.values()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dic)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, tgt = self.inputs[idx], self.targets[idx]\n",
        "\n",
        "        # 入力信号にも <SOW>, <EOW> トークンを付与する場合\n",
        "        #inp = [self.input_cands.index('<SOW>')]  + [self.input_cands.index(x) for x in inp]  + [self.input_cands.index('<EOW>')]\n",
        "\n",
        "        # 入力信号にはスペシャルトークンを付与しない場合\n",
        "        inp = [self.input_cands.index(x) for x in inp]\n",
        "\n",
        "        # ターゲット (教師)信号 には <SOW>, <EOW> を付与する\n",
        "        tgt = [self.target_cands.index('<SOW>')] + [self.target_cands.index(x) for x in tgt] + [self.target_cands.index('<EOW>')]\n",
        "\n",
        "        while len(tgt) < self.maxlen_phon:\n",
        "            tgt = tgt + [self.target_cands.index('<PAD>')]\n",
        "\n",
        "        inp = torch.tensor(inp, dtype=torch.int64)\n",
        "        tgt = torch.tensor(tgt, dtype=torch.int64)\n",
        "        return inp, tgt\n",
        "\n",
        "    def getitem(self, idx):\n",
        "        wrd = self.inputs[idx]\n",
        "        phn = self.targets[idx]\n",
        "        return wrd, phn\n",
        "\n",
        "    def ids2argmax(self, ids):\n",
        "        out = np.array([torch.argmax(idx).numpy() for idx in ids], dtype=np.int32)\n",
        "        return out\n",
        "\n",
        "    def ids2tgt(self, ids):\n",
        "        #out = [self.target_cands[torch.argmax(idx)] for idx in ids]\n",
        "        out = [self.target_cands[idx] for idx in ids]\n",
        "        return out\n",
        "\n",
        "    def ids2inp(self, ids):\n",
        "        out = [self.input_cands[idx] for idx in ids]\n",
        "        return out\n",
        "\n",
        "    def target_ids2target(self, ids:list):\n",
        "        ret = []\n",
        "        for idx in ids:\n",
        "            if idx == self.target_cands.index('<EOW>'):\n",
        "                return ret+['<EOW>']\n",
        "            ret.append(self.target_cands[idx])\n",
        "        return ret\n",
        "\n",
        "\n",
        "psylex71_ds = Psylex71_Dataset()\n",
        "for N in np.random.permutation(psylex71_ds.__len__())[:15]:\n",
        "    inp, tgt = psylex71_ds.__getitem__(N)\n",
        "    print(f'psylex71_ds.ids2inp(inp):{psylex71_ds.ids2inp(inp)}',\n",
        "          f'{inp.numpy()}',\n",
        "          f'psylex71_ds.ids2tgt(tgt):{psylex71_ds.ids2tgt(tgt)}',\n",
        "          f'{tgt.numpy()}')\n",
        "\n",
        "\n",
        "train_size = int(psylex71_ds.__len__() * 0.7)\n",
        "valid_size = psylex71_ds.__len__() - train_size\n",
        "train_ds, valid_ds = torch.utils.data.random_split(dataset=psylex71_ds, lengths=(train_size, valid_size), generator=torch.Generator().manual_seed(seed))\n",
        "#print(train_ds.__len__(), valid_ds.__len__())\n",
        "\n",
        "batch_size = 512\n",
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
        "valid_dl = torch.utils.data.DataLoader(dataset=valid_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def _collate_fn(batch):\n",
        "    inps, tgts = list(zip(*batch))\n",
        "    inps = list(inps)\n",
        "    tgts = list(tgts)\n",
        "    return inps, tgts\n",
        "\n",
        "# batch_size = 4\n",
        "train_dl = torch.utils.data.DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=_collate_fn)\n",
        "\n",
        "valid_dl = torch.utils.data.DataLoader(\n",
        "    dataset=valid_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class TLA(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 # maxlen_phon+2 しているのは単語の前後に <SOW>, <EOW> トークンを付けるため\n",
        "                 inp_size= (len(grph_list)+len(special_tokens)), # * (maxlen_grph + 2),\n",
        "                 inp_len=maxlen_grph, #  + 2,\n",
        "                 out_size=len(phon_list)+len(special_tokens),\n",
        "                 out_len=maxlen_phon+2,\n",
        "                 hid_size=128,\n",
        "                 device='cpu',\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.inp_size=inp_size\n",
        "        self.inp_len=inp_len\n",
        "        self.out_size=out_size\n",
        "        self.out_len=out_len\n",
        "        self.hid_size=hid_size\n",
        "\n",
        "        self.emb_layers = [torch.nn.Embedding(num_embeddings=inp_size, embedding_dim=hid_size, padding_idx=0).to(device) for _ in range(inp_len)]\n",
        "        #self.emb_layer = torch.nn.Embedding(num_embeddings=inp_size, embedding_dim=hid_size, padding_idx=0).to(device)\n",
        "\n",
        "        self.hid_layer = torch.nn.Linear(in_features=hid_size * inp_len, out_features=hid_size).to(device)\n",
        "        #self.hid_layer = torch.nn.Linear(in_features=inp_len * inp_size, out_features=hid_size)\n",
        "\n",
        "        self.out_layers = [torch.nn.Linear(in_features=hid_size, out_features=out_size).to(device) for _ in range(out_len)]\n",
        "\n",
        "    def forward(self, inp):\n",
        "        X = inp\n",
        "        batch_size = X.size(0)\n",
        "        n_grph = X.size(1)\n",
        "\n",
        "        embs = []\n",
        "        for i in range(n_grph):\n",
        "            _emb = self.emb_layers[i](X[:,i])\n",
        "            #print(f'{i}:_emb.size():{_emb.size()}')\n",
        "            embs.append(_emb)\n",
        "\n",
        "        _embs = torch.concat(embs,dim=1)\n",
        "        X = _embs\n",
        "        X = self.hid_layer(X)         # 中間層次元へ変換\n",
        "\n",
        "        # 出力層の音韻表現ごとへ変換\n",
        "        outputs = []\n",
        "        for i in range(self.out_len):\n",
        "            _out = self.out_layers[i](X)\n",
        "            outputs.append(_out)\n",
        "\n",
        "        # softmax 変換\n",
        "        outputs = [torch.nn.functional.softmax(out,dim=1) for out in outputs]\n",
        "\n",
        "        O = torch.empty(self.out_len, batch_size, self.out_size)\n",
        "        for i in range(len(outputs)):\n",
        "            O[i] = outputs[i]\n",
        "        O = O.reshape(batch_size, self.out_len, self.out_size)\n",
        "        return O\n",
        "\n",
        "tla = TLA(device=device)\n",
        "tla.eval()\n",
        "\n",
        "# idx に整数を指定して,対応するデータを取得する\n",
        "idx = np.random.choice(train_ds.__len__())\n",
        "\n",
        "# データセットから返ってくる値は入力信号 inp と教師信号 tch\n",
        "inp, tch = train_ds.__getitem__(idx)\n",
        "print(f'idx:{idx}:', f'inp:{inp}', f'tch:{tch}')\n",
        "\n",
        "# 入出力信号はトークン ID 番号であるため人間が読みやすいように変換して表示\n",
        "print(f'train_ds.dataset.ids2inp({inp}):{train_ds.dataset.ids2inp(inp)}')\n",
        "print(f'train_ds.dataset.ids2tgt({tch}):{train_ds.dataset.ids2tgt(tch)}')\n",
        "\n",
        "inp = pad_sequence(inp.unsqueeze(0), batch_first=True).to(device)\n",
        "\n",
        "outs = tla(inp)\n",
        "outs = [out.cpu() for out in outs]\n",
        "print('出力:', train_ds.dataset.ids2tgt([int(_out.argmax().numpy()) for _out in outs[0]]), end=\": \")\n",
        "print('出力 ids:', [int(_out.argmax().numpy()) for _out in outs[0]])\n",
        "#print('出力 ids:', [int(out.argmax().numpy()) for out in outs])\n",
        "\n",
        "tch = tch.cpu()\n",
        "print('教師:', train_ds.dataset.ids2tgt([idx.numpy() for idx in tch]), end=\": \")\n",
        "print('教師 ids:', [int(_tch.numpy()) for _tch in tch])"
      ],
      "metadata": {
        "id": "or7MvkmPacyE"
      },
      "id": "or7MvkmPacyE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bdb138-4ce6-45d7-b745-bde4934eb29e",
      "metadata": {
        "id": "f0bdb138-4ce6-45d7-b745-bde4934eb29e"
      },
      "outputs": [],
      "source": [
        "# ミニバッチバージョン\n",
        "loss_f = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = torch.optim.Adam(tla.parameters(), lr=1e-6)\n",
        "tla.train()\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    sum_loss = 0.\n",
        "    for inps, tchs in tqdm(train_dl):\n",
        "        inps = pad_sequence(inps, batch_first=True).to(device)\n",
        "        tchs = pad_sequence(tchs, batch_first=True)\n",
        "        outs = tla(inps)\n",
        "\n",
        "        sum_loss = 0.\n",
        "        loss = 0.\n",
        "        for j in range(len(tchs)):\n",
        "            loss += loss_f(outs[j],tchs[j])\n",
        "        sum_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "\n",
        "        out_ids = [out.argmax(dim=1) for out in outs]\n",
        "        count  = 0\n",
        "        for tch, out in zip(tchs[:], out_ids[:]):\n",
        "            yesno = ((tch==out) * 1).sum().numpy() == len(tch)\n",
        "            count += 1 if yesno else 0\n",
        "        p_correct = count / len(tch)\n",
        "        #print('出力 ids:', [int(out.argmax().numpy()) for out in outs[:10]])\n",
        "        #print('教師 ids:', [tch  for tch in tchs[:3]])\n",
        "\n",
        "    print(f'p_correct:{p_correct:.3f}', end=\": \")\n",
        "    #epoch_loss += sum_loss.item()\n",
        "    print(f'sum_loss:{sum_loss:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tla.eval()\n",
        "\n",
        "# idx に整数を指定して,対応するデータを取得する\n",
        "idx = np.random.choice(train_ds.__len__())\n",
        "\n",
        "# データセットから返ってくる値は入力信号 inp と教師信号 tch\n",
        "inp, tch = train_ds.__getitem__(idx)\n",
        "print(f'idx:{idx}:', f'inp:{inp}', f'tch:{tch}')\n",
        "\n",
        "# 入出力信号はトークン ID 番号であるため人間が読みやすいように変換して表示\n",
        "print(f'train_ds.dataset.ids2inp({inp}):{train_ds.dataset.ids2inp(inp)}')\n",
        "print(f'train_ds.dataset.ids2tgt({tch}):{train_ds.dataset.ids2tgt(tch)}')\n",
        "\n",
        "inp = pad_sequence(inp.unsqueeze(0), batch_first=True).to(device)\n",
        "\n",
        "outs = tla(inp)\n",
        "outs = [out.cpu() for out in outs]\n",
        "print('出力:', train_ds.dataset.ids2tgt([int(_out.argmax().numpy()) for _out in outs[0]]), end=\": \")\n",
        "print('出力 ids:', [int(_out.argmax().numpy()) for _out in outs[0]])\n",
        "#print('出力 ids:', [int(out.argmax().numpy()) for out in outs])\n",
        "\n",
        "tch = tch.cpu()\n",
        "print('教師:', train_ds.dataset.ids2tgt([idx.numpy() for idx in tch]), end=\": \")\n",
        "print('教師 ids:', [int(_tch.numpy()) for _tch in tch])"
      ],
      "metadata": {
        "id": "gDvc0S8rXdup"
      },
      "id": "gDvc0S8rXdup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yxg3weiIYIYE"
      },
      "id": "yxg3weiIYIYE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}