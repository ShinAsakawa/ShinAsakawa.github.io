{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2025notebooks/2025_0626psylex71_CDP%2Bja1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db9a97b-ee8e-41b4-b88e-8018bfe1484c",
      "metadata": {
        "id": "2db9a97b-ee8e-41b4-b88e-8018bfe1484c"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_CDP_fig1.svg\" style=\"width:49%;\"><br/>\n",
        "<p>Zorzi+(1998) Fig.1 Architecture of the model. The arrow means full connectivity between layers. Each box stand for a group of letters (26) or phonemes (44).</p>\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_CDP_fig8.svg\" width=\"49%;\"><br/>\n",
        "<p>Zorzi+(1998) Fig.8. Architecture of the model with the hidden layer pathway. In both the direct pathway and the mediated pathway the layers are fully connected (arrows).</p>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_fig10.svg\" width=\"49%\"><br/>\n",
        "<p style=\"align-text:center\">\n",
        "Figure 10. Lexical and sublexical procedures in reading aloud, and their interaction in the phonological decision system, where the final phonological code is computed for articulation.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb9a707e-7d3e-4c46-afb4-94bf7b425bf2",
      "metadata": {
        "id": "fb9a707e-7d3e-4c46-afb4-94bf7b425bf2"
      },
      "source": [
        "# 0. 準備作業"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f00f3ba-87a0-40f2-9b7b-141186681f58",
      "metadata": {
        "id": "1f00f3ba-87a0-40f2-9b7b-141186681f58"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import torch\n",
        "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f'device:{device}')\n",
        "\n",
        "# 必要なライブラリの輸入\n",
        "from collections import OrderedDict\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import operator\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "from IPython import get_ipython\n",
        "isColab =  'google.colab' in str(get_ipython())\n",
        "\n",
        "try:\n",
        "    import ipynbname\n",
        "except ImportError:\n",
        "    !pip install ipynbname\n",
        "    import ipynbname\n",
        "\n",
        "FILEPATH = str(ipynbname.path()).split('/')[-1]\n",
        "print(f'FILEPATH:{FILEPATH}')\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0b5de0-5e4f-4f23-9e38-b20c1255349f",
      "metadata": {
        "id": "7d0b5de0-5e4f-4f23-9e38-b20c1255349f"
      },
      "source": [
        "## 0.1 モーラ tokenizer の定義 (モーラ分かち書き)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "036cb4be-72bf-438b-a0de-48cb61fb0ac8",
      "metadata": {
        "id": "036cb4be-72bf-438b-a0de-48cb61fb0ac8"
      },
      "outputs": [],
      "source": [
        "# モーラ分かち書きの定義 source https://qiita.com/shimajiroxyz/items/a133d990df2bc3affc12\n",
        "import re\n",
        "\n",
        "# # 各条件を正規表現で表す\n",
        "# c1 = '[ウクスツヌフムユルグズヅブプヴ][ァィェォ]' #ウ段＋「ァ/ィ/ェ/ォ」\n",
        "# c2 = '[イキシチニヒミリギジヂビピ][ャュェョ]' #イ段（「イ」を除く）＋「ャ/ュ/ェ/ョ」\n",
        "# c3 = '[テデ][ィュ]' #「テ/デ」＋「ャ/ィ/ュ/ョ」\n",
        "# c4 = '[ァ-ヴー]' #カタカナ１文字（長音含む）\n",
        "#\n",
        "# cond = '('+c1+'|'+c2+'|'+c3+'|'+c4+')'\n",
        "# re_mora = re.compile(cond)\n",
        "#\n",
        "# def moraWakachi(kana_text):\n",
        "#     return re_mora.findall(kana_text)\n",
        "\n",
        "\n",
        "class mora_Tokenizer:\n",
        "    def __init__(self):\n",
        "        special_tokens = ['<PAD>', '<EOW>', '<SOW>', '<UNK>']\n",
        "\n",
        "        mora_list = ['ァ', 'ア', 'ィ', 'イ', 'イェ', 'ゥ', 'ウ', 'ウィ', 'ウェ', 'ウォ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'ガ', 'キ', 'キャ', 'キュ', 'キョ', 'ギ', 'ギャ', 'ギュ', 'ギョ', 'ク', 'クァ', 'クィ', 'クェ', 'クォ', 'グ', 'グァ', 'ケ', 'ゲ', 'コ', 'ゴ', 'サ', 'ザ', 'シ', 'シェ', 'シャ', 'シュ', 'ショ', 'ジ', 'ジェ', 'ジャ', 'ジュ', 'ジョ', 'ス', 'ズ', 'ズィ', 'セ', 'ゼ', 'ソ', 'ゾ', 'タ', 'ダ', 'チ', 'チェ', 'チャ', 'チュ', 'チョ', 'ヂ', 'ヂャ', 'ヂュ', 'ヂョ', 'ッ', 'ツ', 'ツァ', 'ツィ', 'ツェ', 'ツォ', 'ヅ', 'テ', 'ティ', 'テュ', 'デ', 'ディ', 'デュ', 'ト', 'ド', 'ナ', 'ニ', 'ニェ', 'ニャ', 'ニュ', 'ニョ', 'ヌ', 'ネ', 'ノ', 'ハ', 'バ', 'パ', 'ヒ', 'ヒェ', 'ヒャ', 'ヒュ', 'ヒョ', 'ビ', 'ビャ', 'ビュ', 'ビョ', 'ピ', 'ピャ', 'ピュ', 'ピョ', 'フ', 'ファ', 'フィ', 'フェ', 'フォ', 'ブ', 'ブィ', 'プ', 'ヘ', 'ベ', 'ペ', 'ホ', 'ボ', 'ポ', 'マ', 'ミ', 'ミャ', 'ミュ', 'ミョ', 'ム', 'メ', 'モ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'リェ', 'リャ', 'リュ', 'リョ', 'ル', 'レ', 'ロ', 'ヮ', 'ワ', 'ヲ', 'ン', 'ヴ', 'ヴァ', 'ヴィ', 'ヴェ', 'ヴォ', 'ー']\n",
        "\n",
        "        self.tokens = special_tokens + mora_list\n",
        "\n",
        "        # 各条件を正規表現で表す\n",
        "        c1 = '[ウクスツヌフムユルグズヅブプヴ][ァィェォ]' #ウ段＋「ァ/ィ/ェ/ォ」\n",
        "        c2 = '[イキシチニヒミリギジヂビピ][ャュェョ]' #イ段（「イ」を除く）＋「ャ/ュ/ェ/ョ」\n",
        "        c3 = '[テデ][ィュ]' #「テ/デ」＋「ャ/ィ/ュ/ョ」\n",
        "        c4 = '[ァ-ヴー]' #カタカナ１文字（長音含む）\n",
        "\n",
        "        self.cond = '('+c1+'|'+c2+'|'+c3+'|'+c4+')'\n",
        "        self.re_mora = re.compile(self.cond)\n",
        "\n",
        "    def moraWakachi(self, kana_text):\n",
        "        return self.re_mora.findall(kana_text)\n",
        "\n",
        "\n",
        "    def wakachi(self, kana_text):\n",
        "        kana_text = kana_text.replace('ヱ','エ').replace('ヰ','イ')\n",
        "        morae = self.moraWakachi(kana_text)\n",
        "        return morae\n",
        "\n",
        "    def encode(self, kana_text):\n",
        "        kana_text = kana_text.replace('ヱ','エ').replace('ヰ','イ')\n",
        "        morae = self.moraWakachi(kana_text)\n",
        "        ids = [self.tokens.index(_mora) for _mora in morae]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        out = []\n",
        "        for idx in ids:\n",
        "            m = self.tokens[idx]\n",
        "            out.append(m)\n",
        "        return out\n",
        "\n",
        "    def __call__(self, kana_text):\n",
        "        return self.encode(kana_text)\n",
        "\n",
        "mora_tokenizer = mora_Tokenizer()\n",
        "# mora_tokenizer の検証\n",
        "# for _w in ['キチジョージ', 'チキン', 'ガッッキューホーカイー']:\n",
        "#     print(_w, mora_tokenizer(_w), mora_tokenizer.decode(mora_tokenizer(_w)))\n",
        "#     print(_w, mora_tokenizer.encode(_w))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6d57bb2-7e13-452b-a378-b50347596343",
      "metadata": {
        "id": "e6d57bb2-7e13-452b-a378-b50347596343"
      },
      "source": [
        "## 0.2 訓令式ローマ字 tokenizer の定義\n",
        "\n",
        "```\n",
        "ア イ ウ エ オ     a i u e o              \n",
        "カ キ ク ケ コ  キャ キュ キョ ka ki ku ke ko  kya kyu kyo          \n",
        "サ シ ス セ ソ  シャ シュ ショ sa shi su se so  sha shu sho          \n",
        "タ チ ツ テ ト  チャ チュ チョ ta chi tsu te to  cha chu cho          \n",
        "ナ ニ ヌ ネ ノ  ニャ ニュ ニョ na ni nu ne no  nya nyu nyo          \n",
        "ハ ヒ フ へ ホ  ヒャ ヒュ ヒョ ha hi fu he ho  hya hyu hyo          \n",
        "マ ミ ム メ モ  ミャ ミュ ミョ ma mi mu me mo  mya myu myo          \n",
        "ヤ  ユ  ヨ     ya  yu  yo              \n",
        "ラ リ ル レ ロ  リャ リュ リョ ra ri ru re ro  rya  ryu ryo          \n",
        "ワ    ヲ     wa    o              \n",
        "ガ ギ グ ゲ ゴ  ギャ ギュ ギョ ga gi gu ge go  gya gyu gyo          \n",
        "ザ ジ ズ ゼ ゾ  ジャ ジュ ジョ za ji zu ze zo  ja ju jo          \n",
        "ダ ヂ ヅ デ ド  ヂャ ヂュ ヂョ da ji zu de do  ja ju jo          \n",
        "バ ビ ブ ベ ボ  ビャ ビュ ビョ ba bi bu be bo  bya byu byo          \n",
        "パ ピ プ ペ ポ  ピャ ピュ ピョ pa pi pu pe po  pya pyu pyo\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c8bf6fd-5e91-429d-98ce-9e606a8c5698",
      "metadata": {
        "id": "7c8bf6fd-5e91-429d-98ce-9e606a8c5698"
      },
      "outputs": [],
      "source": [
        "# 訓令式ローマ字トークナイザ kunrei_Tokenizer() の定義\n",
        "\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# from kunrei import kunrei\n",
        "#print(dir(kunrei)) # ('キチジョージ').split(' ')\n",
        "#kunrei('ドォーモ')\n",
        "#kunrei('ドォモ')\n",
        "#kunrei('ドォ')    out_str = out_str.replace('プゥ', ' p u:')\n",
        "\n",
        "# print(list(sorted(mora_dict.keys())))\n",
        "# for m in sorted(list(sorted(mora_dict.keys()))):\n",
        "#     print(f'm:{m}, kurei({m}).split(\" \"):{kunrei(m).split(\" \")}')\n",
        "\n",
        "class kunrei_Tokenizer():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.kunrei_trans_dict = {\n",
        "            'ァ':'a',    'ア':'a',    'ィ':'i',    'イ':'i',    'イェ': 'i e',\n",
        "            'ゥ':'u',    'ウ':'u',    'ウィ':'u i',    'ウェ':'u e',    'ウォ':'u o',\n",
        "            'ェ':'e',    'エ':'e',    'ォ':'o',    'オ':'o',    'カ':'k a',\n",
        "            'ガ':'g a',    'キ':'k i',    'キャ':'ky a',    'キュ':'ky u',    'キョ':'ky o',\n",
        "            'ギ':'g i',     'ギャ':'gy a',     'ギュ':'gy u',     'ギョ':'gy o',     'ク':'k u',\n",
        "            'クァ':'k u a',     'クィ':'k u i',     'クェ':'k u e',     'クォ':'k u o',     'グ':'g u',\n",
        "            'グァ':'g u a',     'ケ':'k e',     'ゲ':'g e',     'コ':'k o',     'ゴ':'g o',\n",
        "            'サ':'s a',     'ザ':'z a',     'シ':'s a',     'シェ':'sy e',     'シャ':'sy a',\n",
        "            'シュ':'sy u',     'ショ':'sy o',     'ジ':'g i',     'ジェ':'gy e',     'ジャ':'gy a',\n",
        "            'ジュ':'gy u',     'ジョ':'gy o',     'ス':'s u',     'ズ':'z u',     'ズィ':'z i',\n",
        "            'セ':'s e',     'ゼ':'z e',     'ソ':'s o',     'ゾ':'z o',     'タ':'t a',\n",
        "            'ダ':'d a',     'チ':'t i',     'チェ':'ch e',     'チャ':'ch a',     'チュ':'ch u',\n",
        "            'チョ':'ch o',     'ヂ':'z i',     'ヂャ':'zy a',     'ヂュ':'zy u',     'ヂョ':'zy o',\n",
        "            'ッ':'t u',    'ツ':'t u',    'ツァ':'ty a',    'ツィ':'ty i',    'ツェ':'ty e',\n",
        "            'ツォ':'ty o',     'ヅ':'z u',     'テ':'t e',     'ティ':'t i',     'テュ':'t u',\n",
        "            'デ':'d e',     'ディ':'d i',     'デュ':'d u',     'ト':'t o',     'ド':'d o',\n",
        "            'ナ':'n a',     'ニ':'n i',     'ニェ':'ny e',    'ニャ':'ny a',    'ニュ':'ny u',\n",
        "            'ニョ':'ny o',    'ヌ':'n u',    'ネ':'n e',    'ノ':'n o',\n",
        "            'ハ':'h a',    'バ':'b a',    'パ':'p a',    'ヒ':'h i',    'ヒェ':'hy e',\n",
        "            'ヒャ':'hy a',    'ヒュ':'hy u',    'ヒョ':'hy o',    'ビ':'b i',    'ビャ':'by a',\n",
        "            'ビュ':'by u',    'ビョ':'by o',    'ピ':'p i',    'ピャ':'py a',    'ピュ':'py u',\n",
        "            'ピョ':'py o',    'フ':'f u',    'ファ':'f a',    'フィ':'f u i',    'フェ':'f u e',\n",
        "            'フォ':'f u o',    'ブ':'b u',    'ブィ':'b i',    'プ':'p u',    'ヘ':'h e',\n",
        "            'ベ':'b e',    'ペ':'p e',    'ホ':'h o',    'ボ':'b o',    'ポ':'p o',    'マ':'m a',\n",
        "            'ミ':'m i',    'ミャ':'my a',    'ミュ':'my u',    'ミョ':'my o',\n",
        "            'ム':'m u',    'メ':'m e',    'モ':'m o',    'ヤ':'y a',    'ュ':'y u',\n",
        "            'ユ':'y o',    'ョ':'y o',    'ヨ':'y o',    'ラ':'r a',    'リ':'r i',\n",
        "            'リェ':'ry e',    'リャ':'ry a',    'リュ':'ry u',    'リョ':'ry o',    'ル':'r u',\n",
        "            'レ':'r e',    'ロ':'r o',    'ヮ':'w a',    'ワ':'w a',    'ヲ':'o',    'ン':'N',\n",
        "            'ヴ':'b o',    'ヴァ':'b a',    'ヴィ':'b i',    'ヴェ':'b o',    'ヴォ':'b o',\n",
        "            'ー':':', '〜':':'}\n",
        "        self.mora_tokenizer = mora_Tokenizer()\n",
        "\n",
        "        tokens = [':', 'N', 'a', 'b', 'by', 'ch', 'd', 'e', 'f', 'g', 'gy', 'h', 'hy', 'i', 'k', 'ky', 'm', 'my', 'n', 'ny', 'o', 'p', 'py', 'r', 'ry', 's', 'sy', 't', 'ty', 'u', 'w', 'y', 'z', 'zy']\n",
        "        special_tokens = ['<PAD>', '<EOW>', '<SOW>', '<UNK>']\n",
        "        self.tokens = special_tokens + tokens\n",
        "\n",
        "    def encode(self, kana_text):\n",
        "        # kana_text = kana_text.replace('ヱ','エ').replace('ヰ','イ')\n",
        "        # mora_wakachi = self.mora_tokenizer.moraWakachi(kana_text)\n",
        "\n",
        "        # phon = []\n",
        "        # for mora in mora_wakachi:\n",
        "        #     if not mora in self.kunrei_trans_dict:\n",
        "        #         p = self.tokens.index('<UNK>')\n",
        "        #     else:\n",
        "        #         p = self.kunrei_trans_dict[mora].split(' ')\n",
        "        #     phon = phon + p\n",
        "\n",
        "        phon = self.wakachi(kana_text)\n",
        "        out = []\n",
        "        for p in phon:\n",
        "            if not p in self.tokens:\n",
        "                out.append(self.tokens.index('<UNK>'))\n",
        "            else:\n",
        "                out.append(self.tokens.index(p))\n",
        "        return out\n",
        "\n",
        "    def wakachi(self, kana_text):\n",
        "        kana_text = kana_text.replace('ヱ','エ').replace('ヰ','イ')\n",
        "        mora_wakachi = self.mora_tokenizer.moraWakachi(kana_text)\n",
        "\n",
        "        phon = []\n",
        "        for mora in mora_wakachi:\n",
        "            if not mora in self.kunrei_trans_dict:\n",
        "                p = self.tokens.index('<UNK>')\n",
        "            else:\n",
        "                p = self.kunrei_trans_dict[mora].split(' ')\n",
        "            phon = phon + p\n",
        "        return phon\n",
        "\n",
        "    def decode(self, ids):\n",
        "        out = []\n",
        "        for idx in ids:\n",
        "            m = self.tokens[idx]\n",
        "            out.append(m)\n",
        "        return out\n",
        "\n",
        "    def __call__(self, kana_text):\n",
        "        return self.encode(kana_text)\n",
        "\n",
        "kunrei_tokenizer = kunrei_Tokenizer()\n",
        "word = 'アカサタナ'\n",
        "ids = kunrei_tokenizer(word)\n",
        "print(word, ids, kunrei_tokenizer.decode(ids))\n",
        "print(kunrei_tokenizer.wakachi(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2b66d9b-816f-4cae-b183-7ebb9511b76b",
      "metadata": {
        "id": "b2b66d9b-816f-4cae-b183-7ebb9511b76b"
      },
      "source": [
        "## 0.3 文字の定義，(学習漢字，かな，カナ，数字，記号等)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21896937-d2ae-4377-8356-eb6b421e3532",
      "metadata": {
        "id": "21896937-d2ae-4377-8356-eb6b421e3532"
      },
      "outputs": [],
      "source": [
        "# 書記素の定義，書記素のうちカタカナを音韻表現としても利用\n",
        "\n",
        "seed = 42\n",
        "special_tokens = ['<PAD>', '<EOW>', '<SOW>', '<UNK>']\n",
        "alphabet_upper_chars='ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ'\n",
        "alphabet_lower_chars='ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ'\n",
        "num_chars='０１２３４５６７８９'\n",
        "hira_chars='ぁあぃいぅうぇえぉおかがきぎくぐけげこごさざしじすずせぜそぞただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもゃやゅゆょよらりるれろゎわゐゑをん'\n",
        "kata_chars='ァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモャヤュユョヨラリルレロヮワヰヱヲンヴヵヶ'\n",
        "#kata_chars=kata_chars+'一'  # カタカナ文字に伸ばし記号を加える\n",
        "#phon_list = list(kata_chars+'一')\n",
        "\n",
        "# # 句点コード\n",
        "# from RAM.char_ja import kuten as kuten\n",
        "# kuten_chars=kuten().chars\n",
        "\n",
        "# # 常用漢字\n",
        "# from RAM.char_ja import chars_joyo as chars_joyo\n",
        "# joyo_chars = \"\".join([ch for ch in chars_joyo().char_list])\n",
        "\n",
        "# 学習漢字 学年別\n",
        "_gakushu_list = ['一右雨円王音下火花貝学気休玉金九空月犬見五口校左三山四子糸字耳七車手十出女小上森人水正生青石赤先千川早草足村大男竹中虫町天田土二日入年白八百文本名木目夕立力林六',\n",
        "'引羽雲園遠黄何夏家科歌画会回海絵外角楽活間丸岩顔帰汽記弓牛魚京強教近兄形計元原言古戸午後語交光公工広考行高合国黒今才細作算姉市思止紙寺時自室社弱首秋週春書少場色食心新親図数星晴声西切雪線船前組走多太体台谷知地池茶昼朝長鳥直通弟店点電冬刀東当答頭同道読内南肉馬買売麦半番父風分聞米歩母方北妹毎万明鳴毛門夜野矢友曜用来理里話',\n",
        "'悪安暗委意医育員飲院運泳駅横屋温化荷界開階寒感漢館岸期起客宮急球究級去橋業局曲銀区苦具君係軽決血研県庫湖向幸港号根祭坂皿仕使始指死詩歯事持次式実写者主取守酒受州拾終習集住重宿所暑助勝商昭消章乗植深申真神身進世整昔全想相送息速族他打対待代第題炭短談着柱注丁帳調追定庭笛鉄転登都度島投湯等豆動童農波配倍箱畑発反板悲皮美鼻筆氷表病秒品夫負部服福物平返勉放味命面問役薬油有由遊予様洋羊葉陽落流旅両緑礼列練路和',\n",
        "'愛案以位囲胃衣印栄英塩央億加果課貨芽改械害街各覚完官管観関願喜器希旗機季紀議救求泣給挙漁競共協鏡極訓軍郡型径景芸欠結健建験固候功好康航告差最菜材昨刷察札殺参散産残司史士氏試児治辞失借種周祝順初唱松焼照省笑象賞信臣成清静席積折節説戦浅選然倉巣争側束続卒孫帯隊達単置仲貯兆腸低停底的典伝徒努灯働堂得特毒熱念敗梅博飯費飛必標票不付府副粉兵別変辺便包法望牧末満未脈民無約勇要養浴利陸料良量輪類令例冷歴連労老録',\n",
        "'圧易移因営永衛液益演往応恩仮価可河過賀解快格確額刊幹慣眼基寄規技義逆久旧居許境興均禁句群経潔件券検険減現限個故護効厚構耕講鉱混査再妻採災際在罪財桜雑賛酸師志支枝資飼似示識質舎謝授修術述準序承招証常情条状織職制勢性政精製税績責接設絶舌銭祖素総像増造則測属損態貸退団断築張提程敵適統導銅徳独任燃能破判版犯比肥非備俵評貧婦富布武復複仏編弁保墓報豊暴貿防務夢迷綿輸余預容率略留領',\n",
        "'異遺域宇映延沿我灰拡閣革割株巻干看簡危揮机貴疑吸供胸郷勤筋敬系警劇激穴憲権絹厳源呼己誤后孝皇紅鋼降刻穀骨困砂座済裁策冊蚕姿私至視詞誌磁射捨尺若樹収宗就衆従縦縮熟純処署諸除傷将障城蒸針仁垂推寸盛聖誠宣専泉洗染善創奏層操窓装臓蔵存尊宅担探誕暖段値宙忠著庁潮頂賃痛展党糖討届難乳認納脳派俳拝背肺班晩否批秘腹奮並閉陛片補暮宝訪亡忘棒枚幕密盟模訳優郵幼欲翌乱卵覧裏律臨朗論']\n",
        "\n",
        "_l = []\n",
        "for g in _gakushu_list:\n",
        "    for ch in g:\n",
        "        _l += ch\n",
        "gakushu_chars = \"\".join(ch for ch in _l)\n",
        "\n",
        "grph_list = []\n",
        "#for x in [hira_chars]:                       # 数字は入力文字としない場合\n",
        "#for x in [hira_chars, gakushu_chars]:             # 数字は入力文字としない場合\n",
        "for x in [hira_chars, kata_chars, num_chars, gakushu_chars]: # 数字も入力文字とする場合\n",
        "    for ch in x:\n",
        "        grph_list.append(ch)\n",
        "print(f'len(grph_list):{len(grph_list)}')\n",
        "print(f'全書記素 grph_list:{\"\".join([ch for ch in grph_list])}')\n",
        "\n",
        "# print(f'len(phon_list):{len(phon_list)}')\n",
        "# print(f'全音素 phon_list:{phon_list}')\n",
        "\n",
        "print(f'入力層の素子数 len(grph_list) + len(special_tokens)={len(grph_list) + len(special_tokens)}')\n",
        "# print(f'出力層の素子数 len(phon_list) + len(special_tokens)={len(phon_list) + len(special_tokens)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94d37f68-8b71-4928-877b-498e3f29f853",
      "metadata": {
        "id": "94d37f68-8b71-4928-877b-498e3f29f853"
      },
      "source": [
        "## 0.4 学習文字 tokenizer の定義 gakushu_Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3f627ac-85d5-4b0a-8f62-6bde27793ed8",
      "metadata": {
        "id": "b3f627ac-85d5-4b0a-8f62-6bde27793ed8"
      },
      "outputs": [],
      "source": [
        "class gakushu_Tokenizer():\n",
        "    def __init__(self):\n",
        "        special_tokens = ['<PAD>', '<EOW>', '<SOW>', '<UNK>']\n",
        "        alphabet_upper_chars='ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ'\n",
        "        alphabet_lower_chars='ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ'\n",
        "        num_chars='０１２３４５６７８９'\n",
        "        hira_chars='ぁあぃいぅうぇえぉおかがきぎくぐけげこごさざしじすずせぜそぞただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもゃやゅゆょよらりるれろゎわゐゑをん'\n",
        "        kata_chars='ァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモャヤュユョヨラリルレロヮワヰヱヲンヴヵヶ'\n",
        "\n",
        "        # 学習漢字 学年別\n",
        "        gakushu_chars = '一右雨円王音下火花貝学気休玉金九空月犬見五口校左三山四子糸字耳七車手十出女小上森人水正生青石赤先千川早草足村大男竹中虫町天田土二日入年白八百文本名木目夕立力林六' + '引羽雲園遠黄何夏家科歌画会回海絵外角楽活間丸岩顔帰汽記弓牛魚京強教近兄形計元原言古戸午後語交光公工広考行高合国黒今才細作算姉市思止紙寺時自室社弱首秋週春書少場色食心新親図数星晴声西切雪線船前組走多太体台谷知地池茶昼朝長鳥直通弟店点電冬刀東当答頭同道読内南肉馬買売麦半番父風分聞米歩母方北妹毎万明鳴毛門夜野矢友曜用来理里話' + '悪安暗委意医育員飲院運泳駅横屋温化荷界開階寒感漢館岸期起客宮急球究級去橋業局曲銀区苦具君係軽決血研県庫湖向幸港号根祭坂皿仕使始指死詩歯事持次式実写者主取守酒受州拾終習集住重宿所暑助勝商昭消章乗植深申真神身進世整昔全想相送息速族他打対待代第題炭短談着柱注丁帳調追定庭笛鉄転登都度島投湯等豆動童農波配倍箱畑発反板悲皮美鼻筆氷表病秒品夫負部服福物平返勉放味命面問役薬油有由遊予様洋羊葉陽落流旅両緑礼列練路和' + '愛案以位囲胃衣印栄英塩央億加果課貨芽改械害街各覚完官管観関願喜器希旗機季紀議救求泣給挙漁競共協鏡極訓軍郡型径景芸欠結健建験固候功好康航告差最菜材昨刷察札殺参散産残司史士氏試児治辞失借種周祝順初唱松焼照省笑象賞信臣成清静席積折節説戦浅選然倉巣争側束続卒孫帯隊達単置仲貯兆腸低停底的典伝徒努灯働堂得特毒熱念敗梅博飯費飛必標票不付府副粉兵別変辺便包法望牧末満未脈民無約勇要養浴利陸料良量輪類令例冷歴連労老録' + '圧易移因営永衛液益演往応恩仮価可河過賀解快格確額刊幹慣眼基寄規技義逆久旧居許境興均禁句群経潔件券検険減現限個故護効厚構耕講鉱混査再妻採災際在罪財桜雑賛酸師志支枝資飼似示識質舎謝授修術述準序承招証常情条状織職制勢性政精製税績責接設絶舌銭祖素総像増造則測属損態貸退団断築張提程敵適統導銅徳独任燃能破判版犯比肥非備俵評貧婦富布武復複仏編弁保墓報豊暴貿防務夢迷綿輸余預容率略留領' + '異遺域宇映延沿我灰拡閣革割株巻干看簡危揮机貴疑吸供胸郷勤筋敬系警劇激穴憲権絹厳源呼己誤后孝皇紅鋼降刻穀骨困砂座済裁策冊蚕姿私至視詞誌磁射捨尺若樹収宗就衆従縦縮熟純処署諸除傷将障城蒸針仁垂推寸盛聖誠宣専泉洗染善創奏層操窓装臓蔵存尊宅担探誕暖段値宙忠著庁潮頂賃痛展党糖討届難乳認納脳派俳拝背肺班晩否批秘腹奮並閉陛片補暮宝訪亡忘棒枚幕密盟模訳優郵幼欲翌乱卵覧裏律臨朗論'\n",
        "\n",
        "        self.tokens = special_tokens + list(num_chars) + list(hira_chars) + list(kata_chars) + list(gakushu_chars)\n",
        "\n",
        "    def encode(self, chars):\n",
        "        out = []\n",
        "        for ch in chars:\n",
        "            if not ch in self.tokens:\n",
        "                out.append(self.tokens.index('<UNK>'))\n",
        "            else:\n",
        "                out.append(self.tokens.index(ch))\n",
        "        return out\n",
        "\n",
        "    def decode(self, ids):\n",
        "        out = [self.tokens[idx] for idx in ids]\n",
        "        return out\n",
        "\n",
        "    def __call__(self, chars):\n",
        "        return self.encode(chars)\n",
        "\n",
        "gakushu_tokenizer = gakushu_Tokenizer()\n",
        "# 上記 gakushu_tokenizer の検証\n",
        "# print(gakushu_tokenizer.char_list)\n",
        "# #gakushu_tokenizer.encode('学校')\n",
        "# print(gakushu_tokenizer('学校'))\n",
        "# print(gakushu_tokenizer.decode(gakushu_tokenizer('学校')))\n",
        "#print(len(gakushu_tokenizer.tokens), len(mora_tokenizer.tokens),)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d147219-23e6-43c4-9afe-c0b907d9659c",
      "metadata": {
        "id": "6d147219-23e6-43c4-9afe-c0b907d9659c"
      },
      "source": [
        "## 0.5 NTT 日本語の語彙特性 単語頻度データの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d006cd8-c55f-4a52-ae62-2885f06333ed",
      "metadata": {
        "id": "4d006cd8-c55f-4a52-ae62-2885f06333ed"
      },
      "outputs": [],
      "source": [
        "if isColab:\n",
        "    !pip install googledrivedownloader==0.4\n",
        "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "    import os\n",
        "\n",
        "    # 共有ファイルのIDを指定\n",
        "    file_id = '1eBJDN392BsUckg5LBFbbw5KT9PCsmnxI' # 'psylex71utf8_.txt\n",
        "    # https://drive.google.com/file/d/1eBJDN392BsUckg5LBFbbw5KT9PCsmnxI/view?usp=drive_link\n",
        "\n",
        "    # 保存したい場所とファイル名を指定\\n\",\n",
        "    # 例: /content/ ディレクトリに original_file_name.拡張子 という名前で保存\\n\",\n",
        "    destination_path = '/content/psylex71utf8_.txt' # ファイルの拡張子を適切に設定してください\\n\",\n",
        "    try:\n",
        "        print(f\"ファイルのダウンロードを開始します (ファイルID: {file_id})...\")\n",
        "        gdd.download_file_from_google_drive(file_id=file_id,\n",
        "                                            dest_path=destination_path)\n",
        "                                            # unzip=True if file_id is for a zip file):\n",
        "        print(f\"ファイルのダウンロードが完了しました。'{destination_path}' に保存されました。\")\n",
        "\n",
        "        # ダウンロードしたファイルを読み込む例 (テキストファイルの場合)\n",
        "        if os.path.exists(destination_path):\n",
        "            print(\"ダウンロードしたファイルの内容 (最初の数行):\")\n",
        "            with open(destination_path, 'r') as f:\n",
        "                # ファイルの内容を表示 (例: 最初の5行)\n",
        "                for i in range(5):\n",
        "                    line = f.readline()\n",
        "                    if not line:\n",
        "                        break\n",
        "                    print(line.strip())\n",
        "        else:\n",
        "            print(f\"エラー: ダウンロード先のファイル '{destination_path}' が見つかりません。\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ファイルのダウンロード中にエラーが発生しました: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd402426-c138-419d-8794-6e682d452f51",
      "metadata": {
        "id": "cd402426-c138-419d-8794-6e682d452f51"
      },
      "outputs": [],
      "source": [
        "# NTT 日本語の語彙特性単語頻度データ psylex71.txt の読み込み\n",
        "#HOME = os.environ['HOME']\n",
        "if isColab:\n",
        "    ntt_base = '/content'\n",
        "else:\n",
        "    ntt_base = os.path.join(HOME, 'study/2017_2009AmanoKondo_NTTKanjiData')\n",
        "psy71_fname = os.path.join(ntt_base, 'psylex71utf8_.txt')  # ファイル名\n",
        "psylex71raw = open(psy71_fname, 'r').readlines()\n",
        "psylex71raw = [lin.strip().split(' ')[:6] for lin in psylex71raw]   # 空白 ' ' で分離し，年度ごとの頻度を削除\n",
        "print(f'len(psylex71raw):{len(psylex71raw)}')\n",
        "\n",
        "valid_chars = kata_chars + 'ー〜'\n",
        "\n",
        "# Psylex71 一行のデータは 0:共通ID, 1:独自ID, 2:表記, 3:ヨミ, 4:品詞, 5:頻度 を取り出す。\n",
        "#n_idx=0; n_wrd=2; n_yomi=3; n_pos=4; n_frq=5\n",
        "psylex_ids = {'_idx':0, '_idx2':1, '_wrd':2, '_yomi':3, '_pos':4, '_frq':5, '_mora':6}\n",
        "psylex_ids['_kunrei'] = 7\n",
        "print(f'psylex_ids{psylex_ids}')\n",
        "\n",
        "mora_dict = OrderedDict()\n",
        "kunrei_dict = OrderedDict()\n",
        "\n",
        "for x in tqdm(psylex71raw[1:]):\n",
        "    _word =  x[psylex_ids['_wrd']]\n",
        "    _yomi = x[psylex_ids['_yomi']]\n",
        "    is_valid = True\n",
        "    for ch in _yomi:\n",
        "        if not ch in valid_chars:\n",
        "            is_valid = False\n",
        "    if is_valid:\n",
        "        morae = mora_tokenizer.wakachi(_yomi)\n",
        "        #morae = moraWakachi(_yomi)\n",
        "        for m in morae:\n",
        "            if not m in mora_dict:\n",
        "                mora_dict[m] = 1\n",
        "            else:\n",
        "                mora_dict[m] += 1\n",
        "\n",
        "print(f'len(kunrei_dict):{len(kunrei_dict)}')\n",
        "kunrei_list = sorted(kunrei_dict.keys())\n",
        "print(f'kurei_list:{kunrei_list}')\n",
        "\n",
        "print(f'len(mora_dict):{len(mora_dict)}')\n",
        "mora_list = sorted(mora_dict.keys())\n",
        "print(len(mora_dict), mora_dict)\n",
        "\n",
        "is_graph = True\n",
        "_dict = mora_dict\n",
        "if is_graph:\n",
        "    _N = np.array([v for v in _dict.values()]).sum()\n",
        "    _count_sorted = sorted(_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    figsize=(24,4)\n",
        "    topN = 100 if _N > 100 else _N\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.bar(range(topN), [x[1]/_N for x in _count_sorted[:topN]])\n",
        "    plt.xticks(ticks=range(topN), labels=[c[0] for c in _count_sorted[:topN]])\n",
        "\n",
        "    plt.title(f'モーラ頻度 (上位:{topN} 語)')\n",
        "    plt.ylabel('相対頻度')\n",
        "    plt.show()\n",
        "    #len(mora_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce290850-2544-41d8-b077-3d5a23dbc9fc",
      "metadata": {
        "id": "ce290850-2544-41d8-b077-3d5a23dbc9fc"
      },
      "source": [
        "# 1. データセットの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc83ecee-5079-4d80-bfe5-350739a23978",
      "metadata": {
        "id": "dc83ecee-5079-4d80-bfe5-350739a23978"
      },
      "outputs": [],
      "source": [
        "maxlen_grph = 2        # 書記素最大文字数 + 2 しているのは, 単語の前後に特殊トークン <SOW> <EOW> をつけるため\n",
        "valid_chars=grph_list  # 書記素リスト grph_list を有効文字リスト valid_chars とする\n",
        "ng_yomi_words = []\n",
        "dups_idx = []\n",
        "_psylex71_ = []\n",
        "\n",
        "Psylex71 = OrderedDict()\n",
        "for lin in psylex71raw:\n",
        "    wrd = lin[psylex＿ids['_wrd']]\n",
        "    idx = lin[psylex＿ids['_idx']]\n",
        "    yomi = lin[psylex＿ids['_yomi']]\n",
        "    pos = lin[psylex＿ids['_pos']]\n",
        "    frq = lin[psylex＿ids['_frq']]\n",
        "\n",
        "    # print(f'type(lin):{type(lin)}')\n",
        "    # print(f'lin:{lin}')\n",
        "    # sys.exit()\n",
        "\n",
        "    if len(wrd) == maxlen_grph:  # 長さが maxlen_grph 文字である語に対して処理を行う\n",
        "\n",
        "        # ヨミの中にカタカナ以外の文字が入っていれば NG_flag を True にする\n",
        "        is_kata_yomi = True\n",
        "        for p in yomi:\n",
        "            if not p in kata_chars:\n",
        "                is_kata_yomi = False\n",
        "\n",
        "        # ヨミにカタカナ以外の文字が含まれていれば ng_yomi_words に加える\n",
        "        if is_kata_yomi == False:\n",
        "            ng_yomi_words.append((wrd,yomi))\n",
        "        else:\n",
        "\n",
        "            # valid_chars (学習漢字+)で構成されているか否かを判断\n",
        "            is_valid_grph = True\n",
        "            for i in range(maxlen_grph):\n",
        "                if not wrd[i] in valid_chars:\n",
        "                    is_valid_grph = False\n",
        "\n",
        "            if is_valid_grph == True:\n",
        "\n",
        "                _mora = mora_tokenizer.wakachi(yomi) # .strip()  # モーラ分かち書きを行う\n",
        "                _kunrei = kunrei_tokenizer.wakachi(yomi)\n",
        "                if idx in Psylex71:   # すでに ID 番号が登録されていれば dups_idx リストに加える\n",
        "                    dups_idx.append((idx, lin, (Psylex71[idx]['単語'], Psylex71[idx]['ヨミ'], _mora)))\n",
        "\n",
        "                Psylex71[idx] = {'単語': wrd, 'モーラ':_mora, 'ヨミ': yomi,\n",
        "                                 '訓令':_kunrei, '品詞': pos,'頻度': frq}\n",
        "\n",
        "                _psylex71_.append(lin + [_mora, _kunrei])\n",
        "\n",
        "\n",
        "# 読み (音韻表現) の最大長値の探索\n",
        "maxlen_mora, maxlen_kunrei = 0, 0\n",
        "for a in _psylex71_:\n",
        "    if len(a[psylex_ids['_mora']]) > maxlen_mora:\n",
        "         maxlen_mora = len(a[psylex_ids['_mora']])\n",
        "    if len(a[psylex_ids['_kunrei']]) > maxlen_kunrei:\n",
        "         maxlen_kunrei = len(a[psylex_ids['_kunrei']])\n",
        "\n",
        "# 結果の表示\n",
        "print(f'読み込んだ psylex71.txt の単語数 len(psylex71raw):{len(psylex71raw)}')\n",
        "print(f'Psylex71 の総単語数 len(_psylex71_):{len(_psylex71_)}')\n",
        "print(f'作成したデータベース辞書の項目数 len(Psylex71):{len(Psylex71)}')\n",
        "print(f'ヨミの最長モーラ数 maxlen_mora:{maxlen_mora}')\n",
        "print(f'訓令式の最長トークン数 maxlen_kunrei:{maxlen_kunrei}')\n",
        "print(f'len(mora_list):{len(mora_list)}')\n",
        "#print(f'音素 (読みのカタカナ文字)数 len(phon_cands):{len(phon_cands)}')\n",
        "print(f'Psylex71 におけるカタカナ以外のヨミのある単語数 len(ng_yomi_words):{len(ng_yomi_words)}')\n",
        "print(f'Psylex71 における ID 番号の重複数 len(dups_idx):{len(dups_idx)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef1d47a-7c2d-4171-a920-df4a54263fb3",
      "metadata": {
        "id": "4ef1d47a-7c2d-4171-a920-df4a54263fb3"
      },
      "source": [
        "# 2． Psylex71_Dataset (PyTorch 用のクラス) の作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d8eb12-33ed-46fa-955f-f573ffc8ca44",
      "metadata": {
        "id": "68d8eb12-33ed-46fa-955f-f573ffc8ca44"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class Psylex71_Dataset(torch.utils.data.Dataset):\n",
        "    '''ニューラルネットワークモデルに Psylex71 を学習させるための PyTorch 用データセットのクラス'''\n",
        "\n",
        "    def __init__(self,\n",
        "                 dic=Psylex71,\n",
        "                 input_tokenizer=gakushu_tokenizer,\n",
        "                 output_tokenizer=mora_tokenizer,\n",
        "                 special_tokens=special_tokens,\n",
        "                 device=device):\n",
        "        super().__init__()\n",
        "        self.dic = dic\n",
        "        self.inputs = [v['単語'] for v in dic.values()]\n",
        "        self.targets = [v['ヨミ'] for v in dic.values()]\n",
        "        self.special_tokens = special_tokens\n",
        "        self.device = device\n",
        "\n",
        "        self.input_tokenizer = input_tokenizer\n",
        "        self.output_tokenizer = output_tokenizer\n",
        "\n",
        "        maxlen_out = 0\n",
        "        for k, v in dic.items():\n",
        "            _len = len(self.output_tokenizer(v['ヨミ']))\n",
        "            maxlen_out = _len if _len > maxlen_out else maxlen_out\n",
        "\n",
        "        # ＋2 しているのは <SOW>,<EOW> という 2 つのスペシャルトークンを付加するため\n",
        "        self.maxlen_out = maxlen_out + 2\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dic)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, tgt = self.inputs[idx], self.targets[idx]\n",
        "\n",
        "        # 入力信号にも <SOW>, <EOW> トークンを付与する場合\n",
        "        #inp = [self.input_cands.index('<SOW>')]  + [self.input_cands.index(x) for x in inp]  + [self.input_cands.index('<EOW>')]\n",
        "\n",
        "        # 入力信号にはスペシャルトークンを付与しない場合\n",
        "        #inp = [self.input_tokenizer.index(x) for x in inp]\n",
        "        inp = self.input_tokenizer(inp)\n",
        "\n",
        "        # ターゲット (教師)信号 には <SOW>, <EOW> を付与する\n",
        "        #tgt = [self.target_tokecands.index('<SOW>')] + [self.target_cands.index(x) for x in tgt] + [self.target_cands.index('<EOW>')]\n",
        "        tgt = self.output_tokenizer(tgt)\n",
        "        tgt = [self.output_tokenizer.tokens.index('<SOW>')] + tgt + [self.output_tokenizer.tokens.index('<EOW>')]\n",
        "\n",
        "        while len(tgt) < self.maxlen_out:\n",
        "            #tgt = tgt + [self.target_cands.index('<PAD>')]\n",
        "            tgt = tgt + [self.output_tokenizer.tokens.index('<PAD>')]\n",
        "\n",
        "        inp, tgt = torch.LongTensor(inp), torch.LongTensor(tgt)\n",
        "        inp, tgt = inp.to(self.device), tgt.to(self.device)\n",
        "        return inp, tgt\n",
        "\n",
        "    def getitem(self, idx):\n",
        "        #inp, tgt = self.inputs[idx], self.targets[idx]\n",
        "        wrd = self.inputs[idx]\n",
        "        phn = self.targets[idx]\n",
        "        return wrd, phn\n",
        "\n",
        "    def ids2argmax(self, ids):\n",
        "        out = np.array([torch.argmax(idx).numpy() for idx in ids], dtype=np.int32)\n",
        "        return out\n",
        "\n",
        "    def ids2tgt(self, ids):\n",
        "        # out = [self.target_cands[idx - len(self.special_tokens)] for idx in ids]\n",
        "        out = self.output_tokenizer.decode(ids)\n",
        "        return out\n",
        "\n",
        "    def ids2inp(self, ids):\n",
        "        #out = [self.input_cands[idx] for idx in ids]\n",
        "        out = self.input_tokenizer.decode(ids)\n",
        "        return out\n",
        "\n",
        "    def target_ids2target(self, ids:list):\n",
        "        ret = self.output_tokenizer.decode(ids)\n",
        "        return ret\n",
        "\n",
        "psylex71_ds = Psylex71_Dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e5596b-b653-4be2-a304-454a71f9c6d3",
      "metadata": {
        "id": "41e5596b-b653-4be2-a304-454a71f9c6d3"
      },
      "source": [
        "# 3. モデルの定義\n",
        "## 3.1 TLA モデルの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e45ba26-78b1-4ec8-9a7b-9107c0d2dbde",
      "metadata": {
        "id": "9e45ba26-78b1-4ec8-9a7b-9107c0d2dbde"
      },
      "outputs": [],
      "source": [
        "# 全モデル共通使用するライブラリの輸入\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c4f3f15-e095-4d77-9e4b-602dbaf8896e",
      "metadata": {
        "id": "5c4f3f15-e095-4d77-9e4b-602dbaf8896e"
      },
      "outputs": [],
      "source": [
        "class vanilla_TLA(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 inp_vocab_size:int=len(psylex71_ds.input_tokenizer.tokens),\n",
        "                 out_vocab_size:int=len(psylex71_ds.output_tokenizer.tokens),\n",
        "                 inp_len:int=maxlen_grph,\n",
        "                 out_len:int=psylex71_ds.maxlen_out,\n",
        "                 #out_len:int=maxlen_mora+2,\n",
        "                 n_hid:int=1024,\n",
        "                 device:str=device):\n",
        "\n",
        "        super().__init__()\n",
        "        self.inp_vocab_size=inp_vocab_size\n",
        "        self.inp_len=inp_len\n",
        "        self.out_vocab_size=out_vocab_size\n",
        "        self.out_len=out_len\n",
        "        self.n_hid=n_hid\n",
        "\n",
        "        self.emb_layer = torch.nn.Linear(in_features=inp_vocab_size * inp_len, out_features=n_hid).to(device)\n",
        "        #self.sigmoid = torch.nn.Sigmoid()\n",
        "        #self.tanh = torch.nn.Tanh()\n",
        "        #self.relu = torch.nn.ReLU()\n",
        "        self.emb_outf = torch.nn.Tanh()\n",
        "        self.out_outf = torch.nn.Sigmoid()\n",
        "        #self.out_outf = torch.nn.Tanh()\n",
        "\n",
        "        self.out_layer = torch.nn.Linear(in_features=n_hid, out_features=out_vocab_size * out_len).to(device)\n",
        "\n",
        "    def forward(self, X, Y):\n",
        "        '''互換性のため Y を入力としているが実際には使っていない'''\n",
        "\n",
        "        # 入力 X はトークン ID リストであるので，ワンホットベクトル化する\n",
        "        X = torch.nn.functional.one_hot(X, num_classes=self.inp_vocab_size)\n",
        "\n",
        "        X = X.reshape(X.size(0),-1) # ワンホットベクトルを連接して行ベクトルに変換\n",
        "        X = X.float()               # ワンホットベクトルは整数 int64 なので浮動小数点に変換\n",
        "\n",
        "        X = self.emb_layer(X)       # 埋め込み層への信号伝搬\n",
        "        X = self.emb_outf(X)        # 埋め込み層の非線形変換\n",
        "\n",
        "        X = self.out_layer(X)       # 出力層への信号伝搬\n",
        "        X = self.out_outf(X)        # 出力層での非線形変換\n",
        "\n",
        "        # 各出力ニューロンに分割\n",
        "        X = X.reshape(X.size(0), self.out_len, self.out_vocab_size)\n",
        "\n",
        "        return X\n",
        "\n",
        "vanilla_tla = vanilla_TLA(device=device)\n",
        "print(vanilla_tla.eval())\n",
        "print(f'vanilla_tla.out_len:{vanilla_tla.out_len}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4427deba-7f6d-4d99-85fb-47f72acbcb3d",
      "metadata": {
        "id": "4427deba-7f6d-4d99-85fb-47f72acbcb3d"
      },
      "source": [
        "## 3.2 seq2seq_with_attention model の定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "090b7eea-e840-4ea0-b320-8412e2e430d9",
      "metadata": {
        "id": "090b7eea-e840-4ea0-b320-8412e2e430d9"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq_wAtt(nn.Module):\n",
        "    \"\"\" 注意つき符号化器‐復号化器モデル\n",
        "    Bahdanau, Cho, & Bengio (2015) NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE, arXiv:1409.0473\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 enc_vocab_size:int,\n",
        "                 dec_vocab_size:int,\n",
        "                 n_hid:int,\n",
        "                 n_layers:int=2,\n",
        "                 bidirectional:bool=False,\n",
        "                 device=device):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder 側の入力トークン id を多次元ベクトルに変換\n",
        "        self.encoder_emb = nn.Embedding(num_embeddings=enc_vocab_size,\n",
        "                                        embedding_dim=n_hid,\n",
        "                                        padding_idx=0)\n",
        "\n",
        "        # Decoder 側の入力トークン id を多次元ベクトルに変換\n",
        "        self.decoder_emb = nn.Embedding(num_embeddings=dec_vocab_size,\n",
        "                                        embedding_dim=n_hid,\n",
        "                                        padding_idx=0)\n",
        "\n",
        "        # Encoder LSTM 本体\n",
        "        self.encoder = nn.LSTM(input_size=n_hid,\n",
        "                               hidden_size=n_hid,\n",
        "                               num_layers=n_layers,\n",
        "                               batch_first=True,\n",
        "                               bidirectional=bidirectional)\n",
        "\n",
        "        # Decoder LSTM 本体\n",
        "        self.decoder = nn.LSTM(input_size=n_hid,\n",
        "                               hidden_size=n_hid,\n",
        "                               num_layers=n_layers,\n",
        "                               batch_first=True,\n",
        "                               bidirectional=bidirectional)\n",
        "\n",
        "        # 文脈ベクトルと出力ベクトルの合成を合成する層\n",
        "        bi_fact = 2 if bidirectional else 1\n",
        "        self.combine_layer = nn.Linear(bi_fact * 2 * n_hid, n_hid)\n",
        "\n",
        "        # 最終出力層\n",
        "        self.out_layer = nn.Linear(n_hid, dec_vocab_size)\n",
        "\n",
        "    def forward(self, enc_inp, dec_inp):\n",
        "\n",
        "        enc_emb = self.encoder_emb(enc_inp)\n",
        "        enc_out, (hnx, cnx) = self.encoder(enc_emb)\n",
        "\n",
        "        dec_emb = self.decoder_emb(dec_inp)\n",
        "        dec_out, (hny, cny) = self.decoder(dec_emb,(hnx, cnx))\n",
        "\n",
        "        # enc_out は (バッチサイズ，ソースの単語数，中間層の次元数)\n",
        "        # ソース側 (enc_out) の各単語とターゲット側 (dec_out) の各単語との類似度を測定するため\n",
        "        # 両テンソルの内積をとるため ソース側 (enc_out) の軸を入れ替え\n",
        "        enc_outP = enc_out.permute(0,2,1)\n",
        "\n",
        "        # sim の形状は (バッチサイズ, 中間層の次元数，ソースの単語数)\n",
        "        sim = torch.bmm(dec_out, enc_outP)\n",
        "\n",
        "        # sim の各次元のサイズを記録\n",
        "        batch_size, dec_word_size, enc_word_size = sim.shape\n",
        "\n",
        "        # sim に対して，ソフトマックスを行うため形状を変更\n",
        "        simP = sim.reshape(batch_size * dec_word_size, enc_word_size)\n",
        "\n",
        "        # simP のソフトマックスを用いて注意の重み alpha を算出\n",
        "        alpha = F.softmax(simP,dim=1).reshape(batch_size, dec_word_size, enc_word_size)\n",
        "\n",
        "        # 注意の重み alpha に encoder の出力を乗じて，文脈ベクトル c_t とする\n",
        "        c_t = torch.bmm(alpha, enc_out)\n",
        "\n",
        "        # torch.cat だから c_t と dec_out とで合成\n",
        "        dec_out_ = torch.cat([c_t, dec_out], dim=2)\n",
        "        dec_out_ = self.combine_layer(dec_out_)\n",
        "\n",
        "        return self.out_layer(dec_out_)\n",
        "\n",
        "    def evaluate(self, enc_inp, dec_inp):\n",
        "\n",
        "        enc_emb = self.encoder_emb(enc_inp)\n",
        "        enc_out, (hnx, cnx) = self.encoder(enc_emb)\n",
        "\n",
        "        dec_emb = self.decoder_emb(dec_inp)\n",
        "        dec_out, (hny, cny) = self.decoder(dec_emb,(hnx, cnx))\n",
        "        return self.out_layer(dec_out)\n",
        "\n",
        "# # 以下確認作業\n",
        "# ds = train_ds\n",
        "n_layers=1\n",
        "bidirectional=False\n",
        "n_hid=128\n",
        "tla_seq2seq = Seq2Seq_wAtt(enc_vocab_size=len(gakushu_tokenizer.tokens),\n",
        "                           dec_vocab_size=len(mora_tokenizer.tokens),\n",
        "                           n_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           n_hid=n_hid).to(device)\n",
        "print(tla_seq2seq.eval())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "695fcc6d-f896-49fc-8ca7-1d30b015e109",
      "metadata": {
        "id": "695fcc6d-f896-49fc-8ca7-1d30b015e109"
      },
      "source": [
        "## 3.3 seq2seq_without_attention model の定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "685ac367-d219-43da-ba85-9c26a62dbfd6",
      "metadata": {
        "id": "685ac367-d219-43da-ba85-9c26a62dbfd6"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq_woAtt(nn.Module):\n",
        "    \"\"\" 注意つき符号化器‐復号化器モデル\n",
        "    Bahdanau, Cho, & Bengio (2015) NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE, arXiv:1409.0473\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 enc_vocab_size:int,\n",
        "                 dec_vocab_size:int,\n",
        "                 n_hid:int,\n",
        "                 n_layers:int=2,\n",
        "                 bidirectional:bool=False,\n",
        "                 device=device):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder 側の入力トークン id を多次元ベクトルに変換\n",
        "        self.encoder_emb = nn.Embedding(num_embeddings=enc_vocab_size,\n",
        "                                        embedding_dim=n_hid,\n",
        "                                        padding_idx=0)\n",
        "\n",
        "        # Decoder 側の入力トークン id を多次元ベクトルに変換\n",
        "        self.decoder_emb = nn.Embedding(num_embeddings=dec_vocab_size,\n",
        "                                        embedding_dim=n_hid,\n",
        "                                        padding_idx=0)\n",
        "\n",
        "        # Encoder LSTM 本体\n",
        "        self.encoder = nn.LSTM(input_size=n_hid,\n",
        "                               hidden_size=n_hid,\n",
        "                               num_layers=n_layers,\n",
        "                               batch_first=True,\n",
        "                               bidirectional=bidirectional)\n",
        "\n",
        "        # Decoder LSTM 本体\n",
        "        self.decoder = nn.LSTM(input_size=n_hid,\n",
        "                               hidden_size=n_hid,\n",
        "                               num_layers=n_layers,\n",
        "                               batch_first=True,\n",
        "                               bidirectional=bidirectional)\n",
        "\n",
        "        # 文脈ベクトルと出力ベクトルの合成を合成する層\n",
        "        bi_fact = 2 if bidirectional else 1\n",
        "        self.combine_layer = nn.Linear(bi_fact * 2 * n_hid, n_hid)\n",
        "\n",
        "        # 最終出力層\n",
        "        self.out_layer = nn.Linear(n_hid, dec_vocab_size)\n",
        "\n",
        "    def forward(self, enc_inp, dec_inp):\n",
        "\n",
        "        enc_emb = self.encoder_emb(enc_inp)\n",
        "        enc_out, (hnx, cnx) = self.encoder(enc_emb)\n",
        "\n",
        "        dec_emb = self.decoder_emb(dec_inp)\n",
        "        dec_out, (hny, cny) = self.decoder(dec_emb,(hnx, cnx))\n",
        "\n",
        "        return self.out_layer(dec_out)\n",
        "\n",
        "    def evaluate(self, enc_inp, dec_inp):\n",
        "        return self.forward(enc_inp, dec_inp)\n",
        "\n",
        "\n",
        "# # 以下確認作業\n",
        "# ds = train_ds\n",
        "n_layers=1\n",
        "bidirectional=False\n",
        "n_hid=128\n",
        "tla_seq2seq = Seq2Seq_wAtt(enc_vocab_size=len(gakushu_tokenizer.tokens),\n",
        "                           dec_vocab_size=len(mora_tokenizer.tokens),\n",
        "                           n_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           n_hid=n_hid).to(device)\n",
        "print(tla_seq2seq.eval())\n",
        "\n",
        "tla_seq2seq0 = Seq2Seq_woAtt(enc_vocab_size=len(gakushu_tokenizer.tokens),\n",
        "                             dec_vocab_size=len(mora_tokenizer.tokens),\n",
        "                             n_layers=n_layers,\n",
        "                             bidirectional=bidirectional,\n",
        "                             n_hid=n_hid).to(device)\n",
        "print(tla_seq2seq0.eval())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5829bfe-31dc-4109-b82a-5207335f68bb",
      "metadata": {
        "id": "f5829bfe-31dc-4109-b82a-5207335f68bb"
      },
      "source": [
        "# 4. 訓練 (train) データセット，検証 (valid) データセット，検査 (test) データセットへ分割\n",
        "\n",
        "## 4.1 データセットの選択"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2813f84e-3abc-4cbb-b78d-7ba9ed828eed",
      "metadata": {
        "id": "2813f84e-3abc-4cbb-b78d-7ba9ed828eed"
      },
      "outputs": [],
      "source": [
        "# 以下の 2 つのデータセットは出力用トークナイザによって2つに分かれる\n",
        "psylex71_ds_mora = Psylex71_Dataset(output_tokenizer=mora_tokenizer)\n",
        "psylex71_ds_kunrei = Psylex71_Dataset(output_tokenizer=kunrei_tokenizer)\n",
        "\n",
        "# データセットのチェック\n",
        "for _ds in [psylex71_ds_mora, psylex71_ds_kunrei]:\n",
        "    for N in np.random.permutation(_ds.__len__())[:5]:\n",
        "    #for N in range(3):\n",
        "        inp, tgt = _ds.__getitem__(N)\n",
        "        print(f'_ds.ids2inp(inp):{_ds.ids2inp(inp)}',\n",
        "              f'{inp.cpu().numpy()}',\n",
        "              f'_ds.target_ids2target(tgt):{_ds.target_ids2target(tgt)}',\n",
        "              f'{tgt.cpu().numpy()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f3ba340-d265-45bc-bfc8-65f878ef857b",
      "metadata": {
        "id": "7f3ba340-d265-45bc-bfc8-65f878ef857b"
      },
      "source": [
        "## 4.2 データセットの分割,訓練,検査,検証データセット"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7db8143e-b242-4503-9918-1f287615fc87",
      "metadata": {
        "id": "7db8143e-b242-4503-9918-1f287615fc87"
      },
      "outputs": [],
      "source": [
        "# データセットの分割,訓練,検査,検証データセット\n",
        "_ds = psylex71_ds_mora\n",
        "\n",
        "train_size = int(_ds.__len__() * 0.5)\n",
        "valid_size = int(_ds.__len__() * 0.1)\n",
        "resid_size = _ds.__len__() - train_size - valid_size\n",
        "train_ds, valid_ds, resid_size = torch.utils.data.random_split(\n",
        "    dataset=_ds,\n",
        "    lengths=(train_size, valid_size, resid_size),\n",
        "    generator=torch.Generator().manual_seed(seed))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e5048cb-cdb9-439b-a260-585e00b9cbf4",
      "metadata": {
        "id": "5e5048cb-cdb9-439b-a260-585e00b9cbf4"
      },
      "source": [
        "## 4.3 バッチサイズの定義とデータローダの設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5598baaf-5a2d-46c8-835d-808d7012dcbc",
      "metadata": {
        "id": "5598baaf-5a2d-46c8-835d-808d7012dcbc"
      },
      "outputs": [],
      "source": [
        "# batch_size = 32\n",
        "# batch_size = 64\n",
        "# batch_size = 4096\n",
        "batch_size = 128\n",
        "#batch_size = 512\n",
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
        "valid_dl = torch.utils.data.DataLoader(dataset=valid_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def _collate_fn(batch):\n",
        "    inps, tgts = list(zip(*batch))\n",
        "    inps = list(inps)\n",
        "    tgts = list(tgts)\n",
        "    return inps, tgts\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=_collate_fn)\n",
        "\n",
        "valid_dl = torch.utils.data.DataLoader(\n",
        "    dataset=valid_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=_collate_fn)\n",
        "\n",
        "print(f'train_ds.__len__():{train_ds.__len__()}')\n",
        "print(f'valid_ds.__len__():{valid_ds.__len__()}')\n",
        "\n",
        "# # 以下，検証\n",
        "# _ds = train_ds\n",
        "# for N in range(2):\n",
        "#     inp, tgt = _ds.__getitem__(N)\n",
        "#     print(f'_ds.dataset.ids2inp(inp):{_ds.dataset.ids2inp(inp)}',\n",
        "#           f'{inp.cpu().numpy()}',\n",
        "#           f'_ds.dataset.target_ids2target(tgt):{_ds.dataset.target_ids2target(tgt)}',\n",
        "#           f'{tgt.cpu().numpy()}')\n",
        "\n",
        "# psylex71_ds.maxlen_out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f7279a-6c5f-49e5-8347-9d9d5842bae2",
      "metadata": {
        "id": "d9f7279a-6c5f-49e5-8347-9d9d5842bae2"
      },
      "source": [
        "# 5. 学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d95c41c-007e-4728-a899-6972db77da8c",
      "metadata": {
        "id": "2d95c41c-007e-4728-a899-6972db77da8c"
      },
      "outputs": [],
      "source": [
        "def fit_a_epoch(model:torch.nn.Module=None,\n",
        "                optimizer:torch.optim=None,\n",
        "                loss_f:torch.nn.modules=None,\n",
        "                _dl:torch.utils.data.dataloader.DataLoader=None):\n",
        "\n",
        "    model.train()  # モデルを訓練モードに変更\n",
        "\n",
        "    sum_loss=0\n",
        "    count=0\n",
        "    N = 0\n",
        "\n",
        "    for inps, tchs in _dl:\n",
        "        inps = pad_sequence(inps, batch_first=True).to(device)\n",
        "        tchs = pad_sequence(tchs, batch_first=True).to(device)\n",
        "        outs = model(inps, tchs)\n",
        "\n",
        "        # 正解のカウント\n",
        "        out_ids = [out.argmax(dim=1) for out in outs]\n",
        "        for tch, out in zip(tchs[:], out_ids[:]):\n",
        "            yesno = ((tch==out) * 1).sum().cpu().numpy() == len(tch)\n",
        "            count += 1 if yesno else 0\n",
        "\n",
        "        # 学習の実行\n",
        "        loss = 0.\n",
        "        for j in range(len(tchs)):\n",
        "            loss += loss_f(outs[j],tchs[j])\n",
        "        loss.backward()  # 損失値の計算\n",
        "        optimizer.step() # 学習\n",
        "        sum_loss += loss.item()\n",
        "\n",
        "        N += len(tchs)\n",
        "    p_ = count / N\n",
        "    return model, {'sum_loss':sum_loss, 'count':count, 'N':N, 'P':p_}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b1b618-eebc-4671-8c90-8bb17b0a3297",
      "metadata": {
        "id": "64b1b618-eebc-4671-8c90-8bb17b0a3297"
      },
      "outputs": [],
      "source": [
        "ds = train_ds\n",
        "n_layers=1\n",
        "bidirectional=False\n",
        "n_hid=512\n",
        "input_tokenizer = gakushu_tokenizer\n",
        "output_tokenizer = mora_tokenizer\n",
        "\n",
        "tla_vanilla = vanilla_TLA(inp_vocab_size=len(input_tokenizer.tokens),\n",
        "                          inp_len=2,\n",
        "                          out_vocab_size=len(output_tokenizer.tokens),\n",
        "                          out_len=ds.dataset.maxlen_out,\n",
        "                          device=device,\n",
        "                          n_hid=n_hid).to(device)\n",
        "print(tla_vanilla.eval())\n",
        "\n",
        "tla_seq2seq = Seq2Seq_wAtt(enc_vocab_size=len(input_tokenizer.tokens),\n",
        "                           dec_vocab_size=len(output_tokenizer.tokens),\n",
        "                           n_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           n_hid=n_hid).to(device)\n",
        "print(tla_seq2seq.eval())\n",
        "\n",
        "tla_seq2seq0 = Seq2Seq_woAtt(enc_vocab_size=len(input_tokenizer.tokens),\n",
        "                             dec_vocab_size=len(output_tokenizer.tokens),\n",
        "                             n_layers=n_layers,\n",
        "                             bidirectional=bidirectional,\n",
        "                             n_hid=n_hid).to(device)\n",
        "print(tla_seq2seq0.eval())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08f5e7a6-c089-4f7e-b791-f47fd26b613a",
      "metadata": {
        "id": "08f5e7a6-c089-4f7e-b791-f47fd26b613a"
      },
      "outputs": [],
      "source": [
        "model0 = tla_vanilla\n",
        "model1 = tla_seq2seq\n",
        "model2 = tla_seq2seq0\n",
        "\n",
        "loss_f = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer0 = torch.optim.Adam(model0.parameters(), lr=1e-3)\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
        "# optimizer0 = torch.optim.Adam(model0.parameters(), lr=1e-5)\n",
        "# optimizer1 = torch.optim.Adam(model1.parameters(), lr=1e-5)\n",
        "# optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-5)\n",
        "\n",
        "epochs = 500\n",
        "epochs = 10\n",
        "#epochs = 200\n",
        "interval = 3\n",
        "interval = 1\n",
        "#interval = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for (model, optimizer) in [(model0,optimizer0), (model1,optimizer1), (model2,optimizer2)]:\n",
        "    #for (model, optimizer) in [(model2,optimizer2), (model1,optimizer1), (model0,optimizer0)]:\n",
        "        model, out = fit_a_epoch(model=model, _dl=train_dl, loss_f=loss_f, optimizer=optimizer)\n",
        "        if (epoch % interval) == 0:\n",
        "            print(f\"エポック:{epoch+1:3d}\",\n",
        "                  f\"損失値={out['sum_loss']:10.3f}\",\n",
        "                  f\"正解率={out['P']:5.3f}\",\n",
        "                  f\"({out['count']:5d}/{out['N']:5d})\",\n",
        "                  end=\"\\t\")\n",
        "    if (epoch % interval) == 0:\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a15d892a-9462-423c-b05d-7f9697726027",
      "metadata": {
        "id": "a15d892a-9462-423c-b05d-7f9697726027"
      },
      "outputs": [],
      "source": [
        "model = tla_seq2seq\n",
        "with torch.no_grad():\n",
        "    for i in range(_ds.__len__()):\n",
        "        inp, tgt = _ds.__getitem__(i)\n",
        "        # print(f'インプット:{\"\".join(c for c in chihaya_ds.ids2tkn(inp))}') #i].cpu().numpy()))}')\n",
        "        # print(f'ターゲット:{\"\".join(c for c in chihaya_ds.ids2tkn(tgt))}') #n(c for c in chihaya_ds.ids2tkn(tgt_ids[i].cpu().numpy()))}')\n",
        "\n",
        "        inp_ids = pad_sequence(inp.unsqueeze(0), batch_first=True).to(device)\n",
        "        tgt_ids = pad_sequence(tgt.unsqueeze(0), batch_first=True).to(device)\n",
        "\n",
        "        enc_emb = model.encoder_emb(inp_ids)\n",
        "        enc_out, (hnx, cnx) = model.encoder(enc_emb)\n",
        "\n",
        "        dec_inp = torch.tensor([_ds.output_tokenizer.tokens.index('<SOW>')], device=device)\n",
        "        dec_inp = model.decoder_emb(dec_inp).unsqueeze(0)\n",
        "        dec_state = (hnx, cnx)\n",
        "\n",
        "        print(dec_inp.size(), dec_state[0].size())\n",
        "\n",
        "        for i in range(len(tgt_ids[0])):\n",
        "            print(f'i:{i}', f'dec_inp.size():{dec_inp.size()}', f'tgt_ids:{tgt_ids}')\n",
        "\n",
        "            dec_out, dec_state = model.decoder(dec_inp, dec_state)\n",
        "            dec_inp = dec_out.argmax().unsqueeze(0) # .clone().detach()\n",
        "            #print(dec_inp, tgt_ids, len(tgt_ids))\n",
        "\n",
        "\n",
        "\n",
        "        #print(inp_ids.size(), tgt_ids.size())\n",
        "        #sys.exit()\n",
        "        #model.encoder(inp_ids, tgt_ids)\n",
        "        sys.exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b5d00f-70c6-4321-906b-ceede7b48e80",
      "metadata": {
        "id": "04b5d00f-70c6-4321-906b-ceede7b48e80"
      },
      "outputs": [],
      "source": [
        "model = tla_seq2seq\n",
        "with torch.no_grad():\n",
        "    for i in range(_ds.__len__()):\n",
        "        inp, tgt = chihaya_ds.__getitem__(i)\n",
        "        # print(f'インプット:{\"\".join(c for c in chihaya_ds.ids2tkn(inp))}') #i].cpu().numpy()))}')\n",
        "        # print(f'ターゲット:{\"\".join(c for c in chihaya_ds.ids2tkn(tgt))}') #n(c for c in chihaya_ds.ids2tkn(tgt_ids[i].cpu().numpy()))}')\n",
        "\n",
        "        inp_ids = pad_sequence(inp.unsqueeze(0), batch_first=True).to(device)\n",
        "        tgt_ids = pad_sequence(tgt.unsqueeze(0), batch_first=True).to(device)\n",
        "        #inp_ids = torch.as_tensor(inp, device=device)\n",
        "        #tgt_ids = torch.as_tensor(tgt, device=device)\n",
        "        enc_out, enc_state = model.encoder(inp_ids)\n",
        "        #dec_out, dec_state = decoder(tgt_ids, enc_state)\n",
        "\n",
        "        dec_ids = dec_out.argmax(dim=1).detach().cpu().numpy()\n",
        "        print(f'len(dec_ids):{len(dec_ids)}')\n",
        "        print(\"\".join(c for c in chihaya_ds.ids2tkn(dec_ids)))\n",
        "\n",
        "        dec_inp = torch.tensor([chihaya_ds.chihaya_tokens.index('<SOS>')], device=device)\n",
        "        dec_inp = tgt_ids[0].unsqueeze(0)\n",
        "        for i in range(len(dec_ids)):\n",
        "            dec_out, dec_state = decoder(dec_inp, dec_state)\n",
        "            #print(dec_out.size(), dec_out.argmax().cpu().numpy(), type(dec_out.argmax())) #, dec_out)\n",
        "\n",
        "            if teacher_forcing:\n",
        "                dec_inp = tgt_ids[i].unsqueeze(0)\n",
        "            else:\n",
        "                dec_inp = dec_out.argmax().unsqueeze(0) # .clone().detach()\n",
        "\n",
        "            #dec_inp = dec_out.argmax().unsqueeze(0).clone().detach()\n",
        "            print(f'({dec_inp.cpu().numpy()}',\n",
        "                  f'{chihaya_ds.chihaya_tokens[dec_inp.cpu().numpy()[0]]})', end=\" \") # , type(dec_inp)) # , dec_inp)\n",
        "            #print(dec_inp.size(), dec_inp.argmax().cpu().numpy(), type(dec_inp.argmax())) # , dec_inp)\n",
        "            #sys.exit()\n",
        "        sys.exit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6940bbb6-ffe4-4b94-8232-2e26f9507c69",
      "metadata": {
        "id": "6940bbb6-ffe4-4b94-8232-2e26f9507c69"
      },
      "source": [
        "# Jalex の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "103b18c8-8ed2-45ca-bf30-89dc45c5fe05",
      "metadata": {
        "id": "103b18c8-8ed2-45ca-bf30-89dc45c5fe05"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "try:\n",
        "    import jaconv\n",
        "except:\n",
        "    !pip install jaconv --upgrade\n",
        "    import jaconv\n",
        "# Mecab を使ってヨミを得るために MeCab を import する\n",
        "from ccap.mecab_settings import wakati, yomi #, parser\n",
        "\n",
        "jalex_base = os.path.join(HOME, 'study/2025_2014jalex')\n",
        "jalex_xls_fname = 'JALEX.xlsx'\n",
        "jalex_fname = os.path.join(jalex_base, jalex_xls_fname)\n",
        "jalex_DF = pd.read_excel(jalex_fname)\n",
        "jalex_DF\n",
        "jalex_words = jalex_DF['目標語']\n",
        "print(len(jalex_words))\n",
        "\n",
        "jalex_dic = OrderedDict()\n",
        "for wrd in jalex_words:\n",
        "    if not wrd in jalex_dic:\n",
        "        _yomi = yomi(wrd).strip()\n",
        "        _wakati = wakati(wrd).strip()\n",
        "        _mora = mora_tokenizer.wakachi(_yomi)\n",
        "        _kunrei = kunrei_tokenizer.wakachi(_yomi)\n",
        "        _hira_yomi = jaconv.kata2hira(_yomi)\n",
        "        _julius = jaconv.hiragana2julius(_hira_yomi).split(' ')\n",
        "\n",
        "        jalex_dic[wrd] = {'ヨミ':_yomi, 'モーラ':_mora, '訓令':_kunrei, 'ユリウス':_julius} # , 'Jalex':jalex_DF['wrd']}\n",
        "        print(jalex_dic[wrd])\n",
        "        sys.exit()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}