{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2025notebooks/2025_0626psylex71_CDP%2Bja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db9a97b-ee8e-41b4-b88e-8018bfe1484c",
      "metadata": {
        "id": "2db9a97b-ee8e-41b4-b88e-8018bfe1484c"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_CDP_fig1.svg\">\n",
        "Zorzi+(1998) Fig.1 Architecture of the model. The arrow means full connectivity between layers. Each box stand for a group of letters (26) or phonemes (44).<br/>\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_CDP_fig8.svg\">\n",
        "<p>Zorzi+(1998) Fig.8. Architecture of the model with the hidden layer pathway. In both the direct pathway and the mediated pathway the layers are fully connected (arrows).</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f00f3ba-87a0-40f2-9b7b-141186681f58",
      "metadata": {
        "id": "1f00f3ba-87a0-40f2-9b7b-141186681f58"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import torch\n",
        "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f'device:{device}')\n",
        "\n",
        "# 必要なライブラリの輸入\n",
        "from collections import OrderedDict\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "# import time\n",
        "# import datetime\n",
        "import operator\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "from IPython import get_ipython\n",
        "isColab =  'google.colab' in str(get_ipython())\n",
        "\n",
        "try:\n",
        "    import ipynbname\n",
        "except ImportError:\n",
        "    !pip install ipynbname\n",
        "    import ipynbname\n",
        "\n",
        "FILEPATH = str(ipynbname.path()).split('/')[-1]\n",
        "print(f'FILEPATH:{FILEPATH}')\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0b5de0-5e4f-4f23-9e38-b20c1255349f",
      "metadata": {
        "id": "7d0b5de0-5e4f-4f23-9e38-b20c1255349f"
      },
      "source": [
        "# モーラ分かち書きの定義    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "036cb4be-72bf-438b-a0de-48cb61fb0ac8",
      "metadata": {
        "id": "036cb4be-72bf-438b-a0de-48cb61fb0ac8"
      },
      "outputs": [],
      "source": [
        "# モーラ分かち書きの定義\n",
        "# source https://qiita.com/shimajiroxyz/items/a133d990df2bc3affc12\n",
        "import re\n",
        "\n",
        "# 各条件を正規表現で表す\n",
        "c1 = '[ウクスツヌフムユルグズヅブプヴ][ァィェォ]' #ウ段＋「ァ/ィ/ェ/ォ」\n",
        "c2 = '[イキシチニヒミリギジヂビピ][ャュェョ]' #イ段（「イ」を除く）＋「ャ/ュ/ェ/ョ」\n",
        "c3 = '[テデ][ィュ]' #「テ/デ」＋「ャ/ィ/ュ/ョ」\n",
        "c4 = '[ァ-ヴー]' #カタカナ１文字（長音含む）\n",
        "\n",
        "cond = '('+c1+'|'+c2+'|'+c3+'|'+c4+')'\n",
        "re_mora = re.compile(cond)\n",
        "\n",
        "def moraWakachi(kana_text):\n",
        "    kana_text = kana_text.replace('ヱ','エ').replace('ヰ','イ')\n",
        "    return re_mora.findall(kana_text)\n",
        "\n",
        "# text = 'シンシュンシャンソンショー'\n",
        "# print(text)\n",
        "# print(moraWakachi(text))\n",
        "# print('')\n",
        "\n",
        "# text = 'トーキョートッキョキョカキョク'\n",
        "# print(text)\n",
        "# print(moraWakachi(text))\n",
        "# print('')\n",
        "\n",
        "# text = 'アウトバーン'\n",
        "# print(text)\n",
        "# print(moraWakachi(text))\n",
        "# print('')\n",
        "\n",
        "# text = 'ガッキュウホウカイ'\n",
        "# print(text)\n",
        "# print(moraWakachi(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2b66d9b-816f-4cae-b183-7ebb9511b76b",
      "metadata": {
        "id": "b2b66d9b-816f-4cae-b183-7ebb9511b76b"
      },
      "source": [
        "# 文字の定義，(学習文字，かな，カナ，数字，記号など)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9026f18c-c1b7-44d3-85cf-c91aecacd9eb",
      "metadata": {
        "id": "9026f18c-c1b7-44d3-85cf-c91aecacd9eb"
      },
      "outputs": [],
      "source": [
        "# 書記素の定義，書記素のうちカタカナを音韻表現としても利用\n",
        "\n",
        "seed = 42\n",
        "special_tokens = ['<PAD>', '<EOW>', '<SOW>', '<UNK>']\n",
        "alphabet_upper_chars='ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ'\n",
        "alphabet_lower_chars='ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ'\n",
        "num_chars='０１２３４５６７８９'\n",
        "hira_chars='ぁあぃいぅうぇえぉおかがきぎくぐけげこごさざしじすずせぜそぞただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもゃやゅゆょよらりるれろゎわゐゑをん'\n",
        "kata_chars='ァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモャヤュユョヨラリルレロヮワヰヱヲンヴヵヶ'\n",
        "#kata_chars=kata_chars+'一'  # カタカナ文字に伸ばし記号を加える\n",
        "#phon_list = list(kata_chars+'一')\n",
        "\n",
        "# # 句点コード\n",
        "# from RAM.char_ja import kuten as kuten\n",
        "# kuten_chars=kuten().chars\n",
        "\n",
        "# # 常用漢字\n",
        "# from RAM.char_ja import chars_joyo as chars_joyo\n",
        "# joyo_chars = \"\".join([ch for ch in chars_joyo().char_list])\n",
        "\n",
        "# 学習漢字 学年別\n",
        "_gakushu_list = ['一右雨円王音下火花貝学気休玉金九空月犬見五口校左三山四子糸字耳七車手十出女小上森人水正生青石赤先千川早草足村大男竹中虫町天田土二日入年白八百文本名木目夕立力林六',\n",
        "'引羽雲園遠黄何夏家科歌画会回海絵外角楽活間丸岩顔帰汽記弓牛魚京強教近兄形計元原言古戸午後語交光公工広考行高合国黒今才細作算姉市思止紙寺時自室社弱首秋週春書少場色食心新親図数星晴声西切雪線船前組走多太体台谷知地池茶昼朝長鳥直通弟店点電冬刀東当答頭同道読内南肉馬買売麦半番父風分聞米歩母方北妹毎万明鳴毛門夜野矢友曜用来理里話',\n",
        "'悪安暗委意医育員飲院運泳駅横屋温化荷界開階寒感漢館岸期起客宮急球究級去橋業局曲銀区苦具君係軽決血研県庫湖向幸港号根祭坂皿仕使始指死詩歯事持次式実写者主取守酒受州拾終習集住重宿所暑助勝商昭消章乗植深申真神身進世整昔全想相送息速族他打対待代第題炭短談着柱注丁帳調追定庭笛鉄転登都度島投湯等豆動童農波配倍箱畑発反板悲皮美鼻筆氷表病秒品夫負部服福物平返勉放味命面問役薬油有由遊予様洋羊葉陽落流旅両緑礼列練路和',\n",
        "'愛案以位囲胃衣印栄英塩央億加果課貨芽改械害街各覚完官管観関願喜器希旗機季紀議救求泣給挙漁競共協鏡極訓軍郡型径景芸欠結健建験固候功好康航告差最菜材昨刷察札殺参散産残司史士氏試児治辞失借種周祝順初唱松焼照省笑象賞信臣成清静席積折節説戦浅選然倉巣争側束続卒孫帯隊達単置仲貯兆腸低停底的典伝徒努灯働堂得特毒熱念敗梅博飯費飛必標票不付府副粉兵別変辺便包法望牧末満未脈民無約勇要養浴利陸料良量輪類令例冷歴連労老録',\n",
        "'圧易移因営永衛液益演往応恩仮価可河過賀解快格確額刊幹慣眼基寄規技義逆久旧居許境興均禁句群経潔件券検険減現限個故護効厚構耕講鉱混査再妻採災際在罪財桜雑賛酸師志支枝資飼似示識質舎謝授修術述準序承招証常情条状織職制勢性政精製税績責接設絶舌銭祖素総像増造則測属損態貸退団断築張提程敵適統導銅徳独任燃能破判版犯比肥非備俵評貧婦富布武復複仏編弁保墓報豊暴貿防務夢迷綿輸余預容率略留領',\n",
        "'異遺域宇映延沿我灰拡閣革割株巻干看簡危揮机貴疑吸供胸郷勤筋敬系警劇激穴憲権絹厳源呼己誤后孝皇紅鋼降刻穀骨困砂座済裁策冊蚕姿私至視詞誌磁射捨尺若樹収宗就衆従縦縮熟純処署諸除傷将障城蒸針仁垂推寸盛聖誠宣専泉洗染善創奏層操窓装臓蔵存尊宅担探誕暖段値宙忠著庁潮頂賃痛展党糖討届難乳認納脳派俳拝背肺班晩否批秘腹奮並閉陛片補暮宝訪亡忘棒枚幕密盟模訳優郵幼欲翌乱卵覧裏律臨朗論']\n",
        "\n",
        "_l = []\n",
        "for g in _gakushu_list:\n",
        "    for ch in g:\n",
        "        _l += ch\n",
        "gakushu_chars = \"\".join(ch for ch in _l)\n",
        "\n",
        "grph_list = []\n",
        "#for x in [hira_chars]:                       # 数字は入力文字としない場合\n",
        "#for x in [hira_chars, gakushu_chars]:             # 数字は入力文字としない場合\n",
        "for x in [hira_chars, kata_chars, num_chars, gakushu_chars]: # 数字も入力文字とする場合\n",
        "    for ch in x:\n",
        "        grph_list.append(ch)\n",
        "print(f'len(grph_list):{len(grph_list)}')\n",
        "print(f'全書記素 grph_list:{\"\".join([ch for ch in grph_list])}')\n",
        "\n",
        "# print(f'len(phon_list):{len(phon_list)}')\n",
        "# print(f'全音素 phon_list:{phon_list}')\n",
        "\n",
        "print(f'入力層の素子数 len(grph_list) + len(special_tokens)={len(grph_list) + len(special_tokens)}')\n",
        "# print(f'出力層の素子数 len(phon_list) + len(special_tokens)={len(phon_list) + len(special_tokens)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d147219-23e6-43c4-9afe-c0b907d9659c",
      "metadata": {
        "id": "6d147219-23e6-43c4-9afe-c0b907d9659c"
      },
      "source": [
        "# NTT 日本語の語彙特性 単語頻度データの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if isColab:\n",
        "    !pip install googledrivedownloader==0.4\n",
        "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "    import os\n",
        "\n",
        "    # 共有ファイルのIDを指定\n",
        "    file_id = '1eBJDN392BsUckg5LBFbbw5KT9PCsmnxI' # 'psylex71utf8_.txt\n",
        "    # https://drive.google.com/file/d/1eBJDN392BsUckg5LBFbbw5KT9PCsmnxI/view?usp=drive_link\n",
        "\n",
        "    # 保存したい場所とファイル名を指定\\n\",\n",
        "    # 例: /content/ ディレクトリに original_file_name.拡張子 という名前で保存\\n\",\n",
        "    destination_path = '/content/psylex71utf8_.txt' # ファイルの拡張子を適切に設定してください\\n\",\n",
        "    try:\n",
        "        print(f\"ファイルのダウンロードを開始します (ファイルID: {file_id})...\")\n",
        "        gdd.download_file_from_google_drive(file_id=file_id,\n",
        "                                            dest_path=destination_path)\n",
        "                                            # unzip=True if file_id is for a zip file):\n",
        "        print(f\"ファイルのダウンロードが完了しました。'{destination_path}' に保存されました。\")\n",
        "\n",
        "        # ダウンロードしたファイルを読み込む例 (テキストファイルの場合)\n",
        "        if os.path.exists(destination_path):\n",
        "            print(\"ダウンロードしたファイルの内容 (最初の数行):\")\n",
        "            with open(destination_path, 'r') as f:\n",
        "                # ファイルの内容を表示 (例: 最初の5行)\n",
        "                for i in range(5):\n",
        "                    line = f.readline()\n",
        "                    if not line:\n",
        "                        break\n",
        "                    print(line.strip())\n",
        "        else:\n",
        "            print(f\"エラー: ダウンロード先のファイル '{destination_path}' が見つかりません。\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ファイルのダウンロード中にエラーが発生しました: {e}\")"
      ],
      "metadata": {
        "id": "Xqvqqa0xSDRW"
      },
      "id": "Xqvqqa0xSDRW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "eAo_TOhVSftJ"
      },
      "id": "eAo_TOhVSftJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c06d8b9-36b3-4014-9975-d36f57e16ac2",
      "metadata": {
        "id": "7c06d8b9-36b3-4014-9975-d36f57e16ac2"
      },
      "outputs": [],
      "source": [
        "# NTT 日本語の語彙特性単語頻度データ psylex71.txt の読み込み\n",
        "#HOME = os.environ['HOME']\n",
        "if isColab:\n",
        "    ntt_base = '/content'\n",
        "else:\n",
        "    ntt_base = os.path.join(HOME, 'study/2017_2009AmanoKondo_NTTKanjiData')\n",
        "psy71_fname = os.path.join(ntt_base, 'psylex71utf8_.txt')  # ファイル名\n",
        "psylex71raw = open(psy71_fname, 'r').readlines()\n",
        "psylex71raw = [lin.strip().split(' ')[:6] for lin in psylex71raw]   # 空白 ' ' で分離し，年度ごとの頻度を削除\n",
        "print(f'len(psylex71raw):{len(psylex71raw)}')\n",
        "\n",
        "valid_chars = kata_chars + 'ー'\n",
        "\n",
        "# Psylex71 一行のデータは 0:共通ID, 1:独自ID, 2:表記, 3:ヨミ, 4:品詞, 5:頻度 を取り出す。\n",
        "#n_idx=0; n_wrd=2; n_yomi=3; n_pos=4; n_frq=5\n",
        "psylex_ids = {'_idx':0, '_idx2':1, '_wrd':2, '_yomi':3, '_pos':4, '_frq':5, '_mora':6}\n",
        "print(f'psylex_ids{psylex_ids}')\n",
        "\n",
        "mora_dict = OrderedDict()\n",
        "\n",
        "for x in tqdm(psylex71raw[1:]):\n",
        "    _word =  x[psylex_ids['_wrd']]\n",
        "    _yomi = x[psylex_ids['_yomi']]\n",
        "    is_valid = True\n",
        "    for ch in _yomi:\n",
        "        if not ch in valid_chars:\n",
        "            is_valid = False\n",
        "    if is_valid:\n",
        "        morae = moraWakachi(_yomi)\n",
        "        for m in morae:\n",
        "            if not m in mora_dict:\n",
        "                mora_dict[m] = 1\n",
        "            else:\n",
        "                mora_dict[m] += 1\n",
        "\n",
        "print(f'len(mora_dict):{len(mora_dict)}')\n",
        "mora_list = sorted(mora_dict.keys())\n",
        "\n",
        "is_graph = False\n",
        "print(len(mora_dict), mora_dict)\n",
        "if is_graph:\n",
        "    N_mora=np.array([v for v in mora_dict.values()]).sum()\n",
        "    mora_count_sorted = sorted(mora_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    figsize=(24,4)\n",
        "    topN = 100\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.bar(range(topN), [x[1]/N_mora for x in mora_count_sorted[:topN]])\n",
        "    plt.xticks(ticks=range(topN), labels=[c[0] for c in mora_count_sorted[:topN]])\n",
        "\n",
        "    plt.title(f'モーラ頻度 (上位:{topN} 語)')\n",
        "    plt.ylabel('相対頻度')\n",
        "    plt.show()\n",
        "    #len(mora_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc83ecee-5079-4d80-bfe5-350739a23978",
      "metadata": {
        "id": "dc83ecee-5079-4d80-bfe5-350739a23978"
      },
      "outputs": [],
      "source": [
        "maxlen_grph = 2        # 書記素最大文字数 + 2 しているのは, 単語の前後に特殊トークン <SOW> <EOW> をつけるため\n",
        "valid_chars=grph_list  # 書記素リスト grph_list を有効文字リスト valid_chars とする\n",
        "ng_yomi_words = []\n",
        "dups_idx = []\n",
        "_psylex71_ = []\n",
        "\n",
        "Psylex71 = OrderedDict()\n",
        "for lin in psylex71raw:\n",
        "    wrd = lin[psylex＿ids['_wrd']]\n",
        "    idx = lin[psylex＿ids['_idx']]\n",
        "    yomi = lin[psylex＿ids['_yomi']]\n",
        "    pos = lin[psylex＿ids['_pos']]\n",
        "    frq = lin[psylex＿ids['_frq']]\n",
        "\n",
        "    # print(f'type(lin):{type(lin)}')\n",
        "    # print(f'lin:{lin}')\n",
        "    # sys.exit()\n",
        "\n",
        "    if len(wrd) == maxlen_grph:  # 長さが maxlen_grph 文字である語に対して処理を行う\n",
        "\n",
        "        # ヨミの中にカタカナ以外の文字が入っていれば NG_flag を True にする\n",
        "        is_kata_yomi = True\n",
        "        for p in yomi:\n",
        "            if not p in kata_chars:\n",
        "                is_kata_yomi = False\n",
        "\n",
        "        # ヨミにカタカナ以外の文字が含まれていれば ng_yomi_words に加える\n",
        "        if is_kata_yomi == False:\n",
        "            ng_yomi_words.append((wrd,yomi))\n",
        "        else:\n",
        "\n",
        "            # valid_chars (学習漢字+)で構成されているか否かを判断\n",
        "            is_valid_grph = True\n",
        "            for i in range(maxlen_grph):\n",
        "                if not wrd[i] in valid_chars:\n",
        "                    is_valid_grph = False\n",
        "\n",
        "            if is_valid_grph == True:\n",
        "\n",
        "                _mora = moraWakachi(yomi) # .strip()  # モーラ分かち書きを行う\n",
        "                if idx in Psylex71:   # すでに ID 番号が登録されていれば dups_idx リストに加える\n",
        "                    dups_idx.append((idx, lin, (Psylex71[idx]['単語'], Psylex71[idx]['ヨミ'], _mora)))\n",
        "\n",
        "                Psylex71[idx] = {'単語': wrd, 'モーラ':_mora, 'ヨミ': yomi, '品詞': pos,'頻度': frq}\n",
        "                _psylex71_.append(lin + [_mora])\n",
        "\n",
        "\n",
        "# 読み (音韻表現) の最大長値の探索\n",
        "maxlen_phon = 0\n",
        "for a in _psylex71_:\n",
        "    if len(a[psylex_ids['_mora']]) > maxlen_phon:\n",
        "         maxlen_phon = len(a[psylex_ids['_mora']])\n",
        "\n",
        "# 結果の表示\n",
        "print(f'読み込んだ psylex71.txt の単語数 len(psylex71raw):{len(psylex71raw)}')\n",
        "print(f'Psylex71 の総単語数 len(_psylex71_):{len(_psylex71_)}')\n",
        "print(f'作成したデータベース辞書の項目数 len(Psylex71):{len(Psylex71)}')\n",
        "print(f'ヨミの最長文字数 maxlen_phon:{maxlen_phon}')\n",
        "print(f'len(mora_list):{len(mora_list)}')\n",
        "#print(f'音素 (読みのカタカナ文字)数 len(phon_cands):{len(phon_cands)}')\n",
        "print(f'Psylex71 におけるカタカナ以外のヨミのある単語数 len(ng_yomi_words):{len(ng_yomi_words)}')\n",
        "print(f'Psylex71 における ID 番号の重複数 len(dups_idx):{len(dups_idx)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef1d47a-7c2d-4171-a920-df4a54263fb3",
      "metadata": {
        "id": "4ef1d47a-7c2d-4171-a920-df4a54263fb3"
      },
      "source": [
        "# `Psylex71_Dataset` (モデルに Psylex71 を学習させるためのクラス) の作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bdb138-4ce6-45d7-b745-bde4934eb29e",
      "metadata": {
        "id": "f0bdb138-4ce6-45d7-b745-bde4934eb29e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class Psylex71_Dataset(torch.utils.data.Dataset):\n",
        "    '''ニューラルネットワークモデルに Psylex71 を学習させるための PyTorch 用データセットのクラス'''\n",
        "\n",
        "    def __init__(self,\n",
        "                 dic=Psylex71,\n",
        "                 grph_list=grph_list,\n",
        "                 phon_list=mora_list,\n",
        "                 special_tokens=special_tokens,\n",
        "                 maxlen_phon=maxlen_phon +2, # ＋2 しているのは <SOW>,<EOW> という 2 つのスペシャルトークンを付加するため\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.dic = dic\n",
        "        self.special_tokens = special_tokens\n",
        "        self.maxlen_phon = maxlen_phon\n",
        "        self.grph_list = grph_list\n",
        "        self.phon_list = phon_list\n",
        "        self.input_cands = grph_list\n",
        "        #self.target_cands = special_tokens + phon_list\n",
        "        self.target_cands = special_tokens + mora_list\n",
        "        # self.inputs = [v['単語'] for v in dic.values()]\n",
        "        # self.targets = [v['ヨミ'] for v in dic.values()]\n",
        "        # self.targets = [v['モーラ'] for v in dic.values()]\n",
        "        self.inputs = [v['単語'] for v in dic.values()]\n",
        "        self.targets = [v['ヨミ'] for v in dic.values()]\n",
        "        self.targets = [v['モーラ'] for v in dic.values()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dic)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, tgt = self.inputs[idx], self.targets[idx]\n",
        "\n",
        "        # 入力信号にも <SOW>, <EOW> トークンを付与する場合\n",
        "        #inp = [self.input_cands.index('<SOW>')]  + [self.input_cands.index(x) for x in inp]  + [self.input_cands.index('<EOW>')]\n",
        "\n",
        "        # 入力信号にはスペシャルトークンを付与しない場合\n",
        "        inp = [self.input_cands.index(x) for x in inp]\n",
        "\n",
        "        # ターゲット (教師)信号 には <SOW>, <EOW> を付与する\n",
        "        tgt = [self.target_cands.index('<SOW>')] + [self.target_cands.index(x) for x in tgt] + [self.target_cands.index('<EOW>')]\n",
        "\n",
        "        while len(tgt) < self.maxlen_phon:\n",
        "            tgt = tgt + [self.target_cands.index('<PAD>')]\n",
        "\n",
        "        inp, tgt = torch.LongTensor(inp), torch.LongTensor(tgt)\n",
        "        return inp, tgt\n",
        "\n",
        "    def getitem(self, idx):\n",
        "        #inp, tgt = self.inputs[idx], self.targets[idx]\n",
        "        wrd = self.inputs[idx]\n",
        "        phn = self.targets[idx]\n",
        "        return wrd, phn\n",
        "\n",
        "    def ids2argmax(self, ids):\n",
        "        out = np.array([torch.argmax(idx).numpy() for idx in ids], dtype=np.int32)\n",
        "        return out\n",
        "\n",
        "    def ids2tgt(self, ids):\n",
        "        #out = [self.target_cands[torch.argmax(idx)] for idx in ids]\n",
        "        out = [self.target_cands[idx - len(self.special_tokens)] for idx in ids]\n",
        "        return out\n",
        "\n",
        "    def ids2inp(self, ids):\n",
        "        out = [self.input_cands[idx] for idx in ids]\n",
        "        #out = [self.input_cands[idx - len(self.special_tokens)] for idx in ids]\n",
        "        return out\n",
        "\n",
        "    def target_ids2target(self, ids:list):\n",
        "        ret = []\n",
        "        for idx in ids:\n",
        "            if idx == self.target_cands.index('<EOW>'):\n",
        "                return ret+['<EOW>']\n",
        "            ret.append(self.target_cands[idx])\n",
        "        return ret\n",
        "\n",
        "\n",
        "psylex71_ds = Psylex71_Dataset()\n",
        "\n",
        "_ds = psylex71_ds\n",
        "#for N in np.random.permutation(psylex71_ds.__len__())[:15]:\n",
        "for N in range(15):\n",
        "    inp, tgt = psylex71_ds.__getitem__(N)\n",
        "    print(f'_ds.ids2inp(inp):{_ds.ids2inp(inp)}',\n",
        "          f'{inp.numpy()}',\n",
        "          f'_ds.target_ids2target(tgt):{_ds.target_ids2target(tgt)}',\n",
        "          f'{tgt.numpy()}')\n",
        "\n",
        "\n",
        "train_size = int(_ds.__len__() * 0.7)\n",
        "train_size = int(_ds.__len__() * 0.3)\n",
        "valid_size = _ds.__len__() - train_size\n",
        "train_ds, valid_ds = torch.utils.data.random_split(dataset=_ds, lengths=(train_size, valid_size), generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "batch_size = 64\n",
        "batch_size = 4096\n",
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
        "valid_dl = torch.utils.data.DataLoader(dataset=valid_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def _collate_fn(batch):\n",
        "    inps, tgts = list(zip(*batch))\n",
        "    inps = list(inps)\n",
        "    tgts = list(tgts)\n",
        "    return inps, tgts\n",
        "\n",
        "# batch_size = 4\n",
        "train_dl = torch.utils.data.DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=_collate_fn)\n",
        "\n",
        "valid_dl = torch.utils.data.DataLoader(\n",
        "    dataset=valid_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=_collate_fn)\n",
        "\n",
        "print(f'train_ds.__len__():{train_ds.__len__()}')\n",
        "\n",
        "_ds = train_ds\n",
        "for N in range(15):\n",
        "    inp, tgt = _ds.__getitem__(N)\n",
        "    print(f'_ds.dataset.ids2inp(inp):{_ds.dataset.ids2inp(inp)}',\n",
        "          f'{inp.numpy()}',\n",
        "          f'_ds.datsset.target_ids2target(tgt):{_ds.dataset.target_ids2target(tgt)}',\n",
        "          f'{tgt.numpy()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e5596b-b653-4be2-a304-454a71f9c6d3",
      "metadata": {
        "id": "41e5596b-b653-4be2-a304-454a71f9c6d3"
      },
      "source": [
        "# TLA モデルの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc6f586e-7ea0-4fda-96f0-6b805bdd65a0",
      "metadata": {
        "id": "dc6f586e-7ea0-4fda-96f0-6b805bdd65a0"
      },
      "outputs": [],
      "source": [
        "class TLA(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 # maxlen_phon+2 しているのは単語の前後に <SOW>, <EOW> トークンを付けるため\n",
        "                 inp_size= (len(grph_list)+len(special_tokens)), # * (maxlen_grph + 2),\n",
        "                 inp_len=maxlen_grph, #  + 2,\n",
        "                 out_size=len(mora_list)+len(special_tokens),\n",
        "                 out_len=maxlen_phon+2,\n",
        "                 hid_size=128,\n",
        "                 device=device,\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.inp_size=inp_size\n",
        "        self.inp_len=inp_len\n",
        "        self.out_size=out_size\n",
        "        self.out_len=out_len\n",
        "        self.hid_size=hid_size\n",
        "\n",
        "        self.emb_layers = [torch.nn.Embedding(num_embeddings=inp_size, embedding_dim=hid_size, padding_idx=0).to(device) for _ in range(inp_len)]\n",
        "        #self.emb_layer = torch.nn.Embedding(num_embeddings=inp_size, embedding_dim=hid_size, padding_idx=0).to(device)\n",
        "\n",
        "        self.hid_layer = torch.nn.Linear(in_features=hid_size * inp_len, out_features=hid_size).to(device)\n",
        "        #self.hid_layer = torch.nn.Linear(in_features=inp_len * inp_size, out_features=hid_size)\n",
        "\n",
        "        self.out_layers = [torch.nn.Linear(in_features=hid_size, out_features=out_size).to(device) for _ in range(out_len)]\n",
        "\n",
        "    def forward(self, inp):\n",
        "        X = inp\n",
        "        batch_size = X.size(0)\n",
        "        n_grph = X.size(1)\n",
        "\n",
        "        embs = []\n",
        "        for i in range(n_grph):\n",
        "            _emb = self.emb_layers[i](X[:,i])\n",
        "            #print(f'{i}:_emb.size():{_emb.size()}')\n",
        "            embs.append(_emb)\n",
        "\n",
        "        _embs = torch.concat(embs,dim=1)\n",
        "        X = _embs\n",
        "        X = self.hid_layer(X)         # 中間層次元へ変換\n",
        "\n",
        "        # 出力層の音韻表現ごとへ変換\n",
        "        outputs = []\n",
        "        for i in range(self.out_len):\n",
        "            _out = self.out_layers[i](X)\n",
        "            outputs.append(_out)\n",
        "\n",
        "        # softmax 変換\n",
        "        #outputs = [torch.nn.functional.softmax(out,dim=1) for out in outputs]\n",
        "        outputs = [torch.nn.functional.sigmoid(out) for out in outputs]\n",
        "\n",
        "        #outputs = torch.cat(outputs, dim=0)\n",
        "        # outputs = torch.stack(outputs)\n",
        "        # return outputs\n",
        "\n",
        "        O = torch.empty(self.out_len, batch_size, self.out_size)\n",
        "        for i in range(len(outputs)):\n",
        "            O[i] = outputs[i]\n",
        "        O = O.reshape(batch_size, self.out_len, self.out_size)\n",
        "        O = torch.Tensor(O)\n",
        "        return O\n",
        "\n",
        "tla = TLA(device=device)\n",
        "tla.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c4f3f15-e095-4d77-9e4b-602dbaf8896e",
      "metadata": {
        "id": "5c4f3f15-e095-4d77-9e4b-602dbaf8896e"
      },
      "outputs": [],
      "source": [
        "class vanilla_TLA(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 inp_size= (len(grph_list)+len(special_tokens)),\n",
        "                 inp_len=maxlen_grph,\n",
        "                 out_size=len(mora_list)+len(special_tokens),\n",
        "                 out_len=maxlen_phon+2,\n",
        "                 hid_size=1024,\n",
        "                 device=device,\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.inp_size=inp_size\n",
        "        self.inp_len=inp_len\n",
        "        self.out_size=out_size\n",
        "        self.out_len=out_len\n",
        "        self.hid_size=hid_size\n",
        "\n",
        "        self.emb_layer = torch.nn.Linear(in_features=inp_size * inp_len, out_features=hid_size).to(device)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.out_layer = torch.nn.Linear(in_features=hid_size, out_features=out_size * out_len).to(device)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        X = inp\n",
        "        X = torch.nn.functional.one_hot(X, num_classes=self.inp_size)\n",
        "        X = X.reshape(X.size(0),-1)\n",
        "        X = X.float()\n",
        "        X = self.emb_layer(X)\n",
        "        X = self.tanh(X)\n",
        "        X = self.out_layer(X)\n",
        "        X = self.sigmoid(X)\n",
        "        X = X.reshape(X.size(0), self.out_len, self.out_size)\n",
        "\n",
        "        return X\n",
        "\n",
        "vanilla_tla = vanilla_TLA(device=device)\n",
        "vanilla_tla.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50214a92-8a3f-45b4-938e-b06715f65485",
      "metadata": {
        "id": "50214a92-8a3f-45b4-938e-b06715f65485"
      },
      "source": [
        "# 定義したモデルの試用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d54ba9e-47ad-48fc-b67d-45b4ccc3141b",
      "metadata": {
        "id": "4d54ba9e-47ad-48fc-b67d-45b4ccc3141b"
      },
      "outputs": [],
      "source": [
        "# idx に整数を指定して,対応するデータを取得する\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "#_ds = psylex71_ds\n",
        "_ds = train_ds\n",
        "idx = np.random.choice(_ds.__len__())\n",
        "#idx = 0\n",
        "\n",
        "vanilla_tla.eval()\n",
        "#tla.eval()\n",
        "\n",
        "# N 個のデータを実行してみる\n",
        "N = 5\n",
        "ids = np.random.permutation(_ds.__len__())[:N]\n",
        "for idx in ids:\n",
        "    # データセットから返ってくる値は入力信号 inp と教師信号 tch\n",
        "    inp, tch = _ds.__getitem__(idx)\n",
        "    print(f'idx:{idx}:', f'inp:{inp}', f'tch:{tch}')\n",
        "\n",
        "    # 入出力信号はトークン ID 番号であるため人間が読みやすいように変換して表示\n",
        "    print(f'_ds.dataset.ids2inp({inp}):{_ds.dataset.ids2inp(inp)}')\n",
        "    print(f'_ds.dataset.taregt_ids2target({tch}):{_ds.dataset.target_ids2target(tch)}')\n",
        "\n",
        "    inp = pad_sequence(inp.unsqueeze(0), batch_first=True).to(device)\n",
        "\n",
        "    # outs = tla(inp)\n",
        "    # print('出力:', _ds.dataset.target_ids2target([int(_out.argmax().cpu().numpy()) for _out in outs[0]]), end=\": \")\n",
        "    # print('出力 ids:', [int(_out.argmax().cpu().numpy()) for _out in outs[0]])\n",
        "\n",
        "    print('教師:', train_ds.dataset.target_ids2target([idx.numpy() for idx in tch]), end=\": \")\n",
        "    print('教師 ids:', [int(_tch.numpy()) for _tch in tch])\n",
        "    outs = vanilla_tla(inp)\n",
        "    print('出力:', train_ds.dataset.target_ids2target([int(_out.argmax().cpu().numpy()) for _out in outs[0]]), end=\": \")\n",
        "    print('出力 ids:', [int(_out.argmax().cpu().numpy()) for _out in outs[0]], end=\"\\n===\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b880e29b-8c23-40a1-9b2b-bd333681c827",
      "metadata": {
        "id": "b880e29b-8c23-40a1-9b2b-bd333681c827"
      },
      "outputs": [],
      "source": [
        "# ミニバッチバージョン\n",
        "\n",
        "tla = vanilla_tla\n",
        "loss_f = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = torch.optim.Adam(tla.parameters(), lr=1e-3)\n",
        "epochs = 2\n",
        "epochs = 100\n",
        "\n",
        "_ds = train_ds\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    tla.train()\n",
        "    sum_loss = 0.\n",
        "    count  = 0\n",
        "    _dl = train_dl\n",
        "\n",
        "    for inps, tchs in _dl:\n",
        "    #for inps, tchs in tqdm(_dl):\n",
        "        inps = pad_sequence(inps, batch_first=True).to(device)\n",
        "        tchs = pad_sequence(tchs, batch_first=True).to(device)\n",
        "        outs = tla(inps)\n",
        "\n",
        "        losses = 0.\n",
        "        optimizer.zero_grad()\n",
        "        for j in range(len(tchs)):\n",
        "            loss = loss_f(outs[j],tchs[j])\n",
        "            losses += loss\n",
        "            sum_loss += loss.item()\n",
        "\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        out_ids = [out.argmax(dim=1) for out in outs]\n",
        "        for tch, out in zip(tchs[:], out_ids[:]):\n",
        "            yesno = ((tch==out) * 1).sum().cpu().numpy() == len(tch)\n",
        "            count += 1 if yesno else 0\n",
        "\n",
        "    p_correct = count / _ds.__len__()\n",
        "    print(f'epoch:{epoch+1:03d}', end=\" \")\n",
        "    print(f'p_correct:{p_correct:5.3f}', end=\"\")\n",
        "    print(f'=({count:5d}/{_ds.__len__():5d})', end= \" \")\n",
        "    print(f'sum_loss:{sum_loss/_ds.__len__():.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZcZkSi-RYWdZ"
      },
      "id": "ZcZkSi-RYWdZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}