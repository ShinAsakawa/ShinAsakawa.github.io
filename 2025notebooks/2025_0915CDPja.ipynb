{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2025notebooks/2025_0915CDPja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb9a707e-7d3e-4c46-afb4-94bf7b425bf2",
      "metadata": {
        "id": "fb9a707e-7d3e-4c46-afb4-94bf7b425bf2"
      },
      "source": [
        "# 0. 準備作業"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db9a97b-ee8e-41b4-b88e-8018bfe1484c",
      "metadata": {
        "id": "2db9a97b-ee8e-41b4-b88e-8018bfe1484c"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_CDP_fig1.svg\" style=\"width:49%;\"><br/>\n",
        "<p>Zorzi+(1998) Fig.1 Architecture of the model. The arrow means full connectivity between layers. Each box stand for a group of letters (26) or phonemes (44).</p>\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_CDP_fig8.svg\" width=\"49%;\"><br/>\n",
        "<p>Zorzi+(1998) Fig.8. Architecture of the model with the hidden layer pathway. In both the direct pathway and the mediated pathway the layers are fully connected (arrows).</p>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_fig10.svg\" width=\"49%\"><br/>\n",
        "<p style=\"align-text:center\">\n",
        "Figure 10. Lexical and sublexical procedures in reading aloud, and their interaction in the phonological decision system, where the final phonological code is computed for articulation.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4097c4e9-c490-4021-a06d-b9176667c507",
      "metadata": {
        "id": "4097c4e9-c490-4021-a06d-b9176667c507"
      },
      "source": [
        "## 0.1 必要なライブラリの輸入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f00f3ba-87a0-40f2-9b7b-141186681f58",
      "metadata": {
        "id": "1f00f3ba-87a0-40f2-9b7b-141186681f58"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import torch\n",
        "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f'device:{device}')\n",
        "\n",
        "# 全モデル共通使用するライブラリの輸入\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from collections import OrderedDict\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import operator\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "# 実行環境が Google colablatory であるかを判定\n",
        "from IPython import get_ipython\n",
        "isColab = 'google.colab' in str(get_ipython())\n",
        "print(f'isColab:{isColab}')\n",
        "\n",
        "try: # 図に日本語を表示するため\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "try:\n",
        "    import jaconv\n",
        "except ImportError:\n",
        "    !pip install jaconv\n",
        "\n",
        "try: # 自作ライブラリの輸入\n",
        "    import CDP_ja\n",
        "except ImportError:\n",
        "\n",
        "    !git clone https://github.com/ShinAsakawa/CDP_ja.git\n",
        "    import CDP_ja"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0b5de0-5e4f-4f23-9e38-b20c1255349f",
      "metadata": {
        "id": "7d0b5de0-5e4f-4f23-9e38-b20c1255349f"
      },
      "source": [
        "## CDP_ja の読み込み (トークナイザ tokenizers, モデル)\n",
        "\n",
        "* トークナイザ：モーラ分かち書き, 訓令式ローマ字, 学習漢字, 常用漢字\n",
        "* モデル direct_TLA, indirect_TLA, combined_TLA<br/>\n",
        "をインポートする"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfcda6df-7c13-43b4-a750-8bb91aabdfe3",
      "metadata": {
        "id": "dfcda6df-7c13-43b4-a750-8bb91aabdfe3"
      },
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "# モーラ分かち書き, 訓令式ローマ字表記による音韻表現，学習漢字，常用漢字用トークナイザが定義されている\n",
        "from CDP_ja import mora_Tokenizer, kunrei_Tokenizer, gakushu_Tokenizer, joyo_Tokenizer\n",
        "\n",
        "mora_tokenizer = mora_Tokenizer()\n",
        "kunrei_tokenizer = kunrei_Tokenizer()\n",
        "gakushu_tokenizer = gakushu_Tokenizer()\n",
        "joyo_tokenizer = joyo_Tokenizer()\n",
        "\n",
        "# TLA モデル 3 種の定義， 単層パーセプトロン， 三層パーセプトロン， スキップ結合つき三層パーセプトロン\n",
        "from CDP_ja import direct_TLA, indirect_TLA, combined_TLA\n",
        "\n",
        "# リカレントニューラルネットワークモデルを用いたモデル\n",
        "from CDP_ja import Seq2Seq_wAtt, Seq2Seq_woAtt\n",
        "\n",
        "# そのたの実用的関数群\n",
        "from CDP_ja import fit_an_epoch\n",
        "from CDP_ja import eval_an_epoch\n",
        "from CDP_ja import Psylex71_Dataset\n",
        "from CDP_ja import init_seed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce290850-2544-41d8-b077-3d5a23dbc9fc",
      "metadata": {
        "id": "ce290850-2544-41d8-b077-3d5a23dbc9fc"
      },
      "source": [
        "# NTT 日本語語彙特性 単語頻度データ psylex71 データセットの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8fa0444-c6c5-4f31-b841-7928fdface35",
      "metadata": {
        "id": "a8fa0444-c6c5-4f31-b841-7928fdface35"
      },
      "outputs": [],
      "source": [
        "psylex71_dss={}   # データセットを複数格納するための辞書\n",
        "\n",
        "input_tokenizer = gakushu_tokenizer\n",
        "input_tokenizer = joyo_tokenizer\n",
        "output_tokenizer = mora_tokenizer\n",
        "\n",
        "inp_minlen=1      # 入力情報に与えるための最短単語文字長\n",
        "\n",
        "for inp_maxlen in [2,3,4,5,6,7]:  # 入力情報に与えるための最長単語文字長\n",
        "#for inp_maxlen in [2]:  # 入力情報に与えるための最長単語文字長\n",
        "\n",
        "    if inp_maxlen == inp_minlen:       # 最初だけ全文字候補を出力\n",
        "        display=True\n",
        "    else:\n",
        "        display=False\n",
        "\n",
        "    psylex71_dss[inp_maxlen] = Psylex71_Dataset(\n",
        "        inp_minlen=inp_minlen,\n",
        "        inp_maxlen=inp_maxlen,             # この値だけがループ内で変化する\n",
        "        input_tokenizer=input_tokenizer,   # 入力情報をトークン化するためのトークナイザ\n",
        "        output_tokenizer=output_tokenizer, # 出力情報をトークン化するためのトークナイザ\n",
        "        #excel_fname='psylex71all.xlsx',\n",
        "        excel_fname='psylex71all_sorted.xlsx',\n",
        "        #device=device,\n",
        "        display=display)\n",
        "\n",
        "    print(f'psylex71 最短文字長:{inp_minlen:2d}, 最長文字長:{inp_maxlen:2d}',\n",
        "          f'データセットサイズ (単語数):{psylex71_dss[inp_maxlen].__len__():7,d} 語')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5829bfe-31dc-4109-b82a-5207335f68bb",
      "metadata": {
        "id": "f5829bfe-31dc-4109-b82a-5207335f68bb"
      },
      "source": [
        "# 訓練 (train) データセット，検証 (valid) データセット，検査 (test) データセットへ分割\n",
        "\n",
        "## データセットの選択"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f3ba340-d265-45bc-bfc8-65f878ef857b",
      "metadata": {
        "id": "7f3ba340-d265-45bc-bfc8-65f878ef857b"
      },
      "source": [
        "## データセットの分割,訓練,検査,検証データセット"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7db8143e-b242-4503-9918-1f287615fc87",
      "metadata": {
        "id": "7db8143e-b242-4503-9918-1f287615fc87"
      },
      "outputs": [],
      "source": [
        "# データセットの分割,訓練,検査,検証データセット\n",
        "\n",
        "# 乱数の種を設定\n",
        "seed=42\n",
        "init_seed(seed=seed)\n",
        "\n",
        "# どちらのデータセットを用いるかを _ds に代入することで指定する\n",
        "inp_maxlen = 2\n",
        "#inp_maxlen = 5\n",
        "_ds = psylex71_dss[inp_maxlen]\n",
        "#_ds = psylex71_ds_mora\n",
        "#_ds = psylex71_ds_o2o\n",
        "#_ds = psylex71_ds_p2p\n",
        "#_ds = psylex71_ds_kunrei\n",
        "\n",
        "# ここでは時間節約のため,全体のデータ数のうち 以下の割合だけデータを用いて検証を行う\n",
        "#train_size = int(_ds.__len__() * 0.9)\n",
        "#valid_size = int(_ds.__len__() * 0.1)\n",
        "train_size = int(_ds.__len__() * 0.8)\n",
        "valid_size = int(_ds.__len__() * 0.2)\n",
        "train_size = int(_ds.__len__() * 0.1)\n",
        "valid_size = int(_ds.__len__() * 0.1)\n",
        "resid_size = _ds.__len__() - train_size - valid_size\n",
        "\n",
        "# 実際のデータ分割\n",
        "train_ds, valid_ds, resid_size = torch.utils.data.random_split(\n",
        "    dataset=_ds,\n",
        "    lengths=(train_size, valid_size, resid_size),\n",
        "    generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "print(f'train_size:{train_size}')\n",
        "print(f'valid_size:{valid_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e5048cb-cdb9-439b-a260-585e00b9cbf4",
      "metadata": {
        "id": "5e5048cb-cdb9-439b-a260-585e00b9cbf4"
      },
      "source": [
        "## バッチサイズの定義とデータローダの設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5598baaf-5a2d-46c8-835d-808d7012dcbc",
      "metadata": {
        "id": "5598baaf-5a2d-46c8-835d-808d7012dcbc"
      },
      "outputs": [],
      "source": [
        "# ミニバッチサイズの定義\n",
        "batch_size = 1024\n",
        "batch_size = 128\n",
        "\n",
        "# データセットとミニバッチサイズを用いて PyTorch 用のデータローダを宣言\n",
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
        "valid_dl = torch.utils.data.DataLoader(dataset=valid_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 並列計算のための準備\n",
        "def _collate_fn(batch):\n",
        "    inps, tgts = list(zip(*batch))\n",
        "    inps = list(inps)\n",
        "    tgts = list(tgts)\n",
        "    return inps, tgts\n",
        "\n",
        "# 訓練データセット用データローダ\n",
        "train_dl = torch.utils.data.DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=_collate_fn)\n",
        "\n",
        "# 検証データセット用のデータローダ\n",
        "valid_dl = torch.utils.data.DataLoader(\n",
        "    dataset=valid_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=_collate_fn)\n",
        "\n",
        "print(f'train_ds.__len__():{train_ds.__len__()}')\n",
        "print(f'valid_ds.__len__():{valid_ds.__len__()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39559e4c-2695-4482-809a-cc86c1c81600",
      "metadata": {
        "id": "39559e4c-2695-4482-809a-cc86c1c81600"
      },
      "source": [
        "# モデルの定義\n",
        "## TLA モデルの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c236f2a-220c-48e9-a2f1-e723b05cdda8",
      "metadata": {
        "id": "8c236f2a-220c-48e9-a2f1-e723b05cdda8"
      },
      "outputs": [],
      "source": [
        "#from RAM import Transformer\n",
        "from CDP_ja import Transformer\n",
        "transformer = Transformer(src_vocab_size=len(input_tokenizer.tokens),\n",
        "                          tgt_vocab_size=len(output_tokenizer.tokens),\n",
        "                          model_dim=256,\n",
        "                          num_heads=4,\n",
        "                          num_layers=1,\n",
        "                          max_seq_length=_ds.out_maxlen,\n",
        "                          dropout=0.,\n",
        "                          ff_dim=32,\n",
        "                          device=device)\n",
        "transformer.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37208ed4-cdc7-4d8d-ba0f-a5b1fe159d5d",
      "metadata": {
        "id": "37208ed4-cdc7-4d8d-ba0f-a5b1fe159d5d"
      },
      "source": [
        "## 定義したモデルで動作チェック"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d88089f5-7b32-474f-aa30-ce303287ca3c",
      "metadata": {
        "id": "d88089f5-7b32-474f-aa30-ce303287ca3c"
      },
      "outputs": [],
      "source": [
        "inp_maxlen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d54ba9e-47ad-48fc-b67d-45b4ccc3141b",
      "metadata": {
        "id": "4d54ba9e-47ad-48fc-b67d-45b4ccc3141b"
      },
      "outputs": [],
      "source": [
        "# モデルを宣言するために必要なハイパーパラメータを定義\n",
        "_ds = train_ds                     # データセット\n",
        "n_hid=256                          # 中間層のニューロン数\n",
        "out_f = None\n",
        "hidden_out_f = None\n",
        "n_layers=1\n",
        "bidirectional=False\n",
        "\n",
        "tla_direct = direct_TLA(\n",
        "    inp_vocab_size=len(input_tokenizer.tokens),\n",
        "    out_vocab_size=len(output_tokenizer.tokens),\n",
        "    inp_len = inp_maxlen + 2,\n",
        "    out_f = out_f,\n",
        "    out_len = _ds.dataset.out_maxlen).to(device)\n",
        "print(tla_direct.eval())\n",
        "\n",
        "\n",
        "tla_indirect = indirect_TLA(\n",
        "    inp_vocab_size=len(input_tokenizer.tokens),\n",
        "    out_vocab_size=len(output_tokenizer.tokens),\n",
        "    inp_len = inp_maxlen + 2,\n",
        "    out_f = out_f,\n",
        "    hidden_out_f=hidden_out_f,\n",
        "    out_len = _ds.dataset.out_maxlen).to(device)\n",
        "print(tla_indirect.eval())\n",
        "\n",
        "tla_combined = combined_TLA(\n",
        "    inp_vocab_size=len(input_tokenizer.tokens),\n",
        "    out_vocab_size=len(output_tokenizer.tokens),\n",
        "    inp_len = inp_maxlen + 2,\n",
        "    out_f = out_f,\n",
        "    hidden_out_f=hidden_out_f,\n",
        "    out_len = _ds.dataset.out_maxlen).to(device)\n",
        "print(tla_combined.eval())\n",
        "\n",
        "tla_seq2seq = Seq2Seq_wAtt(\n",
        "    enc_vocab_size=len(input_tokenizer.tokens),\n",
        "    dec_vocab_size=len(output_tokenizer.tokens),\n",
        "    n_layers=n_layers,\n",
        "    bidirectional=bidirectional,\n",
        "    n_hid=n_hid).to(device)\n",
        "print(tla_seq2seq.eval())\n",
        "\n",
        "tla_seq2seq0 = Seq2Seq_woAtt(\n",
        "    enc_vocab_size=len(input_tokenizer.tokens),\n",
        "    dec_vocab_size=len(output_tokenizer.tokens),\n",
        "    n_layers=n_layers,\n",
        "    bidirectional=bidirectional,\n",
        "    n_hid=n_hid).to(device)\n",
        "print(tla_seq2seq.eval())\n",
        "\n",
        "transformer = Transformer(src_vocab_size=len(input_tokenizer.tokens),\n",
        "                          tgt_vocab_size=len(output_tokenizer.tokens),\n",
        "                          model_dim=n_hid,\n",
        "                          num_heads=4,\n",
        "                          num_layers=n_layers,\n",
        "                          max_seq_length=_ds.dataset.out_maxlen,\n",
        "                          dropout=0.,\n",
        "                          ff_dim=32,\n",
        "                          device=device)\n",
        "transformer.eval()\n",
        "\n",
        "# モデルを tla に代入\n",
        "#tla = tla_vanilla\n",
        "model1 = tla_direct\n",
        "model2 = tla_indirect\n",
        "model3 = tla_combined\n",
        "model4 = tla_seq2seq\n",
        "model5 = tla_seq2seq0\n",
        "model6 = transformer\n",
        "\n",
        "model = model1\n",
        "\n",
        "# N 個のデータを実行してみる\n",
        "N = 2\n",
        "ids = np.random.permutation(_ds.__len__())[:N]  # データをシャフルして先頭の N 項目だけ ids に入れる\n",
        "\n",
        "for idx in ids:\n",
        "    # データセットから返ってくる値は入力信号 inp と教師信号 tch\n",
        "    inp, tch = _ds.__getitem__(idx)\n",
        "    print(f'idx:{idx}:', f'inp:{inp}', f'tch:{tch}')\n",
        "\n",
        "    # 入出力信号はトークン ID 番号であるため人間が読みやすいように変換して表示\n",
        "    print(f'_ds.dataset.ids2inp({inp}):{_ds.dataset.ids2inp(inp)}')\n",
        "    print(f'_ds.dataset.taregt_ids2target({tch}):{_ds.dataset.target_ids2target(tch)}')\n",
        "    inp = pad_sequence(inp.unsqueeze(0), batch_first=True).to(device)\n",
        "    tch = pad_sequence(tch.unsqueeze(0), batch_first=True).to(device)\n",
        "\n",
        "    # モデルにデータを与えて出力を得る\n",
        "    outs = model(inp, tch)\n",
        "\n",
        "    # 結果の表示\n",
        "    print('教師:', _ds.dataset.target_ids2target([idx.cpu().numpy() for idx in tch.squeeze(0)]), end=\": \")\n",
        "    print('教師 ids:', [int(_tch.cpu().numpy()) for _tch in tch.squeeze(0)])\n",
        "    print('出力 ids:', [int(_out.argmax().cpu().numpy()) for _out in outs[0]], end=\"\\n===\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c366fdd-4103-43c1-a80f-b7f5d24abd46",
      "metadata": {
        "id": "7c366fdd-4103-43c1-a80f-b7f5d24abd46"
      },
      "outputs": [],
      "source": [
        "from asa_pytorch_lstm import NaiveLSTM\n",
        "#print(dir(NaiveLSTM))\n",
        "\n",
        "class vanilla_LM(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 inp_vocab_size:int=None, # len(psylex71_ds.input_tokenizer.tokens),\n",
        "                 out_vocab_size:int=None, # len(psylex71_ds.output_tokenizer.tokens),\n",
        "                 n_hid:int=512,\n",
        "                 device:str='cpu'):\n",
        "        super().__init__()\n",
        "        self.inp_vocab_size=inp_vocab_size\n",
        "        self.out_vocab_size=out_vocab_size\n",
        "        self.n_hid=n_hid\n",
        "\n",
        "        self.emb_layer = torch.nn.Linear(in_features=inp_vocab_size, out_features=n_hid).to(device)\n",
        "        self.emb_outf = torch.nn.Tanh()\n",
        "        self.out_outf = torch.nn.Sigmoid()\n",
        "        self.lstm = NaiveLSTM(n_inp=n_hid, n_hid=n_hid).to(device)\n",
        "        self.out_layer = torch.nn.Linear(in_features=n_hid, out_features=out_vocab_size).to(device)\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''互換性のため Y を入力としているが実際には使っていない'''\n",
        "\n",
        "        #X = X.float()               # ワンホットベクトルは整数 int64 なので浮動小数点に変換\n",
        "        # 入力 X はトークン ID リストであるので，ワンホットベクトル化する\n",
        "        X = torch.nn.functional.one_hot(X, num_classes=self.inp_vocab_size)\n",
        "\n",
        "        #print(X)\n",
        "        #sys.exit()\n",
        "        X = X.float()\n",
        "        X = self.emb_layer(X)       # 埋め込み層への信号伝搬\n",
        "        X = self.emb_outf(X)        # 埋め込み層の非線形変換\n",
        "\n",
        "        X, (h, c) = self.lstm(X)\n",
        "\n",
        "        X = self.out_layer(X)       # 出力層への信号伝搬\n",
        "        X = self.out_outf(X)        # 出力層での非線形変換\n",
        "\n",
        "        return X\n",
        "\n",
        "lm = vanilla_LM(inp_vocab_size=len(input_tokenizer.tokens),\n",
        "                out_vocab_size=len(output_tokenizer.tokens),\n",
        "                n_hid=1024,\n",
        "                device=device)\n",
        "lm.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f7279a-6c5f-49e5-8347-9d9d5842bae2",
      "metadata": {
        "id": "d9f7279a-6c5f-49e5-8347-9d9d5842bae2"
      },
      "source": [
        "# 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a2364c7-bd94-4469-8b13-c0990514358b",
      "metadata": {
        "id": "8a2364c7-bd94-4469-8b13-c0990514358b"
      },
      "source": [
        "## 訓練に用いるモデルを再定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b1b618-eebc-4671-8c90-8bb17b0a3297",
      "metadata": {
        "id": "64b1b618-eebc-4671-8c90-8bb17b0a3297"
      },
      "outputs": [],
      "source": [
        "# 以下では 3 つのモデルを定義して比較している\n",
        "_ds = train_ds\n",
        "n_layers=1\n",
        "bidirectional=False\n",
        "n_hid=1024\n",
        "n_hid=2048\n",
        "\n",
        "out_f = None\n",
        "# hidden_out_f = 'sigmoid'\n",
        "# out_f = None\n",
        "#out_f = 'tanh'\n",
        "hidden_out_f = 'ReLU'\n",
        "\n",
        "tla_combined = combined_TLA(\n",
        "    inp_vocab_size=len(input_tokenizer.tokens),\n",
        "    out_vocab_size=len(output_tokenizer.tokens),\n",
        "    inp_len=2+2,\n",
        "    out_len=_ds.dataset.out_maxlen,\n",
        "    out_f=out_f,\n",
        "    hidden_out_f=hidden_out_f,\n",
        "    device=device)\n",
        "#print(tla_combined.eval())\n",
        "\n",
        "tla_direct = direct_TLA(\n",
        "    inp_vocab_size=len(input_tokenizer.tokens),\n",
        "    out_vocab_size=len(output_tokenizer.tokens),\n",
        "    inp_len=inp_maxlen+2,\n",
        "    out_len=_ds.dataset.out_maxlen,\n",
        "    out_f=out_f,\n",
        "    device=device)\n",
        "#print(tla_direct.eval())\n",
        "\n",
        "tla_indirect = indirect_TLA(\n",
        "    inp_vocab_size=len(input_tokenizer.tokens),\n",
        "    out_vocab_size=len(output_tokenizer.tokens),\n",
        "    inp_len=inp_maxlen+2,\n",
        "    out_len=_ds.dataset.out_maxlen,\n",
        "    out_f=out_f,\n",
        "    hidden_out_f=hidden_out_f,\n",
        "    # out_f=None,\n",
        "    # hidden_out_f=None,\n",
        "    device=device)\n",
        "#print(tla_indirect.eval())\n",
        "\n",
        "enc_dec = Seq2Seq_wAtt(\n",
        "    enc_vocab_size=len(input_tokenizer.tokens),\n",
        "    dec_vocab_size=len(output_tokenizer.tokens),\n",
        "    n_layers=n_layers,\n",
        "    bidirectional=bidirectional,\n",
        "    n_hid=n_hid).to(device)\n",
        "#print(seq2seq.eval())\n",
        "\n",
        "enc_dec0 = Seq2Seq_woAtt(\n",
        "    enc_vocab_size=len(input_tokenizer.tokens),\n",
        "    dec_vocab_size=len(output_tokenizer.tokens),\n",
        "    n_layers=n_layers,\n",
        "    bidirectional=bidirectional,\n",
        "    n_hid=n_hid).to(device)\n",
        "#print(seq2seq0.eval())\n",
        "\n",
        "transformer = Transformer(\n",
        "    src_vocab_size=len(gakushu_tokenizer.tokens),\n",
        "    tgt_vocab_size=len(mora_tokenizer.tokens),\n",
        "    model_dim=n_hid,\n",
        "    num_heads=4,\n",
        "    num_layers=n_layers,\n",
        "    max_seq_length=_ds.dataset.out_maxlen,\n",
        "    dropout=0.,\n",
        "    ff_dim=32,\n",
        "    device=device)\n",
        "#print(transformer.eval())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94805ae2-e439-4103-bafe-9cc3128a1173",
      "metadata": {
        "id": "94805ae2-e439-4103-bafe-9cc3128a1173"
      },
      "source": [
        "## 実際の訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e1a56e4-0773-49b7-af64-4b552e1340da",
      "metadata": {
        "id": "4e1a56e4-0773-49b7-af64-4b552e1340da"
      },
      "outputs": [],
      "source": [
        "#_ds = psylex71_dss[2]\n",
        "_ds = train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "038c8397-3cba-4ed6-bee4-d309838288cc",
      "metadata": {
        "id": "038c8397-3cba-4ed6-bee4-d309838288cc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# out_f = None\n",
        "model1 = indirect_TLA(\n",
        "    inp_vocab_size=len(input_tokenizer.tokens),\n",
        "    out_vocab_size=len(output_tokenizer.tokens),\n",
        "    inp_len = inp_maxlen + 2,\n",
        "    out_f = out_f,\n",
        "    hidden_out_f=None,\n",
        "    out_len = _ds.dataset.out_maxlen).to(device)\n",
        "\n",
        "model2 = indirect_TLA(\n",
        "    inp_vocab_size=len(input_tokenizer.tokens),\n",
        "    out_vocab_size=len(output_tokenizer.tokens),\n",
        "    inp_len = inp_maxlen + 2,\n",
        "    out_f = out_f,\n",
        "    hidden_out_f='sigmoid',\n",
        "    out_len = _ds.dataset.out_maxlen).to(device)\n",
        "\n",
        "model3 = indirect_TLA(\n",
        "    inp_vocab_size=len(input_tokenizer.tokens),\n",
        "    out_vocab_size=len(output_tokenizer.tokens),\n",
        "    inp_len = inp_maxlen + 2,\n",
        "    out_f = out_f,\n",
        "    hidden_out_f='tanh',\n",
        "    out_len = _ds.dataset.out_maxlen).to(device)\n",
        "\n",
        "model4 = indirect_TLA(\n",
        "    inp_vocab_size=len(input_tokenizer.tokens),\n",
        "    out_vocab_size=len(output_tokenizer.tokens),\n",
        "    inp_len = inp_maxlen + 2,\n",
        "    out_f = out_f,\n",
        "    hidden_out_f='ReLU',\n",
        "    out_len = _ds.dataset.out_maxlen).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af6ea22-3044-417a-a7f2-125b6ce7c24d",
      "metadata": {
        "id": "2af6ea22-3044-417a-a7f2-125b6ce7c24d"
      },
      "outputs": [],
      "source": [
        "model1 = tla_direct\n",
        "model2 = tla_indirect\n",
        "model3 = tla_combined\n",
        "model4 = tla_seq2seq\n",
        "model5 = tla_seq2seq0\n",
        "model6 = transformer\n",
        "\n",
        "#str(models[0]).split(\",\")[0]\n",
        "#str(tla_indirect).split(\"\\n\")[0]\n",
        "for _model in [model1, model2, model3, model4, model5, model6]:\n",
        "    print(str(_model).split('\\n')[0].replace('(','')) #.split('.')[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2de37415-5eb3-4c59-a2c0-2658ad718a8e",
      "metadata": {
        "id": "2de37415-5eb3-4c59-a2c0-2658ad718a8e"
      },
      "outputs": [],
      "source": [
        "model_list = []\n",
        "#out_fs = [None, 'sigmoid', 'tanh', 'ReLU']\n",
        "out_fs = [None]\n",
        "\n",
        "#hidden_out_fs = [None, 'sigmoid', 'tanh', 'ReLU']\n",
        "hidden_out_fs = [None, 'tanh', 'ReLU']\n",
        "\n",
        "#tla_models = [indirect_TLA, combined_TLA, direct_TLA]\n",
        "#models = [model1, model2, model3, model4, model5, model6]\n",
        "models = [enc_dec, enc_dec0, transformer]\n",
        "\n",
        "max_modelname = 0\n",
        "for _model in models:\n",
        "    for hidden_out_f in hidden_out_fs:\n",
        "        for out_f in out_fs:\n",
        "            #model_name = str(_model).split(\"'\")[1].split('.')[-1]\n",
        "            model_name = str(_model).split('\\n')[0].replace('(','')\n",
        "            if 'direct_TLA' == model_name:\n",
        "                model = _model(\n",
        "                    inp_vocab_size=len(input_tokenizer.tokens),\n",
        "                    out_vocab_size=len(output_tokenizer.tokens),\n",
        "                    inp_len= inp_maxlen + 2,\n",
        "                    out_f = out_f,\n",
        "                    out_len = _ds.dataset.out_maxlen).to(device)\n",
        "            else:\n",
        "                model = _model(\n",
        "                    inp_vocab_size=len(input_tokenizer.tokens),\n",
        "                    out_vocab_size=len(output_tokenizer.tokens),\n",
        "                    inp_len= inp_maxlen + 2,\n",
        "                    out_f = out_f,\n",
        "                    hidden_out_f=hidden_out_f,\n",
        "                    out_len = _ds.dataset.out_maxlen).to(device)\n",
        "            model_name += '_OUT' + str(out_f) + '_HID' + str(hidden_out_f)\n",
        "\n",
        "            max_modelname = len(model_name) if len(model_name) > max_modelname else max_modelname\n",
        "            model_list.append((model_name, model))\n",
        "\n",
        "\n",
        "#loss_f = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "loss_f = torch.nn.CrossEntropyLoss()\n",
        "print(len(model_list))\n",
        "print(f'max_modelname:{max_modelname}')\n",
        "for i, _model in enumerate(model_list):\n",
        "    print(i, _model[0], end=\"\\t\\t\")\n",
        "    if ((i+1) % 3) == 0:\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d2fefc4-ced8-4507-984c-d9cc9df91d21",
      "metadata": {
        "id": "5d2fefc4-ced8-4507-984c-d9cc9df91d21"
      },
      "outputs": [],
      "source": [
        "# 学習率リストとエポック数の定義\n",
        "iter_params = [(1e-3, 2), (1e-4, 2), (1e-5, 2)]\n",
        "iter_params = [(1e-3, 20)]\n",
        "iter_params = [(1e-1, 50), (1e-2,20), (1e-3,20), (1e-4,10)]\n",
        "iter_params = [(1e-3, 30)]\n",
        "iter_params = [[1e-3, 10],(1e-4,10)]\n",
        "iter_params = [(1e-2, 10)]\n",
        "# iter_params = [(1e-2, 50), (1e-3, 50), (1e-4, 10)]\n",
        "\n",
        "models = model_list\n",
        "try:\n",
        "    isinstance(results, list)\n",
        "except:\n",
        "    results = [{} for _ in models]\n",
        "\n",
        "# 途中結果を印字するタイミング\n",
        "interval = 1\n",
        "#interval = 3\n",
        "#interval = 10\n",
        "\n",
        "for (lr, epochs) in iter_params: # 学習率とエポック数を定義済のリストに従って変化させる\n",
        "\n",
        "    # 最適化関数の学習率を設定\n",
        "    optimizers = []\n",
        "    for model in models:\n",
        "        optimizers.append(torch.optim.Adam(model[1].parameters(), lr=lr))\n",
        "\n",
        "    print(f'lr:{lr}, epochs:{epochs}')\n",
        "    # エポック数だけ学習を行う\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"エポック:{epoch+1:3d}\")\n",
        "\n",
        "        for N, (_model, optimizer) in enumerate(zip(models, optimizers)):\n",
        "            #model_name = str(type(model)).split(\"'\")[1].split('.')[-1]\n",
        "            model_name = _model[0]\n",
        "            model = _model[1]\n",
        "\n",
        "            # 1 エポックの検証を行う\n",
        "            is_seq2seq = 'Seq2Seq' in str(type(model))\n",
        "            out, model = eval_an_epoch(\n",
        "                model=model,\n",
        "                _dl=valid_dl,\n",
        "                loss_f=loss_f,\n",
        "                is_seq2seq=is_seq2seq,\n",
        "                device=device)\n",
        "            if (epoch % interval) == 0:\n",
        "                print(f\"{model_name:35s} \",\n",
        "                      f\"検証損失値={out['sum_loss']:10.3f}\",\n",
        "                      f\"正解率={out['P']:5.3f}\",\n",
        "                      f\"({out['count']:5d}/{out['N']:5d})\",\n",
        "                      end=\"\\t\")\n",
        "\n",
        "            if not 'valid_loss' in results[N]:\n",
        "                results[N]['valid_loss'] = [out['sum_loss']]\n",
        "            else:\n",
        "                results[N]['valid_loss'].append(out['sum_loss'])\n",
        "            if not 'valid_P' in results[N]:\n",
        "                results[N]['valid_P'] = [out['P']]\n",
        "            else:\n",
        "                results[N]['valid_P'].append(out['P'])\n",
        "\n",
        "\n",
        "            # 1 エポックの訓練を行う\n",
        "            out, model, optimizer = fit_an_epoch(\n",
        "                model=model,\n",
        "                _dl=train_dl,\n",
        "                loss_f=loss_f,\n",
        "                optimizer=optimizer,\n",
        "                is_seq2seq=is_seq2seq,\n",
        "                device=device)\n",
        "            if (epoch % interval) == 0:\n",
        "                print(f\"学習損失値={out['sum_loss']:10.3f}\",\n",
        "                      f\"正解率={out['P']:5.3f}\",\n",
        "                      f\"({out['count']:5d}/{out['N']:5d})\",\n",
        "                      end=\"\\t\")\n",
        "\n",
        "            if not 'train_loss' in results[N]:\n",
        "                results[N]['train_loss'] = [out['sum_loss']]\n",
        "            else:\n",
        "                results[N]['train_loss'].append(out['sum_loss'])\n",
        "            if not 'train_P' in results[N]:\n",
        "                results[N]['train_P'] = [out['P']]\n",
        "            else:\n",
        "                results[N]['train_P'].append(out['P'])\n",
        "\n",
        "            if (epoch % interval) == 0:\n",
        "                print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "878881f9-a74b-4d26-9749-7916be352d47",
      "metadata": {
        "id": "878881f9-a74b-4d26-9749-7916be352d47"
      },
      "source": [
        "## 学習曲線の描画"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee199c61-1db7-4bbe-986f-2d9ce2f8c4f2",
      "metadata": {
        "id": "ee199c61-1db7-4bbe-986f-2d9ce2f8c4f2"
      },
      "outputs": [],
      "source": [
        "for i in range(6):\n",
        "    #plt.plot(results[i]['train_loss'], 'x-', label=f'{i}:訓練データ')\n",
        "    #plt.plot(results[i]['valid_loss'], 'o-', label=f'{i}:検証データ')\n",
        "    plt.plot(results[i]['train_P'], 'x-', label=f'モデル{i}:訓練データ')\n",
        "    #plt.plot(results[i]['valid_P'], 'o-', label=f'モデル{i}:検証データ')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('エポック数')\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n",
        "\n",
        "#print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bb1c490-a3a9-4da9-bc8b-0ce173358c6c",
      "metadata": {
        "id": "8bb1c490-a3a9-4da9-bc8b-0ce173358c6c"
      },
      "source": [
        "# 読めなかった単語を調べてみる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fce753f-38e5-4ac7-9c9b-9908a3b46ac8",
      "metadata": {
        "id": "0fce753f-38e5-4ac7-9c9b-9908a3b46ac8"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8587d4fb-e917-43e3-8901-49428ff6bf75",
      "metadata": {
        "id": "8587d4fb-e917-43e3-8901-49428ff6bf75"
      },
      "outputs": [],
      "source": [
        "_ds = psylex71_ds_mora\n",
        "\n",
        "_errors = {}\n",
        "for N, model in enumerate(models[2:]):\n",
        "    model = model.eval()\n",
        "    model_name = str(type(model)).split('\\'')[1].split('.')[-1]\n",
        "\n",
        "    if not model_name in _errors:\n",
        "        _errors[model_name] = []\n",
        "\n",
        "    is_seq2seq = 'seq2seq' in str(type(model)).lower()\n",
        "    verbose = False\n",
        "    errors = []\n",
        "    for idx in tqdm(range(_ds.__len__() >> 3)):\n",
        "        #_ds.getitem(idx), _ds.__getitem__(idx)\n",
        "        inp, tch = _ds.__getitem__(idx)\n",
        "        inp = pad_sequence(inp.unsqueeze(0), batch_first=True).to(device)\n",
        "        tch = pad_sequence(tch.unsqueeze(0), batch_first=True).to(device)\n",
        "        if is_seq2seq:\n",
        "            out, state = model(inp,tch)\n",
        "        else:\n",
        "            out = model(inp,tch)\n",
        "        out = out.squeeze(0).argmax(dim=1)\n",
        "        tch = tch.squeeze(0)\n",
        "        yesno = (((out == tch) * 1).sum() == len(tch)).detach().cpu().numpy()\n",
        "\n",
        "        if yesno != True:\n",
        "            if verbose:\n",
        "                print(f'{idx:07d}',\n",
        "                      f'{yesno}',\n",
        "                      f'出力:{\"\".join(ch for ch in _ds.output_tokenizer.decode(out)).replace(\"<PAD>\",\"\")}',\n",
        "                      f'{_ds.getitem(N)}')\n",
        "            else:\n",
        "                _errors[model_name].append((f'{idx:07d}',\n",
        "                                            f'{yesno}',\n",
        "                                            f'出力:{\"\".join(ch for ch in _ds.output_tokenizer.decode(out)).replace(\"<PAD>\",\"\")}',\n",
        "                                            f'{_ds.getitem(idx)}'))\n",
        "\n",
        "    print(f'{model_name}',\n",
        "      f'total num. erros:{len(_errors[model_name])}', '/', f'{_ds.__len__()}',\n",
        "      f'P(Correct)={((_ds.__len__() - len(_errors[model_name])) / _ds.__len__())*100:.3f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b900e341-5a7d-494f-880f-0058a2369683",
      "metadata": {
        "id": "b900e341-5a7d-494f-880f-0058a2369683"
      },
      "outputs": [],
      "source": [
        "for model_name in list(_errors.keys()):\n",
        "    for err in _errors[model_name][-5:]:\n",
        "        print(model_name, err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "976d3881-ee4a-42ed-91a6-7157ef65ebb6",
      "metadata": {
        "id": "976d3881-ee4a-42ed-91a6-7157ef65ebb6"
      },
      "outputs": [],
      "source": [
        "# 読めなかった単語を調べてみる\n",
        "\n",
        "loss_f = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "cr = 0\n",
        "model = tla_seq2seq0.eval()\n",
        "for i, (inp, tch) in enumerate(valid_ds):\n",
        "    out = model(inp,tch)\n",
        "    out_ids = out.argmax(dim=1)\n",
        "    yesno = (((tch==out_ids) * 1).sum()  == len(tch)).detach().cpu().numpy()\n",
        "    #if yesno == True:\n",
        "    #    cr += 1\n",
        "    if yesno == False:\n",
        "        print(f'入力:{\"\".join(c for c in gakushu_tokenizer.decode(inp)).replace('<SOW>','').replace('<EOW>','')}',\n",
        "              f'出力:{\"\".join(c for c in mora_tokenizer.decode(out_ids.detach().cpu().numpy())).replace('<PAD>','').replace('<SOW>','').replace('<EOW>','')}',\n",
        "              f'正解:{\"\".join(c for c in mora_tokenizer.decode(tch)).replace('<PAD>','').replace('<SOW>','').replace('<EOW>','')}',\n",
        "              end=\" \"\n",
        "             )\n",
        "        loss = loss_f(out,tch)\n",
        "        loss.backward()\n",
        "        print(f'損失値:{loss.item():.3f}') # , type(loss.item()), loss)\n",
        "    #if cr >= 30:\n",
        "    #    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416b0315-10be-4d78-800e-744de1b17f5f",
      "metadata": {
        "id": "416b0315-10be-4d78-800e-744de1b17f5f"
      },
      "outputs": [],
      "source": [
        "def eval_a_word(model:torch.nn.Module=tla_seq2seq0,\n",
        "                wrd:str=\"\",\n",
        "                input_tokenizer=gakushu_tokenizer,\n",
        "                output_tokenizer=mora_tokenizer):\n",
        "    inps = torch.LongTensor(input_tokenizer(wrd))\n",
        "    inps = torch.LongTensor([input_tokenizer.tokens.index('<SOW>')]+input_tokenizer(wrd)+[input_tokenizer.tokens.index('<EOW>')])\n",
        "\n",
        "    #print(wrd, inps)\n",
        "\n",
        "    inps = pad_sequence(inps.unsqueeze(0), batch_first=True).to(device)\n",
        "    #out0 = pad_sequence(torch.LongTensor([output_tokenizer.tokens.index('<SOW>')]).unsqueeze(0)).to(device)\n",
        "    #outs = model(inps,out0)\n",
        "    #out_ch = output_tokenizer.decode(outs.squeeze(0).argmax(dim=1))\n",
        "    #print(out_ch, outs.argmax(dim=1))\n",
        "    #sys.exit()\n",
        "\n",
        "    tchs = pad_sequence(torch.LongTensor([[  2,  54, 148,  56,  10,   1,   0]]),batch_first=True).to(device)\n",
        "    #tchs = pad_sequence(torch.LongTensor([[  2,  0]]),batch_first=True).to(device)\n",
        "    outs = model(inps,tchs)\n",
        "    print(f'inps:{inps}')\n",
        "    print(f'tchs:{tchs}')\n",
        "    print(f'outs:{outs.squeeze(0).argmax(dim=1)}')\n",
        "    sys.exit()\n",
        "    return inp, wrd, out0, outs\n",
        "\n",
        "model = tla_seq2seq0.eval()\n",
        "inp, wrd, out0, outs = eval_a_word(model=model, wrd='戦争')\n",
        "# print(inp,wrd, out0, outs.squeeze(0).argmax(dim=1))\n",
        "# sys.exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f865f2d9-3ea3-40e7-bfca-7d72b725ac97",
      "metadata": {
        "id": "f865f2d9-3ea3-40e7-bfca-7d72b725ac97"
      },
      "outputs": [],
      "source": [
        "wrd='戦争'\n",
        "\n",
        "if wrd in psylex71_ds_mora.inputs:\n",
        "    idx = psylex71_ds_mora.inputs.index(wrd)\n",
        "    print(idx, wrd, psylex71_ds_mora.getitem(idx), psylex71_ds_mora.__getitem__(idx))\n",
        "\n",
        "output_tokenizer.tokens.index('')\n",
        "\n",
        "#sys.exit()\n",
        "def eval_a_word(model:torch.nn.Module=tla_seq2seq0,\n",
        "                wrd:str=\"\",\n",
        "                input_tokenizer=gakushu_tokenizer,\n",
        "                output_tokenizer=mora_tokenizer):\n",
        "    inps = torch.LongTensor(input_tokenizer(wrd))\n",
        "    print(wrd, inps)\n",
        "    inps = pad_sequence(inps.unsqueeze(0), batch_first=True).to(device)\n",
        "    print(wrd, inps)\n",
        "    return inp, wrd\n",
        "\n",
        "eval_a_word(model=tla_seq2seq0, wrd=wrd)\n",
        "sys.exit()\n",
        "\n",
        "inps,tchs = valid_ds.__getitem__(8)\n",
        "inps = pad_sequence(inps.unsqueeze(0), batch_first=True).to(device)\n",
        "tchs = pad_sequence(tchs.unsqueeze(0), batch_first=True).to(device)\n",
        "\n",
        "# inps, tchs = next(iter(valid_dl))\n",
        "# inps = pad_sequence(inps, batch_first=True).to(device)\n",
        "# tchs = pad_sequence(tchs, batch_first=True).to(device)\n",
        "model.eval()\n",
        "outs = model(inps,tchs)\n",
        "print(inps.size(), tchs.size(), outs.size()) # torch.Size([1024, 2]) torch.Size([1024, 9]) torch.Size([1024, 9, 155])\n",
        "\n",
        "\n",
        "for inp, tch, out in zip(inps, tchs, outs):\n",
        "    print(inp.size(), tch.size(), out.size())\n",
        "    print(gakushu_tokenizer.decode(inp), mora_tokenizer.decode(tch), out.argmax(dim=1))\n",
        "    print(mora_tokenizer.decode(out.argmax(dim=1).detach().cpu().numpy()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a98cf88b-ef18-429f-afba-a364cff708c3",
      "metadata": {
        "id": "a98cf88b-ef18-429f-afba-a364cff708c3"
      },
      "outputs": [],
      "source": [
        "inp = '戦争'\n",
        "tch = 'センソウ'\n",
        "inp_ids = torch.LongTensor(gakushu_tokenizer(inp)).unsqueeze(0)\n",
        "tch_ids = torch.LongTensor([mora_tokenizer.tokens.index('<SOW>')]).unsqueeze(0)\n",
        "model = tla_seq2seq0\n",
        "inps = pad_sequence(inp_ids, batch_first=True).to(device)\n",
        "tchs = pad_sequence(tch_ids, batch_first=True).to(device)\n",
        "outs = model(inps, tchs)\n",
        "print(outs.size(), outs.squeeze(0).argmax(), mora_tokenizer(tch))\n",
        "print(mora_tokenizer.decode([outs.squeeze(0).argmax()]))\n",
        "\n",
        "tch_ids = torch.LongTensor(mora_tokenizer('センソウ')).unsqueeze(0)\n",
        "tch_ids = mora_tokenizer('センソウ')\n",
        "tch_ids = [mora_tokenizer.tokens.index('<SOW>')]+tch_ids+[mora_tokenizer.tokens.index('<EOW>')]\n",
        "tch_ids = torch.LongTensor(tch_ids).unsqueeze(0)\n",
        "outs = model(inps, tchs)\n",
        "print(tch_ids, outs.squeeze(0).argmax())\n",
        "\n",
        "# # 正解のカウント\n",
        "# out_ids = [out.argmax(dim=1) for out in outs]\n",
        "# for tch, out in zip(tchs[:], out_ids[:]):\n",
        "# yesno = ((tch==out) * 1).sum().cpu().numpy() == len(tch)\n",
        "# count += 1 if yesno else 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd787a25-1877-4960-89f0-c27b6471717b",
      "metadata": {
        "id": "cd787a25-1877-4960-89f0-c27b6471717b"
      },
      "outputs": [],
      "source": [
        "#train_ds.dataset.output_tokenizer\n",
        "results[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a15d892a-9462-423c-b05d-7f9697726027",
      "metadata": {
        "id": "a15d892a-9462-423c-b05d-7f9697726027"
      },
      "outputs": [],
      "source": [
        "model = tla_seq2seq0\n",
        "model = model2\n",
        "_ds = valid_ds\n",
        "with torch.no_grad():\n",
        "    for i in range(3):\n",
        "    #for i in range(_ds.__len__()):\n",
        "\n",
        "        idx = _ds.indices[i]\n",
        "        inp, tgt = _ds.dataset.__getitem__(idx)\n",
        "        inp_ids = pad_sequence(inp.unsqueeze(0), batch_first=True).to(device)\n",
        "        tgt_ids = pad_sequence(tgt.unsqueeze(0), batch_first=True).to(device)\n",
        "\n",
        "        enc_emb = model.encoder_emb(inp_ids)\n",
        "        enc_out, (hnx, cnx) = model.encoder(enc_emb)\n",
        "\n",
        "        dec_inp = torch.tensor([_ds.dataset.output_tokenizer.tokens.index('<SOW>')], device=device)\n",
        "        dec_inp = model.decoder_emb(dec_inp).unsqueeze(0)\n",
        "        dec_state = (hnx, cnx)\n",
        "\n",
        "        print(f'inp:{inp.cpu().numpy()}',\n",
        "              f'{\"\".join(c for c in _ds.dataset.ids2inp(inp.cpu().numpy()))}',\n",
        "              f'{tgt.cpu().numpy()}',\n",
        "              f'{\"\".join(c for c in _ds.dataset.target_ids2target(tgt.cpu().numpy()))}'\n",
        "             )\n",
        "\n",
        "        for _i in range(len(tgt_ids[0][1:])):\n",
        "        #for i in range(len(tgt_ids[0])):\n",
        "\n",
        "            dec_out, dec_state = model.decoder(dec_inp, dec_state)\n",
        "            dec_out = dec_out.argmax().unsqueeze(0)\n",
        "            dec_inp = model.decoder_emb(dec_out).unsqueeze(0)\n",
        "            # print(f'_i:{_i}',\n",
        "            #        f'dec_out:{dec_out.detach().cpu().numpy()}',\n",
        "            #        f'tgt_ids[0][{_i+1}]:{tgt_ids[0][_i+1]}')\n",
        "\n",
        "        out_ = model(inp_ids,tgt_ids)\n",
        "        out_ids =  out_.squeeze(0).argmax(dim=1).detach().cpu().numpy()\n",
        "        out_tokens = \"\".join(c for c in psylex71_ds.target_ids2target(out_ids))\n",
        "        print(out_tokens)\n",
        "\n",
        "        #inp, tgt = _ds.__getitem__(i)\n",
        "        #inp_ids = inp\n",
        "\n",
        "        #print(f'out_.squeeze(0).argmax(dim=1){out_.squeeze(0).argmax(dim=1).detach().cpu().numpy()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fca62bb-d2b2-4dda-add3-aab88a656970",
      "metadata": {
        "id": "6fca62bb-d2b2-4dda-add3-aab88a656970"
      },
      "outputs": [],
      "source": [
        "out_ids = out_.squeeze(0).argmax(dim=1).detach().cpu().numpy()\n",
        "print(_ds.dataset.target_ids2target(out_ids), out_ids)\n",
        "#print(psylex71_ds.target_ids2target(out_ids), out_ids)\n",
        "\n",
        "out_tokens = \"\".join(c for c in _ds.dataset.target_ids2target(out_ids))\n",
        "out_tokens\n",
        "inp, tgt = _ds.__getitem__(2)\n",
        "inp_ids = _ds.dataset.ids2inp(inp)\n",
        "inp_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b5d00f-70c6-4321-906b-ceede7b48e80",
      "metadata": {
        "id": "04b5d00f-70c6-4321-906b-ceede7b48e80"
      },
      "outputs": [],
      "source": [
        "model = tla_seq2seq\n",
        "with torch.no_grad():\n",
        "    for i in range(_ds.__len__()):\n",
        "        inp, tgt = chihaya_ds.__getitem__(i)\n",
        "        # print(f'インプット:{\"\".join(c for c in chihaya_ds.ids2tkn(inp))}') #i].cpu().numpy()))}')\n",
        "        # print(f'ターゲット:{\"\".join(c for c in chihaya_ds.ids2tkn(tgt))}') #n(c for c in chihaya_ds.ids2tkn(tgt_ids[i].cpu().numpy()))}')\n",
        "\n",
        "        inp_ids = pad_sequence(inp.unsqueeze(0), batch_first=True).to(device)\n",
        "        tgt_ids = pad_sequence(tgt.unsqueeze(0), batch_first=True).to(device)\n",
        "        #inp_ids = torch.as_tensor(inp, device=device)\n",
        "        #tgt_ids = torch.as_tensor(tgt, device=device)\n",
        "        enc_out, enc_state = model.encoder(inp_ids)\n",
        "        #dec_out, dec_state = decoder(tgt_ids, enc_state)\n",
        "\n",
        "        dec_ids = dec_out.argmax(dim=1).detach().cpu().numpy()\n",
        "        print(f'len(dec_ids):{len(dec_ids)}')\n",
        "        print(\"\".join(c for c in chihaya_ds.ids2tkn(dec_ids)))\n",
        "\n",
        "        dec_inp = torch.tensor([chihaya_ds.chihaya_tokens.index('<SOS>')], device=device)\n",
        "        dec_inp = tgt_ids[0].unsqueeze(0)\n",
        "        for i in range(len(dec_ids)):\n",
        "            dec_out, dec_state = decoder(dec_inp, dec_state)\n",
        "            #print(dec_out.size(), dec_out.argmax().cpu().numpy(), type(dec_out.argmax())) #, dec_out)\n",
        "\n",
        "            if teacher_forcing:\n",
        "                dec_inp = tgt_ids[i].unsqueeze(0)\n",
        "            else:\n",
        "                dec_inp = dec_out.argmax().unsqueeze(0) # .clone().detach()\n",
        "\n",
        "            #dec_inp = dec_out.argmax().unsqueeze(0).clone().detach()\n",
        "            print(f'({dec_inp.cpu().numpy()}',\n",
        "                  f'{chihaya_ds.chihaya_tokens[dec_inp.cpu().numpy()[0]]})', end=\" \") # , type(dec_inp)) # , dec_inp)\n",
        "            #print(dec_inp.size(), dec_inp.argmax().cpu().numpy(), type(dec_inp.argmax())) # , dec_inp)\n",
        "            #sys.exit()\n",
        "        sys.exit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c4a26b2-2624-4508-880c-543ebf7770f7",
      "metadata": {
        "id": "8c4a26b2-2624-4508-880c-543ebf7770f7"
      },
      "source": [
        "# vect2seq model の定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0656623f-d19b-405a-a4f6-066d8eb48024",
      "metadata": {
        "id": "0656623f-d19b-405a-a4f6-066d8eb48024"
      },
      "outputs": [],
      "source": [
        "class Vec2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 sem_dim:int,\n",
        "                 dec_vocab_size:int,\n",
        "                 n_hid:int,\n",
        "                 n_layers:int=2,\n",
        "                 bidirectional:bool=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # 単語の意味ベクトル a.k.a 埋め込み表現 を decoder の中間層に接続するための変換層\n",
        "        # 別解としては，入力層に接続する方法があるが，それはまた別実装にする\n",
        "        self.enc_transform_layer = nn.Linear(\n",
        "            in_features=sem_dim,\n",
        "            out_features=n_hid)\n",
        "        self.decoder_emb = nn.Embedding(\n",
        "            num_embeddings=dec_vocab_size,\n",
        "            embedding_dim=n_hid,\n",
        "            padding_idx=0)\n",
        "\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=n_hid,\n",
        "            hidden_size=n_hid,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional)\n",
        "\n",
        "        # 最終出力層\n",
        "        self.bi_fact = 2 if bidirectional else 1\n",
        "        self.out_layer = nn.Linear(self.bi_fact * n_hid, dec_vocab_size)\n",
        "\n",
        "    def forward(self, enc_inp, dec_inp):\n",
        "        enc_emb = self.enc_transform_layer(enc_inp)\n",
        "        hnx, cnx = enc_emb.clone(), enc_emb.clone()\n",
        "        hnx = hnx.unsqueeze(0)\n",
        "        cnx = cnx.unsqueeze(0)\n",
        "\n",
        "        if self.bi_fact == 2:\n",
        "            hnx = hnx.repeat(2)\n",
        "            cnx = cnx.repeat(2)\n",
        "\n",
        "        dec_emb = self.decoder_emb(dec_inp)\n",
        "\n",
        "        batch_size = enc_inp.size(0)\n",
        "        exp_hid_size = self.decoder.get_expected_hidden_size(enc_inp, batch_sizes=[batch_size])\n",
        "        dec_out, (hny, cny) = self.decoder(dec_emb,(hnx, cnx))\n",
        "\n",
        "        return self.out_layer(dec_out)\n",
        "\n",
        "# 以下確認作業\n",
        "# ds = train_ds\n",
        "# tla_vec2seq = Vec2Seq(\n",
        "#     sem_dim=n_layers,\n",
        "#     dec_vocab_size=len(mora_tokenizer.tokens),\n",
        "#     n_hid=n_hid,\n",
        "#     n_layers=n_layers,\n",
        "#     bidirectional=bidirectional).to(device)\n",
        "# print(tla_vec2seq.eval())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6940bbb6-ffe4-4b94-8232-2e26f9507c69",
      "metadata": {
        "id": "6940bbb6-ffe4-4b94-8232-2e26f9507c69"
      },
      "source": [
        "# Jalex の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "103b18c8-8ed2-45ca-bf30-89dc45c5fe05",
      "metadata": {
        "id": "103b18c8-8ed2-45ca-bf30-89dc45c5fe05"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "try:\n",
        "    import jaconv\n",
        "except:\n",
        "    !pip install jaconv --upgrade\n",
        "    import jaconv\n",
        "# Mecab を使ってヨミを得るために MeCab を import する\n",
        "from ccap.mecab_settings import wakati, yomi #, parser\n",
        "\n",
        "jalex_base = os.path.join(HOME, 'study/2025_2014jalex')\n",
        "jalex_xls_fname = 'JALEX.xlsx'\n",
        "jalex_fname = os.path.join(jalex_base, jalex_xls_fname)\n",
        "jalex_DF = pd.read_excel(jalex_fname)\n",
        "jalex_DF\n",
        "jalex_words = jalex_DF['目標語']\n",
        "print(len(jalex_words))\n",
        "\n",
        "jalex_dic = OrderedDict()\n",
        "for wrd in jalex_words:\n",
        "    if not wrd in jalex_dic:\n",
        "        _yomi = yomi(wrd).strip()\n",
        "        _wakati = wakati(wrd).strip()\n",
        "        _mora = mora_tokenizer.wakachi(_yomi)\n",
        "        _kunrei = kunrei_tokenizer.wakachi(_yomi)\n",
        "        _hira_yomi = jaconv.kata2hira(_yomi)\n",
        "        _julius = jaconv.hiragana2julius(_hira_yomi).split(' ')\n",
        "\n",
        "        jalex_dic[wrd] = {'ヨミ':_yomi, 'モーラ':_mora, '訓令':_kunrei, 'ユリウス':_julius} # , 'Jalex':jalex_DF['wrd']}\n",
        "        print(jalex_dic[wrd])\n",
        "        sys.exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f36a4767-638e-4216-b83d-f4ddf56d2d17",
      "metadata": {
        "id": "f36a4767-638e-4216-b83d-f4ddf56d2d17"
      },
      "outputs": [],
      "source": [
        "#jalex_DF[jalex_DF.iloc('目標語'=='あさって')]\n",
        "jalex_DF['目標語'].str.contains('あさって')\n",
        "#print(dir(jaconv))\n",
        "#help(jaconv.normalize)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f984b1fd-1f36-457f-8d83-cbd1d0c96a33",
      "metadata": {
        "id": "f984b1fd-1f36-457f-8d83-cbd1d0c96a33"
      },
      "source": [
        "## 0.2 NTT 日本語語彙特性 単語頻度データ psylex71.txt のダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e25870-b6d1-4c09-9b36-b903b8335fa5",
      "metadata": {
        "id": "e6e25870-b6d1-4c09-9b36-b903b8335fa5"
      },
      "outputs": [],
      "source": [
        "# if isColab:\n",
        "#     !pip install googledrivedownloader==0.4\n",
        "#     from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "#     import os\n",
        "\n",
        "#     # 共有ファイルのIDを指定\n",
        "#     file_id = '1eBJDN392BsUckg5LBFbbw5KT9PCsmnxI' # 'psylex71utf8_.txt\n",
        "#     # https://drive.google.com/file/d/1eBJDN392BsUckg5LBFbbw5KT9PCsmnxI/view?usp=drive_link\n",
        "\n",
        "#     # 保存したい場所とファイル名を指定\\n\",\n",
        "#     # 例: /content/ ディレクトリに original_file_name.拡張子 という名前で保存\\n\",\n",
        "#     destination_path = '/content/psylex71utf8_.txt' # ファイルの拡張子を適切に設定してください\\n\",\n",
        "#     try:\n",
        "#         print(f\"ファイルのダウンロードを開始します (ファイルID: {file_id})...\")\n",
        "#         gdd.download_file_from_google_drive(file_id=file_id,\n",
        "#                                             dest_path=destination_path)\n",
        "#                                             # unzip=True if file_id is for a zip file):\n",
        "#         print(f\"ファイルのダウンロードが完了しました。'{destination_path}' に保存されました。\")\n",
        "\n",
        "#         # ダウンロードしたファイルを読み込む例 (テキストファイルの場合)\n",
        "#         if os.path.exists(destination_path):\n",
        "#             print(\"ダウンロードしたファイルの内容 (最初の数行):\")\n",
        "#             with open(destination_path, 'r') as f:\n",
        "#                 # ファイルの内容を表示 (例: 最初の5行)\n",
        "#                 for i in range(5):\n",
        "#                     line = f.readline()\n",
        "#                     if not line:\n",
        "#                         break\n",
        "#                     print(line.strip())\n",
        "#         else:\n",
        "#             print(f\"エラー: ダウンロード先のファイル '{destination_path}' が見つかりません。\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"ファイルのダウンロード中にエラーが発生しました: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "651c60b1-e920-4eac-a326-9a5d7774a34c",
      "metadata": {
        "id": "651c60b1-e920-4eac-a326-9a5d7774a34c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class Psylex71_Dataset_Original(torch.utils.data.Dataset):\n",
        "    '''ニューラルネットワークモデルに Psylex71 を学習させるための PyTorch 用データセットのクラス'''\n",
        "\n",
        "    def __init__(self,\n",
        "                 dic=Psylex71,\n",
        "                 grph_list=grph_list,\n",
        "                 phon_list=mora_list,\n",
        "                 special_tokens=special_tokens,\n",
        "                 maxlen_phon=maxlen_phon +2, # ＋2 しているのは <SOW>,<EOW> という 2 つのスペシャルトークンを付加するため\n",
        "                 device=device):\n",
        "        super().__init__()\n",
        "        self.dic = dic\n",
        "        self.special_tokens = special_tokens\n",
        "        self.maxlen_phon = maxlen_phon\n",
        "        self.grph_list = grph_list\n",
        "        self.phon_list = phon_list\n",
        "        self.input_cands = grph_list\n",
        "        #self.target_cands = special_tokens + phon_list\n",
        "        self.target_cands = special_tokens + mora_list\n",
        "        # self.inputs = [v['単語'] for v in dic.values()]\n",
        "        # self.targets = [v['ヨミ'] for v in dic.values()]\n",
        "        # self.targets = [v['モーラ'] for v in dic.values()]\n",
        "        self.inputs = [v['単語'] for v in dic.values()]\n",
        "        self.targets = [v['ヨミ'] for v in dic.values()]\n",
        "        self.targets = [v['モーラ'] for v in dic.values()]\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dic)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, tgt = self.inputs[idx], self.targets[idx]\n",
        "\n",
        "        # 入力信号にも <SOW>, <EOW> トークンを付与する場合\n",
        "        #inp = [self.input_cands.index('<SOW>')]  + [self.input_cands.index(x) for x in inp]  + [self.input_cands.index('<EOW>')]\n",
        "\n",
        "        # 入力信号にはスペシャルトークンを付与しない場合\n",
        "        inp = [self.input_cands.index(x) for x in inp]\n",
        "\n",
        "        # ターゲット (教師)信号 には <SOW>, <EOW> を付与する\n",
        "        tgt = [self.target_cands.index('<SOW>')] + [self.target_cands.index(x) for x in tgt] + [self.target_cands.index('<EOW>')]\n",
        "\n",
        "        while len(tgt) < self.maxlen_phon:\n",
        "            tgt = tgt + [self.target_cands.index('<PAD>')]\n",
        "\n",
        "        inp, tgt = torch.LongTensor(inp), torch.LongTensor(tgt)\n",
        "        inp, tgt = inp.to(self.device), tgt.to(self.device)\n",
        "        return inp, tgt\n",
        "\n",
        "    def getitem(self, idx):\n",
        "        #inp, tgt = self.inputs[idx], self.targets[idx]\n",
        "        wrd = self.inputs[idx]\n",
        "        phn = self.targets[idx]\n",
        "        return wrd, phn\n",
        "\n",
        "    def ids2argmax(self, ids):\n",
        "        out = np.array([torch.argmax(idx).numpy() for idx in ids], dtype=np.int32)\n",
        "        return out\n",
        "\n",
        "    def ids2tgt(self, ids):\n",
        "        #out = [self.target_cands[torch.argmax(idx)] for idx in ids]\n",
        "        out = [self.target_cands[idx - len(self.special_tokens)] for idx in ids]\n",
        "        return out\n",
        "\n",
        "    def ids2inp(self, ids):\n",
        "        out = [self.input_cands[idx] for idx in ids]\n",
        "        #out = [self.input_cands[idx - len(self.special_tokens)] for idx in ids]\n",
        "        return out\n",
        "\n",
        "    def target_ids2target(self, ids:list):\n",
        "        ret = []\n",
        "        for idx in ids:\n",
        "            if idx == self.target_cands.index('<EOW>'):\n",
        "                return ret+['<EOW>']\n",
        "            ret.append(self.target_cands[idx])\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1894cc-8111-4442-8b33-f098b7402a87",
      "metadata": {
        "id": "5b1894cc-8111-4442-8b33-f098b7402a87"
      },
      "source": [
        "## Perry+2007 Appendix\n",
        "\n",
        "#### A. Complex Graphemes Used in the CDP  Sublexical Network\n",
        "\n",
        "The complex graphemes are identical to those implemented in the connectionist model of spelling of Houghton and Zorzi (2003).\n",
        "\n",
        "* The onset consonants were as follows: ch, gh, gn, kn, ph, qu, sh, th, wh, and wr.\n",
        "* The vowels were as follows: air, ai, ar, au, aw, ay, ear, eau, eir, eer, ea, ee, ei, er, eu, ew, ey, ier, ieu, iew, ie, ir, oar, oor, our, oa, oe, oi, oo, ou, or, ow, oy, uar, ua, ue, ui, ur, uy, ye, and yr.\n",
        "* The coda consonants were as follows: ght, tch, que, ch, ck, dd, dg, ff, gh, gn, ll, mb, ng, ph, sh, ss, th, tt, and zz.\n",
        "\n",
        "#### Appendix B. Parameters Used in the Model\n",
        "Parameter type Parameter value\n",
        "\n",
        "##### Lexical route\n",
        "\n",
        "* Features\n",
        "    * Feature-to-letter excitation 0.005\n",
        "    * Feature-to-letter inhibition  0.150\n",
        "* Letters\n",
        "    * Letter-to-letter inhibition 0\n",
        "    * Letter-to-orthography excitation 0.075\n",
        "    * Letter-to-orthography inhibition  0.550\n",
        "* Orthographic lexicon\n",
        "    * Orthography-to-orthography inhibition  0.06\n",
        "    * Orthography-to-phonology excitation 1.40\n",
        "    * Orthography-to-letter excitation 0.30\n",
        "* Phonological lexicon\n",
        "    * Phonology-to-phonology inhibition  0.160\n",
        "    * Phonology-to-phoneme excitation 0.128\n",
        "    * Phonology-to-phoneme inhibition  0.010\n",
        "    * Phonology-to-orthography excitation 1.100\n",
        "* Phonological output buffer\n",
        "    * Phoneme-to-phoneme inhibition  0.040\n",
        "    * Phoneme-to-phonology excitation 0.098\n",
        "    * Phoneme-to-phonology inhibition  0.060\n",
        "\n",
        "##### Overall parameters\n",
        "\n",
        "* Overall activation rate 0.2\n",
        "* Lexicon frequency scaling   >0.4 X log (word frequency)\n",
        "* Phoneme naming activation criterion 0.67\n",
        "* Cycle-to-cycle stopping criterion 0.0023\n",
        "* Maximum number of cycles a word is run for before being timed out and considered an outlier 250\n",
        "\n",
        "##### Parameters used in the sublexical network\n",
        "\n",
        "* Network to phonological output buffer activation 0.085\n",
        "* Number of cycles taken for each letter to be processed 15\n",
        "* Level of activation that a letter must be over before grapheme identification begins 0.21\n",
        "* Temperature ($\\tau$) in the assembly network 3\n",
        "* Learning rate ($\\epsilon$) in the assembly network 0.05\n",
        "\n",
        "### Appendix C\n",
        "\n",
        "#### Activation and Learning Equations Used With the Sublexical Network\n",
        "\n",
        "The sublexical spelling-to-sound network is identical to the two-layer assembly network of Zorzi et al. (1998b), except that instead of letter units, we have grapheme units. These include the complex (i.e., multiletter) graphemes listed in Appendix A in addition to all single letters.\n",
        "\n",
        "#### Activation Function\n",
        "\n",
        "For any given input pattern, the input units are clamped to a value of 1.0 or 0.0, according to the presence or absence of the grapheme they encode; the net input to each output unit is simply\n",
        "$$\n",
        "\\text{net}_{i}=\\sum_{j}w_{ij}a_{j},\n",
        "$$\n",
        "where $a_{j}$ is the activation value of the input unit $j$, and $w_{ij}$ is the weight of the connections linking the unit $j$ to the output unit $i$. The activation of the output unit $i$ is determined by an S shaped squashing function (sigmoid) of the net input, bounding phoneme activations in the range $[0,1]$ and with $f(0)=0$ (i.e., no input and no output):\n",
        "$$\n",
        "O_{i}=\\frac{1}{1+e^{-(\\text{net}_{i}-1)\\tau}},\n",
        "$$\n",
        "where $\\tau$ is a temperature parameter determining the slope of the function ($\\tau=3$ for all simulations). Note that the $-1$ in the\n",
        "exponent shifts the sigmoid to the right, such that $f(0)=0.5$ is very close to 0, rather than the standard $f(0)=0.5$. As in Zorzi et al. (1998b), in the simulations reported here, values less than 0.05 are set to 0, so no input really does mean no output.\n",
        "\n",
        "#### Learning Rule\n",
        "\n",
        "The model was trained with the simple gradient descent technique known as the delta rule (Widrow & Hoff, 1960). For any input pattern, the error correction is made by changing the weights according to the difference between the activation of the output units and desired activation pattern. The desired output is just the correct pronunciation of the orthographic input (nodes that should be on have a target activation of 1, nodes that should be off have a target activation of 0). Formally,\n",
        "$$\n",
        "\\Delta w_{ij}=\\epsilon(t_{i}-o_{i})a_{j},\n",
        "$$\n",
        "where $\\epsilon$ is a learning rate (0.05 in the simulations), $a_j$ is the activation of the $j$-th input unit and $t_{i}$ and $o_{i}$ are the teaching input and the actual output of the ith output unit, respectively (for further details, see Zorzi et al., 1998b, pp. 1136–1137).\n",
        "\n",
        "Zorzi, M., Houghton, G., & Butterworth, B. (1998b). Two routes or one in reading aloud? A connectionist dual-process model. Journal of Experimental Psychology: Human Perception and Performance, 24, 1131–1161.\n",
        "\n",
        "### D.\n",
        "\n",
        "<img src=\"2007Perry_appendixD.svg\" width=\"77%\">\n",
        "\n",
        "### from page 281\n",
        "\n",
        "**Training corpus**. The training corpus was extracted from the English CELEX word form database (Baayen, Piepenbrock, & van Rijn, 1993), and it basically consists of all monosyllables with an orthographic frequency equal to or bigger than one. The database was also cleaned. For example, acronyms (e.g., mph), abbreviations, and proper names were removed. Note that we did not remove words with relatively strange spellings (e.g., isle). A number of errors in the database were also removed. This left 7,383 unique orthographic patterns and 6,663 unique phonological patterns.(footnote 2)\n",
        "\n",
        "**Network training**. In previous simulation work (Hutzler et al., 2004), we have shown that adapting the training regimen to account for explicit teaching methods is important for simulating reading development. Explicit teaching of small-unit correspondences is an important step in early reading and can be simulated by pretraining a connectionist model on a set of grapheme–phoneme correspondences prior to the introduction of a word corpus.\n",
        "\n",
        "The two-layer associative network was initially pretrained for 50 epochs on a set of 115 grapheme–phoneme correspondences selected because they are similar to those found in children’s phonics programs (see Hutzler et al., 2004, for further discussion). They consist of very common correspondences but are by no means all possible grapheme–phoneme relationships. The same correspondence (e.g., the correspondence $L\\rightarrow/l/$) may be exposed in more than one position in the network where it commonly occurs. The list of correspondences used appears in Appendix D. Note that the total number differs from that of Hutzler et al. (2004) because of the different coding scheme (their simulations were based on the CDP model). Learning parameters were identical to those in Zorzi et al. (1998b; see Appendix C).\n",
        "\n",
        "### from page 280\n",
        "\n",
        "### Sublexical Route\n",
        "\n",
        "**入力と出力の表現**。前述の通り、我々は下位語彙経路に書記素法バッファを追加した。この書記素バッファは Houghton&Zorzi(2003) のスペルモデルから採用され、2 層連合ネットワークで使用される入力符号化方式に実装された。これにより、単一の入力ノードは CDP のように個々の文字のみを表すのではなく、ck, th などの複雑な形態素も表すようになった。Houghton＆Zorzi (付録 A) が指定する複雑な形態素集合には、10 のオンセット形態素，41 の母音形態素，19 のコーダ形態素が含まれる。文字がこれらの形態素を形成するために結合する場合、文字ではなく形態素が活性化される (つまり、結合符号化)。注意：これらの複雑な形態素は、基本的に英語で最も頻繁に現れるもの(Perry&Ziegler2004) だが、決して英語で存在するすべての形態素の集合ではない。\n",
        "<!-- Input and output representation. As justified earlier, we added an orthographic buffer to the sublexical route. The orthographic buffer was taken from the spelling model of Houghton&Zorzi(2003) and was implemented in the input coding scheme used with the two-layer associative network. Thus, single input nodes do not represent individual letters only, as in CDP, but also complex graphemes such as ck, th, and so forth. The set of complex graphemes specified by Houghton and Zorzi (see Appendix A) includes 10 onset graphemes, 41 vowel graphemes, and 19 coda graphemes. When letters combine to form one of these graphemes, the grapheme is activated instead of the letters (i.e., conjunctive coding). Note that the complex graphemes are basically the most frequent ones that occur in English (see Perry&Ziegler2004, for a full analysis), although they are by no means the entire set that can be found -->\n",
        "\n",
        "\n",
        "入力表現は、形態素を形態素から graphosyllabic テンプレート(Caramazza&Miceli1990, Houghton&Zorzi2003) に配置することで構築される。このテンプレートには、発音の始まり (オンセット)、母音、および発音の終わり (コーダ) の構成要素が含まれる。発音の始まりスロットが 3 つ、母音スロットが 1 つ、終音スロットが 4 つある。各形態素は 1 つの入力スロットに割り当てられる。文字列の最初の形態素が子音の場合、それは最初の発音の始まりスロットに割り当てられ、続く子音形態素 は 2 番目、3 番目の発音の始まりスロットに割り当てられる。割り当てる形態素がない場合はスロットは空のままになる。母音の形態素は母音スロットに割り当てられる。母音の後の形態素は最初のコーダスロットに割り当てられ、その後の形態素（存在する場合）は順次コーダスロットを埋める。例えば、black は `b-l-*-a-ck-*-*-*` と符号化される。ここで、各アスタリスクは形態素によって活性化されていないスロットを表す。同様に、非単語である sloiched は s-l-*-oi-ch-e-d-* と符号化される。\n",
        "<!-- The input representation is constructed by aligning graphemes to a graphosyllabic template (Caramazza&Miceli1990, Houghton&Zorzi2003) with onset, vowel, and coda constituents. There are three onset slots, one vowel slot, and four coda slots. Each grapheme is assigned to one input slot. If the first grapheme in a letter string is a consonant, it is assigned to the first onset slot, and the following consonant graphemes are assigned to the second and then to the third onset slots. Slots are left empty if there are no graphemes to be assigned. The vowel grapheme is assigned to the vowel slot. The grapheme following the vowel is assigned to the first coda slot, and subsequent graphemes (if any) fill the successive coda slots. Thus, for example, black would be coded as `b-l-*-a-ck-*-*-*`, where each asterisk represents a slot that is not activated by any grapheme. Similarly, a nonword like sloiched would be coded as `s-l-*-oi-ch-e-d-*`. -->\n",
        "\n",
        "ネットワークの音声出力は、Zorzi+(1998b)で説明された表現構造と同一の構造を有しているが、3 つの子音、1 つの母音、3 つの尾音スロットを使用する代わりに、3 つの子音、1 つの母音、4 つの尾音スロットを使用している。したがって、ネットワークに訓練パターンが提示されると、出力（音韻）は子音–母音–尾音 の区別を尊重する形で分解される。4 つ目のコーダスロットを追加した理由は、モデルを訓練するために使用されたデータベースに 4 つのコダ音素を含む単語が存在したためである。したがって、/prɒmpts/ のようなコーダに 4 つの子音を含む単語はモデルで処理され、`p-r-*-ɒ-m-p-t-s`と符号化される。\n",
        "<!-- The phonological output of the network has a representational structure identical to that described in Zorzi+(1998b), except that instead of using three onset, one vowel, and three coda slots, it uses three onset, one vowel, and four coda slots. Thus, when training patterns are presented to the network, the output (phonology) is broken down in a way that respects an onset–vowel–coda distinction. The addition of a fourth coda slot was motivated by the existence of words with four coda phonemes in the database used to train the model. Thus, a word like prompts (/prɒmpts/) with four consonants in the coda can be handled by the model and would be coded as `p-r-*-ɒ-m-p-t-s`. -->\n",
        "\n",
        "TLA ネットワーク。下位語彙ネットワークは、入力ノードと出力ノードの数を除いて CDP の下位語彙経路と同一の単純な 2 層ネットワークである。入力ノードは、前述の形態素バッファ表現に従って単語の書記素を符号化する。したがって、形態素は入力層の   8 つのスロット（3  つのオンセットスロット  ＋1  つの母音スロット ＋4  つのコーダスロット）に符号化され、各スロットは  96 個の形態素ノード (26 個の単文字  +70  個の複合形態素) で構成される。単語の音韻はネットワークの出力層で符号化され、8 つの利用可能なスロット (3 つのオンセットスロット ＋1 つの母音スロット ＋4 つのコーダスロット) それぞれに 43 つの音素ノードが含まれる。これにより、入力ノードは 768 個、出力ノードは 344 個 (つまり 8×96 と 8×43) となる。スロット間で同一の符号化方式を複製することは、書記素 (または音韻論) ユニット全体が任意の位置で利用可能であることを意味する。ただし、この選択は単純化のために行われたものであり、実践的な影響はない。実際、決して活性化されないノード（例えば、発音位置におけるコーダ、コーダ位置における母音など）はネットワークにとって完全に無関係である：つまり、訓練中にいかなる入力も受けないノードは、いかなる出力も引き起こさない (Zorzi+1998b, p. 1136 の式 (2) )、そして表現の構築方法のため、これらの無関係なノードは決して活性化されない。したがって、ネットワークの性能は、発音ユニットを 3 つの主要な節 (発音初期、母音、終音）に分割したスロット固有の書記素 (または音韻) ユニット集合に基づく符号化方式を使用した場合でも同一になる。\n",
        "<!-- TLA network. The sublexical network is a simple two-layer network, identical to the sublexical route of CDP apart from the number of input and output nodes. The input nodes encode the orthography of the word according to the grapheme buffer representation described earlier. Thus, graphemes are encoded over 8 slots in the input layer (3 onset slots  ＋1 vowel slot ＋4 coda slots), where each slot consists of 96 grapheme nodes (26 single letters +70 complex graphemes). The phonology of the word is encoded at the output layer of the network, which contains 43 phoneme nodes for each of the 8 available slots (3 onset slots +1 vowel +4 coda slots). This means that there are 768 input nodes and 344 output nodes (i.e., 8x96 and 8x43). Replicating an identical coding scheme across slots means that the entire set of orthographic (or phonological) units is potentially available at any position. However, this choice was made only for the sake of simplicity, and it has no practical consequences. Indeed, nodes that are never activated (like codas in onset positions, vowels in coda positions, etc.) are completely irrelevant for the network: That is, nodes that never get any input in training never cause any output (see Equation 2 of Zorzi+1998b, p. 1136), and because of the way the representation is constructed, these irrelevant nodes are never activated. Thus, performance of the network would be identical if we had used a coding scheme based on a slot-specific set of orthographic (or phonological) units divided into three main sections (onset, vowel, coda). -->\n",
        "\n",
        "注意：Plaut+(1996)も、発音の初音 (オンセット)、母音、終音 (コーダ) の区別を使用していた。ただし、彼らの書記素と音韻の表象では、文字素子や音素素子の順序をスロットで符号化していなかった。彼らの解決策は、各セット (オンセット、母音、コーダ）内の素子を、書記素的に/音韻的に合法的な順序のみが発生するように配置することであった。子音群に 2 つの可能な順序がある場合（例：/ts/ vs. /st/）、それらを区別するために追加のノードが活性化された。CDP と三角モデルとのもう一つの違いは、CDP  では多文字書記素は書記素素子の活性化のみで符号化されるのに対し、Plaut＋では書記素と個々の文字の両方が活性化される点である。\n",
        "<!-- Note that an onset–vowel–coda distinction was also used by Plaut+(1996). Their orthographic and phonological representations, however, did not use slots to encode the order of grapheme or phoneme units. Their solution was to arrange the units within each set (onset, vowel, or coda) with an order that allows only orthotactically/phonotactically legal sequences to occur. In the case of consonant clusters that have two possible orderings (e.g., /ts/ vs. /st/), an additional node was activated to disambiguate between them. Another difference between CDP  and the triangle model is that a multiletter grapheme is coded only by the activation of the grapheme unit with CDP , whereas in Plaut et al., both the grapheme and the individual letters were activated. -->\n",
        "\n",
        "入力ノードと出力ノードをスロットに分割することは、組み込みのネットワーク構造（例：特定の接続パターン）を意味するものではないん。これは単に、ノードを活性化するための表現方式を反映しているに過ぎない。したがって、入力ノードと出力ノードは完全に接続されており、その間に隠れ素子は存在しない。したがって、任意の入力ノードは、任意の出力ノードを活性化させる可能性がある。出力ノードの活性化は、Zorzi+(1998b) で用いられた方法と同一の仕組みで、入力ノードの活性化に基づいて計算される。実際、同じパラメーターが使用されている(Appendix B)。使用される式とその動作の詳細は、Zorzi+(pp. 1136–1137) および Appendix C で詳細に説明されている。\n",
        "<!-- The breakdown of input and output nodes into slots does not imply a built-in network structure (e.g., a specific pattern of connectivity); it only reflects the representational scheme used to activate the nodes. Accordingly, input and output nodes are fully connected, with no hidden units between them. Thus, any given input node has the potential to activate any given output. Activation of output nodes is calculated on the basis of the activation of input nodes in a manner identical to the one used by Zorzi+(1998b), and indeed the same parameters are used (see Appendix B). The equations used and how they work are described in full detail by Zorzi et al. (pp. 1136–1137) and in Appendix C. -->\n",
        "\n",
        "注意すべき点は、接続主義モデルにおいて、特定の入力-出力関係の学習は、その関係が訓練データセットに存在するか否か、および入力と出力の符号化方法に厳密に依存する点である。例えば、我々のネットワークは、単語の構成要素（例えば、単語の頭子音）から他の構成要素（例えば、単語の末尾位置）への関係を一般化して学習できない。したがって、学習中に特定の位置で活性化されない場合、形態素は音韻に写像されない。この例としては、末尾位置にある子音 j が挙げられる。したがって、非単語である jinje はモデルによって正しく名前付けられない。しかし、英語の書記素上のコーダで j が n の後に現れないという事実から、この単語は書記素レベルでは二音節として扱われる可能性がある (つまり、jin-je 書記素上の音節境界に関する議論は Taft 1979 参照)。ただし、書記素上の音節の問題に関わらず、多音節単語を学習するモデルは jinje に困難を覚えないであろう。なぜなら、injure や banjo のような単語に –nj パターンが存在するためである ( 類似の議論については Plaut+1996 参照)。\n",
        "<!-- It is worth noting that learning of a given input–output relationship, in any connectionist model, strictly depends on its existence in the training corpus and on how the inputs and outputs are coded. For example, our network cannot learn to generalize relationships from parts of words such as onset consonants to parts of words such as coda positions. Thus, a grapheme does not map to any phoneme if it is never activated in a specific position during learning. Though this is rather uncommon, one example is the consonant j in the coda position. Accordingly, a nonword like jinje cannot be correctly named by the model. However, the fact that the letter j never occurs after the letter n in English orthographic codas suggests that the word might be treated as disyllabic at an orthographic level (i.e., jin-je; see Taft, 1979, for a discussion of orthographic syllable boundaries). However, regardless of the issue of orthographic syllables, a model learning multisyllabic words would not have any difficulties with jinje because the –nj pattern does occur in words such as injure and banjo (see Plaut+1996, for a similar argument). -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85d14c39-2b81-41c1-8322-585f014caf9f",
      "metadata": {
        "id": "85d14c39-2b81-41c1-8322-585f014caf9f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}