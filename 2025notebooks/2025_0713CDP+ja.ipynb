{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2025notebooks/2025_0713CDP%2Bja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb9a707e-7d3e-4c46-afb4-94bf7b425bf2",
      "metadata": {
        "id": "fb9a707e-7d3e-4c46-afb4-94bf7b425bf2"
      },
      "source": [
        "# 0. 準備作業"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db9a97b-ee8e-41b4-b88e-8018bfe1484c",
      "metadata": {
        "id": "2db9a97b-ee8e-41b4-b88e-8018bfe1484c"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_CDP_fig1.svg\" style=\"width:49%;\"><br/>\n",
        "<p>Zorzi+(1998) Fig.1 Architecture of the model. The arrow means full connectivity between layers. Each box stand for a group of letters (26) or phonemes (44).</p>\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_CDP_fig8.svg\" width=\"49%;\"><br/>\n",
        "<p>Zorzi+(1998) Fig.8. Architecture of the model with the hidden layer pathway. In both the direct pathway and the mediated pathway the layers are fully connected (arrows).</p>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/refs/heads/master/2025figs/1998Zorzi_fig10.svg\" width=\"49%\"><br/>\n",
        "<p style=\"align-text:center\">\n",
        "Figure 10. Lexical and sublexical procedures in reading aloud, and their interaction in the phonological decision system, where the final phonological code is computed for articulation.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4097c4e9-c490-4021-a06d-b9176667c507",
      "metadata": {
        "id": "4097c4e9-c490-4021-a06d-b9176667c507"
      },
      "source": [
        "## 0.1 必要なライブラリの輸入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f00f3ba-87a0-40f2-9b7b-141186681f58",
      "metadata": {
        "id": "1f00f3ba-87a0-40f2-9b7b-141186681f58"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import torch\n",
        "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f'device:{device}')\n",
        "\n",
        "# 必要なライブラリの輸入\n",
        "from collections import OrderedDict\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import operator\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "from IPython import get_ipython\n",
        "isColab =  'google.colab' in str(get_ipython())\n",
        "print(f'isColab:{isColab}')\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "try:\n",
        "    import ipynbname\n",
        "except ImportError:\n",
        "    !pip install ipynbname\n",
        "    import ipynbname\n",
        "\n",
        "FILEPATH = str(ipynbname.path()).split('/')[-1]\n",
        "print(f'FILEPATH:{FILEPATH}')\n",
        "\n",
        "try:\n",
        "    import CDP_ja\n",
        "except ImportError:\n",
        "    !git clone https://github.com/ShinAsakawa/CDP_ja.git\n",
        "    import CDP_ja"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf CDP_ja"
      ],
      "metadata": {
        "id": "dcac8-wTCamO"
      },
      "id": "dcac8-wTCamO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3a6e8418-8fd0-4843-ba85-435e2225ae02",
      "metadata": {
        "id": "3a6e8418-8fd0-4843-ba85-435e2225ae02"
      },
      "source": [
        "## 0.2 NTT 日本語語彙特性 単語頻度データ psylex71.txt のダウンロード"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0b5de0-5e4f-4f23-9e38-b20c1255349f",
      "metadata": {
        "id": "7d0b5de0-5e4f-4f23-9e38-b20c1255349f"
      },
      "source": [
        "## 0.1 モーラ tokenizer の定義 (モーラ分かち書き)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfcda6df-7c13-43b4-a750-8bb91aabdfe3",
      "metadata": {
        "id": "dfcda6df-7c13-43b4-a750-8bb91aabdfe3"
      },
      "outputs": [],
      "source": [
        "from CDP_ja import mora_Tokenizer, kunrei_Tokenizer, gakushu_Tokenizer, joyo_Tokenizer\n",
        "mora_tokenizer = mora_Tokenizer()\n",
        "kunrei_tokenizer = kunrei_Tokenizer()\n",
        "gakushu_tokenizer = gakushu_Tokenizer()\n",
        "joyo_tokenizer = joyo_Tokenizer()\n",
        "\n",
        "# word = 'アカサタナ'\n",
        "# ids = kunrei_tokenizer(word)\n",
        "# print(word, ids, kunrei_tokenizer.decode(ids))\n",
        "# print(kunrei_tokenizer.wakachi(word))\n",
        "\n",
        "# # 上記 gakushu_tokenizer の検証\n",
        "# print(gakushu_tokenizer('学校'))\n",
        "# print(gakushu_tokenizer.decode(gakushu_tokenizer('学校')))\n",
        "# print(len(gakushu_tokenizer.tokens), len(mora_tokenizer.tokens),)\n",
        "\n",
        "# 常用漢字\n",
        "# from RAM.char_ja import chars_joyo as chars_joyo\n",
        "# joyo_chars = \"\".join([ch for ch in chars_joyo().char_list])\n",
        "# print(joyo_chars)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce290850-2544-41d8-b077-3d5a23dbc9fc",
      "metadata": {
        "id": "ce290850-2544-41d8-b077-3d5a23dbc9fc"
      },
      "source": [
        "# 1. NTT 日本語語彙特性 単語頻度データ psylex71 データセットの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8fa0444-c6c5-4f31-b841-7928fdface35",
      "metadata": {
        "id": "a8fa0444-c6c5-4f31-b841-7928fdface35"
      },
      "outputs": [],
      "source": [
        "from CDP_ja import Psylex71_Dataset\n",
        "\n",
        "psylex71_dss={}\n",
        "inplen_min=2\n",
        "for inplen_max in [2,3,4,5]:\n",
        "\n",
        "    if inplen_max == 2:\n",
        "        display=True\n",
        "    else:\n",
        "        display=False\n",
        "    psylex71_dss[inplen_max] = Psylex71_Dataset(\n",
        "        inplen_min=2,\n",
        "        inplen_max=inplen_max,\n",
        "        # psylex71_dic=None,\n",
        "        input_tokenizer=gakushu_tokenizer,\n",
        "        output_tokenizer=mora_tokenizer,\n",
        "        device=device,\n",
        "        display=display)\n",
        "\n",
        "    print(f'psylex71 最短文字長:{inplen_min}, 最長文字長:{inplen_max}',\n",
        "          f'データセットサイズ (単語数):{psylex71_dss[inplen_max].__len__():7,d} 語')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e5596b-b653-4be2-a304-454a71f9c6d3",
      "metadata": {
        "id": "41e5596b-b653-4be2-a304-454a71f9c6d3"
      },
      "source": [
        "# 3. モデルの定義\n",
        "## 3.1 TLA モデルの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e45ba26-78b1-4ec8-9a7b-9107c0d2dbde",
      "metadata": {
        "id": "9e45ba26-78b1-4ec8-9a7b-9107c0d2dbde"
      },
      "outputs": [],
      "source": [
        "# 全モデル共通使用するライブラリの輸入\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from CDP_ja import vanilla_TLA\n",
        "from CDP_ja import Seq2Seq_wAtt\n",
        "from CDP_ja import Seq2Seq_woAtt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95712d5-f0b9-4d87-851e-7a0adf363c52",
      "metadata": {
        "id": "d95712d5-f0b9-4d87-851e-7a0adf363c52"
      },
      "source": [
        "## Transformer model の定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c236f2a-220c-48e9-a2f1-e723b05cdda8",
      "metadata": {
        "id": "8c236f2a-220c-48e9-a2f1-e723b05cdda8"
      },
      "outputs": [],
      "source": [
        "# from RAM import Transformer\n",
        "# transformer = Transformer(src_vocab_size=len(gakushu_tokenizer.tokens),\n",
        "#                           tgt_vocab_size=len(mora_tokenizer.tokens),\n",
        "#                           model_dim=256,\n",
        "#                           num_heads=4,\n",
        "#                           num_layers=1,\n",
        "#                           max_seq_length=psylex71_dss[2].maxlen_out,\n",
        "#                           #max_seq_length=psylex71_ds.maxlen_out,\n",
        "#                           dropout=0.,\n",
        "#                           ff_dim=32,\n",
        "#                           device=device)\n",
        "# #).to(device)\n",
        "# transformer.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5829bfe-31dc-4109-b82a-5207335f68bb",
      "metadata": {
        "id": "f5829bfe-31dc-4109-b82a-5207335f68bb"
      },
      "source": [
        "# 4. 訓練 (train) データセット，検証 (valid) データセット，検査 (test) データセットへ分割\n",
        "\n",
        "## 4.1 データセットの選択"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2813f84e-3abc-4cbb-b78d-7ba9ed828eed",
      "metadata": {
        "id": "2813f84e-3abc-4cbb-b78d-7ba9ed828eed"
      },
      "outputs": [],
      "source": [
        "# 以下の 2 つのデータセットは出力用トークナイザによって2つに分かれる\n",
        "psylex71_ds_mora   = Psylex71_Dataset(input_tokenizer=gakushu_tokenizer, output_tokenizer=mora_tokenizer)\n",
        "psylex71_ds_kunrei = Psylex71_Dataset(input_tokenizer=gakushu_tokenizer, output_tokenizer=kunrei_tokenizer)\n",
        "\n",
        "# # データセットのチェック\n",
        "# for _ds in [psylex71_ds_mora, psylex71_ds_kunrei]:\n",
        "#     for N in np.random.permutation(_ds.__len__())[:5]:\n",
        "#     #for N in range(3):\n",
        "#         inp, tgt = _ds.__getitem__(N)\n",
        "#         print(f'_ds.ids2inp(inp):{_ds.ids2inp(inp)}',\n",
        "#               f'{inp.cpu().numpy()}',\n",
        "#               f'_ds.target_ids2target(tgt):{_ds.target_ids2target(tgt)}',\n",
        "#               f'{tgt.cpu().numpy()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f3ba340-d265-45bc-bfc8-65f878ef857b",
      "metadata": {
        "id": "7f3ba340-d265-45bc-bfc8-65f878ef857b"
      },
      "source": [
        "## 4.2 データセットの分割,訓練,検査,検証データセット"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7db8143e-b242-4503-9918-1f287615fc87",
      "metadata": {
        "id": "7db8143e-b242-4503-9918-1f287615fc87"
      },
      "outputs": [],
      "source": [
        "# データセットの分割,訓練,検査,検証データセット\n",
        "seed=42\n",
        "\n",
        "_ds = psylex71_ds_mora\n",
        "#_ds = psylex71_ds_kunrei\n",
        "\n",
        "#train_size = int(_ds.__len__() * 0.7)\n",
        "#train_size = int(_ds.__len__() * 0.5)\n",
        "#valid_size = _ds.__len__() - train_size\n",
        "#train_ds, valid_ds = torch.utils.data.random_split(dataset=_ds, lengths=(train_size, valid_size), generator=torch.Generator().manual_seed(seed))\n",
        "#train_size = int(_ds.__len__() * 0.2)\n",
        "\n",
        "train_size = int(_ds.__len__() * 0.07)\n",
        "valid_size = int(_ds.__len__() * 0.03)\n",
        "resid_size = _ds.__len__() - train_size - valid_size\n",
        "\n",
        "train_ds, valid_ds, resid_size = torch.utils.data.random_split(\n",
        "    dataset=_ds,\n",
        "    lengths=(train_size, valid_size, resid_size),\n",
        "    generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "print(f'train_size:{train_size}')\n",
        "print(f'valid_size:{valid_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e5048cb-cdb9-439b-a260-585e00b9cbf4",
      "metadata": {
        "id": "5e5048cb-cdb9-439b-a260-585e00b9cbf4"
      },
      "source": [
        "## 4.3 バッチサイズの定義とデータローダの設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5598baaf-5a2d-46c8-835d-808d7012dcbc",
      "metadata": {
        "id": "5598baaf-5a2d-46c8-835d-808d7012dcbc"
      },
      "outputs": [],
      "source": [
        "# batch_size = 32\n",
        "# batch_size = 64\n",
        "# batch_size = 4096\n",
        "batch_size = 1024\n",
        "#batch_size = 512\n",
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
        "valid_dl = torch.utils.data.DataLoader(dataset=valid_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def _collate_fn(batch):\n",
        "    inps, tgts = list(zip(*batch))\n",
        "    inps = list(inps)\n",
        "    tgts = list(tgts)\n",
        "    return inps, tgts\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=_collate_fn)\n",
        "\n",
        "valid_dl = torch.utils.data.DataLoader(\n",
        "    dataset=valid_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=_collate_fn)\n",
        "\n",
        "print(f'train_ds.__len__():{train_ds.__len__()}')\n",
        "print(f'valid_ds.__len__():{valid_ds.__len__()}')\n",
        "\n",
        "# # 以下，検証\n",
        "# _ds = train_ds\n",
        "# for N in range(2):\n",
        "#     inp, tgt = _ds.__getitem__(N)\n",
        "#     print(f'_ds.dataset.ids2inp(inp):{_ds.dataset.ids2inp(inp)}',\n",
        "#           f'{inp.cpu().numpy()}',\n",
        "#           f'_ds.dataset.target_ids2target(tgt):{_ds.dataset.target_ids2target(tgt)}',\n",
        "#           f'{tgt.cpu().numpy()}')\n",
        "\n",
        "# psylex71_ds.maxlen_out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37208ed4-cdc7-4d8d-ba0f-a5b1fe159d5d",
      "metadata": {
        "id": "37208ed4-cdc7-4d8d-ba0f-a5b1fe159d5d"
      },
      "source": [
        "## 4.4 定義したモデルで動作チェック"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d54ba9e-47ad-48fc-b67d-45b4ccc3141b",
      "metadata": {
        "id": "4d54ba9e-47ad-48fc-b67d-45b4ccc3141b"
      },
      "outputs": [],
      "source": [
        "# N 個のデータを実行してみる\n",
        "N = 5\n",
        "_ds = train_ds\n",
        "ids = np.random.permutation(_ds.__len__())[:N]  # データをシャフルして先頭の N 項目だけ ids に入れる\n",
        "input_tokenizer=gakushu_tokenizer\n",
        "output_tokenizer=mora_tokenizer\n",
        "ds=train_ds\n",
        "n_hid=1024\n",
        "\n",
        "tla_vanilla = vanilla_TLA(inp_vocab_size=len(input_tokenizer.tokens),\n",
        "                          out_vocab_size=len(output_tokenizer.tokens),\n",
        "                          inp_len=2,\n",
        "                          out_len=ds.dataset.maxlen_out,\n",
        "                          device=device,\n",
        "                          n_hid=n_hid).to(device)\n",
        "\n",
        "#print(tla_vanilla.eval())\n",
        "\n",
        "# モデルを tla に代入\n",
        "#tla = tla_seq2seq\n",
        "tla = tla_vanilla\n",
        "_ds = psylex71_dss[2]\n",
        "#print(type(psylex71_dss[2]))\n",
        "#sys.exit()\n",
        "for idx in ids:\n",
        "    # データセットから返ってくる値は入力信号 inp と教師信号 tch\n",
        "    inp, tch = _ds.__getitem__(idx)\n",
        "    print(f'idx:{idx}:', f'inp:{inp}', f'tch:{tch}')\n",
        "\n",
        "    # 入出力信号はトークン ID 番号であるため人間が読みやすいように変換して表示\n",
        "    print(f'_ds.ids2inp({inp}):{_ds.ids2inp(inp)}')\n",
        "    print(f'_ds.taregt_ids2target({tch}):{_ds.target_ids2target(tch)}')\n",
        "    inp = pad_sequence(inp.unsqueeze(0), batch_first=True).to(device)\n",
        "    tch = pad_sequence(tch.unsqueeze(0), batch_first=True).to(device)\n",
        "\n",
        "    outs = tla(inp, tch)\n",
        "\n",
        "    print('教師:', _ds.target_ids2target([idx.cpu().numpy() for idx in tch.squeeze(0)]), end=\": \")\n",
        "    print('教師 ids:', [int(_tch.cpu().numpy()) for _tch in tch.squeeze(0)])\n",
        "    outs = tla(inp,tch)\n",
        "    print('出力 ids:', [int(_out.argmax().cpu().numpy()) for _out in outs[0]], end=\"\\n===\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f7279a-6c5f-49e5-8347-9d9d5842bae2",
      "metadata": {
        "id": "d9f7279a-6c5f-49e5-8347-9d9d5842bae2"
      },
      "source": [
        "# 5. 学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d95c41c-007e-4728-a899-6972db77da8c",
      "metadata": {
        "id": "2d95c41c-007e-4728-a899-6972db77da8c"
      },
      "outputs": [],
      "source": [
        "def fit_an_epoch(model:torch.nn.Module=None,\n",
        "                 optimizer:torch.optim=None,\n",
        "                 loss_f:torch.nn.modules=None,\n",
        "                 _dl:torch.utils.data.dataloader.DataLoader=None):\n",
        "\n",
        "    model.train()  # モデルを訓練モードに変更\n",
        "\n",
        "    sum_loss=0\n",
        "    count=0\n",
        "    N = 0\n",
        "\n",
        "    for inps, tchs in _dl:\n",
        "        inps = pad_sequence(inps, batch_first=True).to(device)\n",
        "        tchs = pad_sequence(tchs, batch_first=True).to(device)\n",
        "        outs = model(inps, tchs)\n",
        "\n",
        "        # 正解のカウント\n",
        "        out_ids = [out.argmax(dim=1) for out in outs]\n",
        "        for tch, out in zip(tchs[:], out_ids[:]):\n",
        "            yesno = ((tch==out) * 1).sum().cpu().numpy() == len(tch)\n",
        "            count += 1 if yesno else 0\n",
        "\n",
        "        # 学習の実行\n",
        "        loss = 0.\n",
        "        optimizer.zero_grad()\n",
        "        for j in range(len(tchs)):\n",
        "            loss += loss_f(outs[j],tchs[j])\n",
        "        loss.backward()  # 損失値の計算\n",
        "        optimizer.step() # 学習\n",
        "        sum_loss += loss.item()\n",
        "\n",
        "        N += len(tchs)\n",
        "    p_ = count / N\n",
        "    return {'sum_loss':sum_loss, 'count':count, 'N':N, 'P':p_}\n",
        "    #return model, {'sum_loss':sum_loss, 'count':count, 'N':N, 'P':p_}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb30678-2b56-4802-8a68-3e4cf0673586",
      "metadata": {
        "id": "6fb30678-2b56-4802-8a68-3e4cf0673586"
      },
      "outputs": [],
      "source": [
        "def eval_an_epoch(model:torch.nn.Module=None,\n",
        "                  loss_f:torch.nn.modules=None,\n",
        "                  _dl:torch.utils.data.dataloader.DataLoader=None):\n",
        "\n",
        "    model.eval()  # モデルを評価モードに変更\n",
        "\n",
        "    sum_loss=0\n",
        "    count=0\n",
        "    N = 0\n",
        "\n",
        "    for inps, tchs in _dl:\n",
        "        inps = pad_sequence(inps, batch_first=True).to(device)\n",
        "        tchs = pad_sequence(tchs, batch_first=True).to(device)\n",
        "        outs = model(inps, tchs)\n",
        "\n",
        "        # 正解のカウント\n",
        "        out_ids = [out.argmax(dim=1) for out in outs]\n",
        "        for tch, out in zip(tchs[:], out_ids[:]):\n",
        "            yesno = ((tch==out) * 1).sum().cpu().numpy() == len(tch)\n",
        "            count += 1 if yesno else 0\n",
        "\n",
        "        # 学習の実行\n",
        "        loss = 0.\n",
        "        for j in range(len(tchs)):\n",
        "            loss += loss_f(outs[j],tchs[j])\n",
        "        sum_loss += loss.item()\n",
        "\n",
        "        N += len(tchs)\n",
        "    p_ = count / N\n",
        "    #return model, {'sum_loss':sum_loss, 'count':count, 'N':N, 'P':p_}\n",
        "    return {'sum_loss':sum_loss, 'count':count, 'N':N, 'P':p_}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a2364c7-bd94-4469-8b13-c0990514358b",
      "metadata": {
        "id": "8a2364c7-bd94-4469-8b13-c0990514358b"
      },
      "source": [
        "## 5.1 訓練に用いるモデルを再定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b1b618-eebc-4671-8c90-8bb17b0a3297",
      "metadata": {
        "id": "64b1b618-eebc-4671-8c90-8bb17b0a3297"
      },
      "outputs": [],
      "source": [
        "ds = train_ds\n",
        "n_layers=1\n",
        "bidirectional=False\n",
        "n_hid=512\n",
        "n_hid=128\n",
        "n_hid=1024\n",
        "\n",
        "input_tokenizer=gakushu_tokenizer\n",
        "output_tokenizer=mora_tokenizer\n",
        "\n",
        "tla_vanilla = vanilla_TLA(inp_vocab_size=len(input_tokenizer.tokens),\n",
        "                          out_vocab_size=len(output_tokenizer.tokens),\n",
        "                          inp_len=2,\n",
        "                          out_len=ds.dataset.maxlen_out,\n",
        "                          device=device,\n",
        "                          n_hid=n_hid).to(device)\n",
        "print(tla_vanilla.eval())\n",
        "\n",
        "tla_seq2seq = Seq2Seq_wAtt(enc_vocab_size=len(input_tokenizer.tokens),\n",
        "                           dec_vocab_size=len(output_tokenizer.tokens),\n",
        "                           n_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           n_hid=n_hid).to(device)\n",
        "print(tla_seq2seq.eval())\n",
        "\n",
        "tla_seq2seq0 = Seq2Seq_woAtt(enc_vocab_size=len(input_tokenizer.tokens),\n",
        "                             dec_vocab_size=len(output_tokenizer.tokens),\n",
        "                             n_layers=n_layers,\n",
        "                             bidirectional=bidirectional,\n",
        "                             n_hid=n_hid).to(device)\n",
        "print(tla_seq2seq0.eval())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94805ae2-e439-4103-bafe-9cc3128a1173",
      "metadata": {
        "id": "94805ae2-e439-4103-bafe-9cc3128a1173"
      },
      "source": [
        "## 5.2 実際の訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08f5e7a6-c089-4f7e-b791-f47fd26b613a",
      "metadata": {
        "id": "08f5e7a6-c089-4f7e-b791-f47fd26b613a"
      },
      "outputs": [],
      "source": [
        "model0 = tla_vanilla\n",
        "model1 = tla_seq2seq\n",
        "model2 = tla_seq2seq0\n",
        "\n",
        "loss_f = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "# 学習率リストとエポック数の定義\n",
        "iter_params = [(1e-3, 2), (1e-4, 2), (1e-5, 2)]\n",
        "iter_params = [(1e-3, 10)]\n",
        "#iter_params = [(1e-3, 20), (1e-4, 10), (1e-5, 5)]\n",
        "\n",
        "results = [{}, {}, {}]\n",
        "\n",
        "# 途中結果を印字するタイミング\n",
        "#interval = 3\n",
        "interval = 1\n",
        "#interval = 10\n",
        "\n",
        "for (lr, epochs) in iter_params: # 学習率とエポック数を定義済のリストに従って変化させる\n",
        "\n",
        "    # 最適化関数の学習率を設定\n",
        "    optimizer0 = torch.optim.Adam(model0.parameters(), lr=lr)\n",
        "    optimizer1 = torch.optim.Adam(model1.parameters(), lr=lr)\n",
        "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=lr)\n",
        "\n",
        "    print(f'lr:{lr}, epochs:{epochs}')\n",
        "    # エポック数だけ学習を行う\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"エポック:{epoch+1:3d}\")\n",
        "\n",
        "        #for (model, optimizer) in [(model2,optimizer2)]:\n",
        "        #for (model, optimizer) in [(model0,optimizer0), (model1,optimizer1), (model2,optimizer2)]:\n",
        "        for N, (model, optimizer) in enumerate([(model2,optimizer2), (model1,optimizer1), (model0,optimizer0)]):\n",
        "\n",
        "            # 1 エポックの訓練を行う\n",
        "            out = fit_an_epoch(model=model, _dl=train_dl, loss_f=loss_f, optimizer=optimizer)\n",
        "            if (epoch % interval) == 0:\n",
        "                print(f\"学習損失値={out['sum_loss']:10.3f}\",\n",
        "                      f\"正解率={out['P']:5.3f}\",\n",
        "                      f\"({out['count']:5d}/{out['N']:5d})\",\n",
        "                      end=\"\\t\")\n",
        "\n",
        "            if not 'train_loss' in results[N]:\n",
        "                results[N]['train_loss'] = [out['sum_loss']]\n",
        "            else:\n",
        "                results[N]['train_loss'].append(out['sum_loss'])\n",
        "            if not 'train_P' in results[N]:\n",
        "                results[N]['train_P'] = [out['P']]\n",
        "            else:\n",
        "                results[N]['train_P'].append(out['P'])\n",
        "\n",
        "\n",
        "        if (epoch % interval) == 0:\n",
        "            print()\n",
        "\n",
        "        for N, (model, optimizer) in enumerate([(model2,optimizer2), (model1,optimizer1), (model0,optimizer0)]):\n",
        "\n",
        "            # 1 エポックの検証を行う\n",
        "            out = eval_an_epoch(model=model, _dl=valid_dl, loss_f=loss_f)\n",
        "            if (epoch % interval) == 0:\n",
        "                print(f\"検証損失値={out['sum_loss']:10.3f}\",\n",
        "                      f\"正解率={out['P']:5.3f}\",\n",
        "                      f\"({out['count']:5d}/{out['N']:5d})\",\n",
        "                      end=\"\\t\")\n",
        "\n",
        "            if not 'valid_loss' in results[N]:\n",
        "                results[N]['valid_loss'] = [out['sum_loss']]\n",
        "            else:\n",
        "                results[N]['valid_loss'].append(out['sum_loss'])\n",
        "            if not 'valid_P' in results[N]:\n",
        "                results[N]['valid_P'] = [out['P']]\n",
        "            else:\n",
        "                results[N]['valid_P'].append(out['P'])\n",
        "\n",
        "        if (epoch % interval) == 0:\n",
        "            print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee199c61-1db7-4bbe-986f-2d9ce2f8c4f2",
      "metadata": {
        "id": "ee199c61-1db7-4bbe-986f-2d9ce2f8c4f2"
      },
      "outputs": [],
      "source": [
        "#print(results)\n",
        "for i in range(2):\n",
        "    plt.plot(results[i]['train_loss'])\n",
        "    plt.plot(results[i]['valid_loss'])\n",
        "    #plt.plot(results[i]['train_P'])\n",
        "    #plt.plot(results[i]['valid_P'])\n",
        "    print(results[i]['train_P'])\n",
        "    print(results[i]['valid_P'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd787a25-1877-4960-89f0-c27b6471717b",
      "metadata": {
        "id": "cd787a25-1877-4960-89f0-c27b6471717b"
      },
      "outputs": [],
      "source": [
        "#train_ds.dataset.output_tokenizer\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PF7zBKz_hTjH"
      },
      "id": "PF7zBKz_hTjH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}