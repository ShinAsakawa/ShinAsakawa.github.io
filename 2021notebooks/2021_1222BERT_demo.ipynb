{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_1222BERT_demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNMgxpJqBT8ofWdZOFn4zs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_1222BERT_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- date: 2021_1222\n",
        "- filename: 2021_1222BERT_demo.ipynb\n",
        "- location: https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_1222BERT_demo.ipynb\n",
        "- author: 浅川伸一\n",
        "\n",
        "# 新納(2001) の再現演習による BERT の理解"
      ],
      "metadata": {
        "id": "b9dAl-J8-GbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "\n",
        "# 必要なライブラリのインストール\n",
        "import platform\n",
        "isColab = True if platform.system() == 'Linux' else False\n",
        "if isColab:\n",
        "    !pip install transformers > /dev/null 2>&1 \n",
        "\n",
        "    # MeCab, fugashi, ipadic のインストール\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1\n",
        "    !pip install mecab-python3 > /dev/null 2>&1\n",
        "    !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "    !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1\n",
        "    \n",
        "    import subprocess\n",
        "    cmd='echo `mecab-config --dicdir`\\\"/mecab-ipadic-neologd\\\"'\n",
        "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                                     shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "    !pip install 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    !python -m unidic download > /dev/null 2>&1\n",
        "    !pip install ipadic > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "ZrGBDms76Jl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "009GB6DO55PP"
      },
      "outputs": [],
      "source": [
        "from transformers import BertJapaneseTokenizer\n",
        "tknz = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
        "tknz_ = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese', do_subword_tokenize=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "# トークナイザの属性値を表示して理解を深める\n",
        "for k in ['sep_token', 'sep_token_id', 'slow_tokenizer_class', \n",
        "          'unk_token', 'unk_token_id', 'verbose', #'vocab', \n",
        "          'vocab_files_names', 'vocab_size', 'word_tokenizer', 'word_tokenizer_type',\n",
        "          'special_tokens_map', 'special_tokens_map_extended', \n",
        "          'subword_tokenizer', 'subword_tokenizer_type',\n",
        "          'all_special_ids', 'all_special_tokens', 'unk_token', 'unk_token_id', \n",
        "          'special_tokens_map', 'special_tokens_map_extended', \n",
        "          'subword_tokenizer', 'subword_tokenizer_type', \n",
        "          'tokenize',\n",
        "          'max_model_input_sizes', 'mecab_kwargs', 'model_input_names', 'model_max_length']:\n",
        "    print(colored(f'{k}','yellow'), colored(f'{getattr(tknz,k)}','green'))"
      ],
      "metadata": {
        "id": "XH1aM3LR6BLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getattr(tknz, 'save_vocabulary')\n",
        "#help(tknz.save_vocabulary)\n",
        "tknz.save_vocabulary('vocab_saved.txt')  # 後に利用可能なように，語彙辞書をテキストファイルとして書き出す\n",
        "\n",
        "# 結果の確認\n",
        "!head vocab_saved.txt\n",
        "!tail vocab_saved.txt"
      ],
      "metadata": {
        "id": "gVNUq_OM8jKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.7 I BertForMaskedLM の利用 (新納2001, p.128)\n",
        "\n",
        "BERT モデルはMasked Language Model によっても学習が行われているので、BERT のモデル自体に MASK された単語を推定する機構が含まれています。\n",
        "MASK された単語を推定するには、モデルからその機構の部分を取り出さなければなりません。\n",
        "これを行うのが ``BertForMaskedLM`` です。\n",
        "例文「私は犬が好き。」の「犬」の部分を MASK した以下の単語列に対して、MASK の単語を推定してみます。\n",
        "\n",
        "> 例文： 私は [MASK] が好き。\n",
        "\n",
        "まずこの単語列をid 列に変換します(※5)\n",
        "\n",
        "```python\n",
        "ids = tknz.encode(\"私は[MASK]が好き。＂）\n",
        "ids\n",
        "[ 2, 1325, 9, 4, 14, 3596, 8, 3]\n",
        "```\n",
        "\n",
        "また、以下により [MASK] の位置を確認しておきます。\n",
        "\n",
        "```python\n",
        "mskpos = ids.index(tknz.mask_token_id)\n",
        "mskpos\n",
        "# 3\n",
        "```\n",
        "\n",
        "BERT モデルからの Masked Language Model の取り出しは以下のように行います。\n",
        "\n",
        "```python\n",
        "from transformers import BertForMaskedLM\n",
        "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese')\n",
        "```\n",
        "\n",
        "\n",
        "次にモデルに単語のid 列を与えると、各単語の位置に現れる単語の分布が得られます。\n",
        "```python\n",
        "x = torch.LongTensor(ids).unsqueeze(O)\n",
        "a = model(x)\n",
        "```\n",
        "\n",
        "上記に示したモデルからの出力 `a` は要素が 1つのタプルです。\n",
        "`a[O]` の形状は以下のとおりです。\n",
        "\n",
        "> ［ バッチサイズ， 単語列の長さ， 登録単語の数］\n",
        "\n",
        "この例の場合、1 データだけなのでバッチサイズは 1、単語列の長さは 8 です。\n",
        "そして登録単語の数はそのモデルの持つ登録単語数です。\n",
        "この登録単語数は `tknz.vocab_size` から参照できます。\n",
        "このモデルの場合は `32000` になっています。\n",
        "\n",
        "```python\n",
        "a[0].shape  # torch.Size([1, 8, 32000])\n",
        "tknz.vocab_size # 32000\n",
        "```\n",
        "\n",
        "<font size=\"+1\" color=\"green\"> 新納本(2021) 近藤先生から自炊版をもらった pdf では，Bert の出力が 3 連 tuple と記載されている (p.124) が実際は違う\n",
        "実際には 出力を .to_tuple() で tuple に変換しないといけない。</font>\n",
        "\n",
        "\n",
        "この例の場合、MASK の位置は `mskpos` だったので、たとえば k=100 番目の登録単語が MASK の位置に現れる程度（確率）は以下のコードで得られます。\n",
        "```python\n",
        "k = 100\n",
        "a[O][O][mskpos][k]\n",
        "# tensor(-5.5299, grad_fn=<SelectBackward>)\n",
        "```\n",
        "変数 `k` を 0 から 31999 まで動かして最も大きな値を持つ $\\hat{k}$ を求めれば、$\\hat{k}$ 番目の登録単語が MASK の位置に最も高い確率で現れる単語と推定できます。\n",
        "このようにベタに調べるよりも、torch には `topk` という便利なメソッドがあります。\n",
        "これはベクトルの要素の中からその値の高いものを上から順に K 個取り出すものです。\n",
        "以下のように使います。\n",
        "\n",
        "```ptyhon\n",
        "b = torch.topk(a[0][0[mskpos],k=5)\n",
        "b[0]  # 上位 5 つの値\n",
        "tensor([8.5864, 8.0724, 7.6974, 7.6480, 7.5863] ,...)\n",
        "b[1］ ＃ 上位 5 つの index\n",
        "tensor([1301, 1201, 705, 6968, 450])\n",
        "```"
      ],
      "metadata": {
        "id": "Hgg2tdYj8mKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "idxs = tknz.encode('私は[MASK]が好き。')\n",
        "print(f'idxs :{idxs}')\n",
        "maskpos = idxs.index(tknz.mask_token_id)\n",
        "print(f'[MASK] の位置:{maskpos}')\n",
        "\n",
        "from transformers import BertForMaskedLM\n",
        "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese')\n",
        "\n",
        "x = torch.LongTensor(idxs).unsqueeze(0)\n",
        "a = model(x)\n",
        "\n",
        "\n",
        "print(a.to_tuple()[0].shape)  # torch.Size([1, 8, 32000])\n",
        "print(tknz.vocab_size) # 32000\n",
        "\n",
        "b = torch.topk(a.to_tuple()[0][0][maskpos],k=5)\n",
        "print(b[0])     # 上位 5 つの値\n",
        "# tensor([8.5864, 8.0724, 7.6974, 7.6480, 7.5863] ,...)\n",
        "\n",
        "print(b[1])     # 上位 5 つの index\n",
        "# tensor([1301, 1201, 705, 6968, 450])\n",
        "\n",
        "for idx in b[1].detach().numpy():\n",
        "    print('idx:', colored(f'{idx}', 'red' ,attrs=['bold']), 'トークン:', colored(f'{tknz.ids_to_tokens[idx]}','red', attrs=['bold']))"
      ],
      "metadata": {
        "id": "RMyMB5Ky8myW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 近藤先生から送っていただいた，オノマトペ文章データを読み込む\n",
        "\n",
        "#ファイルをアップロードします\n",
        "from google.colab import files\n",
        "files.upload()  # ご自身の PC からファイルをアップロードして下さい `original.csv`\"\n",
        "\n",
        "kondo_base = '.'\n",
        "original = {}\n",
        "n = 0\n",
        "with open(os.path.join(kondo_base,'original.csv'), 'r', encoding='utf8') as f:\n",
        "    s = f.read()\n",
        "    for s_ in s.split('\\n'):\n",
        "        if n == 0:\n",
        "            n += 1\n",
        "            continue\n",
        "        idx, sent = s_.split(',')\n",
        "        original[int(idx)] = sent \n",
        "\n",
        "print(f'{len(original)} has been read')\n"
      ],
      "metadata": {
        "id": "2AeeV9r-9Z4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 5\n",
        "for _ in range(N):\n",
        "    N = np.random.randint(low=0, high=len(original))  # ランダムサンプリングしてみる\n",
        "    sent = original[N]\n",
        "    if '(' in sent:\n",
        "        sent = sent.replace(')','').replace('(','')\n",
        "    \n",
        "    print(colored(sent, attrs=['bold']))    # 送っていただいた文\n",
        "    print(colored('分かち書き','blue'), tknz.tokenize(sent)) # その分かち書き\n",
        "    print(colored('分かち書き','green'), tknz_.tokenize(sent)) # その分かち書き\n",
        "    print(colored('ID 化', 'blue'), tknz.encode(sent))   # 分かち書き結果の単語 ID 化\n",
        "    print(colored('ID 化', 'green'), tknz_.encode(sent))   # 分かち書き結果の単語 ID 化\n",
        "\n",
        "\n",
        "# MeCab で単語分割が行われて、MeCab が単語として認識しても、その単語が語鎮リスト vocab.txt に登録されていない場合は\n",
        "# subword である WordPiece が起動され、その単語が適当に分割されます。そのように分割された単語には '##' が単語の前に付与されます。\n",
        "# また、未知語の場合もWordPieceが起動され、同様に分割されます。\n",
        "\n",
        "print(colored(f'\\ntknz.all_special_ids:{tknz.all_special_ids}',attrs=['bold']))  #  [1, 3, 0, 2, 4]\n",
        "print(colored(f'tknz.all_special_tokens:{tknz.all_special_tokens}', attrs=['bold']))  #  ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ],
      "metadata": {
        "id": "hfb3GuSe-uU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MIsB8YaL_JRM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}