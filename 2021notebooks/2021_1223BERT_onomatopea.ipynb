{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_1222BERT_demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0DxTn4/FguTuUG+hXfUcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_1223BERT_onomatopea.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- date: 2021_1223\n",
        "- filename: 2021_1223BERT_onomatopea.ipynb\n",
        "- source: https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_1223BERT_onomatopea.ipynb\n",
        "- author: 浅川伸一\n",
        "\n",
        "# 日本語オノマトペの BERT マスク化言語モデルによる出力例"
      ],
      "metadata": {
        "id": "b9dAl-J8-GbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "\n",
        "# 必要なライブラリのインストール\n",
        "import platform\n",
        "isColab = True if platform.system() == 'Linux' else False\n",
        "if isColab:\n",
        "    !pip install transformers > /dev/null 2>&1 \n",
        "\n",
        "    # MeCab, fugashi, ipadic のインストール\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1\n",
        "    !pip install mecab-python3 > /dev/null 2>&1\n",
        "    !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "    !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1\n",
        "    \n",
        "    import subprocess\n",
        "    cmd='echo `mecab-config --dicdir`\\\"/mecab-ipadic-neologd\\\"'\n",
        "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                                     shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "    !pip install 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    !python -m unidic download > /dev/null 2>&1\n",
        "    !pip install ipadic > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "ZrGBDms76Jl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ここで `日本語オノマトペ辞典4500より.xls` と `original.csv` を指定してアップロードする\n",
        "# 両ファイルは共に著作権保護対象の内容であり，公にできません。\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  "
      ],
      "metadata": {
        "id": "qHcOiYEP0L6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2021/Jan 近藤先生からいただいたオノマトペ辞典のデータの読み込み\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "#import jaconv\n",
        "\n",
        "onomatopea_excel = '日本語オノマトペ辞典4500より.xls'  # オリジナルファイル名，次行は勝手に rename したファイル名\n",
        "#onomatopea_excel = '2021-0325日本語オノマトペ辞典4500より.xls'\n",
        "onmtp2761 = pd.read_excel(onomatopea_excel, sheet_name='2761語')\n",
        "\n",
        "onomatopea = list(sorted(set([o for o in onmtp2761['オノマトペ']])))\n",
        "print(f'データファイル名: {onomatopea_excel}\\n',\n",
        "      f'オノマトペ単語総数: {len(onomatopea)}')\n",
        "\n",
        "# 近藤先生 (2021年12月22日） から送っていただいた，オノマトペ文章データを読み込む\n",
        "original = {}\n",
        "n = 0\n",
        "with open('original.csv', 'r', encoding='utf8') as f:\n",
        "    s = f.read()\n",
        "    for s_ in s.split('\\n'):\n",
        "        if n == 0:\n",
        "            n += 1\n",
        "            continue\n",
        "        idx, sent = s_.split(',')\n",
        "        \n",
        "        # Mac と Windows との unicode 符号化の差分を吸収する\n",
        "        sent = ''.join(unicodedata.normalize('NFKC',x) for x in sent)\n",
        "        original[int(idx)] = {'sent':sent}\n",
        "\n",
        "print(f'{len(original)} has been read')"
      ],
      "metadata": {
        "id": "ROfMZ8RU1-3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertConfig\n",
        "from transformers import BertModel\n",
        "from transformers import BertForPreTraining\n",
        "from transformers import BertJapaneseTokenizer\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "model_ja_name = 'cl-tohoku/bert-base-japanese' \n",
        "model = BertModel.from_pretrained(model_ja_name)\n",
        "config = BertConfig.from_pretrained(model_ja_name)\n",
        "\n",
        "# トークナイザ を 2 つ使ってみる\n",
        "tknz1 = BertJapaneseTokenizer.from_pretrained(model_ja_name)\n",
        "tknz2 = BertJapaneseTokenizer.from_pretrained(model_ja_name, do_subword_tokenize=False)\n",
        "\n",
        "#print(len(tknz.vocab), len(onomatopea))\n",
        "n_added = tknz1.add_tokens(onomatopea)    # 東北大学 BERT に登録されていないオノマトペを加える。\n",
        "tknz1.ids_to_tokens.update()\n",
        "\n",
        "n_added_ = tknz2.add_tokens(onomatopea)  # 東北大学 BERT に登録されていないオノマトペを加える。\n",
        "tknz2.ids_to_tokens.update()\n",
        "model.resize_token_embeddings(len(tknz1))\n",
        "\n",
        "print(len(tknz1), len(tknz1.vocab), tknz1.vocab_size)\n",
        "print(len(tknz2), len(tknz2.vocab), tknz2.vocab_size)\n",
        "\n",
        "print(tknz1.convert_tokens_to_ids(onomatopea[-10:]))\n",
        "print(tknz1.convert_ids_to_tokens(tknz1.convert_tokens_to_ids(onomatopea[-13:])))\n",
        "\n",
        "print(tknz2.convert_tokens_to_ids(onomatopea[-10:]))\n",
        "print(tknz2.convert_ids_to_tokens(tknz2.convert_tokens_to_ids(onomatopea[-13:])))\n"
      ],
      "metadata": {
        "id": "y9CdPPkW2Y92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# ランダムサンプリングしてデータを印字して確認\n",
        "for _ in range(5):\n",
        "    N = np.random.randint(low=0, high=len(original))\n",
        "    sent0 = original[N]['sent']\n",
        "    sent1 = re.sub(r'\\(.*\\)','',sent) # original に含まれる `(と)` のような表現を削除する\n",
        "    \n",
        "    print(colored(sent0, attrs=['bold']))  # 送っていただいた元の文\n",
        "    print(colored('分かち書き','blue'), tknz1.tokenize(sent0)) # その分かち書き\n",
        "    print(colored('分かち書き','green'), tknz2.tokenize(sent0)) # その分かち書き\n",
        "    print(colored('ID 化', 'blue'), tknz1.encode(sent0))   # 分かち書き結果の単語 ID 化\n",
        "    print(colored('ID 化', 'green'), tknz2.encode(sent0))   # 分かち書き結果の単語 ID 化\n",
        "    if sent0 != sent1:\n",
        "        print(colored(sent1, attrs=['bold']))   # (と) を取り去った文\n",
        "        print(colored('分かち書き','blue'), tknz1.tokenize(sent1)) # その分かち書き\n",
        "        print(colored('分かち書き','green'), tknz2.tokenize(sent1)) # その分かち書き\n",
        "        print(colored('ID 化', 'blue'), tknz1.encode(sent1))   # 分かち書き結果の単語 ID 化\n",
        "        print(colored('ID 化', 'green'), tknz2.encode(sent1))   # 分かち書き結果の単語 ID 化\n",
        "\n",
        "\n",
        "# MeCab で単語分割が行われて、MeCab が単語として認識しても、その単語が語鎮リスト vocab.txt に登録されていない場合は\n",
        "# subword である WordPiece が起動され、その単語が適当に分割されます。そのように分割された単語には '##' が単語の前に付与されます。\n",
        "# また、未知語の場合もWordPieceが起動され、同様に分割されます。\n",
        "print(colored(f'\\ntknz.all_special_ids:{tknz1.all_special_ids}',attrs=['bold']))  #  [1, 3, 0, 2, 4]\n",
        "print(colored(f'tknz.all_special_tokens:{tknz1.all_special_tokens}', attrs=['bold']))  #  ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ],
      "metadata": {
        "id": "VQNxO6lq4iHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(dir(tknz))  # トークナイザの内部を調べてみましょう"
      ],
      "metadata": {
        "id": "xKDb0cQg5Jks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "# トークナイザの属性値を表示して理解を深める\n",
        "for k in ['sep_token', 'sep_token_id', 'slow_tokenizer_class', \n",
        "          'unk_token', 'unk_token_id', 'verbose', #'vocab', \n",
        "          'vocab_files_names', 'vocab_size', 'word_tokenizer', 'word_tokenizer_type',\n",
        "          'special_tokens_map', 'special_tokens_map_extended', \n",
        "          'subword_tokenizer', 'subword_tokenizer_type',\n",
        "          'all_special_ids', 'all_special_tokens', 'unk_token', 'unk_token_id', \n",
        "          'special_tokens_map', 'special_tokens_map_extended', \n",
        "          'subword_tokenizer', 'subword_tokenizer_type', \n",
        "          'tokenize',\n",
        "          'max_model_input_sizes', 'mecab_kwargs', 'model_input_names', 'model_max_length']:\n",
        "    print(colored(f'{k}','yellow'), colored(f'{getattr(tknz1,k)}','green'))"
      ],
      "metadata": {
        "id": "XH1aM3LR6BLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# オノマトペにマッチする部分を検索し，[MASK] で置き換える\n",
        "masked_onmtp = {}\n",
        "for N in original:\n",
        "    sent = original[N]['sent']\n",
        "            \n",
        "    for i, o in enumerate(onomatopea):\n",
        "        if sent.find(o) != -1:\n",
        "            target_id = i\n",
        "            o_ = o  # 最後にマッチしたオノマトペだけ取り出す\n",
        "            \n",
        "    if o_ not in masked_onmtp:\n",
        "        masked_onmtp[o_] = [sent.replace(o_,'[MASK]')]\n",
        "    else:\n",
        "        masked_onmtp[o_].append(sent.replace(o_,'[MASK]'))"
      ],
      "metadata": {
        "id": "nkm4F3U-5bL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForMaskedLM\n",
        "masked_lm1 = BertForMaskedLM.from_pretrained(model_ja_name) #'cl-tohoku/bert-base-japanese')\n",
        "masked_lm2 = BertForMaskedLM.from_pretrained(model_ja_name) #'cl-tohoku/bert-base-japanese')\n",
        "\n",
        "tknz1.add_tokens(onomatopea)\n",
        "n_added = tknz1.add_tokens(onomatopea)    # 東北大学 BERT に登録されていないオノマトペを加える。\n",
        "tknz1.ids_to_tokens.update()\n",
        "masked_lm1.resize_token_embeddings(len(tknz1))\n",
        "\n",
        "tknz2.add_tokens(onomatopea)\n",
        "n_added_ = tknz2.add_tokens(onomatopea)  # 東北大学 BERT に登録されていないオノマトペを加える。\n",
        "tknz2.ids_to_tokens.update()\n",
        "masked_lm2.resize_token_embeddings(len(tknz2))\n",
        "\n",
        "n_limit = 3   # デバッグのため総出力回数を 3 にしていますが，すべての出力を得るためには，n_limit の値を len(masked_onmtp) にしてください\n",
        "for i, s in enumerate(masked_onmtp):\n",
        "    print(colored(f'{i} {s}',attrs=['bold']), masked_onmtp[s])\n",
        "    for s_ in masked_onmtp[s]:\n",
        "        print(tknz1.tokenize(s_), end=\"\")\n",
        "        print(colored(tknz1(s_)['input_ids'], 'green'), end=\"\") # with sentencepiece\n",
        "        x = torch.LongTensor(tknz1(s_)['input_ids']).unsqueeze(0)\n",
        "        a = masked_lm1(x)\n",
        "        print(a[0].shape)\n",
        "\n",
        "        print(tknz2.tokenize(s_), end=\"\")\n",
        "        print(colored(tknz2(s_)['input_ids'], 'cyan'), end=\"\") # without sentencepiece\n",
        "        x = torch.LongTensor(tknz2(s_)['input_ids']).unsqueeze(0)\n",
        "        a = masked_lm2(x)\n",
        "        print(a[0].shape)\n",
        "\n",
        "        idxs = tknz2(s_)['input_ids']\n",
        "        maskpos = idxs.index(tknz2.mask_token_id)\n",
        "        top_n = 5\n",
        "        b = torch.topk(a.to_tuple()[0][0][maskpos], k=top_n)\n",
        "        b_idx = b[1].detach().numpy()\n",
        "        b_wrd = list(tknz2.convert_ids_to_tokens(b_idx))\n",
        "        for k in range(top_n):\n",
        "            for j, idx in enumerate(idxs):\n",
        "                if j != maskpos:\n",
        "                    print(colored(tknz2.convert_ids_to_tokens(idx),'blue'), end=\" \")\n",
        "                else:\n",
        "                    print(colored(b_wrd[k],'red', attrs=['bold']),end=\" \")\n",
        "                    \n",
        "            print()\n",
        "    if i > n_limit:\n",
        "        break"
      ],
      "metadata": {
        "id": "LNhPSEfK5h25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_message(a, s_, tokenizer=tknz1, top_n=5):\n",
        "    idxs = tokenizer(s_)['input_ids']\n",
        "    maskpos = idxs.index(tokenizer.mask_token_id)\n",
        "    b = torch.topk(a.to_tuple()[0][0][maskpos], k=top_n)\n",
        "    b_idx = b[1].detach().numpy()\n",
        "    b_wrd = list(tokenizer.convert_ids_to_tokens(b_idx))\n",
        "    for k in range(top_n):\n",
        "        print(k, end=\":\")\n",
        "        for j, idx in enumerate(idxs):\n",
        "            if j != maskpos:\n",
        "                print(colored(tokenizer.convert_ids_to_tokens(idx),'grey'), end=\" \")\n",
        "            else:\n",
        "                print(colored(b_wrd[k],'red', attrs=['bold']),end=\" \")\n",
        "        print()\n",
        "\n",
        "n_limit = 3  \n",
        "# デバッグのため総出力回数を 3 にしていますが，すべての出力を得るためには，n_limit の値を len(masked_onmtp) \n",
        "# にするか，または直下行を有効にして，次々行をコメントアウトしてください\n",
        "#for i, s in enumerate(masked_onmtp):\n",
        "for i in range(5):\n",
        "    N = np.random.choice(len(masked_onmtp))\n",
        "    N = 49\n",
        "    N = 4\n",
        "    ono = list(masked_onmtp.keys())[N]\n",
        "    s = masked_onmtp[ono]\n",
        "    print(colored(f'{i} N:{N} オノマトペ:{ono} {s}', 'blue', attrs=['bold']))\n",
        "    for s_ in s:\n",
        "        print(tknz1.tokenize(s_), end=\"\")\n",
        "        print(colored(tknz1(s_)['input_ids'], 'green'), end=\"\") # with sentencepiece\n",
        "        x = torch.LongTensor(tknz1(s_)['input_ids']).unsqueeze(0)\n",
        "        a = masked_lm1(x)\n",
        "        print(a[0].shape)\n",
        "        \n",
        "        print('=' * 7, 'with wordpice')\n",
        "        print_message(a, s_, tokenizer=tknz1)\n",
        "        print('+' * 7, 'withoout wordpeice')\n",
        "        print_message(a, s_, tokenizer=tknz2)\n"
      ],
      "metadata": {
        "id": "Yscb8cEW5mYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tknz1.save_vocabulary('vocab_saved.txt')  # 後に利用可能なように，語彙辞書をテキストファイルとして書き出す\n",
        "\n",
        "# 結果の確認\n",
        "!head vocab_saved.txt\n",
        "!tail vocab_saved.txt\n",
        "#help(tknz.save_vocabulary)"
      ],
      "metadata": {
        "id": "UyLlNCU15W4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MIsB8YaL_JRM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}