{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_0617lemniscate_pytorch_cifar10.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOudwPTUN2wHq1cPSPI5Xla",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2021notebooks/2021_0617lemniscate_pytorch_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGWxSlGNBI_m"
      },
      "source": [
        "- source: https://github.com/zhirongw/lemniscate.pytorch/cifar.py\n",
        "- date: 2021_0617\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yats5Us27Vc"
      },
      "source": [
        "!git clone https://github.com/zhirongw/lemniscate.pytorch.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mwAeaYT3QPk"
      },
      "source": [
        "import sys\n",
        "sys.path.append('lemniscate.pytorch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akEGx28521kS"
      },
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import lib.custom_transforms as custom_transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "import models\n",
        "import datasets\n",
        "import math\n",
        "\n",
        "from lib.NCEAverage import NCEAverage\n",
        "from lib.LinearAverage import LinearAverage\n",
        "from lib.NCECriterion import NCECriterion\n",
        "from lib.utils import AverageMeter\n",
        "#from test import NN, kNN\n",
        "\n",
        "#from multiprocessing import Process, freeze_support\n",
        "#freeze_support()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kao7kZ6o4HOl"
      },
      "source": [
        "def NN(epoch, net, lemniscate, trainloader, testloader, recompute_memory=0):\n",
        "    net.eval()\n",
        "    net_time = AverageMeter()\n",
        "    cls_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    correct = 0.\n",
        "    total = 0\n",
        "    testsize = testloader.dataset.__len__()\n",
        "\n",
        "    trainFeatures = lemniscate.memory.t()\n",
        "    if hasattr(trainloader.dataset, 'imgs'):\n",
        "        trainLabels = torch.LongTensor([y for (p, y) in trainloader.dataset.imgs]).cuda()\n",
        "    else:\n",
        "        trainLabels = torch.LongTensor(trainloader.dataset.train_labels).cuda()\n",
        "\n",
        "    if recompute_memory:\n",
        "        transform_bak = trainloader.dataset.transform\n",
        "        trainloader.dataset.transform = testloader.dataset.transform\n",
        "        temploader = torch.utils.data.DataLoader(trainloader.dataset, batch_size=100, shuffle=False, num_workers=1)\n",
        "        #for batch_idx, (inputs, targets, indexes) in enumerate(temploader):\n",
        "        for batch_idx, (inputs, targets) in enumerate(temploader):\n",
        "            #targets = targets.cuda(async=True)\n",
        "            batchSize = inputs.size(0)\n",
        "            features = net(inputs)\n",
        "            trainFeatures[:, batch_idx*batchSize:batch_idx*batchSize+batchSize] = features.data.t()\n",
        "        trainLabels = torch.LongTensor(temploader.dataset.train_labels).cuda()\n",
        "        trainloader.dataset.transform = transform_bak\n",
        "    \n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets, indexes) in enumerate(testloader):\n",
        "            #targets = targets.cuda(async=True)\n",
        "            batchSize = inputs.size(0)\n",
        "            features = net(inputs)\n",
        "            net_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            dist = torch.mm(features, trainFeatures)\n",
        "\n",
        "            yd, yi = dist.topk(1, dim=1, largest=True, sorted=True)\n",
        "            candidates = trainLabels.view(1,-1).expand(batchSize, -1)\n",
        "            retrieval = torch.gather(candidates, 1, yi)\n",
        "\n",
        "            retrieval = retrieval.narrow(1, 0, 1).clone().view(-1)\n",
        "            yd = yd.narrow(1, 0, 1)\n",
        "\n",
        "            total += targets.size(0)\n",
        "            correct += retrieval.eq(targets.data).sum().item()\n",
        "            \n",
        "            cls_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            print('Test [{}/{}]\\t'\n",
        "                  'Net Time {net_time.val:.3f} ({net_time.avg:.3f})\\t'\n",
        "                  'Cls Time {cls_time.val:.3f} ({cls_time.avg:.3f})\\t'\n",
        "                  'Top1: {:.2f}'.format(\n",
        "                  total, testsize, correct*100./total, net_time=net_time, cls_time=cls_time))\n",
        "\n",
        "    return correct/total\n",
        "\n",
        "def kNN(epoch, net, lemniscate, trainloader, testloader, K, sigma, recompute_memory=0):\n",
        "    net.eval()\n",
        "    net_time = AverageMeter()\n",
        "    cls_time = AverageMeter()\n",
        "    total = 0\n",
        "    testsize = testloader.dataset.__len__()\n",
        "\n",
        "    trainFeatures = lemniscate.memory.t()\n",
        "    if hasattr(trainloader.dataset, 'imgs'):\n",
        "        trainLabels = torch.LongTensor([y for (p, y) in trainloader.dataset.imgs]).cuda()\n",
        "    else:\n",
        "        trainLabels = torch.LongTensor(trainloader.dataset.train_labels).cuda()\n",
        "    C = trainLabels.max() + 1\n",
        "\n",
        "    if recompute_memory:\n",
        "        transform_bak = trainloader.dataset.transform\n",
        "        trainloader.dataset.transform = testloader.dataset.transform\n",
        "        temploader = torch.utils.data.DataLoader(trainloader.dataset, batch_size=100, shuffle=False, num_workers=1)\n",
        "        for batch_idx, (inputs, targets, indexes) in enumerate(temploader):\n",
        "            #targets = targets.cuda(async=True)\n",
        "            batchSize = inputs.size(0)\n",
        "            features = net(inputs)\n",
        "            trainFeatures[:, batch_idx*batchSize:batch_idx*batchSize+batchSize] = features.data.t()\n",
        "        trainLabels = torch.LongTensor(temploader.dataset.train_labels).cuda()\n",
        "        trainloader.dataset.transform = transform_bak\n",
        "    \n",
        "    top1 = 0.\n",
        "    top5 = 0.\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        retrieval_one_hot = torch.zeros(K, C).cuda()\n",
        "        for batch_idx, (inputs, targets, indexes) in enumerate(testloader):\n",
        "            end = time.time()\n",
        "            #targets = targets.cuda(async=True)\n",
        "            batchSize = inputs.size(0)\n",
        "            features = net(inputs)\n",
        "            net_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            dist = torch.mm(features, trainFeatures)\n",
        "\n",
        "            yd, yi = dist.topk(K, dim=1, largest=True, sorted=True)\n",
        "            candidates = trainLabels.view(1,-1).expand(batchSize, -1)\n",
        "            retrieval = torch.gather(candidates, 1, yi)\n",
        "\n",
        "            retrieval_one_hot.resize_(batchSize * K, C).zero_()\n",
        "            retrieval_one_hot.scatter_(1, retrieval.view(-1, 1), 1)\n",
        "            yd_transform = yd.clone().div_(sigma).exp_()\n",
        "            probs = torch.sum(torch.mul(retrieval_one_hot.view(batchSize, -1 , C), yd_transform.view(batchSize, -1, 1)), 1)\n",
        "            _, predictions = probs.sort(1, True)\n",
        "\n",
        "            # Find which predictions match the target\n",
        "            correct = predictions.eq(targets.data.view(-1,1))\n",
        "            cls_time.update(time.time() - end)\n",
        "\n",
        "            top1 = top1 + correct.narrow(1,0,1).sum().item()\n",
        "            top5 = top5 + correct.narrow(1,0,5).sum().item()\n",
        "\n",
        "            total += targets.size(0)\n",
        "\n",
        "            print('Test [{}/{}]\\t'\n",
        "                  'Net Time {net_time.val:.3f} ({net_time.avg:.3f})\\t'\n",
        "                  'Cls Time {cls_time.val:.3f} ({cls_time.avg:.3f})\\t'\n",
        "                  'Top1: {:.2f}  Top5: {:.2f}'.format(\n",
        "                  total, testsize, top1*100./total, top5*100./total, net_time=net_time, cls_time=cls_time))\n",
        "\n",
        "    print(top1*100./total)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH1Rmy5d9feM"
      },
      "source": [
        "from PIL import Image\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "\n",
        "class CIFAR10Instance(datasets.CIFAR10):\n",
        "    \"\"\"CIFAR10Instance Dataset.\n",
        "    \"\"\"\n",
        "    def __getitem__(self, index):\n",
        "        if self.train:\n",
        "            img, target = self.data[index], self.targets[index]\n",
        "        else:\n",
        "            img, target = self.data[index], self.targets[index]\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target, index\n",
        "\n",
        "class CIFAR100Instance(CIFAR10Instance):\n",
        "    \"\"\"CIFAR100Instance Dataset.\n",
        "\n",
        "    This is a subclass of the `CIFAR10Instance` Dataset.\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar-100-python'\n",
        "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
        "    filename = \"cifar-100-python.tar.gz\"\n",
        "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
        "    train_list = [\n",
        "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
        "    ]\n",
        "\n",
        "    test_list = [\n",
        "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1efbrHa4rut"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=32, scale=(0.2,1.)),\n",
        "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "#trainset = datasets.CIFAR10Instance(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainset = CIFAR10Instance(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "#testset = datasets.CIFAR10Instance(root='./data', train=False, download=True, transform=transform_test)\n",
        "testset = CIFAR10Instance(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "ndata = trainset.__len__()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGWcgWGj5xTp"
      },
      "source": [
        "nce_t = 0.1\n",
        "nce_m = 0.5\n",
        "nce_k = 4096\n",
        "low_dim = 128\n",
        "lr = 0.03\n",
        "test_only = False\n",
        "resume = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsd1_n5e5BPh"
      },
      "source": [
        "print('==> Building model..')\n",
        "net = models.__dict__['ResNet18'](low_dim=low_dim)\n",
        "\n",
        "# define leminiscate\n",
        "if nce_k > 0:\n",
        "    lemniscate = NCEAverage(low_dim, ndata, nce_k, nce_t, nce_m)\n",
        "else:\n",
        "    lemniscate = LinearAverage(low_dim, ndata, nce_t, args.nce_m)\n",
        "\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "# Model\n",
        "if test_only or len(resume)>0:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/'+args.resume)\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    lemniscate = checkpoint['lemniscate']\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    \n",
        "# define loss function\n",
        "if hasattr(lemniscate, 'K'):\n",
        "    criterion = NCECriterion(ndata)\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "net.to(device)\n",
        "lemniscate.to(device)\n",
        "criterion.to(device)\n",
        "\n",
        "if test_only:\n",
        "    acc = kNN(0, net, lemniscate, trainloader, testloader, 200, args.nce_t, 1)\n",
        "    sys.exit(0)\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, lr=0.03):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = lr\n",
        "    if epoch >= 80:\n",
        "        lr = lr * (0.1 ** ((epoch-80) // 40))\n",
        "    print(lr)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVaH-Acw6Q9K"
      },
      "source": [
        "# Training\n",
        "def train(epoch, lr=0.03):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    adjust_learning_rate(optimizer, epoch, lr)\n",
        "    train_loss = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    batch_time = AverageMeter()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # switch to train mode\n",
        "    net.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for batch_idx, (inputs, targets, indexes) in enumerate(trainloader):\n",
        "        data_time.update(time.time() - end)\n",
        "        inputs, targets, indexes = inputs.to(device), targets.to(device), indexes.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        features = net(inputs)\n",
        "        with torch.no_grad():\n",
        "            outputs = lemniscate(features, indexes)\n",
        "        loss = criterion(outputs, indexes)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss.update(loss.item(), inputs.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        print('Epoch: [{}][{}/{}]'\n",
        "              'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
        "              'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
        "              'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f})'.format(\n",
        "              epoch, batch_idx, len(trainloader), batch_time=batch_time, data_time=data_time, train_loss=train_loss))\n",
        "\n",
        "#freeze_support()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYNzhGVe6tl5"
      },
      "source": [
        "for epoch in range(start_epoch, start_epoch+200):\n",
        "    train(epoch, lr=0.03)\n",
        "    acc = kNN(epoch, net, lemniscate, trainloader, testloader, 200, nce_t, 0)\n",
        "\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'lemniscate': lemniscate,\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.t7')\n",
        "        best_acc = acc\n",
        "\n",
        "    print('best accuracy: {:.2f}'.format(best_acc*100))\n",
        "\n",
        "acc = kNN(0, net, lemniscate, trainloader, testloader, 200, nce_t, 1)\n",
        "print('last accuracy: {:.2f}'.format(acc*100))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}