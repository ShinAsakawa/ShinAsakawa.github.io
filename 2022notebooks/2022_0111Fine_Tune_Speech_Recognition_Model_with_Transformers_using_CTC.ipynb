{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022_0111Fine-Tune_Speech_Recognition_Model_with_Transformers_using_CTC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3fce4559ba3142378d886c2a72b5d3f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ec41ccd546d14d7db69f5e3e3a646c37",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2f218a5d35b24f70844bd8c50637e849",
              "IPY_MODEL_4e105fc762ff44a0b854974349e9486d",
              "IPY_MODEL_df754fb9c7264d43bb934a599fa1fb9d"
            ]
          }
        },
        "ec41ccd546d14d7db69f5e3e3a646c37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f218a5d35b24f70844bd8c50637e849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ff297d1a6e8d472ea2449f1fb9f0694b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77c418a0f8c54f708f6d4c4c9b205235"
          }
        },
        "4e105fc762ff44a0b854974349e9486d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f55c0576188e4bfdb520911ab34e5177",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4620,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4620,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b693094bbebc421bb38fa2cbba369ba5"
          }
        },
        "df754fb9c7264d43bb934a599fa1fb9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_74efc78f14f54552a834e6dd2098cab0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4620/4620 [00:00&lt;00:00, 11209.09ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13dbec546b884fa89fb36d41c9f46603"
          }
        },
        "ff297d1a6e8d472ea2449f1fb9f0694b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77c418a0f8c54f708f6d4c4c9b205235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f55c0576188e4bfdb520911ab34e5177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b693094bbebc421bb38fa2cbba369ba5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "74efc78f14f54552a834e6dd2098cab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13dbec546b884fa89fb36d41c9f46603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a177012697974429a5f49b6c7ff061ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_75d478c5208f4033b63a752a3a8bb558",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dec07a1af19148b786be8dffde835899",
              "IPY_MODEL_a359bd4b848648919974e3021cead64c",
              "IPY_MODEL_25c79405826041c4ba5e8dfc45c0b5c3"
            ]
          }
        },
        "75d478c5208f4033b63a752a3a8bb558": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dec07a1af19148b786be8dffde835899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_11c07d44fe424794ac4d75f37e9dd232",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1a9427dd53454b1783a81441e291d6d5"
          }
        },
        "a359bd4b848648919974e3021cead64c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d0d999b1632142f2b606d10af46a98c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1680,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1680,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a418a2f16e34f43bc7d9efbb30edefc"
          }
        },
        "25c79405826041c4ba5e8dfc45c0b5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_20de6e2d5bfc4c68a80c3bbec54d1b0f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1680/1680 [00:00&lt;00:00, 9817.41ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ba53939dc594a81a29e2b08e0c64c1d"
          }
        },
        "11c07d44fe424794ac4d75f37e9dd232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1a9427dd53454b1783a81441e291d6d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0d999b1632142f2b606d10af46a98c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a418a2f16e34f43bc7d9efbb30edefc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "20de6e2d5bfc4c68a80c3bbec54d1b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ba53939dc594a81a29e2b08e0c64c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0111Fine_Tune_Speech_Recognition_Model_with_Transformers_using_CTC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBSYoWbi-45k"
      },
      "source": [
        "# **Fine-tuning Speech Model with ğŸ¤— Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWj4gag6uH2I"
      },
      "source": [
        "ã“ã®ãƒãƒ¼ãƒˆã§ã¯ï¼Œè‡ªå‹•éŸ³å£°èªè­˜ã®ãŸã‚ã®å¤šè¨€èªã®äº‹å‰å­¦ç¿’æ¸ˆã¿éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚\n",
        "<!-- This notebook shows how to fine-tune multi-lingual pretrained speech models for Automatic Speech Recognition. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnXLL0QhuMBD"
      },
      "source": [
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ [TIMITãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](https://huggingface.co/datasets/timit) ä¸Šã§ï¼Œ[ãƒ¢ãƒ‡ãƒ«ãƒãƒ– Model Hub](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads) ã‹ã‚‰ä»»æ„ã®éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã‚ˆã†ã«æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "ãã®ãƒ¢ãƒ‡ãƒ«ã« CTC (Connectionist Temporal Classification) ãƒ˜ãƒƒãƒ‰ã‚’æ­è¼‰ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã‚ã‚‹å ´åˆã«é™ã‚Šã¾ã™ã€‚\n",
        "ãƒ¢ãƒ‡ãƒ«ã‚„ä½¿ç”¨ã—ã¦ã„ã‚‹ GPU ã«ã‚ˆã£ã¦ã¯ï¼Œãƒ¡ãƒ¢ãƒªä¸è¶³ã®ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã™ã‚‹ãŸã‚ã«ï¼Œãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚\n",
        "ã“ã® 2 ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚Œã°ï¼Œã‚ã¨ã¯ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãŒã‚¹ãƒ ãƒ¼ã‚ºã«å‹•ä½œã™ã‚‹ã¯ãšã§ã™ã€‚\n",
        "<!-- This notebook is built to run on the [TIMIT dataset](https://huggingface.co/datasets/timit) with any speech model checkpoint from the [Model Hub](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads) as long as that model has a version with a Connectionist Temporal Classification (CTC) head. \n",
        "Depending on the model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. \n",
        "Set those two parameters, then the rest of the notebook should run smoothly: -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7CftmzCuKYl"
      },
      "source": [
        "model_checkpoint = \"facebook/wav2vec2-base\"\n",
        "batch_size = 32"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTSa1gvYuUX0"
      },
      "source": [
        "å¤šè¨€èªè¨“ç·´æ¸ˆéŸ³å£°ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã®ã‹ï¼Œã‚ˆã‚Šè©³ç´°ãªèª¬æ˜ã«ã¤ã„ã¦ã¯ [ğŸ¤—ãƒ–ãƒ­ã‚°](https://huggingface.co/blog/fine-tune-wav2vec2-english) ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
        "<!-- For a more in-detail explanation of how multi-lingual pretrained speech models function, please take a look at the [ğŸ¤— Blog](https://huggingface.co/blog/fine-tune-wav2vec2-english). -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e335hPmdtASZ"
      },
      "source": [
        "å§‹ã‚ã‚‹å‰ã«ï¼Œãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰ `datasets` ã¨ `transformers` ã®ä¸¡æ–¹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã—ã‚‡ã†ã€‚ \n",
        "ã¾ãŸï¼ŒéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã«ã¯ `librosa` ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒå¿…è¦ã§ï¼Œå¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ [word error rate (WER)](https://huggingface.co/metrics/wer) ãƒ¡ãƒˆãƒªãƒƒã‚¯ ${}^1$ ã‚’ç”¨ã„ã¦è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã¯ `jiwer` ãŒå¿…è¦ã§ã™ã€‚\n",
        "<!-- Before we start, let's install both `datasets` and `transformers` from master. Also, we need the `librosa` package to load audio files and the `jiwer` to evaluate our fine-tuned model using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric ${}^1$. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8eh87Hoee5d"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets==1.14\n",
        "!pip install transformers==4.11.3\n",
        "!pip install librosa\n",
        "!pip install jiwer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_6kYmDMH9lR"
      },
      "source": [
        "æ¬¡ã«ï¼Œè¨“ç·´ä¸­ã«ï¼Œè¨“ç·´ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ [ğŸ¤— Hub](https://huggingface.co/) ã«ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚\n",
        "[ğŸ¤— Hub](https://huggingface.co/) ã«ã¯ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãŒçµ±åˆã•ã‚Œã¦ã„ã‚‹ã®ã§ï¼Œè¨“ç·´ä¸­ã«ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå¤±ã‚ã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "\n",
        "ã“ã‚Œã‚’è¡Œã†ã«ã¯ï¼ŒHugging Face ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‹ã‚‰èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ (ã¾ã ç™»éŒ²ã—ã¦ã„ãªã„å ´åˆã¯ [ã“ã¡ã‚‰ã‹ã‚‰](https://huggingface.co/join) ã§ç™»éŒ²ã—ã¦ãã ã•ã„ï¼)ã€‚\n",
        "<!-- Next we strongly suggest to upload your training checkpoints directly to the [ğŸ¤— Hub](https://huggingface.co/) while training. \n",
        "The [ğŸ¤— Hub](https://huggingface.co/) has integrated version control so you can be sure that no model checkpoint is getting lost during training. \n",
        "\n",
        "To do so you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFLBDyzQIA3R"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCyw5D23IQ1F"
      },
      "source": [
        "ãã—ã¦ï¼Œãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«ï¼ŒGit-LFS ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "<!-- Then you need to install Git-LFS to upload your model checkpoints: -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9BnQDhOITBC"
      },
      "source": [
        "%%capture\n",
        "!apt install git-lfs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn9swf6EQ9Vd"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "${}^1$ Timitã¯é€šå¸¸ï¼ŒéŸ³ç´ èª¤ã‚Šç‡ (PER) ã‚’ç”¨ã„ã¦è©•ä¾¡ã•ã‚Œã¾ã™ãŒ ASR ã§æœ€ã‚‚ä¸€èˆ¬çš„ãªæŒ‡æ¨™ã¯åœ§å€’çš„ã«å˜èªèª¤ã‚Šç‡ (WER) ã§ã™ã€‚\n",
        "ã“ã®ãƒãƒ¼ãƒˆã‚’ã§ãã‚‹ã ã‘ä¸€èˆ¬çš„ãªã‚‚ã®ã«ã™ã‚‹ãŸã‚ã«ï¼ŒWER ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚\n",
        "<!-- ${}^1$ Timit is usually evaluated using the phoneme error rate (PER), but by far the most common metric in ASR is the word error rate (WER). \n",
        "To keep this notebook as general as possible we decided to evaluate the model using WER. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mW-C1Nt-j7k"
      },
      "source": [
        "## ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼Œãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ï¼Œç‰¹å¾´æŠ½å‡ºå™¨\n",
        "<!-- ## Prepare Data, Tokenizer, Feature Extractor -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeBosnY9BH3e"
      },
      "source": [
        "ASR ãƒ¢ãƒ‡ãƒ«ã¯éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ãŸã‚ï¼ŒéŸ³å£°ä¿¡å·ã‚’ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (ä¾‹: ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«) ã«å‡¦ç†ã™ã‚‹ç‰¹å¾´æŠ½å‡ºå™¨ã¨ï¼Œãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å‡¦ç†ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã®ä¸¡æ–¹ãŒå¿…è¦ã¨ãªã‚Šã¾ã™ã€‚\n",
        "<!-- ASR models transcribe speech to text, which means that we both need a feature extractor that processes the speech signal to the model's input format, *e.g.* a feature vector, and a tokenizer that processes the model's output format to text. -->\n",
        "\n",
        "ã“ã®ã‚ˆã†ã«ğŸ¤— Transformersã§ã¯ï¼ŒéŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã«ã¯ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã¨ç‰¹å¾´æŠ½å‡ºå™¨ã®ä¸¡æ–¹ãŒä»˜å±ã—ã¦ã„ã¾ã™ã€‚\n",
        "<!-- In ğŸ¤— Transformers, speech recognition models are thus accompanied by both a tokenizer, and a feature extractor. -->\n",
        "\n",
        "ã¾ãšã¯ï¼Œãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ã‚’å¾©å·åŒ–ã™ã‚‹å½¹å‰²ã‚’æŒã¤ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ä½œã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Let's start by creating the tokenizer responsible for decoding the model's predictions. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEXEWEJGQPqD"
      },
      "source": [
        "### Wav2Vec2CTCTokenizer ã®ä½œæˆ\n",
        "<!-- ### Create Wav2Vec2CTCTokenizer -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bee4g9rpLxll"
      },
      "source": [
        "ã¾ãš [TIMITãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](https://huggingface.co/datasets/timit) ã‚’èª­ã¿è¾¼ã¿ï¼Œãã®æ§‹é€ ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Let's start by loading the [TIMIT dataset](https://huggingface.co/datasets/timit) and taking a look at its structure.-->\n",
        "\n",
        "ã‚‚ã—ï¼Œåˆ¥ã® [éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](https://huggingface.co/datasets?task_categories=task_categories:speech-processing&sort=downloads) ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã„å ´åˆã¯ï¼Œã“ã®éƒ¨åˆ†ã‚’è‡ªç”±ã«ã‚¢ãƒ¬ãƒ³ã‚¸ã—ã¦ãã ã•ã„ã€‚\n",
        "<!-- If you wish to fine-tune the model on a different [speech dataset](https://huggingface.co/datasets?task_categories=task_categories:speech-processing&sort=downloads) feel free to adapt this part. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MMXcWFFgCXU"
      },
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "timit = load_dataset(\"timit_asr\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbIM-L0xdvf4"
      },
      "source": [
        "timit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri5y5N_HMANq"
      },
      "source": [
        "å¤šãã® ASR ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ï¼Œå„éŸ³å£°  `'audioâ€™` ã‚„ãƒ•ã‚¡ã‚¤ãƒ« `'fileâ€™` ã«å¯¾ã—ã¦ï¼Œã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆ `'textâ€™` ã®ã¿ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚\n",
        "Timit ã¯å®Ÿéš›ã«ã¯å„ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦ `'phonetic_detailâ€™` ãªã©ã®ã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’æä¾›ã—ã¦ãŠã‚Šï¼Œå¤šãã®ç ”ç©¶è€…ãŒ Timit ã‚’æ‰±ã†éš›ã«ï¼ŒéŸ³å£°èªè­˜ã§ã¯ãªãéŸ³ç´ åˆ†é¡ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ã‚’é¸æŠã™ã‚‹ã®ã¯ãã®ãŸã‚ã§ã™ã€‚\n",
        "ã—ã‹ã—ï¼Œç§ãŸã¡ã¯ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ã§ãã‚‹ã ã‘ä¸€èˆ¬çš„ãªã‚‚ã®ã«ã—ãŸã„ã®ã§ï¼Œå¾®èª¿æ•´ã®ãŸã‚ã«è»¢å†™ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚\n",
        "<!-- Many ASR datasets only provide the target text, `'text'` for each audio `'audio'` and file `'file'`. \n",
        "Timit actually provides much more information about each audio file, such as the `'phonetic_detail'`, etc., which is why many researchers choose to evaluate their models on phoneme classification instead of speech recognition when working with Timit. \n",
        "However, we want to keep the notebook as general as possible, so that we will only consider the transcribed text for fine-tuning.\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbyq6lDgQc2a"
      },
      "source": [
        "timit = timit.remove_columns([\"phonetic_detail\", \"word_detail\", \"dialect_region\", \"id\", \"sentence_type\", \"speaker_id\"])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9Hq4e4NDT9"
      },
      "source": [
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ãƒ³ãƒ€ãƒ ãªã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹çŸ­ã„é–¢æ•°ã‚’æ›¸ãï¼Œãã‚Œã‚’ 2, 3 å›å®Ÿè¡Œã—ã¦ transcription (è»¢å†™) ã®æ„Ÿè§¦ã‚’ç¢ºã‹ã‚ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Let's write a short function to display some random samples of the dataset and run it a couple of times to get a feeling for the transcriptions. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72737oog2F6U"
      },
      "source": [
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    display(HTML(df.to_html()))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_JUmf3G3b9S"
      },
      "source": [
        "show_random_elements(timit[\"train\"].remove_columns([\"audio\", \"file\"]), num_examples=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fowcOllGNNju"
      },
      "source": [
        "ã‚ˆã—ã£ï¼ã¨æ€ã„ã¾ã—ãŸã€‚ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã¯ã¨ã¦ã‚‚ãã‚Œã„ã§ï¼Œè¨€èªã¯å¯¾è©±ã‚ˆã‚Šã‚‚æ›¸ã‹ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«å¯¾å¿œã—ã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ã€‚\n",
        "ã“ã‚Œã¯ [Timit](https://huggingface.co/datasets/timit_asr) ãŒèª­ã¿ä¸Šã’éŸ³å£°ã‚³ãƒ¼ãƒ‘ã‚¹ã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã™ã‚‹ã¨ç´å¾—ã§ãã¾ã™ã€‚\n",
        "<!-- Alright! The transcriptions look very clean and the language seems to correspond more to written text than dialogue. \n",
        "This makes sense taking into account that [Timit](https://huggingface.co/datasets/timit_asr) is a read speech corpus. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq7OR50LN49m"
      },
      "source": [
        "ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã«ã¯ã€`,.?!;:`ã®ã‚ˆã†ãªç‰¹æ®Šæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "è¨€èªãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã€ã“ã®ã‚ˆã†ãªç‰¹æ®Šæ–‡å­—ã¯ç‰¹å¾´çš„ãªéŸ³ã®å˜ä½ã«å¯¾å¿œã—ã¦ã„ãªã„ãŸã‚ã€ã‚¹ãƒ”ãƒ¼ãƒãƒãƒ£ãƒ³ã‚¯ã‚’åˆ†é¡ã™ã‚‹ã®ã¯éå¸¸ã«å›°é›£ã§ã™ã€‚\n",
        "ä¾‹ãˆã°ã€æ–‡å­—`\"s\"`ã¯å¤šã‹ã‚Œå°‘ãªã‹ã‚Œæ˜ç¢ºãªéŸ³ã‚’æŒã£ã¦ã„ã¾ã™ãŒã€ç‰¹æ®Šæ–‡å­—`\".\"`ã¯ãã†ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
        "ã¾ãŸã€éŸ³å£°ä¿¡å·ã®æ„å‘³ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã¯ã€é€šå¸¸ã€ç‰¹æ®Šæ–‡å­—ã‚’æ–‡å­—èµ·ã“ã—ã«å«ã‚ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
        "<!-- We can see that the transcriptions contain some special characters, such as `,.?!;:`. \n",
        "Without a language model, it is much harder to classify speech chunks to such special characters because they don't really correspond to a characteristic sound unit. \n",
        "E.g., the letter `\"s\"` has a more or less clear sound, whereas the special character `\".\"` does not.\n",
        "Also in order to understand the meaning of a speech signal, it is usually not necessary to include special characters in the transcription.-->\n",
        "\n",
        "ã•ã‚‰ã«ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’å°æ–‡å­—ã®ã¿ã«æ­£è¦åŒ–ã—ã€æœ€å¾Œã«å˜èªåŒºåˆ‡ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã—ã¾ã™ã€‚\n",
        "<!--In addition, we normalize the text to only have lower case letters and append a word separator token at the end. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svKzVJ_hQGK6"
      },
      "source": [
        "import re\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
        "    return batch"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "3fce4559ba3142378d886c2a72b5d3f4",
            "ec41ccd546d14d7db69f5e3e3a646c37",
            "2f218a5d35b24f70844bd8c50637e849",
            "4e105fc762ff44a0b854974349e9486d",
            "df754fb9c7264d43bb934a599fa1fb9d",
            "ff297d1a6e8d472ea2449f1fb9f0694b",
            "77c418a0f8c54f708f6d4c4c9b205235",
            "f55c0576188e4bfdb520911ab34e5177",
            "b693094bbebc421bb38fa2cbba369ba5",
            "74efc78f14f54552a834e6dd2098cab0",
            "13dbec546b884fa89fb36d41c9f46603",
            "a177012697974429a5f49b6c7ff061ee",
            "75d478c5208f4033b63a752a3a8bb558",
            "dec07a1af19148b786be8dffde835899",
            "a359bd4b848648919974e3021cead64c",
            "25c79405826041c4ba5e8dfc45c0b5c3",
            "11c07d44fe424794ac4d75f37e9dd232",
            "1a9427dd53454b1783a81441e291d6d5",
            "d0d999b1632142f2b606d10af46a98c0",
            "4a418a2f16e34f43bc7d9efbb30edefc",
            "20de6e2d5bfc4c68a80c3bbec54d1b0f",
            "6ba53939dc594a81a29e2b08e0c64c1d"
          ]
        },
        "id": "XIHocAuTQbBR",
        "outputId": "f1fa9c4c-cb3f-4bed-fc3e-9abd5a341b84"
      },
      "source": [
        "timit = timit.map(remove_special_characters)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fce4559ba3142378d886c2a72b5d3f4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/4620 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a177012697974429a5f49b6c7ff061ee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1680 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBDRAAYxRE6n"
      },
      "source": [
        "show_random_elements(timit[\"train\"].remove_columns([\"audio\", \"file\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwfaptH5RJwA"
      },
      "source": [
        "ã„ã„ã§ã™ã­ãƒ¼ã€‚\n",
        "ã“ã‚Œã¯è‰¯ã„æ„Ÿã˜ã§ã™ã­ã€‚\n",
        "ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰ã»ã¨ã‚“ã©ã®ç‰¹æ®Šæ–‡å­—ã‚’å–ã‚Šé™¤ãï¼Œå°æ–‡å­—ã®ã¿ã«æ­£è¦åŒ–ã—ã¾ã—ãŸã€‚\n",
        "<!-- Good! This looks better. We have removed most special characters from transcriptions and normalized them to lower-case only.-->\n",
        "\n",
        "CTC ã§ã¯ï¼ŒéŸ³å£°ã®å¡Šã‚’æ–‡å­—ã«åˆ†é¡ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ãªã®ã§ï¼Œã“ã“ã§ã‚‚åŒã˜ã‚ˆã†ã«åˆ†é¡ã—ã¾ã™ã€‚\n",
        "è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç•°ãªã‚‹æ–‡å­—ã‚’ã™ã¹ã¦æŠ½å‡ºã—ï¼Œã“ã®æ–‡å­—ã®ã‚»ãƒƒãƒˆã‹ã‚‰èªå½™ã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- In CTC, it is common to classify speech chunks into letters, so we will do the same here. \n",
        "Let's extract all distinct letters of the training and test data and build our vocabulary from this set of letters. -->\n",
        "\n",
        "ã™ã¹ã¦ã®æ›¸ãèµ·ã“ã—æ–‡å­—ã‚’1ã¤ã®é•·ã„æ›¸ãèµ·ã“ã—æ–‡å­—ã«é€£çµã—ï¼Œãã®æ–‡å­—åˆ—ã‚’æ–‡å­—ã®é›†åˆã«å¤‰æ›ã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°é–¢æ•°ã‚’æ›¸ãã¾ã™ã€‚\n",
        "`map(â€¦)` é–¢æ•°ã«å¼•æ•° `batched=True` ã‚’æ¸¡ã—ã¦ï¼Œãƒãƒƒãƒ”ãƒ³ã‚°é–¢æ•°ãŒã™ã¹ã¦ã®è»¢å†™æ–‡å­—ã«ä¸€åº¦ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚\n",
        "<!--We write a mapping function that concatenates all transcriptions into one long transcription and then transforms the string into a set of chars. \n",
        "It is important to pass the argument `batched=True` to the `map(...)` function so that the mapping function has access to all transcriptions at once. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwCshNbbeRZR"
      },
      "source": [
        "def extract_all_chars(batch):\n",
        "  all_text = \" \".join(batch[\"text\"])\n",
        "  vocab = list(set(all_text))\n",
        "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m6uUjjcfbjH"
      },
      "source": [
        "vocabs = timit.map(\n",
        "  extract_all_chars,\n",
        "  batched=True,\n",
        "  batch_size=-1,\n",
        "  keep_in_memory=True, \n",
        "  remove_columns=timit.column_names[\"train\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oVgE8RZSJNP"
      },
      "source": [
        "ã“ã“ã§è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹ã™ã¹ã¦ã®æ–‡å­—ã®çµ„åˆã‚ã›ã‚’ä½œã‚Šï¼Œãã®çµæœå¾—ã‚‰ã‚ŒãŸãƒªã‚¹ãƒˆã‚’åˆ—æŒ™å‹ã®è¾æ›¸ã«å¤‰æ›ã—ã¾ã™ã€‚\n",
        "<!-- Now, we create the union of all distinct letters in the training dataset and test dataset and convert the resulting list into an enumerated dictionary. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQfneNsmlJI0"
      },
      "source": [
        "vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0kRndSvqaKk"
      },
      "source": [
        "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
        "vocab_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOSzbvs9SXT1"
      },
      "source": [
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ã™ã¹ã¦ã®ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆãŒå«ã¾ã‚Œã¦ãŠã‚Š (ã“ã‚Œã¯é©šãã¹ãã“ã¨ã§ã¯ã‚ã‚Šã¾ã›ã‚“)ï¼Œã¾ãŸï¼Œç‰¹æ®Šæ–‡å­— `\" \"` ã¨ `'` ã‚‚æŠ½å‡ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ç‰¹æ®Šæ–‡å­—ã‚’é™¤å¤–ã—ã¦ã„ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n",
        "<!-- Cool, we see that all letters of the alphabet occur in the dataset (which is not really surprising) and we also extracted the special characters `\" \"` and `'`. Note that we did not exclude those special characters because: -->\n",
        "\n",
        "- ãƒ¢ãƒ‡ãƒ«ã¯ï¼Œå˜èªãŒã„ã¤çµ‚ã‚ã‚‹ã‹ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚\n",
        "ã•ã‚‚ãªã‘ã‚Œã°ï¼Œãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã¯å¸¸ã«æ–‡å­—ã®é€£ç¶šã¨ãªã‚Šï¼Œå˜èªã‚’äº’ã„ã«åˆ†é›¢ã™ã‚‹ã“ã¨ãŒã§ããªããªã£ã¦ã—ã¾ã„ã¾ã™ã€‚\n",
        "- è‹±èªã§ã¯ï¼Œå˜èªã‚’åŒºåˆ¥ã™ã‚‹ãŸã‚ã« `'` ã¨ã„ã†æ–‡å­—ã‚’æ®‹ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "ä¾‹ãˆã°ï¼Œ`\"it's\"` ã¨ `\"its\"` ã¯å…¨ãç•°ãªã‚‹æ„å‘³ã‚’æŒã¡ã¾ã™ã€‚\n",
        "\n",
        "<!--\n",
        "- The model has to learn to predict when a word finished or else the model prediction would always be a sequence of chars which would make it impossible to separate words from each other.\n",
        "- In English, we need to keep the `'` character to differentiate between words, *e.g.*, `\"it's\"` and `\"its\"` which have very different meanings. -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1fBRCn-TRaO"
      },
      "source": [
        "ã¾ãŸï¼Œ`â€\"` ãŒç‹¬è‡ªã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚¯ãƒ©ã‚¹ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‚’æ˜ç¢ºã«ã™ã‚‹ãŸã‚ã«ï¼Œã‚ˆã‚Šç›®ã«ã¤ãã‚„ã™ã„æ–‡å­— `|` ã‚’ä¸ãˆã¦ã„ã¾ã™ã€‚\n",
        "ã•ã‚‰ã« Timit ã®è¨“ç·´ã‚»ãƒƒãƒˆã§ã¯é­é‡ã—ãªã‹ã£ãŸæ–‡å­—ã‚’ãƒ¢ãƒ‡ãƒ«ãŒæ‰±ã†ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã« unknown ãƒˆãƒ¼ã‚¯ãƒ³ã‚‚è¿½åŠ ã—ã¾ã—ãŸã€‚\n",
        "<!-- To make it clearer that `\" \"` has its own token class, we give it a more visible character `|`. \n",
        "In addition, we also add an \"unknown\" token so that the model can later deal with characters not encountered in Timit's training set. -->\n",
        "\n",
        "æœ€å¾Œã« CTC ã® \"*blank token*â€ ã«ç›¸å½“ã™ã‚‹ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒ»ãƒˆãƒ¼ã‚¯ãƒ³ã‚‚è¿½åŠ ã—ã¾ã™ã€‚\n",
        "ã€Œãƒ–ãƒ©ãƒ³ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã€ã¯ CTC ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸­æ ¸ã‚’ãªã™è¦ç´ ã§ã™ã€‚\n",
        "è©³ã—ãã¯ã€Œã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã€[ã“ã¡ã‚‰ã®é …](https://distill.pub/2017/ctc/) ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
        "<!--\n",
        "Finally, we also add a padding token that corresponds to CTC's \"*blank token*\". \n",
        "The \"blank token\" is a core component of the CTC algorithm. \n",
        "For more information, please take a look at the \"Alignment\" section [here](https://distill.pub/2017/ctc/). -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npbIbBoLgaFX"
      },
      "source": [
        "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
        "del vocab_dict[\" \"]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znF0bNunsjbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101e18ac-b654-44cc-923f-774da489dcd8"
      },
      "source": [
        "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
        "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
        "len(vocab_dict)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFPGfet8U5sL"
      },
      "source": [
        "ã“ã‚Œã§ 30 å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãªã‚‹èªå½™ãŒå®Œæˆã—ã¾ã—ãŸã€‚\n",
        "ã¤ã¾ã‚Šï¼Œè¨“ç·´æ¸ˆã¿ã‚¹ãƒ”ãƒ¼ãƒãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¸Šã«è¿½åŠ ã™ã‚‹ç·šå½¢å±¤ã®å‡ºåŠ›æ¬¡å…ƒã¯ 30 ã«ãªã‚Šã¾ã™ã€‚\n",
        "<!-- Cool, now our vocabulary is complete and consists of 30 tokens, which means that the linear layer that we will add on top of the pretrained speech checkpoint will have an output dimension of 30. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CujRgBNVRaD"
      },
      "source": [
        "ãã‚Œã§ã¯ï¼Œèªå½™ã‚’ json ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Let's now save the vocabulary as a json file. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehyUoh9vk191"
      },
      "source": [
        "import json\n",
        "with open('vocab.json', 'w') as vocab_file:\n",
        "    json.dump(vocab_dict, vocab_file)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHJDaKlIVVim"
      },
      "source": [
        "æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ã¦ï¼Œä½œæˆã—ãŸã°ã‹ã‚Šã®èªå½™ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Ÿä½“åŒ–ã—ã¾ã™ã€‚\n",
        "æ­£ã—ã„ `tokenizer_type` ã¯ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰å–å¾—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ã‚‚ã—ï¼Œã‚³ãƒ³ãƒ•ã‚£ã‚°ã§ `tokenizer_class` ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚Œã°ï¼Œãã®å®šç¾©ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ãã†ã§ãªã‘ã‚Œã° `tokenizer_type` ã¯ `model_type` ã«å¯¾å¿œã—ã¦ã„ã‚‹ã¨ä»®å®šã—ã¾ã™ã€‚\n",
        "<!-- In a final step, we use the json file to instantiate a tokenizer object with the just created vocabulary file. \n",
        "The correct `tokenizer_type` can be retrieved from the model configuration. \n",
        "If a `tokenizer_class` is defined in the config, we can use it, else we assume the `tokenizer_type` corresponds to the `model_type`. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx5010dnDTz1"
      },
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_checkpoint)\n",
        "\n",
        "tokenizer_type = config.model_type if config.tokenizer_class is None else None\n",
        "config = config if config.tokenizer_class is not None else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhID90DZDWta"
      },
      "source": [
        "ã“ã‚Œã§ `AutoTokenizer` ã‚’ä½¿ã£ã¦ï¼Œãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’å®Ÿä½“åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ã•ã‚‰ã«ï¼Œãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã®ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¾ã™ã€‚\n",
        "<!-- Now we can instantiate a tokenizer using `AutoTokenizer`. \n",
        "Additionally, we set the tokenizer's special tokens. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xriFGEWQkO4M"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "  \"./\",\n",
        "  config=config,\n",
        "  tokenizer_type=tokenizer_type,\n",
        "  unk_token=\"[UNK]\",\n",
        "  pad_token=\"[PAD]\",\n",
        "  word_delimiter_token=\"|\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6L2EVWwIazG"
      },
      "source": [
        "ä½œæˆã—ãŸã°ã‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ï¼Œã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§å†åˆ©ç”¨ã—ãŸã„å ´åˆã«ã¯ [ğŸ¤— Hub](https://huggingface.co/) ã«`tokenizer` ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚\n",
        "ã“ã“ã§ã¯ï¼Œãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒªãƒã‚¸ãƒˆãƒªã‚’ `\"wav2vec2-base-timit-demo-colab\"` ã¨ã—ã¦ãŠãã¾ã™ã€‚\n",
        "<!-- If one wants to re-use the just created tokenizer with the fine-tuned model of this notebook, it is strongly advised to upload the `tokenizer` to the [ğŸ¤— Hub](https://huggingface.co/). \n",
        "Let's call the repo to which we will upload the files `\"wav2vec2-base-timit-demo-colab\"`: -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mbffBdxIl0M"
      },
      "source": [
        "model_checkpoint_name = model_checkpoint.split(\"/\")[-1]\n",
        "repo_name = f\"{model_checkpoint_name}-demo-colab\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmpG2ftFIu3B"
      },
      "source": [
        "ãã—ã¦ï¼Œãã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ [ğŸ¤— Hub](https://huggingface.co/)ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n",
        "<!-- and upload the tokenizer to the [ğŸ¤— Hub](https://huggingface.co/). -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScBMUz8jIxJi"
      },
      "source": [
        "tokenizer.push_to_hub(repo_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvL12DrNV4cx"
      },
      "source": [
        "ã“ã‚Œã§ `https://huggingface.co/<ã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¶å>/wav2vec2-base-timit-demo-colab`ã«ä½œæˆã•ã‚ŒãŸã°ã‹ã‚Šã®ãƒªãƒã‚¸ãƒˆãƒªã‚’è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- Great, you can see the just created repository under `https://huggingface.co/<your-username>/wav2vec2-base-timit-demo-colab` -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFmShnl7RE35"
      },
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
        "<!-- ### Preprocess Data -->\n",
        "\n",
        "ã“ã“ã¾ã§ã¯ï¼ŒéŸ³å£°ä¿¡å·ã®å®Ÿéš›ã®å€¤ã§ã¯ãªãï¼Œæ›¸ãèµ·ã“ã—ãŸã‚‚ã®ã‚’è¦‹ã¦ãã¾ã—ãŸã€‚\n",
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ `'text'`ã«åŠ ãˆã¦ï¼Œ`'fileâ€™` ã¨ `'audioâ€™` ã¨ã„ã†ã‚«ãƒ©ãƒ åãŒã‚ã‚Šã¾ã™ã€‚\n",
        "`'fileâ€™` ã«ã¯éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹ãŒå…¥ã‚Šã¾ã™ã€‚\n",
        "ã¡ã‚‡ã£ã¨è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- So far, we have not looked at the actual values of the speech signal but just the transcription. \n",
        "In addition to `'text'`, our datasets include two more column names `'file'` and `'audio'`. `'file'` states the absolute path of the audio file. Let's take a look. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTCS7W6XJ9BG"
      },
      "source": [
        "timit[\"train\"][0][\"file\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwxprOw4Nzrl"
      },
      "source": [
        "`Wav2Vec2` ã¯ 16 kHz ã® 1 æ¬¡å…ƒé…åˆ—ã®å½¢å¼ã§å…¥åŠ›ã‚’æ±‚ã‚ã¾ã™ã€‚\n",
        "ã¤ã¾ã‚Šï¼Œã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "<!-- `Wav2Vec2` expects the input in the format of a 1-dimensional array of 16 kHz. \n",
        "This means that the audio file has to be loaded and resampled.-->\n",
        "\n",
        "ã‚ã‚ŠãŒãŸã„ã“ã¨ã« `datasets`  ã¯ï¼Œåˆ— `audio` ã‚’å‘¼ã³å‡ºã™éš›ã«ï¼Œè‡ªå‹•çš„ã«ã“ã‚Œã‚’è¡Œã„ã¾ã™ã€‚\n",
        "ã§ã¯ï¼Œå®Ÿéš›ã«è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Thankfully, `datasets` does this automatically when calling the column `audio`. \n",
        " Let try it out.  -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk9QHuSsN7lf"
      },
      "source": [
        "timit[\"train\"][0][\"audio\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSBIGEiaKHMn"
      },
      "source": [
        "ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ãŒè‡ªå‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "ã“ã‚Œã¯ `datasets == 1.13.3` ã§å°å…¥ã•ã‚ŒãŸæ–°ã—ã„ [`\"Audio\"` feature](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=audio#datasets.Audio) ã®ãŠã‹ã’ã§ï¼Œå‘¼ã³å‡ºã—æ™‚ã«ã‚ªãƒ³ã‚¶ãƒ•ãƒ©ã‚¤ã§ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚\n",
        "<!-- We can see that the audio file has automatically been loaded. \n",
        "This is thanks to the new [`\"Audio\"` feature](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=audio#datasets.Audio) introduced in `datasets == 1.13.3`, which loads and resamples audio files on-the-fly upon calling.-->\n",
        "\n",
        "ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã¯ 16 kHzã«è¨­å®šã•ã‚Œã¦ãŠã‚Šï¼Œã“ã‚Œã¯`Wav2Vec2` ãŒå…¥åŠ›ã¨ã—ã¦æœŸå¾…ã™ã‚‹ã‚‚ã®ã§ã™ã€‚\n",
        "<!-- The sampling rate is set to 16kHz which is what `Wav2Vec2` expects as an input. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOckzFd4Mbzq"
      },
      "source": [
        "ãã‚Œã§ã¯ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç†è§£ã‚’æ·±ã‚ï¼ŒéŸ³å£°ãŒæ­£ã—ãèª­ã¿è¾¼ã¾ã‚ŒãŸã“ã¨ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ï¼Œã„ãã¤ã‹ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è´ã„ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Great, let's listen to a couple of audio files to better understand the dataset and verify that the audio was correctly loaded. -->\n",
        "\n",
        "**æ³¨**: *ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’ä½•åº¦ã‹ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ï¼Œç•°ãªã‚‹éŸ³å£°ã‚µãƒ³ãƒ—ãƒ«ã‚’èãã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- **Note**: *You can click the following cell a couple of times to listen to different speech samples.* -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dueM6U7Ev0OA"
      },
      "source": [
        "import IPython.display as ipd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "rand_int = random.randint(0, len(timit[\"train\"]))\n",
        "\n",
        "print(timit[\"train\"][rand_int][\"text\"])\n",
        "ipd.Audio(data=np.asarray(timit[\"train\"][rand_int][\"audio\"][\"array\"]), autoplay=True, rate=16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MaL9J2dNVtG"
      },
      "source": [
        "è©±è€…ã®è©±ã™é€Ÿåº¦ã‚„ã‚¢ã‚¯ã‚»ãƒ³ãƒˆãªã©ãŒå¤‰åŒ–ã—ã¦ã„ãã®ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "ã—ã‹ã—ï¼Œå…¨ä½“çš„ã«ã¯æ¯”è¼ƒçš„ã‚¯ãƒªã‚¢ãªéŸ³ã§éŒ²éŸ³ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "<!-- It can be heard, that the speakers change along with their speaking rate, accent, etc. Overall, the recordings sound relatively clear though, which is to be expected from a read speech corpus. -->\n",
        "\n",
        "ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãæº–å‚™ã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ï¼ŒéŸ³å£°å…¥åŠ›ã®å½¢çŠ¶ï¼Œè»¢å†™ï¼Œå¯¾å¿œã™ã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’å°åˆ·ã—ã¦æœ€çµ‚ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Let's do a final check that the data is correctly prepared, by printing the shape of the speech input, its transcription, and the corresponding sampling rate. -->\n",
        "\n",
        "**æ³¨** è¤‡æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ï¼Œä»¥ä¸‹ã®ã‚»ãƒ«ã‚’æ•°å›ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- **Note**: *You can click the following cell a couple of times to verify multiple samples.* -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Po2g7YPuRTx"
      },
      "source": [
        "rand_int = random.randint(0, len(timit[\"train\"]))\n",
        "\n",
        "print(\"Target text:\", timit[\"train\"][rand_int][\"text\"])\n",
        "print(\"Input array shape:\", np.asarray(timit[\"train\"][rand_int][\"audio\"][\"array\"]).shape)\n",
        "print(\"Sampling rate:\", timit[\"train\"][rand_int][\"audio\"][\"sampling_rate\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9teZcSwOBJ4"
      },
      "source": [
        "ã„ã„ã§ã™ã­ãƒ¼ã€‚ãƒ‡ãƒ¼ã‚¿ãŒ 1 æ¬¡å…ƒé…åˆ—ã§ã‚ã‚‹ã“ã¨ï¼Œã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãŒå¸¸ã« 16 kHzã«å¯¾å¿œã—ã¦ã„ã‚‹ã“ã¨ï¼Œå¯¾è±¡ã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒæ­£è¦åŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãªã©ï¼Œã™ã¹ã¦é †èª¿ã§ã™ã€‚\n",
        "\n",
        "æ¬¡ã¯ï¼Œãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´æŠ½å‡ºå™¨ã§ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¾ã™ã€‚\n",
        "ç‰¹å¾´æŠ½å‡ºå™¨ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ã‚‡ã†\n",
        "<!-- Good! Everything looks fine - the data is a 1-dimensional array, the sampling rate always corresponds to 16kHz, and the target text is normalized.\n",
        "\n",
        "Next, we should process the data with the model's feature extractor. Let's load the feature extractor -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFQCC3A9vz2r"
      },
      "source": [
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpJDkK8Pv4cM"
      },
      "source": [
        "ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã¨ä¸€ç·’ã« Wav2Vec2Processor ã«åŒ…ã¿è¾¼ã¿ã¾ã™ã€‚\n",
        "<!-- and wrap it into a Wav2Vec2Processor together with the tokenizer. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-EuXD_rv7FP"
      },
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfD3cVnSv9Tp"
      },
      "source": [
        "æœ€å¾Œã« `Wav2Vec2Processor` ã‚’åˆ©ç”¨ã—ã¦ï¼Œå­¦ç¿’ç”¨ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹å½¢å¼ã«ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ãã®ãŸã‚ã« Dataset ã®[`map(...)`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=map#datasets.DatasetDict.map) é–¢æ•°ã‚’åˆ©ç”¨ã—ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Finally, we can leverage `Wav2Vec2Processor` to process the data to the format expected by the model for training. \n",
        "To do so let's make use of Dataset's [`map(...)`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=map#datasets.DatasetDict.map) function.-->\n",
        "\n",
        "ã¾ãš `batch[\"audioâ€]` ã‚’å‘¼ã³å‡ºã™ã ã‘ã§ï¼Œã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼Œå†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚\n",
        "æ¬¡ã«ï¼Œèª­ã¿è¾¼ã‚“ã ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ `input_values` ã‚’æŠ½å‡ºã—ã¾ã™ã€‚ä»Šå›ã®ä¾‹ã§ã¯ `Wav2Vec2Processor` ã¯ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–ã™ã‚‹ã ã‘ã§ã™ã€‚\n",
        "ã—ã‹ã—ï¼Œä»–ã®éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã§ã¯ï¼Œã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã« [Log-Mel feature extraction](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) ã®ã‚ˆã†ãªï¼Œã‚ˆã‚Šè¤‡é›‘ãªç‰¹å¾´æŠ½å‡ºã‚’å«ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "3 ã¤ç›®ã¯ï¼Œãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’ãƒ©ãƒ™ãƒ« ID ã«ç¬¦å·åŒ–ã™ã‚‹ã“ã¨ã§ã™ã€‚\n",
        "<!-- First, we load and resample the audio data, simply by calling `batch[\"audio\"]`.\n",
        "Second, we extract the `input_values` from the loaded audio file. In our case, the `Wav2Vec2Processor` only normalizes the data. For other speech models, however, this step can include more complex feature extraction, such as [Log-Mel feature extraction](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum). \n",
        "Third, we encode the transcriptions to label ids. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJY7I0XAwe9p"
      },
      "source": [
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # batched output is \"un-batched\" to ensure mapping is correct\n",
        "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
        "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
        "    \n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
        "    return batch"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVMZhH4-nP8-"
      },
      "source": [
        "ãƒ‡ãƒ¼ã‚¿æº–å‚™æ©Ÿèƒ½ã‚’ã™ã¹ã¦ã®ä¾‹ã«é©ç”¨ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Let's apply the data preparation function to all examples. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-np9xYK-wl8q"
      },
      "source": [
        "timit = timit.map(prepare_dataset, remove_columns=timit.column_names[\"train\"], num_proc=4)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_MuJSH8nTuQ"
      },
      "source": [
        "**æ³¨**: ç¾åœ¨ `datasets` ã§ã¯ï¼Œã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã®ãƒ­ãƒ¼ãƒ‰ã¨å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã« [`torchaudio`](https://pytorch.org/audio/stable/index.html)  ã¨ [`librosa`](https://librosa.org/doc/latest/index.html) ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\n",
        "ã‚‚ã—ï¼Œç‹¬è‡ªã®ã‚³ã‚¹ãƒˆã§ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã„ãŸã„å ´åˆã¯ï¼Œ`\"pathâ€` åˆ—ã‚’åˆ©ç”¨ã— `\"audioâ€` åˆ—ã¯ç„¡è¦–ã—ã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚\n",
        "<!-- **Note**: Currently `datasets` make use of [`torchaudio`](https://pytorch.org/audio/stable/index.html) and [`librosa`](https://librosa.org/doc/latest/index.html) for audio loading and resampling. \n",
        "If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\"path\"` column instead and disregard the `\"audio\"` column. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4J0bU1WsvAg"
      },
      "source": [
        "é•·ã„å…¥åŠ›ç³»åˆ—ã¯å¤šãã®ãƒ¡ãƒ¢ãƒªã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚\n",
        "`Wav2Vec2` ã¯ `è‡ªå·±æ³¨æ„` ã«åŸºã¥ã„ã¦ã„ã‚‹ãŸã‚ï¼Œé•·ã„å…¥åŠ›ç³»åˆ—ã®å ´åˆï¼Œå¿…è¦ãªãƒ¡ãƒ¢ãƒªé‡ã¯å…¥åŠ›ã®é•·ã•ã«å¯¾ã—ã¦ 2 æ¬¡é–¢æ•°çš„ã«å¢—åŠ ã—ã¾ã™ ( [ã“ã® reddit æŠ•ç¨¿](https://www.reddit.com/r/MachineLearning/comments/genjvb/d_why_is_the_maximum_input_sequence_length_of/) ã‚’å‚ç…§ã—ã¦ãã ã•ã„)ã€‚\n",
        "ä»Šå›ã®ãƒ‡ãƒ¢ã§ã¯ï¼Œè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ 4 ç§’ä»¥ä¸Šã®ç³»åˆ—ã‚’ã™ã¹ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Long input sequences require a lot of memory. Since `Wav2Vec2` is based on `self-attention` the memory requirement scales quadratically with the input length for long input sequences (*cf.* with [this](https://www.reddit.com/r/MachineLearning/comments/genjvb/d_why_is_the_maximum_input_sequence_length_of/) reddit post). \n",
        "For this demo, let's filter all sequences that are longer than 4 seconds out of the training dataset. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqGobEPUvG3v"
      },
      "source": [
        "max_input_length_in_sec = 4.0\n",
        "timit[\"train\"] = timit[\"train\"].filter(lambda x: x < max_input_length_in_sec * processor.feature_extractor.sampling_rate, input_columns=[\"input_length\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Genil2v_Br"
      },
      "source": [
        "ç´ æ™´ã‚‰ã—ã„ï¼ã“ã‚Œã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å§‹ã‚ã‚‹æº–å‚™ãŒã§ãã¾ã—ãŸã€‚\n",
        "<!-- Awesome, now we are ready to start training! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYlQkKVoRUos"
      },
      "source": [
        "## è¨“ç·´\n",
        "<!-- ## Training -->\n",
        "\n",
        "ãƒ‡ãƒ¼ã‚¿ãŒå‡¦ç†ã•ã‚ŒãŸã®ã§ï¼Œå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨­å®šã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒã§ãã¾ã—ãŸã€‚\n",
        "ã“ã“ã§ã¯ğŸ¤—ã® [Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) ã‚’åˆ©ç”¨ã—ã¾ã™ãŒï¼Œãã®ãŸã‚ã«ã¯åŸºæœ¬çš„ã«ä»¥ä¸‹ã®ã“ã¨ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚\n",
        "<!-- The data is processed so that we are ready to start setting up the training pipeline. \n",
        "We will make use of ğŸ¤—'s [Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) for which we essentially need to do the following:-->\n",
        "\n",
        "- ãƒ‡ãƒ¼ã‚¿ã‚³ãƒªãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’å®šç¾©ã™ã‚‹ã€‚å¤šãã® NLP ãƒ¢ãƒ‡ãƒ«ã¨ã¯å¯¾ç…§çš„ã«ï¼ŒéŸ³å£°ãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ï¼Œå…¥åŠ›é•·ãŒå‡ºåŠ›é•·ã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«å¤§ãã„ã§ã™ã€‚\n",
        "*ä¾‹ãˆã°* Wav2Vec2 ã®å…¥åŠ›é•·ãŒ 50000 ã®ã‚µãƒ³ãƒ—ãƒ«ã¯ï¼Œå‡ºåŠ›é•·ãŒ 100 ä»¥ä¸‹ã¨ãªã‚Šã¾ã™ã€‚\n",
        "ã¤ã¾ã‚Šï¼Œã™ã¹ã¦ã®è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«ã¯ï¼Œãã®ãƒãƒƒãƒã®ä¸­ã§æœ€ã‚‚é•·ã„ã‚µãƒ³ãƒ—ãƒ«ã«ã®ã¿ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚Œï¼Œå…¨ä½“ã®ä¸­ã§æœ€ã‚‚é•·ã„ã‚µãƒ³ãƒ—ãƒ«ã«ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚Œãªã„ã¨ã„ã†ã“ã¨ã§ã™ã€‚\n",
        "ãã®ãŸã‚ï¼ŒéŸ³å£°ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«ã¯ï¼Œç‰¹åˆ¥ãªãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚³ãƒ¬ãƒ¼ã‚¿ãŒå¿…è¦ã¨ãªã‚Šã¾ã™ãŒï¼Œã“ã‚Œã‚’ä»¥ä¸‹ã«å®šç¾©ã—ã¾ã™ã€‚\n",
        "\n",
        "<!-- - Define a data collator. In contrast to most NLP models, speech models usually have a much larger input length than output length. \n",
        "*E.g.*, a sample of input length 50000 for Wav2Vec2 has an output length of no more than 100. \n",
        "Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. \n",
        "Therefore, fine-tuning speech models requires a special padding data collator, which we will define below -->\n",
        "\n",
        "- è©•ä¾¡æŒ‡æ¨™ã€‚\n",
        "å­¦ç¿’ä¸­ï¼Œãƒ¢ãƒ‡ãƒ«ã¯å˜èªã®èª¤ã‚Šç‡ã§è©•ä¾¡ã•ã‚Œã‚‹ã¹ãã§ã™ã€‚\n",
        "ãã®ãŸã‚ã« `compute_metrics` é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚\n",
        "\n",
        "<!-- - Evaluation metric. \n",
        "During training, the model should be evaluated on the word error rate. \n",
        "We should define a `compute_metrics` function accordingly -->\n",
        "\n",
        "- å­¦ç¿’æ¸ˆã¿ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€ã€‚\n",
        "å­¦ç¿’å‰ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ï¼Œå­¦ç¿’ç”¨ã«æ­£ã—ãè¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "<!--  - Load a pretrained checkpoint. \n",
        "We need to load a pretrained checkpoint and configure it correctly for training. -->\n",
        "\n",
        "- å­¦ç¿’è¨­å®šã®å®šç¾©ã€‚\n",
        "\n",
        "<!-- - Define the training configuration. -->\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸå¾Œã¯ï¼Œãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’æ­£ã—ãè©•ä¾¡ã—ï¼ŒéŸ³å£°ã‚’æ­£ã—ãæ›¸ãå–ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚\n",
        "<!-- After having fine-tuned the model, we will correctly evaluate it on the test data and verify that it has indeed learned to correctly transcribe speech. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slk403unUS91"
      },
      "source": [
        "###  ãƒˆãƒ¬ãƒ¼ãƒŠã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
        "<!-- ### Set-up Trainer -->\n",
        "\n",
        "ã¾ãšï¼Œãƒ‡ãƒ¼ã‚¿ã‚³ãƒªãƒ¬ãƒ¼ã‚¿ã®å®šç¾©ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n",
        "ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ã®ã‚³ãƒ¼ãƒ‰ã¯ [ã“ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰](https://github.com/huggingface/transformers/blob/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3/examples/research_projects/wav2vec2/run_asr.py#L81) ã‹ã‚‰ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸã€‚\n",
        "<!-- Let's start by defining the data collator. The code for the data collator was copied from [this example](https://github.com/huggingface/transformers/blob/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3/examples/research_projects/wav2vec2/run_asr.py#L81). -->\n",
        "\n",
        "è©³ç´°ã¯çœãã¾ã™ãŒï¼Œä¸€èˆ¬çš„ãªãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ã¨ã¯ç•°ãªã‚Šï¼Œã“ã®ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ã¯ `input_values` ã¨ `labels` ã‚’åˆ¥å€‹ã«æ‰±ã„ï¼Œãã‚Œã‚‰ã«å¯¾ã—ã¦åˆ¥ã€…ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é–¢æ•°ã‚’é©ç”¨ã—ã¾ã™ã€‚\n",
        "ã“ã‚Œã¯ï¼ŒéŸ³å£°ã®å…¥åŠ›ã¨å‡ºåŠ›ã¯ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã§ã‚ã‚‹ãŸã‚ï¼ŒåŒã˜ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é–¢æ•°ã§å‡¦ç†ã™ã¹ãã§ã¯ãªã„ã¨ã„ã†ç†ç”±ã‹ã‚‰ã§ã™ã€‚\n",
        "ä¸€èˆ¬çš„ãªãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ã¨åŒæ§˜ï¼Œãƒ©ãƒ™ãƒ«å†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ `-100` ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ï¼Œã“ã‚Œã‚‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯æå¤±ã‚’è¨ˆç®—ã™ã‚‹éš›ã« **è€ƒæ…®ã•ã‚Œãªã„**  ã“ã¨ã«ãªã‚Šã¾ã™ã€‚\n",
        "<!-- Without going into too many details, in contrast to the common data collators, this data collator treats the `input_values` and `labels` differently and thus applies to separate padding functions on them. \n",
        "This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function.\n",
        "Analogous to the common data collators, the padding tokens in the labels with `-100` so that those tokens are **not** taken into account when computing the loss. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tborvC9hx88e"
      },
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbQf5GuZyQ4_"
      },
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO-Zdj-5cxXp"
      },
      "source": [
        "æ¬¡ã«ï¼Œè©•ä¾¡æŒ‡æ¨™ã‚’å®šç¾©ã—ã¾ã™ã€‚\n",
        "å‰è¿°ã—ãŸã‚ˆã†ã« ASR ã§ã¯ WER (å˜èªèª¤ã‚Šç‡) ãŒä¸»æµã§ã‚ã‚‹ãŸã‚ï¼Œã“ã®ãƒãƒ¼ãƒˆã§ã‚‚ WER ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "<!-- Next, the evaluation metric is defined. As mentioned earlier, the predominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xsux2gmyXso"
      },
      "source": [
        "wer_metric = load_metric(\"wer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1qZU5p-deqB"
      },
      "source": [
        "ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç¢ºç‡æ¯”ãƒ™ã‚¯ãƒˆãƒ«ã®ç³»åˆ—ã‚’è¿”ã—ã¾ã™ã€‚\n",
        "ã“ã“ã§  $\\mathbf{y}_1 = f_{theta}(x_{1}, \\ldots, x_{n})[0]$ ã¨ã—ï¼Œ$n >> m$ ã¨ã™ã‚‹ã¨ï¼Œ$\\mathbf{y}_{1}=f_{\\theta}(x_{1},\\dots,x_{n})[0]$  ã¨ãªã‚Šã¾ã™ã€‚\n",
        "<!-- The model will return a sequence of logit vectors:\n",
        "$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.-->\n",
        "\n",
        "ç¢ºç‡æ¯”ãƒ™ã‚¯ãƒˆãƒ« $\\mathbf{y} _{1}$ ã«ã¯ï¼Œå…ˆã»ã©å®šç¾©ã—ãŸèªå½™ã®å„å˜èªã®å¯¾æ•°ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã®ã§ï¼Œ$\\text{len}(\\mathbf{y}_{i}) =$ `config.vocab_size` ã¨ãªã‚Šã¾ã™ã€‚\n",
        "ãƒ¢ãƒ‡ãƒ«ã®æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„äºˆæ¸¬å€¤ã«èˆˆå‘³ãŒã‚ã‚‹ã®ã§ï¼Œç¢ºç‡æ¯”ã®`argmax(...)`ã‚’å–ã‚Šã¾ã™ã€‚\n",
        "ã¾ãŸï¼Œç¬¦å·åŒ–ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‚’å…ƒã®æ–‡å­—åˆ—ã«æˆ»ã™ãŸã‚ã« `-100` ã‚’`pad_token_id` ã«ç½®ãæ›ãˆï¼ŒCTC ã‚¹ã‚¿ã‚¤ãƒ«ã® ${}^1$ã§ï¼Œé€£ç¶šã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ãŒåŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã•ã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèªã—ãªãŒã‚‰ï¼Œids ã‚’å¾©å·åŒ–ã—ã¾ã™ã€‚\n",
        "<!-- A logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. \n",
        "We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. \n",
        "Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XZ-kjweyTy_"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmgrx4bRwLIH"
      },
      "source": [
        "ã“ã‚Œã§ï¼Œäº‹å‰è¨“ç·´ã•ã‚ŒãŸ `Wav2Vec2` ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã® `pad_token_id` ã¯ï¼Œãƒ¢ãƒ‡ãƒ«ã® `pad_token_id` ã‚’å®šç¾©ã™ã‚‹ã‚‚ã®ã§ãªã‘ã‚Œã°ãªã‚‰ãšï¼ŒCTC éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã«ã¯ CTC ã® *blank token* ${}^2$ ã‚‚å®šç¾©ã—ã¾ã™ã€‚\n",
        "<!-- Now, we can load the pretrained `Wav2Vec2` checkpoint. \n",
        "The tokenizer's `pad_token_id` must be to define the model's `pad_token_id` or in the case of a CTC speech model also CTC's *blank token* ${}^2$. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7cqAWIayn6w"
      },
      "source": [
        "from transformers import AutoModelForCTC\n",
        "\n",
        "model = AutoModelForCTC.from_pretrained(\n",
        "    model_checkpoint, \n",
        "    ctc_loss_reduction=\"mean\", \n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PDAoxYzwb-U"
      },
      "source": [
        "ã»ã¨ã‚“ã©ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ™ãƒ¼ã‚¹ã®éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã®æœ€åˆã®è¦ç´ ã¯ï¼Œç”Ÿã®éŸ³å£°ä¿¡å·ã‹ã‚‰éŸ³éŸ¿çš„ã«æ„å‘³ã®ã‚ã‚‹ï¼Œã—ã‹ã—æ–‡è„ˆçš„ã«ã¯ç‹¬ç«‹ã—ãŸç‰¹å¾´ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ CNN å±¤ã®ã‚¹ã‚¿ãƒƒã‚¯ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "ãƒ¢ãƒ‡ãƒ«ã®ã“ã®éƒ¨åˆ†ã¯äº‹å‰è¨“ç·´ã§ã™ã§ã«ååˆ†ã«è¨“ç·´ã•ã‚Œã¦ãŠã‚Šï¼Œ[è«–æ–‡](https://arxiv.org/pdf/2006.13979.pdf) ã§è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ï¼Œã“ã‚Œä»¥ä¸Šã®å¾®èª¿æ•´ã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
        "ãã®ãŸã‚ï¼Œ*ç‰¹å¾´æŠ½å‡º* éƒ¨åˆ†ã®ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦ï¼Œ`requires_grad` ã‚’ `False` ã«è¨­å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- The first component of most transformer-based speech models consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. \n",
        "This part of the model has already been sufficiently trained during pretraining and as stated in the [paper](https://arxiv.org/pdf/2006.13979.pdf) does not need to be fine-tuned anymore. \n",
        "Thus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD4aGhQM0K-D"
      },
      "source": [
        "æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ï¼Œè¨“ç·´ã«é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™ã€‚\n",
        "ã„ãã¤ã‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¾ã™ã€‚\n",
        "<!-- In a final step, we define all parameters related to training. \n",
        "To give more explanation on some of the parameters:-->\n",
        "\n",
        "- `group_by_length` ã¯ï¼Œå…¥åŠ›ã®é•·ã•ãŒä¼¼ã¦ã„ã‚‹å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«ã‚’ 1 ã¤ã®ãƒãƒƒãƒã«ã¾ã¨ã‚ã‚‹ã“ã¨ã§ï¼Œå­¦ç¿’ã®åŠ¹ç‡åŒ–ã‚’å›³ã‚Šã¾ã™ã€‚\n",
        "ã“ã‚Œã«ã‚ˆã‚Šï¼Œãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã•ã‚Œã‚‹ç„¡é§„ãªãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®æ•°ã‚’å¤§å¹…ã«æ¸›ã‚‰ã™ã“ã¨ãŒã§ãï¼Œå­¦ç¿’æ™‚é–“ã‚’å¤§å¹…ã«çŸ­ç¸®ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "\n",
        "<!-- - `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model -->\n",
        "\n",
        "- `learning_rate` ã¨ `weight_decay` ã¯ ï¼Œå¾®èª¿æ•´ãŒå®‰å®šã™ã‚‹ã¾ã§ï¼Œãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¾ã—ãŸã€‚\n",
        "ãªãŠï¼Œã“ã‚Œã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ Timit ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¼·ãä¾å­˜ã—ã¦ãŠã‚Šï¼Œä»–ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯æœ€é©ã§ã¯ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "<!--\n",
        "- `learning_rate` and `weight_decay` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Timit dataset and might be suboptimal for other speech datasets.-->\n",
        "\n",
        "ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã®è©³ã—ã„èª¬æ˜ã¯ [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
        "<!-- For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments). -->\n",
        "\n",
        "å­¦ç¿’ä¸­ã¯ 400 ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒãƒãƒ–ã«éåŒæœŸã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚\n",
        "ã“ã‚Œã«ã‚ˆã‚Šï¼Œãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ä¸­ã§ã‚‚ï¼Œãƒ‡ãƒ¢ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’ä½¿ã£ã¦éŠã¶ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- During training, a checkpoint will be uploaded asynchronously to the hub every 400 training steps. \n",
        "It allows you to also play around with the demo widget even while your model is still training. -->\n",
        "\n",
        "**æ³¨æ„** ã‚‚ã—ï¼Œãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸããªã„å ´åˆã¯ï¼Œå˜ç´”ã« `push_to_hub=False` ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚\n",
        "<!-- \n",
        "**Note**: If one does not want to upload the model checkpoints to the hub, simply set `push_to_hub=False`. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbeKSV7uzGPP"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=repo_name,\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=32,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=30,\n",
        "  fp16=True,\n",
        "  gradient_checkpointing=True,\n",
        "  save_steps=500,\n",
        "  eval_steps=500,\n",
        "  logging_steps=500,\n",
        "  learning_rate=1e-4,\n",
        "  weight_decay=0.005,\n",
        "  warmup_steps=1000,\n",
        "  save_total_limit=2,\n",
        "  push_to_hub=True,\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsW-WZcL1ZtN"
      },
      "source": [
        "ã“ã‚Œã§ï¼Œã™ã¹ã¦ã®å®Ÿä½“ ãŒ Trainer ã«æ¸¡ã•ã‚Œï¼Œè¨“ç·´ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚\n",
        "<!-- Now, all instances can be passed to Trainer and we are ready to start training! -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY7vBmFCPFgC"
      },
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=timit[\"train\"],\n",
        "    eval_dataset=timit[\"test\"],\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoXBx1JAA0DX"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "${}^1$ CTC ã§ã¯ï¼Œãƒ¢ãƒ‡ãƒ«ãŒè©±è€…ã®å‰²åˆã«ä¾å­˜ã—ãªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã«ï¼Œé€£ç¶šã—ãŸåŒä¸€ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯å˜ç´”ã« 1 ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã•ã‚Œã¾ã™ã€‚\n",
        "ã—ã‹ã—ï¼Œç¬¦å·åŒ–ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã¯ï¼Œãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾å¿œã—ã¦ã„ãªã„ãŸã‚ï¼Œãƒ‡ã‚³ãƒ¼ãƒ‰æ™‚ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã•ã‚Œã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
        "ã“ã‚ŒãŒã€`group_tokens=False`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã•ãªã‘ã‚Œã°ãªã‚‰ãªã„ç†ç”±ã§ã™ã€‚\n",
        "ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã•ãªã‘ã‚Œã° `\"helloâ€` ã®ã‚ˆã†ãªå˜èªã¯èª¤ã£ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œ `\"heloâ€` ã¨ã—ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã—ã¾ã„ã¾ã™ã€‚\n",
        "<!-- \n",
        "${}^1$ To allow models to become independent of the speaker rate, in CTC, consecutive tokens that are identical are simply grouped as a single token. \n",
        "However, the encoded labels should not be grouped when decoding since they don't correspond to the predicted tokens of the model, which is why the `group_tokens=False` parameter has to be passed. \n",
        "If we wouldn't pass this parameter a word like `\"hello\"` would incorrectly be encoded, and decoded as `\"helo\"`.-->\n",
        "\n",
        "${}^2$ ç©ºç™½ãƒˆãƒ¼ã‚¯ãƒ³ã¯ï¼Œ2 ã¤ã® l ã®é–“ã«ç©ºç™½ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å¼·åˆ¶çš„ã«æŒ¿å…¥ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šï¼Œãƒ¢ãƒ‡ãƒ«ã« `\"helloâ€` ã®ã‚ˆã†ãªå˜èªã‚’äºˆæ¸¬ã•ã›ã¾ã™ã€‚\n",
        "æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ `\"helloâ€` ã® CTC é©åˆäºˆæ¸¬ã¯ `[PAD] [PAD] \"h\" \"e\" \"e\" \"l\" [PAD] \"l\" \"o\" \"o\" [PAD]` ã¨ãªã‚Šã¾ã™ã€‚\n",
        "<!-- ${}^2$ The blank token allows the model to predict a word, such as `\"hello\"` by forcing it to insert the blank token between the two l's. \n",
        "A CTC-conform prediction of `\"hello\"` of our model would be `[PAD] [PAD] \"h\" \"e\" \"e\" \"l\" \"l\" [PAD] \"l\" \"o\" \"o\" [PAD]`. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpvZHM1xReIW"
      },
      "source": [
        "### è¨“ç·´ <!--### Training-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-3oKSzZ1hGq"
      },
      "source": [
        "è¨“ç·´ã¯ï¼Œã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸ GPU ã«å¿œã˜ã¦ 2, 3 æ™‚é–“ã‹ã‹ã‚Šã¾ã™ã€‚\n",
        "<!-- Training will take a couple of hours depending on the GPU allocated to this notebook. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "_UEjJqGsQw24",
        "outputId": "5ec2d080-79b7-46c2-c72a-5f84d4cb5b9b"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.\n",
            "***** Running training *****\n",
            "  Num examples = 3978\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3750\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/feature_extraction_utils.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  tensor = as_tensor(value)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:882: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  return (input_length - kernel_size) // stride + 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='181' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 181/3750 04:44 < 1:34:29, 0.63 it/s, Epoch 1.44/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCyp-v3n4Zlt"
      },
      "source": [
        "æœ€çµ‚çš„ãª WER ã¯ 0.3 ç¨‹åº¦ã«ãªã‚‹ã¯ãšã§ã™ã€‚\n",
        "ã“ã‚Œã¯ï¼Œæœ€å…ˆç«¯ã®éŸ³ç´ èª¤ã‚Šç‡ (PER) ãŒ 0.1 ã‚’ã‚ãšã‹ã«ä¸‹å›ã‚‹ã“ã¨ï¼ˆ[leaderboard](https://paperswithcode.com/sota/speech-recognition-on-timit) å‚ç…§) ã‚„ï¼ŒWER ã¯é€šå¸¸ PER ã‚ˆã‚Šã‚‚æ‚ªã„ã“ã¨ã‚’è€ƒãˆã‚‹ã¨å¦¥å½“ãªå€¤ã§ã™ã€‚\n",
        "<!-- The final WER should be around 0.3 which is reasonable given that state-of-the-art phoneme error rates (PER) are just below 0.1 (see [leaderboard](https://paperswithcode.com/sota/speech-recognition-on-timit)) and that WER is usually worse than PER.-->\n",
        "\n",
        "è¨“ç·´çµæœã‚’ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ï¼Œä»¥ä¸‹ã®å‘½ä»¤ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
        "<!-- \n",
        "You can now upload the result of the training to the Hub, just execute this instruction: -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYGQYBhHNsvj"
      },
      "source": [
        "trainer.push_to_hub()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djzwS5WeNu16"
      },
      "source": [
        "ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å‹äººã‚„å®¶æ—ï¼ŒãŠæ°—ã«å…¥ã‚Šã®ãƒšãƒƒãƒˆã¨å…±æœ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ä¾‹ãˆã° ã€Œã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¶å/ã‚ãªãŸãŒé¸ã‚“ã åå‰ã€ã¨ã„ã†è­˜åˆ¥å­ã§ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier \"your-username/the-name-you-picked\" so for instance: -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adm0LngNNxq7"
      },
      "source": [
        "```python\n",
        "from transformers import AutoModelForCTC, Wav2Vec2Processor\n",
        "\n",
        "model = AutoModelForCTC.from_pretrained(\"patrickvonplaten/wav2vec2-base-timit-demo-colab\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"patrickvonplaten/wav2vec2-base-timit-demo-colab\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw0_ygXSw_MT"
      },
      "source": [
        "CTC æå¤±ã‚’ä½¿ã£ã¦ã‚ˆã‚Šå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ï¼Œå…¬å¼ã®éŸ³å£°èªè­˜ã®ä¾‹ [ã“ã¡ã‚‰](https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition#connectionist-temporal-classification-without-language-model-ctc-wo-lm) ã‚’è¦‹ã¦ã¿ã‚‹ã¨ã‚ˆã„ã§ã—ã‚‡ã†ğŸ¤—ã€‚\n",
        "<!-- To fine-tune larger models on larger datasets using CTC loss, one should take a look at the official speech-recognition examples [here](https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition#connectionist-temporal-classification-without-language-model-ctc-wo-lm) ğŸ¤—. -->"
      ]
    }
  ]
}