{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022_0111Fine-Tune_Speech_Recognition_Model_with_Transformers_using_CTC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3fce4559ba3142378d886c2a72b5d3f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ec41ccd546d14d7db69f5e3e3a646c37",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2f218a5d35b24f70844bd8c50637e849",
              "IPY_MODEL_4e105fc762ff44a0b854974349e9486d",
              "IPY_MODEL_df754fb9c7264d43bb934a599fa1fb9d"
            ]
          }
        },
        "ec41ccd546d14d7db69f5e3e3a646c37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f218a5d35b24f70844bd8c50637e849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ff297d1a6e8d472ea2449f1fb9f0694b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77c418a0f8c54f708f6d4c4c9b205235"
          }
        },
        "4e105fc762ff44a0b854974349e9486d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f55c0576188e4bfdb520911ab34e5177",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4620,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4620,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b693094bbebc421bb38fa2cbba369ba5"
          }
        },
        "df754fb9c7264d43bb934a599fa1fb9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_74efc78f14f54552a834e6dd2098cab0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4620/4620 [00:00&lt;00:00, 11209.09ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13dbec546b884fa89fb36d41c9f46603"
          }
        },
        "ff297d1a6e8d472ea2449f1fb9f0694b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77c418a0f8c54f708f6d4c4c9b205235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f55c0576188e4bfdb520911ab34e5177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b693094bbebc421bb38fa2cbba369ba5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "74efc78f14f54552a834e6dd2098cab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13dbec546b884fa89fb36d41c9f46603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a177012697974429a5f49b6c7ff061ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_75d478c5208f4033b63a752a3a8bb558",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dec07a1af19148b786be8dffde835899",
              "IPY_MODEL_a359bd4b848648919974e3021cead64c",
              "IPY_MODEL_25c79405826041c4ba5e8dfc45c0b5c3"
            ]
          }
        },
        "75d478c5208f4033b63a752a3a8bb558": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dec07a1af19148b786be8dffde835899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_11c07d44fe424794ac4d75f37e9dd232",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1a9427dd53454b1783a81441e291d6d5"
          }
        },
        "a359bd4b848648919974e3021cead64c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d0d999b1632142f2b606d10af46a98c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1680,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1680,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a418a2f16e34f43bc7d9efbb30edefc"
          }
        },
        "25c79405826041c4ba5e8dfc45c0b5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_20de6e2d5bfc4c68a80c3bbec54d1b0f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1680/1680 [00:00&lt;00:00, 9817.41ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ba53939dc594a81a29e2b08e0c64c1d"
          }
        },
        "11c07d44fe424794ac4d75f37e9dd232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1a9427dd53454b1783a81441e291d6d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0d999b1632142f2b606d10af46a98c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a418a2f16e34f43bc7d9efbb30edefc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "20de6e2d5bfc4c68a80c3bbec54d1b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ba53939dc594a81a29e2b08e0c64c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0111Fine_Tune_Speech_Recognition_Model_with_Transformers_using_CTC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBSYoWbi-45k"
      },
      "source": [
        "# **Fine-tuning Speech Model with 🤗 Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWj4gag6uH2I"
      },
      "source": [
        "このノートでは，自動音声認識のための多言語の事前学習済み音声モデルを微調整する方法を紹介します。\n",
        "<!-- This notebook shows how to fine-tune multi-lingual pretrained speech models for Automatic Speech Recognition. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnXLL0QhuMBD"
      },
      "source": [
        "このノートブックは [TIMITデータセット](https://huggingface.co/datasets/timit) 上で，[モデルハブ Model Hub](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads) から任意の音声モデルのチェックポイントを実行するように構築されています。\n",
        "そのモデルに CTC (Connectionist Temporal Classification) ヘッドを搭載したバージョンがある場合に限ります。\n",
        "モデルや使用している GPU によっては，メモリ不足のエラーを回避するために，バッチサイズを調整する必要があるかもしれません。\n",
        "この 2 つのパラメータを設定すれば，あとはノートブックがスムーズに動作するはずです。\n",
        "<!-- This notebook is built to run on the [TIMIT dataset](https://huggingface.co/datasets/timit) with any speech model checkpoint from the [Model Hub](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads) as long as that model has a version with a Connectionist Temporal Classification (CTC) head. \n",
        "Depending on the model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. \n",
        "Set those two parameters, then the rest of the notebook should run smoothly: -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7CftmzCuKYl"
      },
      "source": [
        "model_checkpoint = \"facebook/wav2vec2-base\"\n",
        "batch_size = 32"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTSa1gvYuUX0"
      },
      "source": [
        "多言語訓練済音声モデルがどのように機能するのか，より詳細な説明については [🤗ブログ](https://huggingface.co/blog/fine-tune-wav2vec2-english) をご覧ください。\n",
        "<!-- For a more in-detail explanation of how multi-lingual pretrained speech models function, please take a look at the [🤗 Blog](https://huggingface.co/blog/fine-tune-wav2vec2-english). -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e335hPmdtASZ"
      },
      "source": [
        "始める前に，マスターから `datasets` と `transformers` の両方をインストールしましょう。 \n",
        "また，音声ファイルを読み込むためには `librosa` パッケージが必要で，微調整されたモデルを [word error rate (WER)](https://huggingface.co/metrics/wer) メトリック ${}^1$ を用いて評価するためには `jiwer` が必要です。\n",
        "<!-- Before we start, let's install both `datasets` and `transformers` from master. Also, we need the `librosa` package to load audio files and the `jiwer` to evaluate our fine-tuned model using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric ${}^1$. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8eh87Hoee5d"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets==1.14\n",
        "!pip install transformers==4.11.3\n",
        "!pip install librosa\n",
        "!pip install jiwer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_6kYmDMH9lR"
      },
      "source": [
        "次に，訓練中に，訓練チェックポイントを [🤗 Hub](https://huggingface.co/) に直接アップロードすることを強くお勧めします。\n",
        "[🤗 Hub](https://huggingface.co/) にはバージョン管理が統合されているので，訓練中にモデルのチェックポイントが失われていないことを確認することができます。\n",
        "\n",
        "これを行うには，Hugging Face ウェブサイトから認証トークンを保存する必要があります (まだ登録していない場合は [こちらから](https://huggingface.co/join) で登録してください！)。\n",
        "<!-- Next we strongly suggest to upload your training checkpoints directly to the [🤗 Hub](https://huggingface.co/) while training. \n",
        "The [🤗 Hub](https://huggingface.co/) has integrated version control so you can be sure that no model checkpoint is getting lost during training. \n",
        "\n",
        "To do so you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFLBDyzQIA3R"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCyw5D23IQ1F"
      },
      "source": [
        "そして，モデルのチェックポイントをアップロードするために，Git-LFS をインストールする必要があります。\n",
        "<!-- Then you need to install Git-LFS to upload your model checkpoints: -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9BnQDhOITBC"
      },
      "source": [
        "%%capture\n",
        "!apt install git-lfs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn9swf6EQ9Vd"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "${}^1$ Timitは通常，音素誤り率 (PER) を用いて評価されますが ASR で最も一般的な指標は圧倒的に単語誤り率 (WER) です。\n",
        "このノートをできるだけ一般的なものにするために，WER を使ってモデルを評価することにしました。\n",
        "<!-- ${}^1$ Timit is usually evaluated using the phoneme error rate (PER), but by far the most common metric in ASR is the word error rate (WER). \n",
        "To keep this notebook as general as possible we decided to evaluate the model using WER. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mW-C1Nt-j7k"
      },
      "source": [
        "## データの準備，トークン化器，特徴抽出器\n",
        "<!-- ## Prepare Data, Tokenizer, Feature Extractor -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeBosnY9BH3e"
      },
      "source": [
        "ASR モデルは音声をテキストに変換するため，音声信号をモデルの入力フォーマット (例: 特徴ベクトル) に処理する特徴抽出器と，モデルの出力フォーマットをテキストに処理するトークン化器の両方が必要となります。\n",
        "<!-- ASR models transcribe speech to text, which means that we both need a feature extractor that processes the speech signal to the model's input format, *e.g.* a feature vector, and a tokenizer that processes the model's output format to text. -->\n",
        "\n",
        "このように🤗 Transformersでは，音声認識モデルにはトークン化器と特徴抽出器の両方が付属しています。\n",
        "<!-- In 🤗 Transformers, speech recognition models are thus accompanied by both a tokenizer, and a feature extractor. -->\n",
        "\n",
        "まずは，モデルの予測値を復号化する役割を持つトークン化器を作ってみましょう。\n",
        "<!-- Let's start by creating the tokenizer responsible for decoding the model's predictions. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEXEWEJGQPqD"
      },
      "source": [
        "### Wav2Vec2CTCTokenizer の作成\n",
        "<!-- ### Create Wav2Vec2CTCTokenizer -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bee4g9rpLxll"
      },
      "source": [
        "まず [TIMITデータセット](https://huggingface.co/datasets/timit) を読み込み，その構造を見てみましょう。\n",
        "<!-- Let's start by loading the [TIMIT dataset](https://huggingface.co/datasets/timit) and taking a look at its structure.-->\n",
        "\n",
        "もし，別の [音声データセット](https://huggingface.co/datasets?task_categories=task_categories:speech-processing&sort=downloads) でモデルを微調整したい場合は，この部分を自由にアレンジしてください。\n",
        "<!-- If you wish to fine-tune the model on a different [speech dataset](https://huggingface.co/datasets?task_categories=task_categories:speech-processing&sort=downloads) feel free to adapt this part. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MMXcWFFgCXU"
      },
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "timit = load_dataset(\"timit_asr\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbIM-L0xdvf4"
      },
      "source": [
        "timit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri5y5N_HMANq"
      },
      "source": [
        "多くの ASR データセットでは，各音声  `'audio’` やファイル `'file’` に対して，ターゲットとなるテキスト `'text’` のみを提供しています。\n",
        "Timit は実際には各オーディオファイルについて `'phonetic_detail’` などのより多くの情報を提供しており，多くの研究者が Timit を扱う際に，音声認識ではなく音素分類でモデルを評価することを選択するのはそのためです。\n",
        "しかし，私たちはノートブックをできるだけ一般的なものにしたいので，微調整のために転写されたテキストのみを考慮することになります。\n",
        "<!-- Many ASR datasets only provide the target text, `'text'` for each audio `'audio'` and file `'file'`. \n",
        "Timit actually provides much more information about each audio file, such as the `'phonetic_detail'`, etc., which is why many researchers choose to evaluate their models on phoneme classification instead of speech recognition when working with Timit. \n",
        "However, we want to keep the notebook as general as possible, so that we will only consider the transcribed text for fine-tuning.\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbyq6lDgQc2a"
      },
      "source": [
        "timit = timit.remove_columns([\"phonetic_detail\", \"word_detail\", \"dialect_region\", \"id\", \"sentence_type\", \"speaker_id\"])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9Hq4e4NDT9"
      },
      "source": [
        "データセットのランダムなサンプルを表示する短い関数を書き，それを 2, 3 回実行して transcription (転写) の感触を確かめてみましょう。\n",
        "<!-- Let's write a short function to display some random samples of the dataset and run it a couple of times to get a feeling for the transcriptions. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72737oog2F6U"
      },
      "source": [
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    display(HTML(df.to_html()))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_JUmf3G3b9S"
      },
      "source": [
        "show_random_elements(timit[\"train\"].remove_columns([\"audio\", \"file\"]), num_examples=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fowcOllGNNju"
      },
      "source": [
        "よしっ！と思いました。トランスクリプションはとてもきれいで，言語は対話よりも書かれたテキストに対応しているように見えます。\n",
        "これは [Timit](https://huggingface.co/datasets/timit_asr) が読み上げ音声コーパスであることを考慮すると納得できます。\n",
        "<!-- Alright! The transcriptions look very clean and the language seems to correspond more to written text than dialogue. \n",
        "This makes sense taking into account that [Timit](https://huggingface.co/datasets/timit_asr) is a read speech corpus. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq7OR50LN49m"
      },
      "source": [
        "トランスクリプションには、`,.?!;:`のような特殊文字が含まれていることがわかります。\n",
        "言語モデルがない場合、このような特殊文字は特徴的な音の単位に対応していないため、スピーチチャンクを分類するのは非常に困難です。\n",
        "例えば、文字`\"s\"`は多かれ少なかれ明確な音を持っていますが、特殊文字`\".\"`はそうではありません。\n",
        "また、音声信号の意味を理解するためには、通常、特殊文字を文字起こしに含める必要はありません。\n",
        "<!-- We can see that the transcriptions contain some special characters, such as `,.?!;:`. \n",
        "Without a language model, it is much harder to classify speech chunks to such special characters because they don't really correspond to a characteristic sound unit. \n",
        "E.g., the letter `\"s\"` has a more or less clear sound, whereas the special character `\".\"` does not.\n",
        "Also in order to understand the meaning of a speech signal, it is usually not necessary to include special characters in the transcription.-->\n",
        "\n",
        "さらに、テキストを小文字のみに正規化し、最後に単語区切りのトークンを追加します。\n",
        "<!--In addition, we normalize the text to only have lower case letters and append a word separator token at the end. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svKzVJ_hQGK6"
      },
      "source": [
        "import re\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
        "    return batch"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "3fce4559ba3142378d886c2a72b5d3f4",
            "ec41ccd546d14d7db69f5e3e3a646c37",
            "2f218a5d35b24f70844bd8c50637e849",
            "4e105fc762ff44a0b854974349e9486d",
            "df754fb9c7264d43bb934a599fa1fb9d",
            "ff297d1a6e8d472ea2449f1fb9f0694b",
            "77c418a0f8c54f708f6d4c4c9b205235",
            "f55c0576188e4bfdb520911ab34e5177",
            "b693094bbebc421bb38fa2cbba369ba5",
            "74efc78f14f54552a834e6dd2098cab0",
            "13dbec546b884fa89fb36d41c9f46603",
            "a177012697974429a5f49b6c7ff061ee",
            "75d478c5208f4033b63a752a3a8bb558",
            "dec07a1af19148b786be8dffde835899",
            "a359bd4b848648919974e3021cead64c",
            "25c79405826041c4ba5e8dfc45c0b5c3",
            "11c07d44fe424794ac4d75f37e9dd232",
            "1a9427dd53454b1783a81441e291d6d5",
            "d0d999b1632142f2b606d10af46a98c0",
            "4a418a2f16e34f43bc7d9efbb30edefc",
            "20de6e2d5bfc4c68a80c3bbec54d1b0f",
            "6ba53939dc594a81a29e2b08e0c64c1d"
          ]
        },
        "id": "XIHocAuTQbBR",
        "outputId": "f1fa9c4c-cb3f-4bed-fc3e-9abd5a341b84"
      },
      "source": [
        "timit = timit.map(remove_special_characters)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fce4559ba3142378d886c2a72b5d3f4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/4620 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a177012697974429a5f49b6c7ff061ee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1680 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBDRAAYxRE6n"
      },
      "source": [
        "show_random_elements(timit[\"train\"].remove_columns([\"audio\", \"file\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwfaptH5RJwA"
      },
      "source": [
        "いいですねー。\n",
        "これは良い感じですね。\n",
        "トランスクリプションからほとんどの特殊文字を取り除き，小文字のみに正規化しました。\n",
        "<!-- Good! This looks better. We have removed most special characters from transcriptions and normalized them to lower-case only.-->\n",
        "\n",
        "CTC では，音声の塊を文字に分類するのが一般的なので，ここでも同じように分類します。\n",
        "訓練データとテストデータの異なる文字をすべて抽出し，この文字のセットから語彙を構築しましょう。\n",
        "<!-- In CTC, it is common to classify speech chunks into letters, so we will do the same here. \n",
        "Let's extract all distinct letters of the training and test data and build our vocabulary from this set of letters. -->\n",
        "\n",
        "すべての書き起こし文字を1つの長い書き起こし文字に連結し，その文字列を文字の集合に変換するマッピング関数を書きます。\n",
        "`map(…)` 関数に引数 `batched=True` を渡して，マッピング関数がすべての転写文字に一度にアクセスできるようにすることが重要です。\n",
        "<!--We write a mapping function that concatenates all transcriptions into one long transcription and then transforms the string into a set of chars. \n",
        "It is important to pass the argument `batched=True` to the `map(...)` function so that the mapping function has access to all transcriptions at once. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwCshNbbeRZR"
      },
      "source": [
        "def extract_all_chars(batch):\n",
        "  all_text = \" \".join(batch[\"text\"])\n",
        "  vocab = list(set(all_text))\n",
        "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m6uUjjcfbjH"
      },
      "source": [
        "vocabs = timit.map(\n",
        "  extract_all_chars,\n",
        "  batched=True,\n",
        "  batch_size=-1,\n",
        "  keep_in_memory=True, \n",
        "  remove_columns=timit.column_names[\"train\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oVgE8RZSJNP"
      },
      "source": [
        "ここで訓練データセットとテストデータセットに含まれるすべての文字の組合わせを作り，その結果得られたリストを列挙型の辞書に変換します。\n",
        "<!-- Now, we create the union of all distinct letters in the training dataset and test dataset and convert the resulting list into an enumerated dictionary. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQfneNsmlJI0"
      },
      "source": [
        "vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0kRndSvqaKk"
      },
      "source": [
        "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
        "vocab_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOSzbvs9SXT1"
      },
      "source": [
        "データセットにはすべてのアルファベットが含まれており (これは驚くべきことではありません)，また，特殊文字 `\" \"` と `'` も抽出されています。これらの特殊文字を除外していないことに注意してください。\n",
        "<!-- Cool, we see that all letters of the alphabet occur in the dataset (which is not really surprising) and we also extracted the special characters `\" \"` and `'`. Note that we did not exclude those special characters because: -->\n",
        "\n",
        "- モデルは，単語がいつ終わるかを予測することを学習しなければなりません。\n",
        "さもなければ，モデルの予測は常に文字の連続となり，単語を互いに分離することができなくなってしまいます。\n",
        "- 英語では，単語を区別するために `'` という文字を残しておく必要があります。\n",
        "例えば，`\"it's\"` と `\"its\"` は全く異なる意味を持ちます。\n",
        "\n",
        "<!--\n",
        "- The model has to learn to predict when a word finished or else the model prediction would always be a sequence of chars which would make it impossible to separate words from each other.\n",
        "- In English, we need to keep the `'` character to differentiate between words, *e.g.*, `\"it's\"` and `\"its\"` which have very different meanings. -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1fBRCn-TRaO"
      },
      "source": [
        "また，`”\"` が独自のトークンクラスを持っていることを明確にするために，より目につきやすい文字 `|` を与えています。\n",
        "さらに Timit の訓練セットでは遭遇しなかった文字をモデルが扱うことができるように unknown トークンも追加しました。\n",
        "<!-- To make it clearer that `\" \"` has its own token class, we give it a more visible character `|`. \n",
        "In addition, we also add an \"unknown\" token so that the model can later deal with characters not encountered in Timit's training set. -->\n",
        "\n",
        "最後に CTC の \"*blank token*” に相当するパディング・トークンも追加します。\n",
        "「ブランクトークン」は CTC アルゴリズムの中核をなす要素です。\n",
        "詳しくは「アライメント」[こちらの項](https://distill.pub/2017/ctc/) をご覧ください。\n",
        "<!--\n",
        "Finally, we also add a padding token that corresponds to CTC's \"*blank token*\". \n",
        "The \"blank token\" is a core component of the CTC algorithm. \n",
        "For more information, please take a look at the \"Alignment\" section [here](https://distill.pub/2017/ctc/). -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npbIbBoLgaFX"
      },
      "source": [
        "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
        "del vocab_dict[\" \"]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znF0bNunsjbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101e18ac-b654-44cc-923f-774da489dcd8"
      },
      "source": [
        "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
        "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
        "len(vocab_dict)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFPGfet8U5sL"
      },
      "source": [
        "これで 30 個のトークンからなる語彙が完成しました。\n",
        "つまり，訓練済みスピーチチェックポイントの上に追加する線形層の出力次元は 30 になります。\n",
        "<!-- Cool, now our vocabulary is complete and consists of 30 tokens, which means that the linear layer that we will add on top of the pretrained speech checkpoint will have an output dimension of 30. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CujRgBNVRaD"
      },
      "source": [
        "それでは，語彙を json ファイルとして保存してみましょう。\n",
        "<!-- Let's now save the vocabulary as a json file. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehyUoh9vk191"
      },
      "source": [
        "import json\n",
        "with open('vocab.json', 'w') as vocab_file:\n",
        "    json.dump(vocab_dict, vocab_file)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHJDaKlIVVim"
      },
      "source": [
        "最後のステップでは json ファイルを使って，作成したばかりの語彙ファイルを使ってトークン化器オブジェクトを実体化します。\n",
        "正しい `tokenizer_type` はモデルのコンフィグレーションから取得することができます。\n",
        "もし，コンフィグで `tokenizer_class` が定義されていれば，その定義を使用することができます。\n",
        "そうでなければ `tokenizer_type` は `model_type` に対応していると仮定します。\n",
        "<!-- In a final step, we use the json file to instantiate a tokenizer object with the just created vocabulary file. \n",
        "The correct `tokenizer_type` can be retrieved from the model configuration. \n",
        "If a `tokenizer_class` is defined in the config, we can use it, else we assume the `tokenizer_type` corresponds to the `model_type`. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx5010dnDTz1"
      },
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_checkpoint)\n",
        "\n",
        "tokenizer_type = config.model_type if config.tokenizer_class is None else None\n",
        "config = config if config.tokenizer_class is not None else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhID90DZDWta"
      },
      "source": [
        "これで `AutoTokenizer` を使って，トークン化器を実体化することができます。\n",
        "さらに，トークン化器の特別なトークンを設定します。\n",
        "<!-- Now we can instantiate a tokenizer using `AutoTokenizer`. \n",
        "Additionally, we set the tokenizer's special tokens. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xriFGEWQkO4M"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "  \"./\",\n",
        "  config=config,\n",
        "  tokenizer_type=tokenizer_type,\n",
        "  unk_token=\"[UNK]\",\n",
        "  pad_token=\"[PAD]\",\n",
        "  word_delimiter_token=\"|\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6L2EVWwIazG"
      },
      "source": [
        "作成したばかりのトークン化器を，このノートブックの微調整されたモデルで再利用したい場合には [🤗 Hub](https://huggingface.co/) に`tokenizer` をアップロードすることを強くお勧めします。\n",
        "ここでは，ファイルをアップロードするリポジトリを `\"wav2vec2-base-timit-demo-colab\"` としておきます。\n",
        "<!-- If one wants to re-use the just created tokenizer with the fine-tuned model of this notebook, it is strongly advised to upload the `tokenizer` to the [🤗 Hub](https://huggingface.co/). \n",
        "Let's call the repo to which we will upload the files `\"wav2vec2-base-timit-demo-colab\"`: -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mbffBdxIl0M"
      },
      "source": [
        "model_checkpoint_name = model_checkpoint.split(\"/\")[-1]\n",
        "repo_name = f\"{model_checkpoint_name}-demo-colab\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmpG2ftFIu3B"
      },
      "source": [
        "そして，そのトークン化器を [🤗 Hub](https://huggingface.co/)にアップロードします。\n",
        "<!-- and upload the tokenizer to the [🤗 Hub](https://huggingface.co/). -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScBMUz8jIxJi"
      },
      "source": [
        "tokenizer.push_to_hub(repo_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvL12DrNV4cx"
      },
      "source": [
        "これで `https://huggingface.co/<あなたのユーザ名>/wav2vec2-base-timit-demo-colab`に作成されたばかりのリポジトリを見ることができます。\n",
        "<!-- Great, you can see the just created repository under `https://huggingface.co/<your-username>/wav2vec2-base-timit-demo-colab` -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFmShnl7RE35"
      },
      "source": [
        "### データの前処理\n",
        "<!-- ### Preprocess Data -->\n",
        "\n",
        "ここまでは，音声信号の実際の値ではなく，書き起こしたものを見てきました。\n",
        "データセットには `'text'`に加えて，`'file’` と `'audio’` というカラム名があります。\n",
        "`'file’` には音声ファイルの絶対パスが入ります。\n",
        "ちょっと見てみましょう。\n",
        "<!-- So far, we have not looked at the actual values of the speech signal but just the transcription. \n",
        "In addition to `'text'`, our datasets include two more column names `'file'` and `'audio'`. `'file'` states the absolute path of the audio file. Let's take a look. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTCS7W6XJ9BG"
      },
      "source": [
        "timit[\"train\"][0][\"file\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwxprOw4Nzrl"
      },
      "source": [
        "`Wav2Vec2` は 16 kHz の 1 次元配列の形式で入力を求めます。\n",
        "つまり，オーディオファイルを読み込んでリサンプリングする必要があります。\n",
        "<!-- `Wav2Vec2` expects the input in the format of a 1-dimensional array of 16 kHz. \n",
        "This means that the audio file has to be loaded and resampled.-->\n",
        "\n",
        "ありがたいことに `datasets`  は，列 `audio` を呼び出す際に，自動的にこれを行います。\n",
        "では，実際に試してみましょう。\n",
        "<!-- Thankfully, `datasets` does this automatically when calling the column `audio`. \n",
        " Let try it out.  -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk9QHuSsN7lf"
      },
      "source": [
        "timit[\"train\"][0][\"audio\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSBIGEiaKHMn"
      },
      "source": [
        "オーディオファイルが自動的に読み込まれていることがわかります。\n",
        "これは `datasets == 1.13.3` で導入された新しい [`\"Audio\"` feature](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=audio#datasets.Audio) のおかげで，呼び出し時にオンザフライでオーディオファイルをロードして再サンプリングします。\n",
        "<!-- We can see that the audio file has automatically been loaded. \n",
        "This is thanks to the new [`\"Audio\"` feature](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=audio#datasets.Audio) introduced in `datasets == 1.13.3`, which loads and resamples audio files on-the-fly upon calling.-->\n",
        "\n",
        "サンプリングレートは 16 kHzに設定されており，これは`Wav2Vec2` が入力として期待するものです。\n",
        "<!-- The sampling rate is set to 16kHz which is what `Wav2Vec2` expects as an input. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOckzFd4Mbzq"
      },
      "source": [
        "それでは，データセットの理解を深め，音声が正しく読み込まれたことを確認するために，いくつかの音声ファイルを聴いてみましょう。\n",
        "<!-- Great, let's listen to a couple of audio files to better understand the dataset and verify that the audio was correctly loaded. -->\n",
        "\n",
        "**注**: *以下のセルを何度かクリックすると，異なる音声サンプルを聞くことができます。\n",
        "<!-- **Note**: *You can click the following cell a couple of times to listen to different speech samples.* -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dueM6U7Ev0OA"
      },
      "source": [
        "import IPython.display as ipd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "rand_int = random.randint(0, len(timit[\"train\"]))\n",
        "\n",
        "print(timit[\"train\"][rand_int][\"text\"])\n",
        "ipd.Audio(data=np.asarray(timit[\"train\"][rand_int][\"audio\"][\"array\"]), autoplay=True, rate=16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MaL9J2dNVtG"
      },
      "source": [
        "話者の話す速度やアクセントなどが変化していくのがわかります。\n",
        "しかし，全体的には比較的クリアな音で録音されています。\n",
        "<!-- It can be heard, that the speakers change along with their speaking rate, accent, etc. Overall, the recordings sound relatively clear though, which is to be expected from a read speech corpus. -->\n",
        "\n",
        "データが正しく準備されているかどうか，音声入力の形状，転写，対応するサンプリングレートを印刷して最終確認してみましょう。\n",
        "<!-- Let's do a final check that the data is correctly prepared, by printing the shape of the speech input, its transcription, and the corresponding sampling rate. -->\n",
        "\n",
        "**注** 複数のサンプルを確認するために，以下のセルを数回クリックすることができます。\n",
        "<!-- **Note**: *You can click the following cell a couple of times to verify multiple samples.* -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Po2g7YPuRTx"
      },
      "source": [
        "rand_int = random.randint(0, len(timit[\"train\"]))\n",
        "\n",
        "print(\"Target text:\", timit[\"train\"][rand_int][\"text\"])\n",
        "print(\"Input array shape:\", np.asarray(timit[\"train\"][rand_int][\"audio\"][\"array\"]).shape)\n",
        "print(\"Sampling rate:\", timit[\"train\"][rand_int][\"audio\"][\"sampling_rate\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9teZcSwOBJ4"
      },
      "source": [
        "いいですねー。データが 1 次元配列であること，サンプリングレートが常に 16 kHzに対応していること，対象となるテキストが正規化されていることなど，すべて順調です。\n",
        "\n",
        "次は，モデルの特徴抽出器でデータを処理します。\n",
        "特徴抽出器をロードしましょう\n",
        "<!-- Good! Everything looks fine - the data is a 1-dimensional array, the sampling rate always corresponds to 16kHz, and the target text is normalized.\n",
        "\n",
        "Next, we should process the data with the model's feature extractor. Let's load the feature extractor -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFQCC3A9vz2r"
      },
      "source": [
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpJDkK8Pv4cM"
      },
      "source": [
        "トークン化器と一緒に Wav2Vec2Processor に包み込みます。\n",
        "<!-- and wrap it into a Wav2Vec2Processor together with the tokenizer. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-EuXD_rv7FP"
      },
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfD3cVnSv9Tp"
      },
      "source": [
        "最後に `Wav2Vec2Processor` を利用して，学習用モデルが期待する形式にデータを処理することができます。\n",
        "そのために Dataset の[`map(...)`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=map#datasets.DatasetDict.map) 関数を利用しましょう。\n",
        "<!-- Finally, we can leverage `Wav2Vec2Processor` to process the data to the format expected by the model for training. \n",
        "To do so let's make use of Dataset's [`map(...)`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=map#datasets.DatasetDict.map) function.-->\n",
        "\n",
        "まず `batch[\"audio”]` を呼び出すだけで，オーディオデータを読み込み，再サンプリングします。\n",
        "次に，読み込んだオーディオファイルから `input_values` を抽出します。今回の例では `Wav2Vec2Processor` はデータを正規化するだけです。\n",
        "しかし，他の音声モデルでは，このステップに [Log-Mel feature extraction](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) のような，より複雑な特徴抽出を含めることができます。\n",
        "3 つ目は，トランスクリプションをラベル ID に符号化することです。\n",
        "<!-- First, we load and resample the audio data, simply by calling `batch[\"audio\"]`.\n",
        "Second, we extract the `input_values` from the loaded audio file. In our case, the `Wav2Vec2Processor` only normalizes the data. For other speech models, however, this step can include more complex feature extraction, such as [Log-Mel feature extraction](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum). \n",
        "Third, we encode the transcriptions to label ids. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJY7I0XAwe9p"
      },
      "source": [
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # batched output is \"un-batched\" to ensure mapping is correct\n",
        "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
        "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
        "    \n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
        "    return batch"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVMZhH4-nP8-"
      },
      "source": [
        "データ準備機能をすべての例に適用してみましょう。\n",
        "<!-- Let's apply the data preparation function to all examples. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-np9xYK-wl8q"
      },
      "source": [
        "timit = timit.map(prepare_dataset, remove_columns=timit.column_names[\"train\"], num_proc=4)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_MuJSH8nTuQ"
      },
      "source": [
        "**注**: 現在 `datasets` では，オーディオのロードと再サンプリングに [`torchaudio`](https://pytorch.org/audio/stable/index.html)  と [`librosa`](https://librosa.org/doc/latest/index.html) を使用しています。\n",
        "もし，独自のコストでデータの読み込みやサンプリングを行いたい場合は，`\"path”` 列を利用し `\"audio”` 列は無視しても構いません。\n",
        "<!-- **Note**: Currently `datasets` make use of [`torchaudio`](https://pytorch.org/audio/stable/index.html) and [`librosa`](https://librosa.org/doc/latest/index.html) for audio loading and resampling. \n",
        "If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\"path\"` column instead and disregard the `\"audio\"` column. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4J0bU1WsvAg"
      },
      "source": [
        "長い入力系列は多くのメモリを必要とします。\n",
        "`Wav2Vec2` は `自己注意` に基づいているため，長い入力系列の場合，必要なメモリ量は入力の長さに対して 2 次関数的に増加します ( [この reddit 投稿](https://www.reddit.com/r/MachineLearning/comments/genjvb/d_why_is_the_maximum_input_sequence_length_of/) を参照してください)。\n",
        "今回のデモでは，訓練データセットから 4 秒以上の系列をすべてフィルタリングしてみましょう。\n",
        "<!-- Long input sequences require a lot of memory. Since `Wav2Vec2` is based on `self-attention` the memory requirement scales quadratically with the input length for long input sequences (*cf.* with [this](https://www.reddit.com/r/MachineLearning/comments/genjvb/d_why_is_the_maximum_input_sequence_length_of/) reddit post). \n",
        "For this demo, let's filter all sequences that are longer than 4 seconds out of the training dataset. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqGobEPUvG3v"
      },
      "source": [
        "max_input_length_in_sec = 4.0\n",
        "timit[\"train\"] = timit[\"train\"].filter(lambda x: x < max_input_length_in_sec * processor.feature_extractor.sampling_rate, input_columns=[\"input_length\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Genil2v_Br"
      },
      "source": [
        "素晴らしい！これでトレーニングを始める準備ができました。\n",
        "<!-- Awesome, now we are ready to start training! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYlQkKVoRUos"
      },
      "source": [
        "## 訓練\n",
        "<!-- ## Training -->\n",
        "\n",
        "データが処理されたので，学習パイプラインの設定を開始する準備ができました。\n",
        "ここでは🤗の [Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) を利用しますが，そのためには基本的に以下のことが必要になります。\n",
        "<!-- The data is processed so that we are ready to start setting up the training pipeline. \n",
        "We will make use of 🤗's [Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) for which we essentially need to do the following:-->\n",
        "\n",
        "- データコリレーターを定義する。多くの NLP モデルとは対照的に，音声モデルは通常，入力長が出力長よりもはるかに大きいです。\n",
        "*例えば* Wav2Vec2 の入力長が 50000 のサンプルは，出力長が 100 以下となります。\n",
        "つまり，すべての訓練サンプルは，そのバッチの中で最も長いサンプルにのみパディングされ，全体の中で最も長いサンプルにはパディングされないということです。\n",
        "そのため，音声モデルの微調整には，特別なパディング・データ・コレータが必要となりますが，これを以下に定義します。\n",
        "\n",
        "<!-- - Define a data collator. In contrast to most NLP models, speech models usually have a much larger input length than output length. \n",
        "*E.g.*, a sample of input length 50000 for Wav2Vec2 has an output length of no more than 100. \n",
        "Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. \n",
        "Therefore, fine-tuning speech models requires a special padding data collator, which we will define below -->\n",
        "\n",
        "- 評価指標。\n",
        "学習中，モデルは単語の誤り率で評価されるべきです。\n",
        "そのために `compute_metrics` 関数を定義します。\n",
        "\n",
        "<!-- - Evaluation metric. \n",
        "During training, the model should be evaluated on the word error rate. \n",
        "We should define a `compute_metrics` function accordingly -->\n",
        "\n",
        "- 学習済みのチェックポイントを読み込む。\n",
        "学習前のチェックポイントをロードして，学習用に正しく設定する必要があります。\n",
        "\n",
        "<!--  - Load a pretrained checkpoint. \n",
        "We need to load a pretrained checkpoint and configure it correctly for training. -->\n",
        "\n",
        "- 学習設定の定義。\n",
        "\n",
        "<!-- - Define the training configuration. -->\n",
        "\n",
        "モデルを微調整した後は，テストデータでモデルを正しく評価し，音声を正しく書き取ることができるようになったことを確認します。\n",
        "<!-- After having fine-tuned the model, we will correctly evaluate it on the test data and verify that it has indeed learned to correctly transcribe speech. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slk403unUS91"
      },
      "source": [
        "###  トレーナのセットアップ\n",
        "<!-- ### Set-up Trainer -->\n",
        "\n",
        "まず，データコリレータの定義を説明します。\n",
        "データコレータのコードは [このサンプルコード](https://github.com/huggingface/transformers/blob/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3/examples/research_projects/wav2vec2/run_asr.py#L81) からコピーしました。\n",
        "<!-- Let's start by defining the data collator. The code for the data collator was copied from [this example](https://github.com/huggingface/transformers/blob/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3/examples/research_projects/wav2vec2/run_asr.py#L81). -->\n",
        "\n",
        "詳細は省きますが，一般的なデータコレータとは異なり，このデータコレータは `input_values` と `labels` を別個に扱い，それらに対して別々のパディング関数を適用します。\n",
        "これは，音声の入力と出力は異なるモダリティであるため，同じパディング関数で処理すべきではないという理由からです。\n",
        "一般的なデータコレータと同様，ラベル内のトークンを `-100` でパディングすることで，これらのトークンは損失を計算する際に **考慮されない**  ことになります。\n",
        "<!-- Without going into too many details, in contrast to the common data collators, this data collator treats the `input_values` and `labels` differently and thus applies to separate padding functions on them. \n",
        "This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function.\n",
        "Analogous to the common data collators, the padding tokens in the labels with `-100` so that those tokens are **not** taken into account when computing the loss. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tborvC9hx88e"
      },
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbQf5GuZyQ4_"
      },
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO-Zdj-5cxXp"
      },
      "source": [
        "次に，評価指標を定義します。\n",
        "前述したように ASR では WER (単語誤り率) が主流であるため，このノートでも WER を使用します。\n",
        "<!-- Next, the evaluation metric is defined. As mentioned earlier, the predominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xsux2gmyXso"
      },
      "source": [
        "wer_metric = load_metric(\"wer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1qZU5p-deqB"
      },
      "source": [
        "このモデルは確率比ベクトルの系列を返します。\n",
        "ここで  $\\mathbf{y}_1 = f_{theta}(x_{1}, \\ldots, x_{n})[0]$ とし，$n >> m$ とすると，$\\mathbf{y}_{1}=f_{\\theta}(x_{1},\\dots,x_{n})[0]$  となります。\n",
        "<!-- The model will return a sequence of logit vectors:\n",
        "$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.-->\n",
        "\n",
        "確率比ベクトル $\\mathbf{y} _{1}$ には，先ほど定義した語彙の各単語の対数が含まれているので，$\\text{len}(\\mathbf{y}_{i}) =$ `config.vocab_size` となります。\n",
        "モデルの最も可能性の高い予測値に興味があるので，確率比の`argmax(...)`を取ります。\n",
        "また，符号化されたラベルを元の文字列に戻すために `-100` を`pad_token_id` に置き換え，CTC スタイルの ${}^1$で，連続したトークンが同じトークンにグループ化されていないことを確認しながら，ids を復号化します。\n",
        "<!-- A logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. \n",
        "We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. \n",
        "Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XZ-kjweyTy_"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmgrx4bRwLIH"
      },
      "source": [
        "これで，事前訓練された `Wav2Vec2` チェックポイントをロードすることができます。\n",
        "トークン化器の `pad_token_id` は，モデルの `pad_token_id` を定義するものでなければならず，CTC 音声モデルの場合には CTC の *blank token* ${}^2$ も定義します。\n",
        "<!-- Now, we can load the pretrained `Wav2Vec2` checkpoint. \n",
        "The tokenizer's `pad_token_id` must be to define the model's `pad_token_id` or in the case of a CTC speech model also CTC's *blank token* ${}^2$. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7cqAWIayn6w"
      },
      "source": [
        "from transformers import AutoModelForCTC\n",
        "\n",
        "model = AutoModelForCTC.from_pretrained(\n",
        "    model_checkpoint, \n",
        "    ctc_loss_reduction=\"mean\", \n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PDAoxYzwb-U"
      },
      "source": [
        "ほとんどのトランスフォーマーベースの音声モデルの最初の要素は，生の音声信号から音響的に意味のある，しかし文脈的には独立した特徴を抽出するために使用される CNN 層のスタックで構成されています。\n",
        "モデルのこの部分は事前訓練ですでに十分に訓練されており，[論文](https://arxiv.org/pdf/2006.13979.pdf) で述べられているように，これ以上の微調整は必要ありません。\n",
        "そのため，*特徴抽出* 部分のすべてのパラメータについて，`requires_grad` を `False` に設定することができます。\n",
        "<!-- The first component of most transformer-based speech models consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. \n",
        "This part of the model has already been sufficiently trained during pretraining and as stated in the [paper](https://arxiv.org/pdf/2006.13979.pdf) does not need to be fine-tuned anymore. \n",
        "Thus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD4aGhQM0K-D"
      },
      "source": [
        "最後のステップでは，訓練に関連するすべてのパラメータを定義します。\n",
        "いくつかのパラメータについて詳しく説明します。\n",
        "<!-- In a final step, we define all parameters related to training. \n",
        "To give more explanation on some of the parameters:-->\n",
        "\n",
        "- `group_by_length` は，入力の長さが似ている学習サンプルを 1 つのバッチにまとめることで，学習の効率化を図ります。\n",
        "これにより，モデルに渡される無駄なパディングトークンの数を大幅に減らすことができ，学習時間を大幅に短縮することができます。\n",
        "\n",
        "<!-- - `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model -->\n",
        "\n",
        "- `learning_rate` と `weight_decay` は ，微調整が安定するまで，ヒューリスティックにチューニングされました。\n",
        "なお，これらのパラメータは Timit データセットに強く依存しており，他の音声データセットでは最適ではない可能性があります。\n",
        "\n",
        "<!--\n",
        "- `learning_rate` and `weight_decay` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Timit dataset and might be suboptimal for other speech datasets.-->\n",
        "\n",
        "他のパラメータについての詳しい説明は [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments) を参照してください。\n",
        "<!-- For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments). -->\n",
        "\n",
        "学習中は 400 ステップごとにチェックポイントがハブに非同期でアップロードされます。\n",
        "これにより，モデルの訓練中でも，デモウィジェットを使って遊ぶことができます。\n",
        "<!-- During training, a checkpoint will be uploaded asynchronously to the hub every 400 training steps. \n",
        "It allows you to also play around with the demo widget even while your model is still training. -->\n",
        "\n",
        "**注意** もし，モデルのチェックポイントをハブにアップロードしたくない場合は，単純に `push_to_hub=False` を設定してください。\n",
        "<!-- \n",
        "**Note**: If one does not want to upload the model checkpoints to the hub, simply set `push_to_hub=False`. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbeKSV7uzGPP"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=repo_name,\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=32,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=30,\n",
        "  fp16=True,\n",
        "  gradient_checkpointing=True,\n",
        "  save_steps=500,\n",
        "  eval_steps=500,\n",
        "  logging_steps=500,\n",
        "  learning_rate=1e-4,\n",
        "  weight_decay=0.005,\n",
        "  warmup_steps=1000,\n",
        "  save_total_limit=2,\n",
        "  push_to_hub=True,\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsW-WZcL1ZtN"
      },
      "source": [
        "これで，すべての実体 が Trainer に渡され，訓練を開始する準備が整いました。\n",
        "<!-- Now, all instances can be passed to Trainer and we are ready to start training! -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY7vBmFCPFgC"
      },
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=timit[\"train\"],\n",
        "    eval_dataset=timit[\"test\"],\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoXBx1JAA0DX"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "${}^1$ CTC では，モデルが話者の割合に依存しないようにするために，連続した同一のトークンは単純に 1 つのトークンとしてグループ化されます。\n",
        "しかし，符号化されたラベルは，モデルの予測トークンに対応していないため，デコード時にグループ化されるべきではありません。\n",
        "これが、`group_tokens=False`パラメータを渡さなければならない理由です。\n",
        "このパラメータを渡さなければ `\"hello”` のような単語は誤ってエンコードされ `\"helo”` としてデコードされてしまいます。\n",
        "<!-- \n",
        "${}^1$ To allow models to become independent of the speaker rate, in CTC, consecutive tokens that are identical are simply grouped as a single token. \n",
        "However, the encoded labels should not be grouped when decoding since they don't correspond to the predicted tokens of the model, which is why the `group_tokens=False` parameter has to be passed. \n",
        "If we wouldn't pass this parameter a word like `\"hello\"` would incorrectly be encoded, and decoded as `\"helo\"`.-->\n",
        "\n",
        "${}^2$ 空白トークンは，2 つの l の間に空白トークンを強制的に挿入することにより，モデルに `\"hello”` のような単語を予測させます。\n",
        "我々のモデルによる `\"hello”` の CTC 適合予測は `[PAD] [PAD] \"h\" \"e\" \"e\" \"l\" [PAD] \"l\" \"o\" \"o\" [PAD]` となります。\n",
        "<!-- ${}^2$ The blank token allows the model to predict a word, such as `\"hello\"` by forcing it to insert the blank token between the two l's. \n",
        "A CTC-conform prediction of `\"hello\"` of our model would be `[PAD] [PAD] \"h\" \"e\" \"e\" \"l\" \"l\" [PAD] \"l\" \"o\" \"o\" [PAD]`. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpvZHM1xReIW"
      },
      "source": [
        "### 訓練 <!--### Training-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-3oKSzZ1hGq"
      },
      "source": [
        "訓練は，このノートブックに割り当てられた GPU に応じて 2, 3 時間かかります。\n",
        "<!-- Training will take a couple of hours depending on the GPU allocated to this notebook. -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "_UEjJqGsQw24",
        "outputId": "5ec2d080-79b7-46c2-c72a-5f84d4cb5b9b"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.\n",
            "***** Running training *****\n",
            "  Num examples = 3978\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3750\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/feature_extraction_utils.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  tensor = as_tensor(value)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:882: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  return (input_length - kernel_size) // stride + 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='181' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 181/3750 04:44 < 1:34:29, 0.63 it/s, Epoch 1.44/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCyp-v3n4Zlt"
      },
      "source": [
        "最終的な WER は 0.3 程度になるはずです。\n",
        "これは，最先端の音素誤り率 (PER) が 0.1 をわずかに下回ること（[leaderboard](https://paperswithcode.com/sota/speech-recognition-on-timit) 参照) や，WER は通常 PER よりも悪いことを考えると妥当な値です。\n",
        "<!-- The final WER should be around 0.3 which is reasonable given that state-of-the-art phoneme error rates (PER) are just below 0.1 (see [leaderboard](https://paperswithcode.com/sota/speech-recognition-on-timit)) and that WER is usually worse than PER.-->\n",
        "\n",
        "訓練結果をハブにアップロードするには，以下の命令を実行します。\n",
        "<!-- \n",
        "You can now upload the result of the training to the Hub, just execute this instruction: -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYGQYBhHNsvj"
      },
      "source": [
        "trainer.push_to_hub()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djzwS5WeNu16"
      },
      "source": [
        "このモデルを友人や家族，お気に入りのペットと共有することができます。\n",
        "例えば 「あなたのユーザ名/あなたが選んだ名前」という識別子でロードすることができます。\n",
        "<!-- You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier \"your-username/the-name-you-picked\" so for instance: -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adm0LngNNxq7"
      },
      "source": [
        "```python\n",
        "from transformers import AutoModelForCTC, Wav2Vec2Processor\n",
        "\n",
        "model = AutoModelForCTC.from_pretrained(\"patrickvonplaten/wav2vec2-base-timit-demo-colab\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"patrickvonplaten/wav2vec2-base-timit-demo-colab\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw0_ygXSw_MT"
      },
      "source": [
        "CTC 損失を使ってより大きなデータセットでより大きなモデルを微調整するには，公式の音声認識の例 [こちら](https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition#connectionist-temporal-classification-without-language-model-ctc-wo-lm) を見てみるとよいでしょう🤗。\n",
        "<!-- To fine-tune larger models on larger datasets using CTC loss, one should take a look at the official speech-recognition examples [here](https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition#connectionist-temporal-classification-without-language-model-ctc-wo-lm) 🤗. -->"
      ]
    }
  ]
}