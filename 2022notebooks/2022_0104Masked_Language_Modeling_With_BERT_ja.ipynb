{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0104Masked_Language_Modeling_With_BERT_ja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "457ed60a-a3d7-42ee-8640-f1a87d8ad87b",
      "metadata": {
        "id": "457ed60a-a3d7-42ee-8640-f1a87d8ad87b"
      },
      "source": [
        "- date: 2022_0104\n",
        "- source: https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c\n",
        "- github: https://github.com/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0104Masked_Language_Modeling_With_BERT_ja.ipynb\n",
        "\n",
        "- author: James Briggs\n",
        "- date_origin: May 19, 2021\n",
        "\n",
        "# Masked-Language Modeling With BERT\n",
        "\n",
        "みんなのお気に入りの変換ツールである BERT は ，Google が学習するのに約 7,000 ドルかかりました [1]。\n",
        "しかし，私たちは，同じモデルを使用するために数行のコードを書くだけで，すべて無料で利用できます。\n",
        "<!-- BERT, everyone’s favorite transformer costs Google ~\\$7K to train [1] (and who knows how much in R&D costs). \n",
        "From there, we write a couple of lines of code to use the same model — all for free. -->\n",
        "\n",
        "BERT は，マスクド・言語モデル (MLM) と次文予測 (NSP) という  2  つの独自の学習アプ ローチにより，NLP  において比類のない成功を収めています。\n",
        "<!-- BERT has enjoyed unparalleled success in NLP thanks to two unique training approaches, masked-language modeling (MLM), and next sentence prediction (NSP). -->\n",
        "\n",
        "多くの場合，事前に学習済  BERT モデルをそのまま使用して，自分の言語課題にうまく適用することができるかもしれません。\n",
        "<!-- In many cases, we might be able to take the pre-trained BERT model out-of-the-box and apply it successfully to our own language tasks.-->\n",
        "\n",
        "しかし，多くの場合，モデルを微調整する必要があります。\n",
        "<!-- But often, we might need to fine-tune the model. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1427f78c-5b1a-456d-8d41-b9b689270555",
      "metadata": {
        "id": "1427f78c-5b1a-456d-8d41-b9b689270555"
      },
      "outputs": [],
      "source": [
        "from IPython.display import YouTubeVideo, display\n",
        "tube_id = 'q9NS5WpfkrU'\n",
        "display(YouTubeVideo(tube_id, width=600, height=400))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee42f3c-7c20-4b6c-a59b-9e41eda76f1a",
      "metadata": {
        "id": "2ee42f3c-7c20-4b6c-a59b-9e41eda76f1a"
      },
      "source": [
        "MLM でさらに訓練を重ねることで，より具体的な領域での言語の特殊な使用方法をよりよく理解できるように BERT を微調整することができます。\n",
        "\n",
        "すぐに使える BERT - 汎用的な使用に最適。\n",
        "MLM で微調整された BERT - ドメイン固有の使用に最適。\n",
        "\n",
        "この記事では MLM とは何か，どのように機能するのか，そして MLM を使ってどのようにモデルを改善できるのかについて，詳しく説明します。\n",
        "<!-- Further training with MLM allows us to fine-tune BERT to better understand the particular use of language in a more specific domain.\n",
        "\n",
        "Out-of-the-box BERT — great for general purpose use. Fine-tuned with MLM BERT — great for domain-specific use.\n",
        "\n",
        "In this article, we’ll go into depth about what MLM is, how it works, and how we can improve our models with it.\n",
        "-->\n",
        "\n",
        "# マスク化言語モデル MLM\n",
        "<!-- # Masked-Language Modeling -->\n",
        "\n",
        "MLM は BERT に文を与え，BERT 内部の重みを最適化して，相手側に同じ文を出力することで構成されています。\n",
        "<!--MLM consists of giving BERT a sentence and optimizing the weights inside BERT to output the same sentence on the other side.-->\n",
        "\n",
        "つまり，文を入力し，BERT が同じ文を出力するように要求します。\n",
        "<!-- So we input a sentence and ask that BERT outputs the same sentence. -->\n",
        "\n",
        "しかし，実際に BERT にその入力文を与える前に，いくつかのトークンを必要とします。\n",
        "<!-- However, before we actually give BERT that input sentence — we mask a few tokens. -->\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*phTLnQ8itb3ZX5_h9BWjWw.png\" width=\"600px\"><br/>\n",
        "この画像では，トークン を BERT に渡す前に，リンカーン・トークンを [MASK] に置き換えてマスクしています。\n",
        "    <!-- In this image, before passing our tokens into BERT — we have masked the lincoln token, replacing it with [MASK]. -->\n",
        "</center>\n",
        "\n",
        "つまり，実際には不完全な文章を入力して，BERT に完成させるように依頼しているのです。\n",
        "<!-- So we’re actually inputting an incomplete sentence and asking BERT to complete it for us. -->\n",
        "\n",
        "## 間隙を埋める <!--Fill In The Gaps-->\n",
        "\n",
        "これはどのような効果があるのでしょうか？\n",
        "それは、多くの人が学校で与えられた問題のようなものです。つまり、文章が与えられたら、その穴を埋めなければなりません。\n",
        "<!-- What is the effect of this? \n",
        "Well, it’s like those questions many of us were given in school — where, given a sentence, we had to fill in the gaps. -->\n",
        "\n",
        "\n",
        "`秋 に は ， ___ が 木 から 落ちる 。`\n",
        "<!-- `In Autumn the ______ fall from the trees.`-->\n",
        "\n",
        "答えがわかりますか？\n",
        "ほとんど、あなたは分かっているでしょう。\n",
        "それは，文脈を考慮したからです。\n",
        "<!-- Do you know the answer? \n",
        "Most likely you do, and you do because you have considered the context of the sentence.-->\n",
        "\n",
        "「落ちる」 と 「木」 という単語が出てきましたが，足りない単語は木から落ちるものだということがわかります。\n",
        "<!-- We see the words fall and trees — we know that the missing word is something that falls from trees. -->\n",
        "\n",
        "どんぐり，枝，葉など，木から落ちるものはたくさんありますが，秋という別の条件があるので，秋に木から落ちる可能性が最も高いのは葉だということで，検索対象が絞られます。\n",
        "<!-- A lot of things fall from trees, acorns, branches, leaves — but we have another condition, in Autumn — that narrows down our search, the most probable thing to fall from a tree in Autumn are leaves. -->\n",
        "\n",
        "人間として，私たちは一般的な世界の知識と言語的な理解を組み合わせて，その結論を導き出します。\n",
        "BERT の場合，この推測は，たくさんの本を読み，言語パターンを非常によく学んでいることから得られます。\n",
        "<!-- As humans, we use a mix of general world knowledge, and linguistic understanding to come to that conclusion. \n",
        "For BERT, this guess will come from reading a lot — and learning linguistic patterns incredibly well. -->\n",
        "\n",
        "BERT は，秋，木，葉が何であるかを知らないかもしれませんが，言語パターンとこれらの単語の文脈から，答えが葉である可能性が最も高いことを知っています。\n",
        "<!-- BERT may not know what Autumn, trees, and leaves are — but it does know that given linguistic patterns, and the context of these words, the answer is most likely to be leaves. -->\n",
        "\n",
        "この処理の結果，BERT にとっては，使用されている言語のスタイルの理解度が向上します。\n",
        "<!-- The outcome of this process — for BERT — is an improved comprehension of the style of language being used. -->\n",
        "\n",
        "## 実際の処理 <!--The Process-->\n",
        "\n",
        "MLM が何をしているかは理解できましたが，実際にはどのように機能するのでしょうか？\n",
        "コード上で必要となる論理的なステップは何でしょうか？\n",
        "<!-- So we understand what MLM is doing, but how does this actually work? \n",
        "What are the logical steps that we’ll need to follow in code? -->\n",
        "\n",
        "\n",
        "1.  テキストをトークン化します。\n",
        "通常の変換機と同じように，まずテキストのトークン化を行います。\n",
        "トークン化からは 3 つの異なるテンソルが得られます。\n",
        "\n",
        "<!-- 1. We tokenize our text. Just like we usually would with transformers, we begin with text tokenization.\n",
        "From tokenization we will receive three different tensors:\n",
        "-->\n",
        "\n",
        "* input_ids\n",
        "* token_type_ids\n",
        "* attention_mask\n",
        "\n",
        "MLM には token_type_ids は必要ないし，この例では attention_mask はそれほど重要ではありません。\n",
        "<!-- We don’t need token_type_ids for MLM — and in this example attention_mask is not so important.-->\n",
        "\n",
        "我々にとっては input_ids テンソルが最も重要です。\n",
        "ここには，トークン化されたテキスト表現があり，これを今後修正していくことになるでしょう。\n",
        "<!--For us, the input_ids tensor is most important. Here, we will have a tokenized representation of our text — which is what we will be modifying moving forwards.-->\n",
        "\n",
        "2. `labels tensor` を作成する。\n",
        "ここではモデルを訓練しているので，損失を計算して最適化するためのラベルテンソルが必要です。\n",
        "`labels tensor` は単純に `input_ids` なので，これをコピーするだけです。\n",
        "<!-- 2. Create a labels tensor. We’re training our model here, so we need a labels tensor to calculate loss against — and optimize towards.\n",
        "The labels tensor is simply input_ids — so all we need to do is make a copy.-->\n",
        "\n",
        "3. `input_ids` のトークンをマスクする。\n",
        "label 用の `input_ids` のコピーを作成したので，先にトークンのランダムな選択をマスクすることができます。\n",
        "<!-- 3. Mask tokens in input_ids. Now that we’ve created a copy of input_ids for labels, we can go ahead and mask a random selection of tokens. -->\n",
        "\n",
        "BERT 論文では，モデルの事前訓練中に，いくつかの追加ルールを用いて，各トークンを 15％ の確率でマスク化していますが，ここではこれを簡略化して，各単語を 15％ の確率でマスク化することにします。\n",
        "<!-- The BERT paper uses a 15% probability of masking each token during model pre-training, with a few additional rules — we’ll use a simplified version of this and assign a 15% probability of each word being masked. -->\n",
        "\n",
        "4. 損失を計算します。\n",
        "`input_ids` と `labels` のテンソルを BERT モデルで処理し，両者の間の損失を計算します。\n",
        "この損失を用いて，BERT による必要な勾配変化を計算し，モデルの重みを最適化します。\n",
        "<!--4. Calculate loss. We process the input_ids and labels tensors through our BERT model and calculate the loss between them both.\n",
        "Using this loss, we calculate the required gradient changes with BERT — and optimize our model weights. -->\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*0KvOrY6rY055m9oq36HRkg.png\" width=\"600px\"><br/>\n",
        "<div style=\"text-align:left; width:40%; background-color:cornsilk\" width=\"400px\">\n",
        "512 個のトークンはすべて，モデルの語彙サイズに等しいベクトル長を持ちます。\n",
        "最終的な出力埋め込みベクトルであるロジット(確率比) を生成します。\n",
        "予測されたトークン ID は，lソフトマックスと argmax 変換を用いて，このロジットから抽出されます。\n",
        "<!-- All 512 tokens produce a final output embedding — the logits — which has a vector length equal to the model vocab size. \n",
        "The predicted token_id is extracted from this logit using a softmax and argmax transformation. -->\n",
        "</div>    \n",
        "</center>    \n",
        "\n",
        "損失は，各出力「トークン」の出力確率分布と，真のワンホット符号化ラベルとの差として計算されます。\n",
        "<!-- The loss is calculated as the difference between the output probability distributions for each output ‘token’, and the true one-hot encoded labels.\n",
        "-->\n",
        "\n",
        "# マスク化言語モデル MLM のコード MLM In Code\n",
        "\n",
        "さて，それはすごいことですが，MLM をコードで実証するにはどうすればよいでしょうか？\n",
        "<!-- Okay, that’s all great, but how can we demonstrate MLM in code? -->\n",
        "\n",
        "HuggingFace のトランスフォーマーと PyTorch そして bert-base-uncased モデルを使用します。\n",
        "では，まず全てをインポートして初期化しましょう。\n",
        "<!-- We’ll be using HuggingFace’s transformers and PyTorch, alongside the bert-base-uncased model. So, let’s import and initialize everything first:\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "isColab = True if platform.system() == 'Linux' else False\n",
        "if isColab:\n",
        "    !pip install transformers > /dev/null 2>&1 "
      ],
      "metadata": {
        "id": "Mr4HrlsdKdOo"
      },
      "id": "Mr4HrlsdKdOo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0898c1f7-4fa0-4b6e-b5ad-aea8222c19ec",
      "metadata": {
        "id": "0898c1f7-4fa0-4b6e-b5ad-aea8222c19ec"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dbdccc7-c653-4513-a008-51605b2bb289",
      "metadata": {
        "id": "3dbdccc7-c653-4513-a008-51605b2bb289"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "text = (\"After Abraham Lincoln won the November 1860 presidential \"\n",
        "        \"election on an anti-slavery platform, an initial seven \"\n",
        "        \"slave states declared their secession from the country \"\n",
        "        \"to form the Confederacy. War broke out in April 1861 \"\n",
        "        \"when secessionist forces attacked Fort Sumter in South \"\n",
        "        \"Carolina, just over a month after Lincoln's \"\n",
        "        \"inauguration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5579783-97d8-4617-a5ca-1a951f3b5b50",
      "metadata": {
        "id": "b5579783-97d8-4617-a5ca-1a951f3b5b50"
      },
      "source": [
        "それでは，それぞれの論理的なステップへと進んでいきます。\n",
        "<!-- And now we move into each one of our logical steps, starting with:-->\n",
        "\n",
        "<font size=\"+2\">1.  **トークン化**</font> - トークン化は簡単です。\n",
        "すでに BertTokenizer を初期化しましたので，あとは入力テキストをトークン化するだけです。\n",
        "<!--1. Tokenization — tokenization is simple, we’ve already initialized a BertTokenizer, all we do now is tokenize our input text. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84223328-e263-4789-9b80-b55488b3a99a",
      "metadata": {
        "id": "84223328-e263-4789-9b80-b55488b3a99a"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(text, return_tensors='pt')\n",
        "print(inputs.keys())\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f2b89ea-9312-48c3-9c48-3740128d245f",
      "metadata": {
        "id": "1f2b89ea-9312-48c3-9c48-3740128d245f"
      },
      "source": [
        "ここでは，埋め草(パディング) や 切断(トランケーション)については気にしません。\n",
        "`token_type_ids` と `attention_mask` は特に気にする必要はありませんが `input_ids` は気になります。\n",
        "<!-- We won’t worry about padding/truncation for now. \n",
        "What we should be aware of are the three tensors described earlier — token_type_ids and attention_mask don’t need any attention from us — but input_ids does.-->\n",
        "\n",
        "<font size=\"+2\">2. **ラベル作成**</font> - 次のステップは簡単です。\n",
        "ここで必要なのは `input_ids` テンソルを新しい labels テンソルにクローンすることです。\n",
        "これも `inputs` 変数に格納します。\n",
        "<!--2. Create labels — The next step is easy, all we need to do here is clone our input_ids tensor into a new labels tensor. \n",
        "We’ll store this within the inputs variable too. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4acb8daf-c4ce-4023-970a-0d42ed60dd12",
      "metadata": {
        "id": "4acb8daf-c4ce-4023-970a-0d42ed60dd12"
      },
      "outputs": [],
      "source": [
        "inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0daaa75d-6ff3-4f35-b334-a7f68d8f12f9",
      "metadata": {
        "id": "0daaa75d-6ff3-4f35-b334-a7f68d8f12f9"
      },
      "source": [
        "<font size=\"+2\"> 3. **マスキング**</font> -  今度は `input_ids` テンソル内のトークンのランダムな選択をマスキングする必要があります。\n",
        "<!-- 3. Masking — Now we need to mask a random selection of tokens in our input_ids tensor. -->\n",
        "\n",
        "トークンを 15% の確率でマスク化するためには `torch.rand` と各値 $<0.15$ という条件を用います。\n",
        "これらの条件を満たすと，マスキング配列 `mask_arr` が生成されます。\n",
        "<!-- To create our 15% probability of masking any one token, we can use torch.rand alongside a condition of each value < 0.15. \n",
        "Together, these will produce our masking array mask_arr. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5457822d-795d-4fcd-9bed-99203c82aabb",
      "metadata": {
        "id": "5457822d-795d-4fcd-9bed-99203c82aabb"
      },
      "outputs": [],
      "source": [
        "# input_idsと同じ次元の浮動小数点数のランダムな配列を作る # create random array of floats in equal dimension to input_ids\n",
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "# 乱数配列が0.15 より小さい場合は true を設定 # where the random array is less than 0.15, we set true\n",
        "mask_arr = rand < 0.15\n",
        "print(mask_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b91d159d-9756-4eb4-8871-55f18cb2833b",
      "metadata": {
        "id": "b91d159d-9756-4eb4-8871-55f18cb2833b"
      },
      "source": [
        "さて MASK トークンを配置する場所を選ぶために mask_arr を使いますが，CLS トークンや SEP トークンなどの他の特別なトークン (それぞれ 101と 102) の上に MASK トークンを配置したくはありません。\n",
        "<!-- Now, we use mask_arr to select where to place our MASK tokens — but we don’t want to place a MASK token over other special tokens such as CLS or SEP tokens (101 and 102 respectively).-->\n",
        "\n",
        "そこで，さらに条件を追加する必要があります。\n",
        "トークン ID 101 または 102 を含むポジションのチェックです。\n",
        "<!--So, we need to add an additional condition. A check for positions containing the token ids 101 or 102. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37ad4d9e-22f0-4f42-acdf-c067c96dbc00",
      "metadata": {
        "id": "37ad4d9e-22f0-4f42-acdf-c067c96dbc00"
      },
      "outputs": [],
      "source": [
        "(inputs.input_ids != 101) * (inputs.input_ids != 102)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60184306-6692-4d9b-b652-0ecdc8d3d88a",
      "metadata": {
        "id": "60184306-6692-4d9b-b652-0ecdc8d3d88a"
      },
      "outputs": [],
      "source": [
        "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102)\n",
        "print(mask_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a8945e-4d61-4193-8cad-c26a85766bb4",
      "metadata": {
        "id": "46a8945e-4d61-4193-8cad-c26a85766bb4"
      },
      "source": [
        "これでマスキング・テンソルができあがりました。\n",
        "これを適用するために，まず `True` の値が見つかったインデックス位置を抽出し，この選択を使ってこれらのポジションの値を 103 (MASK トークンID) に設定します。\n",
        "<!-- Now that is our masking tensor, to apply it we will first extract the index positions where we find a True value — then use this selection to set values in these positions to 103 (the MASK token id). -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87339afc-6595-49fe-8186-c1deccf0977c",
      "metadata": {
        "id": "87339afc-6595-49fe-8186-c1deccf0977c"
      },
      "outputs": [],
      "source": [
        "# mask_arr から selection を作成 #create selection from mask_arr\n",
        "selection = torch.flatten((mask_arr[0]).nonzero()).tolist()\n",
        "print(selection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8494bcbd-266c-43d9-9196-d2e156837505",
      "metadata": {
        "id": "8494bcbd-266c-43d9-9196-d2e156837505"
      },
      "outputs": [],
      "source": [
        "# create selection from mask_arr\n",
        "selection = torch.flatten((mask_arr[0]).nonzero()).tolist()\n",
        "selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6bbf659-6a1e-4f83-b4c2-3496838591eb",
      "metadata": {
        "id": "b6bbf659-6a1e-4f83-b4c2-3496838591eb"
      },
      "outputs": [],
      "source": [
        "# selection インデックスを inputs.input_ids へ適用し MASK トークンを追加 ＃apply selection index to inputs.input_ids, adding MASK tokens\n",
        "inputs.input_ids[0, selection] = 103\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c60e1eae-a6d5-42ec-af2b-e7799827b653",
      "metadata": {
        "id": "c60e1eae-a6d5-42ec-af2b-e7799827b653"
      },
      "source": [
        "これで，上の `input_ids` テンソルで  103 で表された MASK トークンを見ることができます。\n",
        "<!-- Now we can see our MASK tokens represented by 103 in the input_ids tensor above.-->\n",
        "\n",
        "<font size=\"+2\">4. **損失の計算**</font> - ここでの最後のステップは，一般的なモデルの訓練処理と変わりません。\n",
        "`input_ids` テンソルと `labels` テンソルが `input dictionary` に入っているので，これをモデルに渡してモデルの損失を返すことができます。\n",
        "<!-- \n",
        "4. Calculate Loss — Our final step here no different from the typical model training process.\n",
        "With both input_ids and labels tensors inside our inputs dictionary, we can pass this to our model and return the model loss. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "473a800b-0837-43d0-b843-3710336c6aed",
      "metadata": {
        "id": "473a800b-0837-43d0-b843-3710336c6aed"
      },
      "outputs": [],
      "source": [
        "# pass inputs as kwarg to model\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a4ee8c-0fdf-46b4-a614-a41f50224f2b",
      "metadata": {
        "id": "26a4ee8c-0fdf-46b4-a614-a41f50224f2b"
      },
      "outputs": [],
      "source": [
        "# we get two output tensors, loss and logits\n",
        "print(outputs.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "403cf369-ecac-4e90-b4c2-2a0021ff0a80",
      "metadata": {
        "id": "403cf369-ecac-4e90-b4c2-2a0021ff0a80"
      },
      "outputs": [],
      "source": [
        "print(outputs.loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed6efb85-6236-4b3f-9b69-da5003f53c84",
      "metadata": {
        "id": "ed6efb85-6236-4b3f-9b69-da5003f53c84"
      },
      "source": [
        "## 訓練 <!-- ## Training -->\n",
        "\n",
        "さて，ここまで基本的なことを説明してきましたが，モデルを微調整する際にはどのようにすればよいのでしょうか？\n",
        "訓練には 2 つの方法があります。\n",
        "(1) これまでに学んだことを全て使って，独自のバージョンの訓練関数を実装する，あるいは\n",
        "(2) HuggingFace の `Trainer` を使う。\n",
        "<!-- Cool, we’ve run through all of the essentials — but how would all of this look when fine-tuning a model?\n",
        "There are two ways, \n",
        "(1) we implement our own version of the training function using everything we have learned so far, or \n",
        "(2) we use HuggingFace’s Trainer. -->\n",
        "\n",
        "最適化された使いやすい解法としては，明らかに `Trainer` がお勧めです。\n",
        "その使い方を紹介しますが，まずは自分で実装してみましょう。\n",
        "<!-- Trainer is clearly the way to go for an optimized, easy-to-use solution. \n",
        "And we’ll take a look at how we can use it — but first, let’s try implementing it ourselves. -->\n",
        "\n",
        "### 私たちの実装 <!-- ### Our Implementation -->\n",
        "\n",
        "せっかく学んだのだから，自分で訓練関数を実装してみないともったいないですよね。\n",
        "<!-- After learning all of this, it would be a waste to not give implementing our own training function a go. -->\n",
        "\n",
        "<iframe width=\"665\" height=\"382\" src=\"https://www.youtube.com/embed/R6hcxMMOrPE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "915e1f05-324f-4641-861b-3023f2c21ba1",
      "metadata": {
        "id": "915e1f05-324f-4641-861b-3023f2c21ba1"
      },
      "outputs": [],
      "source": [
        "from IPython.display import YouTubeVideo, display\n",
        "tube_id = 'R6hcxMMOrPE'\n",
        "display(YouTubeVideo(tube_id, width=600, height=400))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ea2087-9d63-4b09-a5e5-84ffb99ce93d",
      "metadata": {
        "id": "d4ea2087-9d63-4b09-a5e5-84ffb99ce93d"
      },
      "source": [
        "まず，データが必要です。\n",
        "トークンをランダムにマスキングするだけなので，ほとんどのテキストを使うことができます。\n",
        "ラベル付きのデータや特別なデータは必要ありません。\n",
        "<!-- First, we need data. \n",
        "Because we’re just randomly masking a selection of tokens — we can use almost any text. We don’t need labeled or special data.-->\n",
        "\n",
        "ここでは，Marcus Aurelius の Meditations を使用することにします。\n",
        "ソースは [こちら](http://classics.mit.edu/Antoninus/meditations.html) で，すこし前処理が必要です。([クリアバージョン](https://github.com/jamescalam/transformers/blob/main/data/text/meditations/clean.txt))\n",
        "\n",
        "<!-- Here, we’ll use Meditations by Marcus Aurelius, sourced from here and preprocessed a little (clean version). -->\n",
        "\n",
        "まず，テキストデータをインポート/初期化して読み込みます。\n",
        "<!-- First, we’ll import/initialize and load our text data.  -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c609fad0-11f8-4bd6-b81b-3f037d986170",
      "metadata": {
        "id": "c609fad0-11f8-4bd6-b81b-3f037d986170"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d8e3b4-c183-49d1-917e-d6b84bc3045e",
      "metadata": {
        "id": "e0d8e3b4-c183-49d1-917e-d6b84bc3045e"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/jamescalam/transformers/main/data/text/meditations/clean.txt -O clean.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f293ed3-388d-4f4b-b975-f91827737b5d",
      "metadata": {
        "id": "1f293ed3-388d-4f4b-b975-f91827737b5d"
      },
      "outputs": [],
      "source": [
        "with open('clean.txt', 'r') as fp:\n",
        "    text = fp.read().split('\\n')\n",
        "\n",
        "print(text[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ef8c7e-bd1d-439d-a769-b1099ab1b0f9",
      "metadata": {
        "id": "f2ef8c7e-bd1d-439d-a769-b1099ab1b0f9"
      },
      "source": [
        "次に，**トークン化** を行います。\n",
        "今回は，長さの異なる多くの系列があるため，各系列を切り詰めてパッド化します。\n",
        "<!-- Then we tokenize — this time we do truncate and pad each sequence — as we have many sequences of different lengths. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19fcaf06-818d-4a64-b04c-89271a75f8e1",
      "metadata": {
        "id": "19fcaf06-818d-4a64-b04c-89271a75f8e1"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79302614-2bd9-41e7-adb8-9b394b5951f9",
      "metadata": {
        "id": "79302614-2bd9-41e7-adb8-9b394b5951f9"
      },
      "source": [
        "次に `input_id` をクローンして，ラベルテンソルを作成します。\n",
        "<!-- Now we clone the input_ids to create our labels tensor. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df818e04-e23a-4f3b-ace5-fc8effc93fea",
      "metadata": {
        "id": "df818e04-e23a-4f3b-ace5-fc8effc93fea"
      },
      "outputs": [],
      "source": [
        "inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "print(inputs.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ae58d68-4ca8-4671-9054-8818d0c2f46c",
      "metadata": {
        "id": "8ae58d68-4ca8-4671-9054-8818d0c2f46c"
      },
      "source": [
        "次に，マスクのコードですが，今回は 2 つの理由から少し異なります。\n",
        "<!-- Next is our masking code, which is a little different this time for two reasons:-->\n",
        "\n",
        "* マスクに PAD トークンを含めてはいけない (CLS や SEP は従来どおりに扱う)。\n",
        "* 多くのデータ系列がある。ひとつではない。\n",
        "\n",
        "<!--\n",
        "* Our mask should not include PAD tokens (as before with CLS and SEP).\n",
        "* We have many sequences — not just one. \n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cb4f46c-4890-451b-a149-ab9b1ef4a4ec",
      "metadata": {
        "id": "3cb4f46c-4890-451b-a149-ab9b1ef4a4ec"
      },
      "outputs": [],
      "source": [
        "# input_ids テンソルと同じ次元の浮動小数点数のランダムな配列を作成 # create random array of floats with equal dimensions to input_ids tensor\n",
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "# mask 配列の作成 # create mask array\n",
        "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
        "           (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
        "print(mask_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3b97497-4638-465f-b825-a58f10b58842",
      "metadata": {
        "id": "d3b97497-4638-465f-b825-a58f10b58842"
      },
      "source": [
        "そして今度は，それぞれのベクターの中で，各 `True` 値のインデックスを取ります。\n",
        "<!-- And now we take take the indices of each True value, within each individual vector. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e780f605-aecf-44e1-8edb-86abcfd7db6e",
      "metadata": {
        "id": "e780f605-aecf-44e1-8edb-86abcfd7db6e"
      },
      "outputs": [],
      "source": [
        "selection = []\n",
        "\n",
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    selection.append(\n",
        "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "    )\n",
        "print(selection[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64b10201-a14a-4c72-b651-f3e7b00b31ae",
      "metadata": {
        "id": "64b10201-a14a-4c72-b651-f3e7b00b31ae"
      },
      "source": [
        "次に，これらのインデックスを `input_ids` の各行に適用し，これらのインデックスの値をそれぞれ 103 として割り当てます。\n",
        "<!-- Then apply these indices to each respective row in input_ids, assigning each of the values at these indices as 103. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deaa0dea-2b84-484b-80d4-42ff51965cba",
      "metadata": {
        "id": "deaa0dea-2b84-484b-80d4-42ff51965cba"
      },
      "outputs": [],
      "source": [
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    inputs.input_ids[i, selection[i]] = 103\n",
        "\n",
        "print(inputs.input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dce8a30b-9781-406e-b80b-cf8bc2b88866",
      "metadata": {
        "id": "dce8a30b-9781-406e-b80b-cf8bc2b88866"
      },
      "source": [
        "`mask_arr` テンソルの `True` 値と同じ位置に 103 という値が割り当てられているのがわかります。\n",
        "<!-- We can see the value 103 assigned in the same position as the True value is found in the mask_arr tensor.-->\n",
        "\n",
        "これで入力テンソルの準備が整い，学習時にモデルに入力するための設定を始めることができます。\n",
        "<!-- The inputs tensors are now ready — and we can begin setting them up to be fed into our model during training. -->\n",
        "\n",
        "学習時には PyTorch の `DataLoader` を使ってデータを読み込みます。\n",
        "これを使うには，データを PyTorch の `Dataset` オブジェクトにフォーマットする必要があります。\n",
        "<!-- During training, we’ll be using a PyTorch DataLoader to load our data. To use this, we’ll need to format our data into a PyTorch Dataset object. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83281c33-a70b-45f9-80bc-d6e3734a4230",
      "metadata": {
        "id": "83281c33-a70b-45f9-80bc-d6e3734a4230"
      },
      "outputs": [],
      "source": [
        "class MeditationsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9a2639e-2f3b-4e83-8343-ba6e673ff3b3",
      "metadata": {
        "id": "b9a2639e-2f3b-4e83-8343-ba6e673ff3b3"
      },
      "source": [
        "`MeditationsDataset` クラスを使って，データを初期化します。\n",
        "<!-- Initialize our data using the MeditationsDataset class. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d3de9b3-b530-4d3a-9ba7-3a447fed37a2",
      "metadata": {
        "id": "2d3de9b3-b530-4d3a-9ba7-3a447fed37a2"
      },
      "outputs": [],
      "source": [
        "dataset = MeditationsDataset(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "067eadbb-fe7b-4adb-8bcc-ee79eae969d9",
      "metadata": {
        "id": "067eadbb-fe7b-4adb-8bcc-ee79eae969d9"
      },
      "source": [
        "そして `dataloader` を初期化します。\n",
        "`dataloader` は，訓練時にモデルにデータを読み込むために使用します。\n",
        "<!-- And initialize the dataloader, which we'll be using to load our data into the model during training. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f16e4a1-dab6-43ef-9672-47595f7ce7cc",
      "metadata": {
        "id": "3f16e4a1-dab6-43ef-9672-47595f7ce7cc"
      },
      "outputs": [],
      "source": [
        "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "#loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ef6c23-7ce6-4259-a4c5-0e7f8a07c209",
      "metadata": {
        "id": "e1ef6c23-7ce6-4259-a4c5-0e7f8a07c209"
      },
      "source": [
        "これで，反復訓練に入る準備が整いました。\n",
        "ループを始める前に，3 つのことを設定する必要があります。\n",
        "<!-- Now we’re ready to move onto our training loop. Before starting our loop we need to set up three things:-->\n",
        "\n",
        "* モデルをGPU/CPU (GPU が利用可能あれば) に移動させる\n",
        "* モデルの訓練モードを有効にする\n",
        "* 重み付けされた重み崩壊付き最適化 `AdamW` を初期化する\n",
        "\n",
        "<!--* Move the model to GPU/CPU (GPU if available).\n",
        "* Activate model training mode.\n",
        "* Initialize an Adam with weighted decay optimizer. -->\n",
        "\n",
        "Setup GPU/CPU usage and activate the training mode of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a8e965c-0b5e-4d7b-bbaa-a0014dd93a6f",
      "metadata": {
        "id": "3a8e965c-0b5e-4d7b-bbaa-a0014dd93a6f"
      },
      "outputs": [],
      "source": [
        "# GPU/CPU 使用を設定し，モデルの訓練モードを起動 #Setup GPU/CPU usage and activate the training mode of our model.\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# そしてモデルを選択したデバイスに移動 # and move our model over to the selected device\n",
        "model.to(device)\n",
        "# 訓練モードに設定 #activate training mode\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "Pv3T1QWBRuAB"
      },
      "id": "Pv3T1QWBRuAB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e46c699c-53a8-493f-a283-7c4f2124eec5",
      "metadata": {
        "id": "e46c699c-53a8-493f-a283-7c4f2124eec5"
      },
      "outputs": [],
      "source": [
        "# 最適化関数を初期化 (AdamW は重み付き崩壊で，過学習の可能性を減らします) #Initialize our optimizer (Adam with weighted decay - reduces chance of overfitting).\n",
        "from transformers import AdamW\n",
        "# 最適化関数を初期化 # initialize optimizer\n",
        "optim = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aa9f846-ab13-4719-9b46-75701d0a4150",
      "metadata": {
        "id": "2aa9f846-ab13-4719-9b46-75701d0a4150"
      },
      "source": [
        "これでようやくセットアップが完了し，訓練を開始することができます。\n",
        "ここでは PyTorch の典型的な訓練ループを導入します。\n",
        "<!-- Now we’re finally set up — we can begin training! We format this as a typical training loop in PyTorch. -->"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab の無料環境では GPU メモリ不足で動作せず。残念\n",
        "# import torch\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
      ],
      "metadata": {
        "id": "FJvVYnfgLvge"
      },
      "id": "FJvVYnfgLvge",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0fde4f9-5d6d-4171-873b-23cb13285d43",
      "metadata": {
        "id": "b0fde4f9-5d6d-4171-873b-23cb13285d43"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm  # for our progress bar\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # setup loop with TQDM and dataloader\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch in loop:\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optim.zero_grad()\n",
        "        # pull all tensor batches required for training\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        # process\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                        labels=labels)\n",
        "        # extract loss\n",
        "        loss = outputs.loss\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "        # print relevant info to progress bar\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee2f975-2c2e-4e4c-a901-a8e0963f2dbb",
      "metadata": {
        "id": "5ee2f975-2c2e-4e4c-a901-a8e0963f2dbb"
      },
      "source": [
        "これで，我々は MLM の微調整用のスクリプトを導入したことになります。\n",
        "<!-- And with that we’re done — we’ve implemented our own MLM fine-tuning script.-->\n",
        "\n",
        "### トレーナー <!-- ### Trainer -->\n",
        "\n",
        "`Trainer` の実装に移りますが，データセットを作成するまでの作業は，`Trainer` が学習のための入力として期待されているため，引き続き行う必要があります。\n",
        "<!-- Switching across to our Trainer implementation — we’ll still need to do everything we did before up to to point that we created our dataset — as Trainer will be expecting this as input for training. -->\n",
        "\n",
        "まず，訓練用の引数を定義し，`Trainer` を初期化してから，訓練を行います。\n",
        "<!-- We’ll first define our training arguments, initialize the Trainer — then train! -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c14d9d48-3686-4399-8cf5-b504b12d5f2e",
      "metadata": {
        "id": "c14d9d48-3686-4399-8cf5-b504b12d5f2e"
      },
      "outputs": [],
      "source": [
        "# We'll pass a training args dictionary to the Trainer defining our training arguments.\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='out',\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b8beb8-bb02-4345-a1a6-6b9be21b65de",
      "metadata": {
        "id": "e7b8beb8-bb02-4345-a1a6-6b9be21b65de"
      },
      "outputs": [],
      "source": [
        "# Now we'll import and initialize our Trainer.\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db81c067-e7f1-4736-86d2-920727e0a77e",
      "metadata": {
        "id": "db81c067-e7f1-4736-86d2-920727e0a77e"
      },
      "outputs": [],
      "source": [
        "# And train.\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bdf26dc-9138-4111-899a-cece7990d227",
      "metadata": {
        "id": "9bdf26dc-9138-4111-899a-cece7990d227"
      },
      "source": [
        "`Trainer` のアプローチは確かにはるかにシンプルで，初期化時にチェックポイントやその他の機能を指定するだけで実装することができます。\n",
        "<!-- So the Trainer approach is certainly much simpler — and allows us to implement checkpoints and other features by simply specifying them at initialization time.-->\n",
        "\n",
        "以上が MLM を用いてモデルの微調整を始めるために必要な知識のすべてです。\n",
        "<!-- Okay, so that’s everything we really need to know to get started with fine-tuning our models using MLM. -->\n",
        "\n",
        "MLM には多くの機能がありますが，そのコンセプトと実装はそれほど複雑ではなく，非常に強力なものとなっています。\n",
        "<!-- There’s a lot to MLM, but the concept and implementations are not too complex — and incredibly powerful. -->\n",
        "\n",
        "ここで学んだことを利用すれば，NLP の最高のモデルを，よりドメインに特化した言語のユースケースに合わせて微調整することができます。\n",
        "必要なのは，ラベルのないテキストだけで，データソースは簡単に見つかります。\n",
        "<!-- Using what we’ve learned here, we can take the best models in NLP and fine-tune them to our more domain-specific language use-cases — needing nothing more than unlabelled text — often an easy data source to find. -->\n",
        "\n",
        "この記事を楽しんでいただけましたか？\n",
        "ご質問がありましたら，Twitter または下記のコメント欄でお知らせください。\n",
        "このようなコンテンツをもっと見たいという方は，YouTube にも投稿しています。\n",
        "<!-- I hope you enjoyed this article! If you have any questions, let me know via Twitter or in the comments below. \n",
        "If you’d like more content like this, I post on YouTube too. -->\n",
        "\n",
        "お読みいただきありがとうございました。\n",
        "<!-- Thanks for reading! -->"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "2022_0104Masked-Language_Modeling_With_BERT_ja.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}