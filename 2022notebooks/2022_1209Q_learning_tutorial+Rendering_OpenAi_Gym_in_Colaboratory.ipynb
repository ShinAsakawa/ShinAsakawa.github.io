{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1209Q_learning_tutorial%2BRendering_OpenAi_Gym_in_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "odNaDE1zyrL2"
      },
      "cell_type": "markdown",
      "source": [
        "# 参考: [Rendering OpenAi Gym in Google Colaboratory](https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/).\n",
        "<!-- \n",
        "# install dependancies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies -->\n",
        "\n",
        "M1 MacBook Pro などでは，[XQuattz](https://www.xquartz.org/) をダウンロードして，\n",
        "以下のコマンドを実行してから動作させること\n",
        "\n",
        "```bash\n",
        "xhost +\n",
        "```\n",
        "\n",
        "参考: https://stackoverflow.com/questions/65890804/xstarterror-in-pyvirtualdisplay\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "isColab =  'google.colab' in str(get_ipython())\n",
        "\n",
        "if isColab:\n",
        "    !apt-get update > /dev/null 2>&1\n",
        "    !apt-get install cmake > /dev/null 2>&1\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg\n",
        "\n",
        "    !pip install --upgrade setuptools 2>&1\n",
        "    !pip install ez_setup > /dev/null 2>&1\n",
        "\n",
        "    !pip install gym\n",
        "    !pip install 'gym[atari]'\n",
        "    !pip install 'gym[classic_control]'\n",
        "    !pip install 'gym[accept-rom-license]'    \n",
        "    !pip install pyvirtualdisplay\n"
      ],
      "metadata": {
        "id": "7fxVmDgK5nzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "APXSx7hg19TH"
      },
      "cell_type": "markdown",
      "source": [
        "# ヘルパー関数など\n",
        "<!-- # Imports and Helper functions -->\n"
      ]
    },
    {
      "metadata": {
        "id": "pdb2JwZy4jGj"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "#from gym.wrappers import Monitor\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQEtc28G4niA"
      },
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G9UWeToN4r7D"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
        "                        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                        </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "    #env = Monitor(env, './video', force=True)\n",
        "    #env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
        "    env = RecordVideo(env, './video',  episode_trigger = lambda episode_number: True)\n",
        "    env.reset()\n",
        "    return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dGEFMfDOzLen"
      },
      "cell_type": "code",
      "source": [
        "#env = wrap_env(gym.make(\"MsPacman-v0\"))\n",
        "env = wrap_env(gym.make(\"CartPole-v1\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7BmIlXhe9Q89"
      },
      "cell_type": "code",
      "source": [
        "#check out the pacman action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8nj5sjsk15IT"
      },
      "cell_type": "code",
      "source": [
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "        \n",
        "    if done:\n",
        "        break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 強化学習(DQN)チュートリアル\n",
        "<!-- # Reinforcement Learning (DQN) Tutorial -->\n",
        "\n",
        "**著者**: [Adam Paszke](https://github.com/apaszke)\n",
        "\n",
        "このチュートリアルでは PyTorch を使って [OpenAI Gym](https://gym.openai.com/) の CartPole-v0 課題で Deep Q Learning (DQN) エージェントを訓練する方法を紹介する。\n",
        "<!-- This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the [OpenAI Gym](https://gym.openai.com/)-->\n",
        "\n",
        "**課題**\n",
        "<!-- **Task** -->\n",
        "\n",
        "エージェントは，カートに取り付けられたポールが直立するように，カートを左右に動かすという 2 つの行動のどちらかを決定しなければならない。\n",
        "様々なアルゴリズムと視覚化による公式リーダーボードは [Gym website](https://gym.openai.com/envs/CartPole-v0) で見ることができる。\n",
        "<!-- The agent has to decide between two actions - moving the cart left or right - so that the pole attached to it stays upright. \n",
        "You can find an official leaderboard with various algorithms and visualizations at the [Gym website](https://gym.openai.com/envs/CartPole-v0) -->\n",
        "\n",
        "<center>\n",
        "<img src=\"https://pytorch.org/tutorials/_images/cartpole.gif\" width=\"33%\" alt=\"cartpole\">\n",
        "</center>\n",
        "\n",
        "エージェントが環境の現在の状態を観察し，行動を選択すると，環境は新しい状態に **遷移**し，また行動の結果を示す報酬を返す。\n",
        "この課題では，報酬はタイムステップごとに +1 され，ポールが倒れすぎるか，カートが中心から 2.4 単位以上離れると，環境は終了する。\n",
        "これは，より良い成績をとるシナリオは，より長い時間実行され，より大きなリターンを蓄積することを意味する。\n",
        "<!-- As the agent observes the current state of the environment and chooses an action, the environment *transitions* to a new state, and also returns a reward that indicates the consequences of the action. \n",
        "In this task, rewards are +1 for every incremental timestep and the environment terminates if the pole falls over too far or the cart moves more then 2.4 units away from center. \n",
        "This means better performing scenarios will run for longer duration, accumulating larger return.-->\n",
        "\n",
        "CartPole 課題は，エージェントへの入力が環境の状態 (位置，速度など) を表す 4 つの実数値であるように設計されている。\n",
        "しかし，ニューラルネットワークは純粋にシーンを見るだけで課題を解くことができるので，カートを中心とした画面のパッチを入力として使用することにする。\n",
        "このため，公式のリーダーボードの結果と直接比較することはできないが，このような課題の方がはるかに難しい。\n",
        "残念ながら，すべてのフレームをレンダリングする必要があるため，訓練速度が遅くなる。\n",
        "<!-- The CartPole task is designed so that the inputs to the agent are 4 real values representing the environment state (position, velocity, etc.).\n",
        "However, neural networks can solve the task purely by looking at the scene, so we'll use a patch of the screen centered on the cart as an input. \n",
        "Because of this, our results aren't directly comparable to the ones from the official leaderboard - our task is much harder.\n",
        "Unfortunately this does slow down the training, because we have to render all the frames. -->\n",
        "\n",
        "厳密には，現在の画面パッチと前回の画面パッチとの差分として状態を提示する。\n",
        "これにより，エージェントは 1 枚の画像からポールの速度を考慮することができるようになる。\n",
        "<!-- Strictly speaking, we will present the state as the difference between the current screen patch and the previous one. \n",
        "This will allow the agent to take the velocity of the pole into account from one image. -->\n",
        "\n",
        "**パッケージ**\n",
        "<!-- **Packages** -->\n",
        "\n",
        "まず，必要なパッケージをインポートする。\n",
        "環境として [gym](https://gym.openai.com/docs) が必要である\n",
        " (`pip install gym` でインストール)。\n",
        "また PyTorch から以下を使用する\n",
        "<!-- First, let's import needed packages. \n",
        "Firstly, we need [gym](https://gym.openai.com/docs) for the environment (Install using `pip install gym`).\n",
        "We'll also use the following from PyTorch: -->\n",
        "\n",
        "- ニューラルネットワーク (``torch.nn``)\n",
        "- 最適化関数 (``torch.optim``)\n",
        "- 自動微分 (``torch.autograd``)\n",
        "- 視覚課題のためのユーティリティ (``torchvision`` - [別パッケージ](https://github.com/pytorch/vision)).\n",
        "\n",
        "<!-- -  neural networks (``torch.nn``)\n",
        "-  optimization (``torch.optim``)\n",
        "-  automatic differentiation (``torch.autograd``)\n",
        "-  utilities for vision tasks (``torchvision`` - [a separate package](https://github.com/pytorch/vision)). -->"
      ],
      "metadata": {
        "id": "Dmn0Fu_XaBED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "env = gym.make('CartPole-v0').unwrapped\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "cAmqO7o2ZkPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# リプレイメモリー\n",
        "<!-- # Replay Memory-->\n",
        "\n",
        "DQN の学習には、経験値リプレイメモリを使用することにする。\n",
        "これはエージェントが観測した遷移を保存し，後でこのデータを再利用することができる。\n",
        "このメモリからランダムにサンプリングすることで，バッチを構成する遷移を復号化することができる。\n",
        "これにより，DQN の学習手順が大幅に安定化し，改善されることが知られている。\n",
        "<!-- We'll be using experience replay memory for training our DQN. \n",
        "It stores the transitions that the agent observes, allowing us to reuse this data later. \n",
        "By sampling from it randomly, the transitions that build up a batch are decorrelated. \n",
        "It has been shown that this greatly stabilizes and improves the DQN training procedure.-->\n",
        "\n",
        "これには以下の 2 つのクラスが必要とな:\n",
        "<!-- For this, we're going to need two classses: -->\n",
        "\n",
        "- `Transition` - 環境の遷移を表す名前付きタプル。\n",
        "これは基本的に (state, action) の対を (next_state, reward) の結果に写像するもので，state は後述するように画面の差分画像となる。\n",
        "- `ReplayMemory` - 最近観測された遷移を保持する，サイズに制限のある循環バッファ cyclic buffer 。 \n",
        "また，学習用の遷移をランダムに選択するための `.sample()` メソッドも実装されている。\n",
        "\n",
        "<!--\n",
        "-  ``Transition`` - a named tuple representing a single transition in our environment. \n",
        "It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.\n",
        "-  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a ``.sample()`` method for selecting a random batch of transitions for training. -->\n",
        "\n"
      ],
      "metadata": {
        "id": "8gNdAQDVaJX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "uwj3dpXwaGTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "では，モデルを定義してみよう。\n",
        "その前に，DQN とは何か，簡単におさらいしておく。\n",
        "<!-- Now, let's define our model. \n",
        "But first, let's quickly recap what a DQN is. -->\n",
        "\n",
        "# DQN アルゴリズム\n",
        "<!-- # DQN algorithm -->\n",
        "\n",
        "我々の環境は決定論的であるため，ここで示される全ての式も簡略化のため決定論的に定式化されている。\n",
        "強化学習の文献では，環境中の確率的な遷移に対する期待値も含まれる。\n",
        "<!-- Our environment is deterministic, so all equations presented here are also formulated deterministically for the sake of simplicity. \n",
        "In the reinforcement learning literature, they would also contain expectations over stochastic transitions in the environment.-->\n",
        "\n",
        "我々の目的は、割引された累積報酬 $R_{t_0}=\\sum_{t=t_0}^{\\infty}\\gamma^{t-t_0}r_t$ を最大化しようとするポリシーを訓練することであり，R_{t_0} は **リターン** としても知られている。\n",
        "割引 $\\gamma$ は，合計が収束することを保証する $0$ と $1$ の間の定数である。\n",
        "これは，エージェントにとって，不確実な遠い将来からの報酬は，かなり確信できる近い将来の報酬よりも重要ではなくなる。\n",
        "<!-- Our aim will be to train a policy that tries to maximize the discounted, cumulative reward $R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where $R_{t_0}$ is also known as the *return*. \n",
        "The discount, $\\gamma$, should be a constant between $0$ and $1$ that ensures the sum converges. \n",
        "It makes rewards from the uncertain far future less important for our agent than the ones in the near future that it can be fairly confident about. -->\n",
        "\n",
        "Q 学習の主な考え方は，もし $Q^{\\star}: \\text{状態} \\times \\text{行為} \\rightarrow \\mathbb{R}$  という関数があれば，ある状態で行動を起こした場合のリターンがどうなるかを教えてくれるので，報酬を最大化するポリリーを簡単に構築できる，というものである: \n",
        "<!-- The main idea behind Q-learning is that if we had a function $Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards:-->\n",
        "\n",
        "$$\\pi^{\\star}(s) = \\arg\\!\\max_{a} Q^{\\star}(s, a)$$\n",
        "\n",
        "しかし，我々は世界の全てを知っているわけではないので，$Q^{\\star}$ にアクセスすることはできない。\n",
        "しかし，ニューラルネットワークは普遍的な関数近似器なので，単純にニューラルネットワークを作り，$Q^{\\star}$ に近似するように訓練すればよい。\n",
        "<!-- However, we don't know everything about the world, so we don't have access to $Q^*$. \n",
        "But, since neural networks are universal function approximators, we can simply create one and train it to resemble $Q^*$.-->\n",
        "\n",
        "学習更新則には，あるポリシーに対するあらゆる $Q$ 関数がベルマン方程式に従うという事実を利用することにする: \n",
        "<!-- For our training update rule, we'll use a fact that every $Q$ function for some policy obeys the Bellman equation: -->\n",
        "\n",
        "$$Q^{\\pi}(s,a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))$$\n",
        "\n",
        "両者の等式の差は，時間差誤差 (TD エラー) $\\delta$と呼ばれる:\n",
        "<!-- The difference between the two sides of the equality is known as the temporal difference error, $\\delta$:-->\n",
        "$$\\delta = Q(s,a) - (r + \\gamma \\max_{a} Q(s',a))$$\n",
        "\n",
        "この誤差を最小化するために [フーバー損失 (Huber loss)] (https://en.wikipedia.org/wiki/Huber_loss) を使用する。\n",
        "フーバー損失は誤差が小さいときは二乗平均誤差のように作用し，誤差が大きいときは平均絶対誤差のように作用する。\n",
        "これにより $Q$ の推定値が非常に雑音が多いときに外れ値に対してより頑健になる。\n",
        "リプレイメモリからサンプリングした遷移のバッチ $B$ に対して以下を計算する:\n",
        "<!-- To minimise this error, we will use the `Huber loss <https://en.wikipedia.org/wiki/Huber_loss>`. \n",
        "The Huber loss acts like the mean squared error when the error is small, but like the mean absolute error when the error is large - this makes it more robust to outliers when the estimates of $Q$ are very noisy. \n",
        "We calculate this over a batch of transitions, $B$, sampled from the replay memory: -->\n",
        "\n",
        "$${L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} {L}(\\delta)$$\n",
        "ここで，\n",
        "$$\\begin{align}\n",
        "{L}(\\delta) = \n",
        "\\begin{cases}\n",
        "     \\frac{1}{2}{\\delta^2}  & \\text{ } |\\delta| \\le 1 \\text{ の場合}, \\\\\n",
        "     |\\delta| - \\frac{1}{2} & \\text{それ以外}\n",
        "\\end{cases}\n",
        "\\end{align}$$\n",
        "\n",
        "<!-- $$\\begin{align}\n",
        "\\text{where} \\quad \\mathcal{L}(\\delta) = \n",
        "\\begin{cases}\n",
        "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
        "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
        "\\end{cases}\n",
        "\\end{align}$$-->\n",
        "\n",
        "## Q 学習\n",
        "<!-- ## Q-network -->\n",
        "\n",
        "ここでのモデルは，現在の画面パッチと以前の画面パッチの差を取り込む畳み込みニューラルネットワークとなる。\n",
        "これは 2 つの出力を持ち$Q(s,\\text{left})$ と $Q(s,\\text{right})$ を表す (ここで $s$ はネットワークへの入力である)。\n",
        "事実上，このネットワークは，現在の入力が与えられたときに各行為を行うことの **期待収益 expected return** を予測しようとしているのである。\n",
        "<!-- Our model will be a convolutional neural network that takes in the difference between the current and previous screen patches. \n",
        "It has two outputs, representing $Q(s, \\mathrm{left})$ and $Q(s, \\mathrm{right})$ (where $s$ is the input to the network). \n",
        "In effect, the network is trying to predict the *expected return* of taking each action given the current input. -->"
      ],
      "metadata": {
        "id": "udsBSgm7aNmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Linear input connections の数は conv2d layer の出力に依存する．\n",
        "        # したがって，入力画像サイズに依存するので，それを計算する．\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        \n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    # 次の行為を決定するために 1 つの要素で呼び出されるか，最適化中に一括で呼び出され，\n",
        "    # テンソル ([[left_exp,right_exp]…]) を返す．\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "metadata": {
        "id": "Rr8EXgf9aLis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 入力抽出\n",
        "<!-- ## Input extraction-->\n",
        "\n",
        "以下のコードは，環境からレンダリング画像を取り出して処理するためのユーティリティである。\n",
        "これは `torchvision` パッケージを使用しており，画像変換の合成を簡単に行うことができる。\n",
        "セルを実行すると，抽出されたパッチの例が表示される。\n",
        "<!-- The code below are utilities for extracting and processing rendered images from the environment. \n",
        "It uses the `torchvision` package, which makes it easy to compose image transforms. \n",
        "Once you run the cell it will display an example patch that it extracted. -->"
      ],
      "metadata": {
        "id": "fxpLxleZai4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # カートの中央\n",
        "\n",
        "def get_screen():\n",
        "    # ジムで要求された画面は 400x600x3 だが, 800x1200x3 などもっと大きい場合もある\n",
        "    # これを torch で用いられる順番 (CHW) に転置 (transpose) する\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    \n",
        "    # カートは下半分にあるので，画面の上下部分を切り剥がす\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # カートを中心とした正方形の画像を作成するために縁を切り落す\n",
        "    screen = screen[:, :, slice_range]\n",
        "    \n",
        "    # 浮動小数点に変換，再スケール，torch Tensorに変換\n",
        "    # (これはコピーを必要としない)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    \n",
        "    # サイズを変更し、バッチディメンションを追加（BCHW）\n",
        "    return resize(screen).unsqueeze(0)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "           interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DQ2AQsf5agIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render(mode='rgb_array');"
      ],
      "metadata": {
        "id": "SVbTq2U9akzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 訓練\n",
        "<!-- # Training -->\n",
        "\n",
        "## ハイパーパラメータとユーティリティ\n",
        "<!-- ## Hyperparameters and utilities-->\n",
        "\n",
        "このセルはモデルと最適化関数を実体化し，いくつかのユーティリティを定義する:\n",
        "<!-- This cell instantiates our model and its optimizer, and defines some utilities: -->\n",
        "\n",
        "- `select_action` - イプシロン貪欲ポリシーに従って行為を選択する。\n",
        "簡単に言うと，行為を選択するためにモデルを使用することもあれば，一様にサンプリングすることもある。\n",
        "ランダムな行為を選択する確率は `EPS_START` から始まり，`EPS_END` に向かって指数関数的に減衰していく。\n",
        "`EPS_DECAY` は減衰の速度を制御する。\n",
        "- `plot_durations` - エピソードの継続時間を，過去 100 エピソードの平均値 (公式の評価で使用される指標) とともにプロットするためのヘルパー関数。\n",
        "プロットは，メインの学習反復ループを含むセルの下に表示され，各エピソードの後に更新される。\n",
        "\n",
        "<!-- - `select_action` - will select an action accordingly to an epsilon greedy policy. \n",
        "Simply put, we'll sometimes use our model for choosing the action, and sometimes we'll just sample one uniformly. \n",
        "The probability of choosing a random action will start at `EPS_START` and will decay exponentially towards `EPS_END`. \n",
        "`EPS_DECAY` controls the rate of the decay.\n",
        "- `plot_durations` - a helper for plotting the durations of episodes, along with an average over the last 100 episodes (the measure used in the official evaluations). \n",
        "The plot will be underneath the cell containing the main training loop, and will update after every episode.-->\n"
      ],
      "metadata": {
        "id": "8s6Be-0Aas1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "# AI gymから返された形状に基づいて層を正しく初期化できるように，スクリーンサイズを取得する。\n",
        "# この時点での典型的な寸法は 3x40x90 に近く，これは `get_screen()` \n",
        "# でレンダーバッファをクランプしてダウンスケールした結果である。\n",
        "init_screen = get_screen()\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "# Gym の行為空間から行為の数を取得\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) は各行で最大の列の値を返す\n",
        "            # max の結果の 2 列目は，最大の要素が見つかった場所のインデックス\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    \n",
        "    # 100 エピソードの平均をとってプロットもする\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # プロットが更新されるように少停止\n",
        "    if is_ipython:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())"
      ],
      "metadata": {
        "id": "EYDOJsRmarRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習ループ\n",
        "<!-- ## Training loop-->\n",
        "\n",
        "最後に，モデルを学習するコードである。\n",
        "<!-- Finally, the code for training our model. -->\n",
        "\n",
        "以下に，最適化の一段階を実行する `optimize_model` 関数がある。\n",
        "まずバッチをサンプリングし，すべてのテンソルを一つにまとめ，$Q(s_t, a_t)$ と $V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$ を計算し，それらを組み合わせて損失を計算する。\n",
        "定義上，$s$ が終端状態であれば $V(s) = 0$ とする。\n",
        "また，安定性を高めるために，ターゲットネットワークを使って $V(s_{t+1})$ を計算する。\n",
        "ターゲットネットワークはその重みがほとんど凍結されたままであるが，ポリシーネットワークの重みで時々更新される。\n",
        "これは通常決まったステップ数であるが，ここでは簡単のためエピソードとする。\n",
        "<!-- Here, you can find an `optimize_model` function that performs a single step of the optimization. \n",
        "It first samples a batch, concatenates all the tensors into a single one, computes $Q(s_t, a_t)$ and $V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our loss. \n",
        "By definition we set $V(s) = 0$ if $s$ is a terminal state. \n",
        "We also use a target network to compute $V(s_{t+1})$ for added stability. \n",
        "The target network has its weights kept frozen most of the time, but is updated with the policy network's weights every so often.\n",
        "This is usually a set number of steps but we shall use episodes for simplicity.-->\n"
      ],
      "metadata": {
        "id": "Ioqh09AFaxho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # バッチを転置 (詳しい説明は https://stackoverflow.com/a/19343/3343043 参照)\n",
        "    # Transitions のバッチ配列からバッチ配列の Transition に変換される\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # 最終状態でないマスクを計算し，そのバッチ要素を連結する\n",
        "    # (最終状態とは，シミュレーションが終了した後の状態のこと)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Q(s_t, a) を計算する - モデルが Q(s_t) を計算し，次に行われた行動の列を選択する。\n",
        "    # これらは `policy_net` に従って各バッチの状態に対して取られたであろう行動である\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # すべての次の状態について V(s_{t+1}) を計算する。\n",
        "    # 最終状態でない次の状態に対する行動の期待値は「古い」target_net に基づいて計算される; \n",
        "    # max(1)[0] でその最高の報酬を選択する。\n",
        "    # これはマスクに基づいてマージされ，状態の期待値か，状態が最終的であった場合には 0 を得る\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    \n",
        "    # Q 値の期待値を計算\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Huber 損失を計算\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # モデルの最適化\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "0YY6xUDuavUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下に，メイン訓練反復を示す。\n",
        "最初に環境をリセットし，`state`  テンソルを初期化する。\n",
        "次に，行動をサンプリングし，実行し，次の画面と報酬 (常に 1) を観察し，モデルを一度最適化する。\n",
        "エピソードが終了すると (モデルが失敗すると)，ループを再開する。\n",
        "<!-- Below, you can find the main training loop. \n",
        "At the beginning we reset the environment and initialize the `state` Tensor. \n",
        "Then, we sample an action, execute it, observe the next screen and the reward (always 1), and optimize our model once. When the episode ends (our model fails), we restart the loop.-->\n",
        "\n",
        "以下では `num_episodes` は小さく設定されている。\n",
        "ノートブックをダウンロードし，300以上のエピソードを実行することで，持続時間の改善が期待できる。\n",
        "<!-- Below, `num_episodes` is set small. You should download the notebook and run lot more epsiodes, such as 300+ for meaningful duration improvements. -->"
      ],
      "metadata": {
        "id": "axobsNqXa2L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 50\n",
        "for i_episode in range(num_episodes):\n",
        "    \n",
        "    # 環境と状態を初期化\n",
        "    env.reset()\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen\n",
        "    \n",
        "    for t in count():\n",
        "        # 選択して実行するアクション\n",
        "        action = select_action(state)\n",
        "        _, reward, done, _, _ = env.step(action.item())\n",
        "        #_, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # 新しい状態を観察\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # 遷移をメモリに格納\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # 次の状態に移動\n",
        "        state = next_state\n",
        "\n",
        "        # 最適化の 1 ステップを実行 (ポリシーネットワーク上で)\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "            \n",
        "    # ターゲットネットワークを更新し DQN のすべての重みとバイアスをコピー\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print('Complete')\n",
        "#env.render()\n",
        "env.close()\n",
        "#plt.ioff()\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "96i7zqFAaziZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下は，結果として得られる全体のデータフローを示す図である。\n",
        "<!-- Here is the diagram that illustrates the overall resulting data flow. -->\n",
        "\n",
        "<center>\n",
        "<img src=\"https://pytorch.org/tutorials/_images/reinforcement_learning_diagram.jpg\" width=\"66%\" alt=\"reinforcement learning diagram\">\n",
        "</center>\n",
        "<!-- .. figure:: /_static/img/reinforcement_learning_diagram.jpg -->\n",
        "\n",
        "行動はランダムに，またはポリシーに基づいて選択され，Gym  環境から次のステップのサンプルを取得する。\n",
        "その結果をリプレイメモリに記録し，さらに反復毎に最適化ステップを実行する。\n",
        "最適化では，リプレイメモリからランダムなバッチを選び，新しいポリシーの学習を行う。\n",
        "また，最適化では  「古い」target_net を使用して Q 値の期待値を計算するため，時々更新して最新の状態を保つ。\n",
        "<!-- Actions are chosen either randomly or based on a policy, getting the next step sample from the gym environment. \n",
        "We record the results in the replay memory and also run optimization step on every iteration.\n",
        "Optimization picks a random batch from the replay memory to do training of the new policy. \n",
        "\"Older\" target_net is also used in optimization to compute the expected Q values; it is updated occasionally to keep it current. -->\n",
        "\n"
      ],
      "metadata": {
        "id": "onoMrbG5a9j8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aJea9rEKa4Wf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}