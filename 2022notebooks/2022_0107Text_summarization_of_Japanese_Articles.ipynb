{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022_0107Text_summarization_of_Japanese_Articles.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN3SWGK067DEjX4E8txF3gY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0107Text_summarization_of_Japanese_Articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* url: https://medium.com/@shubhamsingh_31435/text-summarization-of-japanese-articles-using-python-and-nlp-47a214d769b\n",
        "* date: 2022_0107\n",
        "* title: Text Summarization of Japanese Articles using python and NLP\n",
        "* filename: 2022_0107Text_summarization_of_Japanese_Articles.ipynb\n",
        "\n",
        "# Text Summarization of Japanese Articles using python and NLP\n"
      ],
      "metadata": {
        "id": "II0BZ8WXbAk2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdxSl-EDTyb3"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "isColab = platform.system() == 'Linux'\n",
        "if isColab:\n",
        "    !pip install --upgrade wikipedia > /dev/null 2>&1 \n",
        "    !pip install --upgrade jamdict jamdict-data nagisa pykakasi > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://nagisa.readthedocs.io/en/latest/basic_usage.html\n",
        "import nagisa\n",
        "\n",
        "text = 'Pythonで簡単に使えるツールです'\n",
        "words = nagisa.tagging(text)\n",
        "print(words)\n",
        "#=> Python/名詞 で/助詞 簡単/形状詞 に/助動詞 使える/動詞 ツール/名詞 です/助動詞\n",
        "\n",
        "# Get a list of words\n",
        "print(words.words)\n",
        "#=> ['Python', 'で', '簡単', 'に', '使える', 'ツール', 'です']\n",
        "\n",
        "# Get a list of POS-tags\n",
        "print(words.postags)\n",
        "#=> ['名詞', '助詞', '形状詞', '助動詞', '動詞', '名詞', '助動詞']"
      ],
      "metadata": {
        "id": "4BzFi4ZeT1vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extarcting all nouns from a text\n",
        "words = nagisa.extract(text, extract_postags=['名詞'])\n",
        "print(words)\n",
        "#=> Python/名詞 ツール/名詞\n",
        "\n",
        "# Filtering specific POS-tags from a text\n",
        "words = nagisa.filter(text, filter_postags=['助詞', '助動詞'])\n",
        "print(words)\n",
        "#=> Python/名詞 簡単/形状詞 使える/動詞 ツール/名詞\n",
        "\n",
        "# A list of available POS-tags\n",
        "print(nagisa.tagger.postags)\n",
        "#=> ['補助記号', '名詞', ... , 'URL']\n"
      ],
      "metadata": {
        "id": "q5n2Xc6zUvcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# default\n",
        "text = \"3月に見た「3月のライオン」\"\n",
        "print(nagisa.tagging(text))\n",
        "#=> 3/名詞 月/名詞 に/助詞 見/動詞 た/助動詞 「/補助記号 3/名詞 月/名詞 の/助詞 ライオン/名詞 」/補助記号\n",
        "\n",
        "# If a word (\"3月のライオン\") is included in the single_word_list, it is recognized as a single word.\n",
        "new_tagger = nagisa.Tagger(single_word_list=['3月のライオン'])\n",
        "print(new_tagger.tagging(text))\n",
        "#=> 3/名詞 月/名詞 に/助詞 見/動詞 た/助動詞 「/補助記号 3月のライオン/名詞 」/補助記号"
      ],
      "metadata": {
        "id": "iVorfYx0U4n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '(人•ᴗ•♡)こんばんは♪'\n",
        "words = nagisa.tagging(text)\n",
        "print(words)\n",
        "#=> (人•ᴗ•♡)/補助記号 こんばんは/感動詞 ♪/補助記号\n",
        "\n",
        "url = 'https://github.com/taishi-i/nagisaでコードを公開中(๑¯ω¯๑)'\n",
        "words = nagisa.tagging(url)\n",
        "print(words)\n",
        "#=> https://github.com/taishi-i/nagisa/URL で/助詞 コード/名詞 を/助詞 公開/名詞 中/接尾辞 (๑　̄ω　̄๑)/補助記号\n",
        "\n",
        "words = nagisa.filter(url, filter_postags=['URL', '補助記号', '助詞'])\n",
        "print(words)\n",
        "#=> コード/名詞 公開/名詞 中/接尾辞"
      ],
      "metadata": {
        "id": "p5TmU7quVGWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/neocl/jamdict\n",
        "from jamdict import Jamdict\n",
        "jam = Jamdict()\n",
        "\n",
        "# use wildcard matching to find anything starts with 食べ and ends with る\n",
        "result = jam.lookup('食べ%る')\n",
        "\n",
        "# print all word entries\n",
        "for entry in result.entries:\n",
        "     print(entry)\n"
      ],
      "metadata": {
        "id": "ob-10spCVLRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for k in ['chars', 'entries', 'names', 'text']:\n",
        "#     print(f'{k} : {type(getattr(result, k))}')\n",
        "#     if isinstance(getattr(result,k), list):\n",
        "#         _list = getattr(result,k)\n",
        "#         for x in _list:\n",
        "#             print(k, x)\n",
        "#             #print(getattr(result,k)[x])\n",
        "\n",
        "#dir(result)\n",
        "for l in result.text().split('。'):\n",
        "    print(l)"
      ],
      "metadata": {
        "id": "AUAy1C_NV29w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print all related characters\n",
        "for c in result.chars:\n",
        "    print(repr(c))\n"
      ],
      "metadata": {
        "id": "-bQ2lvPQVSTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m jamdict lookup 日本語教育学\n",
        "# ========================================\n",
        "# Found entries\n",
        "# ========================================\n",
        "# Entry: 1264430 | Kj:  言語学 | Kn: げんごがく\n",
        "# --------------------\n",
        "# 1. linguistics ((noun (common) (futsuumeishi)))\n",
        "\n",
        "# ========================================\n",
        "# Found characters\n",
        "# ========================================\n",
        "# Char: 言 | Strokes: 7\n",
        "# --------------------\n",
        "# Readings: yan2, eon, 언, Ngôn, Ngân, ゲン, ゴン, い.う, こと\n",
        "# Meanings: say, word\n",
        "# Char: 語 | Strokes: 14\n",
        "# --------------------\n",
        "# Readings: yu3, yu4, eo, 어, Ngữ, Ngứ, ゴ, かた.る, かた.らう\n",
        "# Meanings: word, speech, language\n",
        "# Char: 学 | Strokes: 8\n",
        "# --------------------\n",
        "# Readings: xue2, hag, 학, Học, ガク, まな.ぶ\n",
        "# Meanings: study, learning, science\n",
        "\n",
        "# No name was found."
      ],
      "metadata": {
        "id": "WyDh32wSVfbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using KRAD/RADK mapping\n",
        "# Jamdict has built-in support for KRAD/RADK (i.e. kanji-radical and radical-kanji mapping). The terminology of radicals/components used by Jamdict can be different from else where.\n",
        "\n",
        "# A radical in Jamdict is a principal component, each character has only one radical.\n",
        "# A character may be decomposed into several writing components.\n",
        "# By default jamdict provides two maps:\n",
        "\n",
        "# jam.krad is a Python dict that maps characters to list of components.\n",
        "# jam.radk is a Python dict that maps each available components to a list of characters.\n",
        "\n",
        "# Find all writing components (often called \"radicals\") of the character 雲\n",
        "print(jam.krad['雲'])\n",
        "# ['一', '雨', '二', '厶']\n",
        "\n",
        "# Find all characters with the component 鼎\n",
        "chars = jam.radk['鼎']\n",
        "print(chars)\n",
        "# {'鼏', '鼒', '鼐', '鼎', '鼑'}\n",
        "\n",
        "# look up the characters info\n",
        "result = jam.lookup(''.join(chars))\n",
        "for c in result.chars:\n",
        "    print(c, c.meanings())\n",
        "# 鼏 ['cover of tripod cauldron']\n",
        "# 鼒 ['large tripod cauldron with small']\n",
        "# 鼐 ['incense tripod']\n",
        "# 鼎 ['three legged kettle']\n",
        "# 鼑 []"
      ],
      "metadata": {
        "id": "TByEi_ixVnDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding name entities\n",
        "# Find all names with 鈴木 inside\n",
        "#result = jam.lookup('%鈴木%')\n",
        "result = jam.lookup('%岩下%')\n",
        "for name in result.names:\n",
        "    print(name)\n",
        "\n",
        "# [id#5025685] キューティーすずき (キューティー鈴木) : Kyu-ti- Suzuki (1969.10-) (full name of a particular person)\n",
        "# [id#5064867] パパイヤすずき (パパイヤ鈴木) : Papaiya Suzuki (full name of a particular person)\n",
        "# [id#5089076] ラジカルすずき (ラジカル鈴木) : Rajikaru Suzuki (full name of a particular person)\n",
        "# [id#5259356] きつねざきすずきひなた (狐崎鈴木日向) : Kitsunezakisuzukihinata (place name)\n",
        "# [id#5379158] こすずき (小鈴木) : Kosuzuki (family or surname)\n",
        "# [id#5398812] かみすずき (上鈴木) : Kamisuzuki (family or surname)\n",
        "# [id#5465787] かわすずき (川鈴木) : Kawasuzuki (family or surname)\n",
        "# [id#5499409] おおすずき (大鈴木) : Oosuzuki (family or surname)\n",
        "# [id#5711308] すすき (鈴木) : Susuki (family or surname)\n",
        "# ..."
      ],
      "metadata": {
        "id": "tLEpyvngXuMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jam.lookup('花火')\n",
        "for entry in jam.lookup('花火').entries:\n",
        "     print(entry)\n"
      ],
      "metadata": {
        "id": "bbJ45FyuX7lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All necessary imports \n",
        "import wikipedia # for fetching japanese article from wikipedia\n",
        "import re # regex library for searching patterns and pre-processing the text\n",
        "import nagisa # library used for Natural Language Processing for japanese\n",
        "import pykakasi # library for conversion of Kanji into Hirigana, Katakana and Romaji\n",
        "import heapq # library for implementing priority queues where the queue item with higher weight is given more priority in processing\n",
        "import pandas as pd # library for managing the data in form of table\n",
        "from jamdict import Jamdict # library for searching the japanese vocabulary\n",
        "\n",
        "# set the language as Japanese for wikipedia article\n",
        "wikipedia.set_lang(\"ja\")\n",
        "\n",
        "# search article on any topic\n",
        "wikipedia.search(\"COVID-19\") # searching for article related to \"COVID-19\" across wikipedia"
      ],
      "metadata": {
        "id": "zuC2Kj2zZsa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = wikipedia.page(\"2019新型コロナウイルス\") # getting the article for topic: \"2019新型コロナウイルス\"\n",
        "article_content = article.content # getting the content of the article\n",
        "\n",
        "# Cleaning the article using regex for pre-processing\n",
        "text = re.sub(r'\\[[0-9]+\\]','',article_content) # removing references such [1] or [2] etc from paragraph\n",
        "text = re.sub(r\"\\s+\",' ',text) # for removing the extra spaces"
      ],
      "metadata": {
        "id": "NYxj7Zz0Z7Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Processing the japanese data using regex\n",
        "clean_text = text.lower() # converts any english word in lower case\n",
        "clean_text = re.sub(r\"\\W\",\" \",clean_text) # removing any non-words characters which include special characters, comma, punctuation\n",
        "clean_text = re.sub(r\"\\d\",\" \",clean_text) # removing any digits\n",
        "clean_text = re.sub(r\"\\s+\",' ',clean_text) # removing any extra spaces in middle \n",
        "clean_text = re.sub(r\"^\\s\",' ',clean_text) # removing any extra spaces in beginning\n",
        "clean_text = re.sub(r\"\\s$\",' ',clean_text) # removing any extra spaces in end\n",
        "\n",
        "# After cleaning and pre-processing, article is broken into individual sentences\n",
        "sentences = text.split(\"。\") # getting all the sentences using \"。\" as delimiter\n",
        "\n",
        "# using \"nagisa\" library to get individual words extracted using following Parts of Speech:\n",
        "# 英単語   : for English words\n",
        "# 接頭辞   : for conjunctions\n",
        "# 形容詞   : for adjective\n",
        "# 名詞     : for noun\n",
        "# 動詞     : for verb\n",
        "# 助動詞   : for auxilary verbs\n",
        "# 副詞     : for adverbs\n",
        "jp_tokenised_words = nagisa.extract(clean_text, extract_postags=['英単語','接頭辞','形容詞','名詞','動詞','助動詞','副詞'])\n",
        "tokenised_words = jp_tokenised_words.words\n",
        "\n",
        "# list of stop-words. Stop words are words which are filtered out before or after processing of natural language data  \n",
        "jp_stopwords = [\"あそこ\",\"あっ\",\"あの\",\"あのかた\",\"あの人\",\"あり\",\"あります\",\"ある\",\"あれ\",\"い\",\"いう\",\"います\",\"いる\",\"う\",\"うち\",\"え\",\"お\",\"および\",\"おり\",\"おります\",\"か\",\"かつて\",\"から\",\"が\",\"き\",\"ここ\",\"こちら\",\"こと\",\"この\",\"これ\",\"これら\",\"さ\",\"さらに\",\"し\",\"しかし\",\"する\",\"ず\",\"せ\",\"せる\",\"そこ\",\"そして\",\"その\",\"その他\",\"その後\",\"それ\",\"それぞれ\",\"それで\",\"た\",\"ただし\",\"たち\",\"ため\",\"たり\",\"だ\",\"だっ\",\"だれ\",\"つ\",\"て\",\"で\",\"でき\",\"できる\",\"です\",\"では\",\"でも\",\"と\",\"という\",\"といった\",\"とき\",\"ところ\",\"として\",\"とともに\",\"とも\",\"と共に\",\"どこ\",\"どの\",\"な\",\"ない\",\"なお\",\"なかっ\",\"ながら\",\"なく\",\"なっ\",\"など\",\"なに\",\"なら\",\"なり\",\"なる\",\"なん\",\"に\",\"において\",\"における\",\"について\",\"にて\",\"によって\",\"により\",\"による\",\"に対して\",\"に対する\",\"に関する\",\"の\",\"ので\",\"のみ\",\"は\",\"ば\",\"へ\",\"ほか\",\"ほとんど\",\"ほど\",\"ます\",\"また\",\"または\",\"まで\",\"も\",\"もの\",\"ものの\",\"や\",\"よう\",\"より\",\"ら\",\"られ\",\"られる\",\"れ\",\"れる\",\"を\",\"ん\",\"何\",\"及び\",\"彼\",\"彼女\",\"我々\",\"特に\",\"私\",\"私達\",\"貴方\",\"貴方方\"]\n"
      ],
      "metadata": {
        "id": "QGXXC-uyaCU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://gist.github.com/shubh2016shiv/f409b89b19303f1ba4e8b5f23b981e46#file-text_summarization-py\n",
        "# Calculate the frequency of each word \n",
        "word2count = {} # dictionary stores the word as a key and frequency as its value\n",
        "for word in tokenised_words:\n",
        "    if word not in jp_stopwords:  # We dont want to include any stop word\n",
        "        if word not in word2count.keys():\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "            \n",
        "\n",
        "# Calculate the weighted frequency of each word by dividing the frequency of the word by maximum frequency of word in whole article            \n",
        "for key in word2count.keys():\n",
        "    word2count[key] = word2count[key]/max(word2count.values()) # Weighted Frequency\n",
        "\n",
        "    \n",
        "'''After Calculating the weighted frequency of each word,\n",
        "Importance score of the sentence is calculated by adding all weighted frequency of words in that sentence'''\n",
        "\n",
        "# Below function , \"getSpaceSeperatedJpWords(text)\" inserts spaces among words in Japanese sentence by using 'pykakasi' library\n",
        "'''For example:\n",
        "sentence is  \"日本は素晴らしい国です\", then,\n",
        "result will be \"日本 は 素晴ら しい 国 です\" \n",
        "with each word has either proper meaning or grammar meaning\n",
        "\n",
        "日本 means \"Japan\"\n",
        "\n",
        "は is a particle for topic marker.\n",
        "\n",
        "素晴ら しい means \"amazing\". Even though it is a single word but there is space between 素晴ら and しい. \n",
        "Reason is that \"しい\" is grammatically significant as it i-adjective and can be conjugated.\n",
        "\n",
        "国 is \"Country\"\n",
        "\n",
        "です is \"is / are\"\n",
        "'''\n",
        "def getSpaceSeperatedJpWords(text):\n",
        "    wakati = pykakasi.wakati()\n",
        "    conv = wakati.getConverter()\n",
        "    result_with_spaces = conv.do(text)\n",
        "    return result_with_spaces\n",
        "  \n",
        "\n",
        "sent2score={} # This dictionary stores each sentence and its score as value\n",
        "for sentence in sentences: # for each sentence in all sentences\n",
        "    # get each word as a token using \"'英単語','接頭辞','形容詞','名詞','動詞','助動詞','副詞'\" as list of filters\n",
        "    tokenised_sentence = nagisa.extract(sentence, extract_postags=['英単語','接頭辞','形容詞','名詞','動詞','助動詞','副詞'])\n",
        "    words = tokenised_sentence.words\n",
        "    for word in words: # if each word of all words in that sentence and\n",
        "        if word in word2count.keys(): # if that word is available in \"word2count\" dictionary\n",
        "            if len(getSpaceSeperatedJpWords(sentence).split(\" \")) < 20: # threshold of 20 is chosen for removing the sentences which are long and not important\n",
        "                if sentence not in sent2score.keys(): # then add its corresponding weighted freqency \n",
        "                    sent2score[sentence] = word2count[word] \n",
        "                else:\n",
        "                    sent2score[sentence] += word2count[word]\n",
        "    "
      ],
      "metadata": {
        "id": "SAlImpLeYV54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "閾値 20 は，この閾値より短い文章を選択するために考慮されています。\n",
        "これは，長い文章は不必要に冗長な情報を含んでいるため，ｌ要約を生成するために長い文章を避けるために行われます。\n",
        "各文章にスコアを割り当てた結果は以下のようになります。\n",
        "<!-- The threshold value of 20 is considered to select those sentences which are shorter than this threshold value. \n",
        "This is done to avoid any long sentences for generating the summary, as long sentences unnecessarily contain redundant information. After each sentence is assigned its own score, the result is: -->"
      ],
      "metadata": {
        "id": "yI9cIEvuaigB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent2score"
      ],
      "metadata": {
        "id": "pCk_ugg1ZBir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JUqoOex9aVQp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}