{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1024CNN_layer_visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b183e88-4bd5-452c-aeb3-2013e5ab78f0",
      "metadata": {
        "id": "9b183e88-4bd5-452c-aeb3-2013e5ab78f0"
      },
      "source": [
        "# CNN 層の可視化デモ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "208c8d6d-0e10-4f5a-8441-49fa171d403f",
      "metadata": {
        "id": "208c8d6d-0e10-4f5a-8441-49fa171d403f"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "try:\n",
        "    import bit\n",
        "except ImportError:\n",
        "    !pip install ipynbname --upgrade > /dev/null 2>&1\n",
        "    !git clone https://github.com/ShinAsakawa/bit.git\n",
        "    import bit\n",
        "\n",
        "isColab = bit.isColab\n",
        "HOME = bit.HOME\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib    \n",
        "    import japanize_matplotlib\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63fb01df-2986-438e-b2ba-98ddf760741d",
      "metadata": {
        "id": "63fb01df-2986-438e-b2ba-98ddf760741d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "from termcolor import colored\n",
        "\n",
        "# 各モデルを定義し，訓練済み結合係数をダウンロード\n",
        "DNNs = {}\n",
        "DNNs['resnet18'] = models.resnet18(weights='DEFAULT', progress=True)\n",
        "# DNNs['alexnet'] = models.alexnet(weights='DEFAULT', progress=True)\n",
        "# DNNs['vgg16'] = models.vgg16(weights='DEFAULT', progress=True)\n",
        "# DNNs['squeezenet']= models.squeezenet1_0(weights='DEFAULT', progress=True)\n",
        "# DNNs['densenet'] = models.densenet161(weights='DEFAULT', progress=True)\n",
        "# DNNs['inception'] = models.inception_v3(weights='DEFAULT', progress=True)\n",
        "# DNNs['googlenet'] = models.googlenet(weights='DEFAULT', progress=True)\n",
        "# DNNs['shufflenet'] = models.shufflenet_v2_x1_0(weights='DEFAULT', progress=True)\n",
        "# DNNs['mobilenet'] = models.mobilenet_v2(weights='DEFAULT', progress=True)\n",
        "# DNNs['resnext50_32x4d'] = models.resnext50_32x4d(weights='DEFAULT', progress=True)\n",
        "# DNNs['wide_resnet50_2'] = models.wide_resnet50_2(weights='DEFAULT', progress=True)\n",
        "# DNNs['mnasnet'] = models.mnasnet1_0(weights='DEFAULT', progress=True)\n",
        "\n",
        "# 上の中から試したいモデルを選んでください。最後のモデルが有効になります。\n",
        "net = DNNs['resnet18'] \n",
        "#net = DNNs['squeezenet']\n",
        "#net = DNNs['googlenet']\n",
        "#net = DNNs['shufflenet']\n",
        "#net = DNNs['mobilenet']\n",
        "#net = DNNs['vgg16']\n",
        "#net = DNNs['alexnet']\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "# # RGB 各チャンネルの平均と分散の定義。CNN 唯一の前処理\n",
        "# mean=[0.485, 0.456, 0.406]\n",
        "# std=[0.229, 0.224, 0.225]\n",
        "\n",
        "resnet_pt = net\n",
        "resnet_pt.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchsummary\n",
        "torchsummary.summary(resnet_pt,input_size=(3,224,224))"
      ],
      "metadata": {
        "id": "Bb6XOuW3yPPN"
      },
      "id": "Bb6XOuW3yPPN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5d8a4101-b65e-4fbe-8553-da5bd3fa1246",
      "metadata": {
        "id": "5d8a4101-b65e-4fbe-8553-da5bd3fa1246"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import torchviz\n",
        "except ImportError:\n",
        "    !pip install torchviz\n",
        "    import torchviz\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchsummary\n",
        "torchsummary.summary(resnet_pt,input_size=(3,224,224))"
      ],
      "metadata": {
        "id": "Nk5ews9RyH4h"
      },
      "id": "Nk5ews9RyH4h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea418fc2-6621-4b3a-a793-4ab52fb37bd0",
      "metadata": {
        "id": "ea418fc2-6621-4b3a-a793-4ab52fb37bd0"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torchviz import make_dot\n",
        "\n",
        "x = torch.rand([1,3,224,224])\n",
        "y = resnet_pt.forward(x)\n",
        "make_dot(y.mean(), params=dict(resnet_pt.named_parameters()))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff952aa-3657-406c-acd5-12d76d8cc929",
      "metadata": {
        "id": "cff952aa-3657-406c-acd5-12d76d8cc929"
      },
      "outputs": [],
      "source": [
        "#for name, module in resnet_pt.named_children():\n",
        "#    print(name) # , module)\n",
        "#print(resnet_pt.layer4)    \n",
        "for i, layer in enumerate(resnet_pt.named_modules()):\n",
        "    print(i, layer[0], type(layer[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93cd32f1-7193-46aa-a9bc-c1fe111447b9",
      "metadata": {
        "id": "93cd32f1-7193-46aa-a9bc-c1fe111447b9"
      },
      "outputs": [],
      "source": [
        "class SaveOutput:\n",
        "    def __init__(self):\n",
        "        self.outputs = []\n",
        "\n",
        "    def __call__(self, module, module_in, module_out):\n",
        "        self.outputs.append(module_out)\n",
        "\n",
        "    def clear(self):\n",
        "        self.outputs = []\n",
        "\n",
        "saved_output = SaveOutput()\n",
        "hook_handles = []\n",
        "hooked_modules = ['conv1', 'layer1', 'layer2', 'layer3', 'layer4']\n",
        "for layer in resnet_pt.named_modules():\n",
        "    if layer[0] in hooked_modules:\n",
        "        print(f'{layer[0]} hooked {type(layer[1])}')\n",
        "        handle = layer[1].register_forward_hook(saved_output)\n",
        "        hook_handles.append(handle)\n",
        "    \n",
        "#resnet_pt.eval()    \n",
        "hook_handles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O cat.jpg https://www.cats.org.uk/media/6189/manja-vitolic-gkxkby-c-dk-unsplash-web.jpg\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "ABViNYUoy0aR"
      },
      "id": "ABViNYUoy0aR",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08fdf11b-2e4b-4c17-8388-283a54ae0a1b",
      "metadata": {
        "id": "08fdf11b-2e4b-4c17-8388-283a54ae0a1b"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "image = Image.open('cat.jpg')\n",
        "transform = T.Compose([T.Resize((224, 224)), T.ToTensor()])\n",
        "X = transform(image).unsqueeze(dim=0).to(device)\n",
        "\n",
        "out = resnet_pt(X)\n",
        "len(saved_output.outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1cb69e-7482-437e-abca-184b4eed9795",
      "metadata": {
        "id": "1e1cb69e-7482-437e-abca-184b4eed9795"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "for i in range(len(saved_output.outputs)):\n",
        "    plt.subplot(1,5,i+1)\n",
        "    plt.gca().set_title(f'層 {i+1}')\n",
        "    plt.imshow(saved_output.outputs[i].detach().numpy().mean(axis=1)[0], cmap='gray')\n",
        "\n",
        "plt.show()    \n",
        "    \n",
        "len(saved_output.outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここから下は，当面無関係です。\n",
        "無視してください。"
      ],
      "metadata": {
        "id": "SkXFFuY94LZH"
      },
      "id": "SkXFFuY94LZH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8862fd73-bc75-4928-840d-d0a10f6a8f92",
      "metadata": {
        "id": "8862fd73-bc75-4928-840d-d0a10f6a8f92"
      },
      "outputs": [],
      "source": [
        "X = torch.randn(1000, 3)\n",
        "y = 3*X[:, 0] + 2*X[:, 1]**2 + X[:, 2]**3 + torch.randn(1000)\n",
        "y = y.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df4b520b-1bb5-43be-8818-37b7606a3143",
      "metadata": {
        "id": "df4b520b-1bb5-43be-8818-37b7606a3143"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = torch.nn.Sequential(\n",
        "            BlockLinear(3, 1, 20),\n",
        "            torch.nn.ReLU(),\n",
        "            BlockLinear(3, 20, 20),\n",
        "            torch.nn.ReLU(),\n",
        "            BlockLinear(3, 20, 20),\n",
        "            torch.nn.ReLU(),\n",
        "            BlockLinear(3, 20, 1),\n",
        "        )\n",
        "\n",
        "        self.lr = torch.nn.Linear(3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_pre = self.features(x)\n",
        "        return self.lr(x_pre)\n",
        "\n",
        "model = Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0887d35-1d9e-4848-b9f8-10fe4ba67399",
      "metadata": {
        "id": "e0887d35-1d9e-4848-b9f8-10fe4ba67399"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = torch.nn.MSELoss()\n",
        "for i in range(2000):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X)\n",
        "    loss = criterion(y, y_pred)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 100 == 0:\n",
        "        print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944686ae-bc7c-4525-8040-68148e4665b1",
      "metadata": {
        "id": "944686ae-bc7c-4525-8040-68148e4665b1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = torch.linspace(-5, 5, 100).reshape(-1, 1)\n",
        "x = torch.hstack(3*[x])\n",
        "\n",
        "for i in range(3):\n",
        "    plt.plot(\n",
        "        x[:, 0].detach().numpy(),\n",
        "        model.get_submodule('lr').weight[0][i].item() * model.get_submodule('features')(x)[:, i].detach().numpy())\n",
        "    plt.title(f'Feature {i+1}')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/rinnakk/japanese-clip.git"
      ],
      "metadata": {
        "id": "Wus1XJ6C3lxj"
      },
      "id": "Wus1XJ6C3lxj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import japanese_clip as ja_clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# ja_clip.available_models()\n",
        "# ['rinna/japanese-clip-vit-b-16', 'rinna/japanese-cloob-vit-b-16']\n",
        "# If you want v0.1.0 models, set `revision='v0.1.0'`\n",
        "model, preprocess = ja_clip.load(\"rinna/japanese-clip-vit-b-16\", cache_dir=\"/tmp/japanese_clip\", device=device)\n",
        "tokenizer = ja_clip.load_tokenizer()\n",
        "\n",
        "#image = preprocess(Image.open(\"./data/dog.jpeg\")).unsqueeze(0).to(device)\n",
        "image = preprocess(Image.open(\"cat.jpg\")).unsqueeze(0).to(device)\n",
        "encodings = ja_clip.tokenize(\n",
        "    texts=[\"犬\", \"猫\", \"象\"],\n",
        "    max_seq_len=77,\n",
        "    device=device,\n",
        "    tokenizer=tokenizer, # this is optional. if you don't pass, load tokenizer each time\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.get_image_features(image)\n",
        "    text_features = model.get_text_features(**encodings)\n",
        "    \n",
        "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "print(\"Label probs:\", text_probs)  # prints: [[1.0, 0.0, 0.0]]"
      ],
      "metadata": {
        "id": "ECr7FR0A3ntQ"
      },
      "id": "ECr7FR0A3ntQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}