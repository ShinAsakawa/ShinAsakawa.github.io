{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO4PKzSMwJS6U+VJp4qSEKX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0918T5_demo_filling_blank_question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5 による，文章穴埋め問題\n",
        "- date: 2022_0918\n",
        "- author: 浅川伸一\n",
        "- filename: `2022_0918T5_demo_filling_blank_question.ipynb`\n",
        "\n",
        "## 使用上の注意\n",
        "\n",
        "* `[CLS]` を使用すること。`[CLS]` はモデルの学習時に使用されるため，マスクされたトークンを予測するには，必ず文の前に `[CLS]` トークンを追加して，モデルが正しく符号化できるようにする。\n",
        "* トークン化した後に `[MASK]` を使用する。入力文字列に直接 [MASK] を入力するのと，トークン化した後に `[MASK]` で置き換えるのとでは、トークンの並びが異なるので，予測結果も異なる。\n",
        "トークン化後に `[MASK]` を使用する方がよい。\n",
        "これは，モデルがどのように事前学習されたかと一致するためである。\n",
        "しかし Huggingface Inference API は入力文字列に `[MASK]` をタイプすることしかサポートしておらず，よりロバストな予測を生成することができない。\n",
        "* `position_ids` を明示的に引数として与える。\n",
        "`Roberta` モデルでは，モデルに対して `position_ids` が与えられない場合 Huggingface 版の transformers は自動的に `position_ids` を構築する。だが， 0 ではなく `padding_idx` から作成する。これでは `rinna/japanese-roberta-base` では期待通りに動作しない。\n",
        "なぜなら対応するトークナイザーの `padding_idx` が 0 ではないからである。\n",
        "そのため，必ず自分で `position_ids` を `constrcut` して，位置 ID 0 から開始するようにすること。"
      ],
      "metadata": {
        "id": "IspVXxuD1yO-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehuM3_nmyyx-"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "try:\n",
        "    import bit\n",
        "except ImportError:\n",
        "    !pip install --upgrade 'fugashi[ipadic]' > /dev/null 2>&1\n",
        "    !pip install --upgrade 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    !python -m unidic download\n",
        "    !pip install --upgrade ipadic > /dev/null 2>&1\n",
        "    !pip install --upgrade sentencepiece > /dev/null 2>&1\n",
        "    !pip install --upgrade transformers > /dev/null 2>&1\n",
        "    !pip install --upgrade termcolor > /dev/null 2>&1\n",
        "    !pip install --upgrade jaconv  > /dev/null 2>&1      \n",
        "    !pip install ipynbname --upgrade > /dev/null 2>&1\n",
        "    !git clone https://github.com/ShinAsakawa/bit.git\n",
        "    import bit\n",
        "\n",
        "isColab = bit.isColab\n",
        "HOME = bit.HOME"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, RobertaForMaskedLM\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-roberta-base\")\n",
        "tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n",
        "\n",
        "# load model\n",
        "model = RobertaForMaskedLM.from_pretrained(\"rinna/japanese-roberta-base\")\n",
        "model = model.eval()\n",
        "\n",
        "# 入力文\n",
        "text = \"ぐっすり寝たので、気持ちがいい。\"\n",
        "\n",
        "# prepend [CLS]\n",
        "text = \"[CLS]\" + text\n",
        "print(f'text:{text}')"
      ],
      "metadata": {
        "id": "ZYQ-tO1uy-BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(f'tokens:{tokens}')  \n",
        "# tokens:['[CLS]', '▁', 'ぐ', 'っ', 'すり', '寝', 'た', 'の', 'で', '、', '気持ち', 'が', 'いい', '。']"
      ],
      "metadata": {
        "id": "YYoV89E8zPhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mask a token; masked_idx の index をマスクする\n",
        "masked_idx = 2\n",
        "tokens[masked_idx] = tokenizer.mask_token\n",
        "print(f'tokens:{tokens}')  \n",
        "# tokens:['[CLS]', '▁', '[MASK]', 'っ', 'すり', '寝', 'た', 'の', 'で', '、', '気持ち', 'が', 'いい', '。']"
      ],
      "metadata": {
        "id": "i1STl9Y722v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to ids\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f'token_ids:{token_ids}')\n",
        "# token_ids:[4, 9, 6, 1315, 14073, 4518, 40, 10, 19, 7, 8053, 12, 2505, 8]\n",
        "print(f'トークン ID をもう一度トークンに戻すと:\\n{tokenizer.convert_ids_to_tokens(token_ids)}')\n",
        "# ['[CLS]', '▁', '[MASK]', 'っ', 'すり', '寝', 'た', 'の', 'で', '、', '気持ち', 'が', 'いい', '。']"
      ],
      "metadata": {
        "id": "VeDGLFTK3Gr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to tensor\n",
        "token_tensor = torch.LongTensor([token_ids])\n",
        "\n",
        "# position ids を明示的に与える\n",
        "position_ids = list(range(0, token_tensor.size(1)))\n",
        "print(f'position_ids:{position_ids}')  \n",
        "# position_ids:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
        "position_id_tensor = torch.LongTensor([position_ids])  # torch.LongTensor とは倍精度整数\n",
        "\n",
        "# マスクに対応する上位 10 候補を得る\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=token_tensor, position_ids=position_id_tensor)\n",
        "    predictions = outputs[0][0, masked_idx].topk(10)\n",
        "\n",
        "for i, index_t in enumerate(predictions.indices):\n",
        "    index = index_t.item()\n",
        "    token = tokenizer.convert_ids_to_tokens([index])[0]\n",
        "    print(i, token)"
      ],
      "metadata": {
        "id": "CfCD83Xv3MFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 入力文\n",
        "text = \"ぐっすり寝たので、気持ちがいい。\"\n",
        "\n",
        "# prepend [CLS]\n",
        "text = \"[CLS]\" + text\n",
        "print(f'text:{text}')\n",
        "\n",
        "# tokenize\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "# convert to tensor\n",
        "token_tensor = torch.LongTensor([token_ids])\n",
        "\n",
        "print(f'tokens:{tokens}')  \n",
        "# tokens:['[CLS]', '▁', 'ぐ', 'っ', 'すり', '寝', 'た', 'の', 'で', '、', '気持ち', 'が', 'いい', '。']\n",
        "\n",
        "# mask a token; masked_idx の index をマスクする\n",
        "for masked_idx in [2, 3, 4]:\n",
        "    tokens[masked_idx] = tokenizer.mask_token\n",
        "print(f'tokens:{tokens}')  \n",
        "# tokens:['[CLS]', '▁', '[MASK]', '[MASK]', '[MASK]', '寝', 'た', 'の', 'で', '、', '気持ち', 'が', 'いい', '。']\n",
        "\n",
        "# position ids を明示的に与える\n",
        "position_ids = list(range(0, token_tensor.size(1)))\n",
        "print(f'position_ids:{position_ids}')  \n",
        "# position_ids:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
        "position_id_tensor = torch.LongTensor([position_ids])\n",
        "\n",
        "# マスクに対応する上位 10 候補を得る\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=token_tensor, position_ids=position_id_tensor)\n",
        "    for masked_idx in [2,3,4]:\n",
        "        predictions = outputs[0][0, masked_idx].topk(10)\n",
        "        print(f'masked_idx:{masked_idx}')\n",
        "        for i, index_t in enumerate(predictions.indices):\n",
        "            index = index_t.item()\n",
        "            token = tokenizer.convert_ids_to_tokens([index])[0]\n",
        "            print(f'\\t{i}:{token}', end=\" \")\n",
        "        print()            "
      ],
      "metadata": {
        "id": "t9FKgl_W3ajC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_predict(\n",
        "    full_text:str, \n",
        "    blank:str,\n",
        "    top_k:int=10,\n",
        "    model=model,\n",
        "    ):\n",
        "    model = model.eval()                \n",
        "    \n",
        "    text = '[CLS]' + full_text\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    \n",
        "    # convert to ids\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    \n",
        "    masked_tokens = tokenizer.tokenize(blank)[1:]\n",
        "    masked_pos = []\n",
        "    for masked_token in masked_tokens:\n",
        "        #print(f'masked_token:{masked_token}')\n",
        "        try:\n",
        "            if tokens.index(masked_token):\n",
        "                pos = tokens.index(masked_token)\n",
        "        except ValueError:\n",
        "            pos = -1\n",
        "        if pos != -1:\n",
        "            token_ids[pos] = tokenizer.mask_token_id\n",
        "            masked_pos.append(pos)\n",
        "\n",
        "    print(f'トークン ID をもう一度トークンに戻す:{tokenizer.convert_ids_to_tokens(token_ids)}')\n",
        "            \n",
        "    # convert to tensor\n",
        "    token_tensor = torch.LongTensor([token_ids])\n",
        "\n",
        "    position_ids = list(range(0, token_tensor.size(1)))\n",
        "    position_id_tensor = torch.LongTensor([position_ids])\n",
        "    \n",
        "    # マスクに対応する上位 top_k 候補を得る\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=token_tensor, \n",
        "                        position_ids=position_id_tensor)\n",
        "    for _masked_pos in masked_pos:\n",
        "        predictions = outputs[0][0, _masked_pos].topk(10)\n",
        "        print(f'_masked_pos:{_masked_pos}')\n",
        "        for i, index_t in enumerate(predictions.indices):\n",
        "            index = index_t.item()\n",
        "            token = tokenizer.convert_ids_to_tokens([index])[0]\n",
        "            print(f'\\t{i}:{token}', end=\" \")\n",
        "        print()                            \n",
        "    #print(tokens)\n",
        "\n",
        "gpt_predict(\n",
        "    full_text='このパソコンは誰でも使えますが，コピーは有料です', \n",
        "    blank='有料',\n",
        "    model=model)\n",
        "\n",
        "gpt_predict(\n",
        "    full_text='次々に新しいゲームが作られる。', \n",
        "    blank='次々に',\n",
        "    model=model)\n"
      ],
      "metadata": {
        "id": "umW86BW-35uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DDe2NQv36gKv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}