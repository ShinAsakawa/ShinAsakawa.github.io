{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022_0126onomatopea_bert_mlm.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPcbbgHVZHLN2vrsJPycjpg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0126onomatopea_bert_mlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- filename: 2022_0126onomatopea_bert_mlm.ipynb\n",
        "- memo: 2022年01月22日現在，\n",
        "- author: 浅川伸一 asakawa@ieee.org\n",
        "- lincense: MIT\n",
        "\n",
        "* transformers は M1 Mac では動作しない。\n",
        "具体的には，最適化関数 Adamw を呼び出すと halt する。\n",
        "そこで，このコードでは，transformers 版の AdamW ではなく，PyTorch 版の AdamW を呼び出すように変更している。\n",
        "ただし，ホスト名の判断は決め打ちしているので，各自変更しなければならない。\n",
        "\n",
        "# 0 このコードの概要，ねらい\n",
        "\n",
        "- huggingface が提供する `transformers` から BERT を呼び出す。\n",
        "`transformers` に登録されているモデルのうち，東北大学乾研提供の 日本語化 BERT モデルを微調整 fine-tuning する。\n",
        "モデル名としては `cl-tohoku/bert-base-japanese` である。\n",
        "\n",
        "- この日本語化された BERT モデル (BERT-MLM) を `BertForMaskedLM` (マスク化言語モデルに特化した BERT) として呼び出し，オノマトペ予測課題としみなして訓練を行うことである。\n",
        "\n",
        "## 本コードの具体的な手順\n",
        "\n",
        "1. 必要なライブラリを輸入 import \n",
        "2. 小野編 「オノマトペ辞典4500」の読み込み\n",
        "3. 訓練済 日本語 BERT モデルの読み込み\n",
        "4. 日本語 BERT モデルで提供されているトークナイザに，小野編「オノマトペ辞典」を登録\n",
        "5. 訓練テキストデータ (original.csv) の読み込み\n",
        "6. 小野版オノマトペ辞典の，各オノマトペ記述文に出てくるオノマトペを [MASK] で置換する\n",
        "7. PyTorch の流儀に従って Dataset, DataLoader を定義する\n",
        "8. データセットを，訓練，検証，テストデータセットに 3 分割する\n",
        "9. 最適化関数を定義する\n",
        "10. 訓練と評価の定義\n",
        "10. 結果の損失関数の減衰曲線を描画\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-86UZiKBHgxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1  必要なライブラリを輸入 import "
      ],
      "metadata": {
        "id": "ZpyxNLpxWgWC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA-F1cfnHZiz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "from termcolor import colored\n",
        "\n",
        "# 本ファイルを Google Colaboratory 上で実行する場合に，必要となるライブラリをインストールする\n",
        "import platform\n",
        "isColab = platform.system() == 'Linux'\n",
        "if isColab:\n",
        "    !pip install transformers > /dev/null 2>&1 \n",
        "\n",
        "    # MeCab, fugashi, ipadic のインストール\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1\n",
        "    !pip install mecab-python3 > /dev/null 2>&1\n",
        "    !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "    !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1\n",
        "    \n",
        "    import subprocess\n",
        "    cmd='echo `mecab-config --dicdir`\\\"/mecab-ipadic-neologd\\\"'\n",
        "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                                     shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "    !pip install 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    !python -m unidic download > /dev/null 2>&1\n",
        "    !pip install ipadic > /dev/null 2>&1\n",
        "    !pip install jaconv > /dev/null 2>&1\n",
        "    !pip install japanize_matplotlib > /dev/null 2>&1    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch の seed の設定関連 再現性確保のため\n",
        "# https://qiita.com/takubb/items/7d45ae701390912c7629\n",
        "# https://qiita.com/si1242/items/d2f9195c08826d87d6ad\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# リソースの選択（CPU/GPU）\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 乱数シード固定（再現性の担保）\n",
        "def fix_seed(seed):\n",
        "    # random\n",
        "    random.seed(seed)\n",
        "    # numpy\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # pytorch\\n\",\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed = 42\n",
        "fix_seed(seed)\n",
        "# データローダーのサブプロセスの乱数のseedが固定\n",
        "def worker_init_fn(worker_id):\n",
        "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
        "    print(worker_init_fn(1))\n",
        "    \n",
        "# # データローダーの作成\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "#                                            batch_size=16,  # バッチサイズ\n",
        "#                                            shuffle=True,  # データシャッフル\n",
        "#                                            num_workers=2,  # 高速化\n",
        "#                                            pin_memory=True,  # 高速化\n",
        "#                                            worker_init_fn=worker_init_fn\n",
        "#                                            )"
      ],
      "metadata": {
        "id": "Chh77SjQHriA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 小野編 「オノマトペ辞典4500」の読み込み"
      ],
      "metadata": {
        "id": "_aHjOIAXH3kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2021/Jan 近藤先生からいただいたオノマトペ辞典のデータの読み込み\n",
        "\n",
        "#'日本語オノマトペ辞典4500より.xls' は著作権の問題があり，公にできません。\n",
        "# そのため Google Colab での解法，ローカルファイルよりアップロードしてください\n",
        "if isColab:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # ここで `日本語オノマトペ辞典4500より.xls` を指定してアップロードする\n",
        "    data_dir = '.'\n",
        "else:\n",
        "    data_dir = '/Users/asakawa/study/2021ccap/notebooks'\n",
        "\n",
        "import pandas as pd\n",
        "import jaconv\n",
        "\n",
        "onomatopea_excel = '2021-0325日本語オノマトペ辞典4500より.xls'\n",
        "onmtp2761 = pd.read_excel(os.path.join(data_dir, onomatopea_excel), sheet_name='2761語')\n",
        "\n",
        "#すべてカタカナ表記にしてデータとして利用する場合\n",
        "#`日本語オノマトペ辞典4500` はすべてひらがな表記だが，一般にオノマトペはカタカナ表記されることが多いはず\n",
        "#onomatopea = list(sorted(set([jaconv.hira2kata(o) for o in onmtp2761['オノマトペ']])))\n",
        "\n",
        "# Mac と Windows の表記の相違を吸収\n",
        "onomatopea = list(sorted(set([jaconv.normalize(o) for o in onmtp2761['オノマトペ']])))\n",
        "print(f'データファイル名: {os.path.join(data_dir, onomatopea_excel)}\\n',\n",
        "      f'オノマトペ単語総数: len(onomatopea):{len(onomatopea)}')"
      ],
      "metadata": {
        "id": "1__hASpmH0U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 訓練済 日本語 BERT モデルの読み込みと，小野編「オノマトペ辞典」のトークナイザへの登録"
      ],
      "metadata": {
        "id": "oV0uu5fsH90t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformers, huggingface 版の BERT 実装の読み込み\n",
        "import torch\n",
        "from transformers import BertConfig\n",
        "from transformers import BertForPreTraining\n",
        "from transformers import BertJapaneseTokenizer\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "model_ja_name = 'cl-tohoku/bert-base-japanese'  # 東北大学乾研による 日本語 BERT 実装\n",
        "model = BertForMaskedLM.from_pretrained(model_ja_name) # マスク化言語モデルを指定\n",
        "config = BertConfig.from_pretrained(model_ja_name)\n",
        "\n",
        "# GPU が利用可能であれば利用する\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "tknz1 = BertJapaneseTokenizer.from_pretrained(model_ja_name)\n",
        "# BPE (or sentencepiece) による下位単語分割あり"
      ],
      "metadata": {
        "id": "bYdnOFq4H-Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 日本語 BERT モデルで提供されているトークナイザに，小野編「オノマトペ辞典」を登録\n"
      ],
      "metadata": {
        "id": "vCQKyKKRIDAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# トークナイザ の修正，実際には onomatopea 単語リストを引数に指定して `add_tokens()` を呼び出すだけ\n",
        "# ただし，語彙数 tknz.vocab は変更されない。追加された語彙，本コードの場合はオノマトペは，\n",
        "# `tknz1.added_tokens_encoder` と `tknz1.added_tokens_decoder` に反映されているためである\n",
        "num_added = tknz1.add_tokens(onomatopea)\n",
        "print(f'追加されたトークン数:{num_added}/オノマトペ数:{len(onomatopea)}') \n",
        "model.resize_token_embeddings(len(tknz1))\n",
        "\n",
        "print(f' len(tknz1):{len(tknz1)}\\n', \n",
        "      f'len(tknz1.vocab):{len(tknz1.vocab)}\\n',  # 一見すると，この数字からオノマトペが追加されていないように見える。\n",
        "      f'tknz1.vocab_size:{tknz1.vocab_size}')    # 駄菓子菓子，下で見るように，正しく動作しているように見受けられる\n",
        "\n",
        "# print('# 確認用')\n",
        "# for w in onomatopea[-5:]:\n",
        "#     idx = tknz1.convert_tokens_to_ids(w)\n",
        "#     w_ = tknz1.convert_ids_to_tokens(idx)\n",
        "#     print(f'単語:{w}(id:{idx}) -> token:{w_}')    "
      ],
      "metadata": {
        "id": "_776KAyqIDaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 訓練テキストデータ (original.csv) の読み込み"
      ],
      "metadata": {
        "id": "BUo6bbQMIJwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 近藤先生 (2021年12月22日） から送っていただいた，オノマトペ文章データ 'original.csv' を読み込む\n",
        "import jaconv\n",
        "\n",
        "if isColab:\n",
        "    uploaded = files.upload()  # original.csv をアップロード\n",
        "    data_dir = '.'\n",
        "else:\n",
        "    data_dir = '/Users/asakawa/study/2021kondo_project'\n",
        "\n",
        "original = []\n",
        "n = 0\n",
        "with open(os.path.join(data_dir,'original.csv'), 'r', encoding='utf8') as f:\n",
        "    s = f.read()\n",
        "    for s_ in s.split('\\n'):\n",
        "        if n == 0:\n",
        "            n += 1\n",
        "            continue\n",
        "        idx, sent = s_.split(',')\n",
        "        \n",
        "        # Mac と Windows との unicode 符号化の差分を吸収する\n",
        "        # jaconv.normalize は内部で unicodedata.normalize('NFKC') を呼び出しているので\n",
        "        # 差異 between Mac and Windows を吸収できる\n",
        "        sent = ''.join(jaconv.normalize(x) for x in sent)\n",
        "        original.append(sent)\n",
        "        #original[int(idx)] = sent\n",
        "\n",
        "print(f'{len(original)} has been read')"
      ],
      "metadata": {
        "id": "ab0-wgS8IKGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 小野版オノマトペ辞典の，各オノマトペ記述文に出てくるオノマトペを [MASK] で置換する"
      ],
      "metadata": {
        "id": "61uMV6pHINMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_token_len = np.array([len(tknz1(s).input_ids) for s in original]).max()\n",
        "max_token_len += 2  # 保険のため 2 くらい加えておく\n",
        "print(f'max_token_len:{max_token_len}')\n",
        "\n",
        "# トークナイザにかけて出力を得る。`max_length` のデフォルトは 512 だが，今回は長文である必要がないと考えられる。\n",
        "# ここでは `max_token_len = 23` にしている。512 でも動作するが，学習に要する時間が増える\n",
        "text = tuple(original)  # 全文をタプルに変換\n",
        "inputs = tknz1(text, \n",
        "               return_tensors='pt', \n",
        "               max_length=max_token_len, \n",
        "               truncation=True, \n",
        "               padding='max_length')\n",
        "\n",
        "#`labels` キーを追加する。実際には inputs_ids なのでラベルではなくトークンID の系列\n",
        "inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "\n",
        "#トークン ID を走査して，オノマトペ単語であれば，[MASK] トークンに置き換える。\n",
        "l_ = []\n",
        "for l in inputs['labels']:\n",
        "    l_.append([tknz1.mask_token_id if w in onomatopea else tknz1.convert_tokens_to_ids(w) for w in tknz1.convert_ids_to_tokens(l)])\n",
        "\n",
        "inputs['input_ids'] = torch.LongTensor(l_)\n",
        "#print(inputs['input_ids'].shape)"
      ],
      "metadata": {
        "id": "7Y2gkpwrIP9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 PyTorch の流儀に従って Dataset, DataLoader を定義する\n"
      ],
      "metadata": {
        "id": "gBz16Gd4IUkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#データセットのためのクラスを定義\n",
        "class onmtpDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encoder):\n",
        "        self.encoder = encoder\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return {key:torch.tensor(val[idx]) for key, val in self.encoder.items()}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encoder.input_ids)\n",
        "    \n",
        "dataset = onmtpDataset(inputs)\n",
        "\n",
        "#データローダを準備\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# GPU/CPU 使用を設定し，モデルの訓練モードを起動 #Setup GPU/CPU usage and activate the training mode of our model.\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device) # モデルを選択したデバイスに移動 # and move our model over to the selected device\n",
        "model.train()  # 訓練モードに設定 #activate training mode"
      ],
      "metadata": {
        "id": "OobVI0zSIU6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 訓練データセットを，訓練，検証，テストデータセットの 3 つに分割する\n",
        "<!-- # Split train dataset into train, validation and test sets -->"
      ],
      "metadata": {
        "id": "PgoE8pbPIYfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#データセットを 7:1.5:1.5 に分割して 訓練データセット，検証データセット，テストデータセットに分割\n",
        "train_size = int(dataset.__len__() * 0.70)\n",
        "valid_size = int(dataset.__len__() * 0.15)\n",
        "test_size = dataset.__len__() - train_size - valid_size\n",
        "\n",
        "train_dataset, \\\n",
        "valid_dataset, \\\n",
        "test_dataset = torch.utils.data.random_split(dataset, \n",
        "                                             lengths=[train_size, test_size, valid_size], \n",
        "                                             generator=torch.Generator().manual_seed(seed))"
      ],
      "metadata": {
        "id": "WVAKvjdIIako"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 最適化関数を定義する"
      ],
      "metadata": {
        "id": "IfU9ng9wJO2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import socket \n",
        "#実行しているホスト名によって，M1 Mac であれば，Pytorch 版の AdamW を輸入し，そうでなければ Huggingface transformers 版の AdamW を輸入する  \n",
        "if not 'Sinope' in socket.gethostname():\n",
        "    # 以下だと M1 mac では halt する。\n",
        "    #最適化関数を初期化 (AdamW は重み付き崩壊で，過学習の可能性を減らす) \n",
        "    #Initialize our optimizer (Adam with weighted decay - reduces chance of overfitting).\n",
        "    from transformers import AdamW\n",
        "    #最適化関数を初期化 # initialize optimizer\n",
        "    optim = AdamW(model.parameters(), lr=5e-5)\n",
        "else:\n",
        "    # なので，transformers.AdamW ではなく，PyTorch の標準関数である Adam で代用する\n",
        "    from torch.optim import AdamW\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "ttN4Iql1JLd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 訓練と評価の定義"
      ],
      "metadata": {
        "id": "dk1wXANGJUUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import typing\n",
        "import transformers\n",
        "\n",
        "n_batch_size = 128\n",
        "traindataset_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                                  batch_size=n_batch_size, \n",
        "                                                  shuffle=True,\n",
        "                                                  pin_memory=True,\n",
        "                                                  worker_init_fn=worker_init_fn,\n",
        "                                                 )\n",
        "testdataset_loader  = torch.utils.data.DataLoader(test_dataset,  \n",
        "                                                  batch_size=n_batch_size, \n",
        "                                                  shuffle=False,\n",
        "                                                  pin_memory=True,\n",
        "                                                  worker_init_fn=worker_init_fn,\n",
        "                                                 )\n",
        "validdataset_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "                                                  batch_size=n_batch_size, \n",
        "                                                  shuffle=False,\n",
        "                                                  pin_memory=True,\n",
        "                                                  worker_init_fn=worker_init_fn,\n",
        "                                                 )\n",
        "\n",
        "def forward(data:transformers.tokenization_utils_base.BatchEncoding, \n",
        "        model:transformers.models.bert.modeling_bert.BertForMaskedLM=model) -> transformers.modeling_outputs.MaskedLMOutput:\n",
        "    _input_ids = data['input_ids'].clone().detach().to(device)  # ミニバッチサイズだけデータを取得\n",
        "    _attention_mask = data['attention_mask'].clone().detach().to(device)\n",
        "    _labels = data['labels'].clone().detach().to(device)\n",
        "    _out = model(_input_ids,\n",
        "                 attention_mask=_attention_mask,\n",
        "                 labels=_labels)\n",
        "    return _out\n",
        "\n",
        "\n",
        "def eval(data:transformers.tokenization_utils_base.BatchEncoding, \n",
        "         model:transformers.models.bert.modeling_bert.BertForMaskedLM=model) -> transformers.modeling_outputs.MaskedLMOutput:\n",
        "    model.eval()\n",
        "    _input_ids = data['input_ids'].clone().detach().to(device)  # ミニバッチサイズだけデータを取得\n",
        "    _attention_mask = data['attention_mask'].clone().detach().to(device)\n",
        "    _labels = data['labels'].clone().detach().to(device)\n",
        "    _out = model(_input_ids,\n",
        "                 attention_mask=_attention_mask,\n",
        "                 labels=_labels)\n",
        "    return _out\n",
        "\n"
      ],
      "metadata": {
        "id": "Npr4qGR-JR8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11 訓練の実施"
      ],
      "metadata": {
        "id": "KOXlJDz_XsQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tqdm import tqdm\n",
        "\n",
        "epochs = 40\n",
        "#train_losses, valid_losses = [], []\n",
        "for epoch in range(epochs):\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    valid_loop = tqdm(validdataset_loader)\n",
        "    for data in valid_loop:\n",
        "        _out = eval(data)\n",
        "        _loss = _out.loss\n",
        "        #valid_loss += _loss.item()\n",
        "        valid_loss = _loss.item() # * data['input_ids'].size(0)\n",
        "        valid_loop.set_description(f'\\t検証エポック {epoch}') # 進行状況の表示\n",
        "        valid_loop.set_postfix(loss=_loss.item())\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    train_loop = tqdm(traindataset_loader, leave=True)\n",
        "    for data in train_loop:\n",
        "        optim.zero_grad()   # 勾配情報の 0 クリア\n",
        "        _out = forward(data)\n",
        "        _loss = _out.loss   # 損失値を取得\n",
        "        _loss.backward()    # 取得した損失値に基づいて BERT のパラメータを逆伝播\n",
        "        optim.step()        # BERT パラメータの更新 すなわち学習\n",
        "        train_loop.set_description(f'訓練エポック {epoch}') # 進行状況の表示\n",
        "        train_loop.set_postfix(loss=_loss.item())\n",
        "        train_loss += _loss.item()\n",
        "    train_losses.append(train_loss)\n"
      ],
      "metadata": {
        "id": "04yrgJklKcZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12 損失の減衰曲線を描画"
      ],
      "metadata": {
        "id": "Bj0mbjxKXYQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "\n",
        "plt.plot(train_losses[1:], color='red', label='訓練')\n",
        "plt.plot(valid_losses[1:], color='green', label='検証')\n",
        "plt.xlabel('訓練時間')\n",
        "plt.ylabel('損失値')\n",
        "plt.legend()\n",
        "plt.title('オノマトペ微調整における学習の推移 (損失値) の減少')\n",
        "plt.savefig('2022_0126onomatopea_mlm_train.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0poxr8VVLNrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#上記グラフのダウンロード\n",
        "files.download('2022_0126onomatopea_mlm_train.pdf')"
      ],
      "metadata": {
        "id": "mPwHezf4oFiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13 全オノマトペデータに対して予測を行う"
      ],
      "metadata": {
        "id": "X3cwIsDlLaCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_an_output(N, original=original, tknz1=tknz1, inputs=inputs, print_flag=True):\n",
        "    \"\"\"\n",
        "    引数として 数字を 1 つ入力すると (N)，`original.csv` の N 行目のデータを読み込んで，\n",
        "    その文のオノマトペを [MASK] に置き換えて，マスク化言語モデルで [MASK] を予測する。\n",
        "    結果を表示する場合には 引数 `print_flag=True` として呼び出す\n",
        "    \"\"\"\n",
        "    if N >= len(original) or (not isinstance(N, int)):\n",
        "        return\n",
        "\n",
        "    _out = model(inputs.input_ids[N].unsqueeze(0).to(device), attention_mask=inputs.attention_mask[N].unsqueeze(0).to(device), labels=inputs.labels[N].to(device))\n",
        "    _x = _out.logits.detach()\n",
        "    __x = _x.squeeze(0).detach().clone()\n",
        "    _pred_idx  = torch.argmax(__x, dim=1, keepdim=True)\n",
        "    _pred_s    = \"/\".join(tknz1.convert_ids_to_tokens(_pred_idx)).replace('/[PAD]','')\n",
        "    \n",
        "    _orig      = original[N] # 原文\n",
        "    _inp_idx   = tknz1.convert_ids_to_tokens(inputs.input_ids[N]) # 入力トークンID\n",
        "    _inp_s     = \"/\".join(_inp_idx).replace('/[PAD]','')          # 入力文\n",
        "    _teach_idx = tknz1.convert_ids_to_tokens(inputs.labels[N])    # 教師信号トークンID\n",
        "    _teach_s   = \"/\".join(_teach_idx).replace('/[PAD]','')        # 教師信号文\n",
        "    \n",
        "    _mask_pos = np.where(inputs.input_ids[N].detach().numpy() == tknz1.mask_token_id)\n",
        "    _teach_tokens = inputs.labels[N][_mask_pos].detach().squeeze().numpy()\n",
        "    _pred_tokens  = _pred_idx[_mask_pos].detach().squeeze().cpu().numpy()\n",
        "\n",
        "    _n_hit = np.array([_teach_tokens == _pred_tokens]).sum()       # 正解したか否か\n",
        "    if print_flag:\n",
        "        color = 'grey' if _n_hit > 0 else 'red'\n",
        "        print(f'{N:5,d}   原文:{_orig}')\n",
        "        print(f'\\t入力:{_inp_s}')\n",
        "        print(f'\\t正解:{_teach_s}')\n",
        "        print(colored(f'\\t出力:{_pred_s}',color))\n",
        "        print(f'\\tmask 位置:{_mask_pos}')\n",
        "        print(f'\\t正解トークン:{_teach_tokens}', f'予測トークン:{_pred_tokens}', \n",
        "              f'{np.array([_teach_tokens == _pred_tokens]).sum() > 0}')\n",
        "        print(f'\\t_out.loss:{_out.loss:.3f}')\n",
        "    \n",
        "    return _out.loss, _n_hit\n",
        "\n"
      ],
      "metadata": {
        "id": "c-PGg7XCMRx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "model.eval()\n",
        "total_hit = 0\n",
        "for i in range(len(original)):\n",
        "    _, hit = eval_an_output(i, print_flag=False) # print_flag = True にすると推論結果を表示します. 逆に False にすれば正解率だけ計算します\n",
        "    total_hit += hit\n",
        "\n",
        "print(f'正解数:{total_hit}/{len(original)}= {total_hit/len(original) * 100:.3f} %')    "
      ],
      "metadata": {
        "id": "KydHjkRqO2Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14 任意の文章を入力して，オノマトペ語の予測を行う"
      ],
      "metadata": {
        "id": "Oea4T1s5LsS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import typing\n",
        "\n",
        "def eval_a_line(inp:str or list, \n",
        "                model:transformers.models.bert.modeling_bert.BertForMaskedLM=model, \n",
        "                tknz1:transformers.models.bert_japanese.tokenization_bert_japanese.BertJapaneseTokenizer=tknz1,\n",
        "                onomatopea_vocab=onomatopea,\n",
        "                max_token_len:int=max_token_len,\n",
        "                print_flag:bool = True):\n",
        "    \"\"\"任意の文章を入れて，オノマトペを [MASK] に入れ替えて評価する\"\"\"\n",
        "    if isinstance(inp, str):\n",
        "        _inp = tknz1(inp,\n",
        "                     return_tensors='pt', \n",
        "                     max_length=max_token_len, \n",
        "                     truncation=True, \n",
        "                     padding='max_length')\n",
        "        \n",
        "        #`labels` キーを追加する。実際には inputs_ids なのでラベルではなくトークン ID の系列\n",
        "        _inp['labels'] = _inp.input_ids.detach().clone().squeeze()\n",
        "\n",
        "        #トークン ID を走査して，オノマトペ単語であれば，[MASK] トークンに置き換える。\n",
        "        _inp['input_ids'] = torch.LongTensor([tknz1.mask_token_id if w in onomatopea_vocab else tknz1.convert_tokens_to_ids(w) for w in tknz1.convert_ids_to_tokens(_inp['input_ids'].squeeze())]).unsqueeze(0)\n",
        "\n",
        "        _str =  \"\".join(tknz1.convert_ids_to_tokens(_inp.input_ids.squeeze()))\n",
        "        _orig = _str.replace('[CLS]','').replace('[PAD]','').replace('[SEP]','')\n",
        "        \n",
        "        _str =  \"\".join(tknz1.convert_ids_to_tokens(_inp.input_ids.squeeze()))\n",
        "        _mask = _str.replace('[CLS]','').replace('[PAD]','').replace('[SEP]','')\n",
        "        _out = model(_inp.input_ids.to(device), attention_mask=_inp.attention_mask.to(device), labels=_inp.labels.to(device))\n",
        "        _logit = _out.logits.clone().detach().squeeze()\n",
        "        _pred_idx  = torch.argmax(_logit, dim=1, keepdim=True)\n",
        "        _pred_s    = \"\".join(tknz1.convert_ids_to_tokens(_pred_idx.squeeze())).replace('[PAD]','').replace('[CLS]','').replace('[SEP','')\n",
        "\n",
        "\n",
        "        _positions =  _inp['input_ids'].clone().detach().squeeze().cpu().numpy()\n",
        "        _mask_pos = np.where(_positions == tknz1.mask_token_id)\n",
        "        _teach_tokens = _inp['labels'][_mask_pos].clone().detach().squeeze().cpu().numpy()\n",
        "        _pred_tokens  = _pred_idx[_mask_pos].clone().detach().squeeze().cpu().numpy()\n",
        "        _is_hit = np.array([_teach_tokens == _pred_tokens]).sum() > 0       # 正解したか否か\n",
        "\n",
        "        if print_flag:\n",
        "            print(f'ソース:{inp}')\n",
        "            print(f'入力文:{_orig}')\n",
        "            #print(f'マスク:{_mask}')\n",
        "            print(f'予測文:{_pred_s}')\n",
        "            (color, attrs) = ('red',['bold','reverse']) if not _is_hit else ('grey', ['bold'])\n",
        "            print(f'({_teach_tokens}=={_pred_tokens})=', \n",
        "                  colored(f'{_is_hit}', color, attrs=attrs), \n",
        "                  colored(f'loss:{_out.loss:.3f}', color, attrs=attrs))\n",
        "        return _is_hit, _out.loss\n",
        "    elif isinstance(inp,list):\n",
        "        return [eval_a_line(l) for l in inp]\n",
        "    "
      ],
      "metadata": {
        "id": "d3fm7Ft-v2Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset2list(_dataset:torch.utils.data.dataset.Subset=valid_dataset)-> list:\n",
        "    \"\"\"subdataset を list に変換する\"\"\"\n",
        "    ret = []\n",
        "    for d in _dataset:\n",
        "        _s = tknz1.convert_ids_to_tokens(d['labels'].clone().detach())\n",
        "        s = \"\".join(w for w in _s)\n",
        "        ret.append(s.replace('[PAD]','').replace('[CLS]','').replace('[SEP]',''))\n",
        "    return ret\n",
        "\n",
        "_train = dataset2list(train_dataset)\n",
        "_valid = dataset2list(valid_dataset)\n",
        "_test  = dataset2list(test_dataset) "
      ],
      "metadata": {
        "id": "2ERPPCR0z2nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15 訓練，検証，テスト，各データセットについての精度を計算"
      ],
      "metadata": {
        "id": "BlB6V5ydLz17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_hits = 0\n",
        "for l in _train:\n",
        "    (hit, loss) = eval_a_line(l, print_flag=False)\n",
        "    total_hits += hit\n",
        "print(f'total_hits:{total_hits}/len(train_dataset):{len(_train)}={total_hits/len(_train):.2f}%')\n",
        "\n",
        "total_hits = 0\n",
        "for l in _test:\n",
        "    (hit, loss) = eval_a_line(l, print_flag=False)\n",
        "    total_hits += hit\n",
        "print(f'total_hits:{total_hits}/len(test_dataset):{len(_test)}={total_hits/len(_test):.2f}%')\n",
        "\n",
        "total_hits = 0\n",
        "for l in _valid:\n",
        "    (hit, loss) = eval_a_line(l, print_flag=False)\n",
        "    total_hits += hit\n",
        "print(f'total_hits:{total_hits}/len(valid_dataset):{len(_valid)}={total_hits/len(_valid):.2f}%')"
      ],
      "metadata": {
        "id": "1g7JhXMpEw94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16 適当な文章を入力して結果を観察してみる"
      ],
      "metadata": {
        "id": "rt-D_VIdMt8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#任意の文章を入力して，オノマトペ語の予測を行う\n",
        "\n",
        "s = '今日の朝，彼女に壁どんした'\n",
        "eval_a_line(s)\n",
        "\n",
        "s = '私ははっ（と）驚いた。'\n",
        "eval_a_line(s)"
      ],
      "metadata": {
        "id": "m31F_VboFQiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# X あと，上位 ｎ 個の予測を作らねばならんなー"
      ],
      "metadata": {
        "id": "18x_BB9lMpF_"
      }
    }
  ]
}