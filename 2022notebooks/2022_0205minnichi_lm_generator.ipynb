{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0205minnichi_lm_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- date: 2022_0206\n",
        "- filename: 2022_0205minnichi_lm_generator.ipynb\n",
        "\n",
        "# [「みんなの日本語」](https://www.3anet.co.jp/np/books/2300/) 語彙を用いた [符号化ー復号化 (encoder-decoder) モデル](https://arxiv.org/pdf/1409.3215) による言語生成\n",
        "\n",
        "- 想定発表媒体: 某日本語教育研究会論文誌\n",
        "- 著者: 岩下 智彦，吉原 将大，浅川伸一\n",
        "\n",
        "\n",
        "### 注:\n",
        "\n",
        "このファイルを自分のローカル PC で動作させるためには，Python の実行環境とブラウザベースのインタフェイス jupyter-notebook または jupyter-lab が必要です。\n",
        "OS が MacOSX であれば，[homebrew](https://brew.sh/) と [anaconda](https://www.anaconda.com/products/individual) をインストール済である必要があります。\n",
        "加えて，`PyTorch`, `MeCab`, `jaconv`, `japanize_matplotlib`, `konoha` と言った，標準的なライブラリをインストールしておく必要があります。\n",
        "以下に上記 4 つのライブラリのインスールを行うサンプルオペレーションを示します。\n",
        "\n",
        "```bash\n",
        "conda install pytorch torchvision torchaudio -c pytorch\n",
        "\n",
        "brew install mecab\n",
        "brew install mecab-ipadic\n",
        "pip install mecab-python3brew install mecab\n",
        "\n",
        "pip install jaconv\n",
        "pip install japanize-matplotlib\n",
        "pip install 'konoha[mecab]'\n",
        "```\n",
        "\n",
        "ローカル Mac に自力で，環境構築をする場合の老婆 (翁?) 心的基本方針を記して起きます。\n",
        "homebrew にパッケージ管理を極力任せる方が，後々の管理が楽になるでしょう。\n",
        "自力で複数のパッケージを管理すると，依存関係が煩雑になって心折れてしまいます。\n",
        "Homebrew が対応していない場合のみ，anaconda 付属の pip でイントールするという方針で行くと，自分のミスや誤解に起因するインストール済ライブラリの依存関係の不一致に悩む可能性が減ります。\n",
        "\n",
        "本コードをローカル Mac で動作させるためには，git コマンドを用いて Github 上にアップロードした自作クラス `Minnichi` をローカルディスクに保存する必要があります。\n",
        "これは直下セルの 12 行目で実行しているコマンドを，自身のローカル Mac 上で行うことを意味します。\n",
        "具体的には，Mac の「端末エミュレータ」あるいは類似のエミュレータ上で以下のコマンドを実行します。\n",
        "\n",
        "```bash\n",
        "git clone https://github.com/ShinAsakawa/ccap.git\n",
        "```\n",
        "\n",
        "こうすることで，カレントディレクトリ直下に `ccap` というディレクトリが作成されます。\n",
        "この `ccap` ディレクトリ内に `minnichi.py` というファイルがあります。\n",
        "`ccap` とは，本プロジェクトとは異なるプロジェクトです。\n",
        "もちろん，将来的には，別の github レポジトリを作成した方が良いと考えています。\n",
        "ですが，当面のお試しコードの意味合いもありますので，今回は `ccap` プロジェクトのレポジトリに間借りして作成してあります。\n"
      ],
      "metadata": {
        "id": "1AtaCmENiTa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 準備: 必要となるライブラリのインストールなど"
      ],
      "metadata": {
        "id": "B1X23XhftLg5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv01za6x05Uj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ローカルと colab との相違を吸収するために\n",
        "# 本ファイルを Google Colaboratory 上で実行する場合に，必要となるライブラリをインストール\n",
        "import platform\n",
        "isColab = platform.system() == 'Linux'\n",
        "if isColab:\n",
        "    ![ -d ccap ] & /bin/rm -rf ccap\n",
        "    !git clone https://github.com/ShinAsakawa/ccap.git\n",
        "    !pip install japanize_matplotlib > /dev/null 2>&1\n",
        "    !pip install jaconv > /dev/null 2>&1\n",
        "\n",
        "    # MeCab, fugashi, ipadic のインストール\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y # > /dev/null 2>&1\n",
        "    !pip install mecab-python3 > /dev/null 2>&1\n",
        "    #!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "    #!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1\n",
        "    \n",
        "    import subprocess\n",
        "    cmd='echo `mecab-config --dicdir`\\\"/usr/share/mecab/dic/ipadic\\\"'\n",
        "    path_ipadic = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                                    shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "    !pip install 'konoha[mecab]'\n",
        "    #!pip install 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    #!python -m unidic download > /dev/null 2>&1\n",
        "    #!pip install ipadic > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O mecab-ipadic.tar.gz 'https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7MWVlSDBCSXZMTXM'\n",
        "!tar zxfv mecab-ipadic.tar.gz\n",
        "!cd /content/mecab-ipadic-2.7.0-20070801/\n",
        "#!/content/mecab-ipadic-2.7.0-20070801/configure --with-charset=utf8\n",
        "#!make install\n",
        "##!ls mecab-ipadic-2.7.0-20070801/\n",
        "!/usr/lib/mecab/mecab-dict-index -d /content/mecab-ipadic-2.7.0-20070801 -o /content/mecab-ipadic-2.7.0-20070801 -f EUC-JP -t utf8\n",
        "#上記 mecab-dict-index をして dicrc やら mecabrc やらが，consistent でないと mecab が動作しないなー。"
      ],
      "metadata": {
        "id": "hnCQR3NA1N7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!echo \"dicdir =  /opt/homebrew/lib/mecab/dic/ipadic\" > mecabrc\n",
        "#!mecab --rcfile=./mecabrc -Owakati -d /usr/share/mecab/dic/ipadic\n",
        "!mecab --rcfile=/content/mecabrc -Owakati -d /content/mecab-ipadic-2.7.0-20070801\n"
      ],
      "metadata": {
        "id": "SdL_bUrUwmKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn7xiIJQnH4A"
      },
      "outputs": [],
      "source": [
        "#%reload_ext autoreload\n",
        "#%autoreload 2\n",
        "import MeCab\n",
        "import jaconv\n",
        "from ccap.minnichi import Minnichi\n",
        "\n",
        "Minn = Minnichi(reload=True)\n",
        "#Minn.save_data()\n",
        "Minn = Minnichi(reload=False)\n",
        "\n",
        "print(Minn.tokenize('お前はトラだ。虎になるのだ。', max_length=25,pad=False)['input_ids'])\n",
        "print(Minn.convert_ids2tokens(Minn.tokenize('お前はトラだ。虎になるのだ。', max_length=20,pad=False)['input_ids']))\n",
        "#X = np.random.randint(M.__len__())\n",
        "\n",
        "# for X in range(3):\n",
        "#     for _M in [Minn, Minn]:\n",
        "#         print(f'X:{X}\\n',\n",
        "#               f\"_M(X)['input_ids']:{_M(X)['input_ids']}\\n\",\n",
        "#               f\"_M.convert_ids2tokens(_M(X)['input_ids']):{_M.convert_ids2tokens(_M(X)['input_ids'])}\\n\",\n",
        "#               f\"_M.convert_tokens2ids(_M(X)['tokens']):{_M.convert_tokens2ids(_M(X)['tokens'])}\\n\",\n",
        "#               f\"_M.convert_ids2tokens(_M.convert_tokens2ids(_M(X)['tokens'])):{_M.convert_ids2tokens(_M.convert_tokens2ids(_M(X)['tokens']))}\\n\",\n",
        "#               f\"_M(X)['tokens']:{_M(X)['tokens']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MeCab.Tagger(f'-Owakati -d /usr/share/mecab/dic/')\n",
        "!ls /usr/share/mecab/dic/ipadic"
      ],
      "metadata": {
        "id": "DUOLkF0UvAxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtm9XMO1guoe"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    print(Minn.lines[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeP7fFMo1Fb5"
      },
      "outputs": [],
      "source": [
        "Minn.draw_freq(figsize=(28,8),rotation=25)\n",
        "MAX_LENGTH = Minn.max_length + 1\n",
        "Minn(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTWxlIIr1z6R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#データセットのためのクラスを定義\n",
        "class minnichiDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encoder:Minnichi):\n",
        "        self.encoder = encoder\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return self.encoder(idx)['input_ids']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.encoder.__len__()\n",
        "\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    \"\"\"RNNによる符号化器\"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    \"\"\"注意付き復号化器の定義\"\"\"\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyy7anAe18OQ"
      },
      "outputs": [],
      "source": [
        "def tensorFromIds(sentence_ids):\n",
        "    return torch.tensor(sentence_ids, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "teacher_forcing_ratio = 0.5  # 訳注：教師強制率。文献によっては，訓練中にこの値を徐々に減衰させることも行われます\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden() # 符号化器の中間層を初期化\n",
        "    encoder_optimizer.zero_grad()         # 符号化器の最適化関数の初期化\n",
        "    decoder_optimizer.zero_grad()         # 復号化器の最適化関数の初期化\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[Minn.vocab.index('<SOS>')]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            #if decoder_input.item() == EOS_token:\n",
        "            if decoder_input.item() == Minn.vocab.index('<EOS>'):\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL4a602x2XGu"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    \"\"\"時間変数を見やすいように，分と秒に変換して返す\"\"\"\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return f'{int(m):2d}分 {int(s):2d}秒'\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    \"\"\"開始時刻 since と，現在の処理が全処理中に示す割合 percent を与えて，経過時間と残り時間を計算して表示する\"\"\"\n",
        "    now = time.time()  #現在時刻を取得\n",
        "    s = now - since    # 開始時刻から現在までの経過時間を計算\n",
        "    #s = since - now    \n",
        "    es = s / (percent) # 経過時間を現在までの処理割合で割って終了予想時間を計算\n",
        "    rs = es - s        # 終了予想時刻から経過した時間を引いて残り時間を計算\n",
        "    #return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "    return f'経過時間:{asMinutes(s)} (残り時間 {asMinutes(rs)})'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A92raujjguoi"
      },
      "outputs": [],
      "source": [
        "from termcolor import colored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX0oQr2u2g5Z"
      },
      "outputs": [],
      "source": [
        "def fit(encoder:nn.Module, \n",
        "        decoder:nn.Module, \n",
        "        epochs:int=20, \n",
        "        lr:float=0.001, \n",
        "        n_sample:int=3)->list:\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    #encoder_optimizer = optim.SGD(encoder.parameters(), lr=lr)\n",
        "    #decoder_optimizer = optim.SGD(decoder.parameters(), lr=lr)\n",
        "    encoder_optimizer = optim.adamw(encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = optim.adamw(decoder.parameters(), lr=lr)\n",
        "    criterion = nn.NLLLoss()\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        #エポックごとに学習順をシャッフルする\n",
        "        learning_order = np.random.permutation(len(Minn.lines)) \n",
        "        for i in range(len(Minn.lines)):\n",
        "            x = learning_order[i]   # ランダムにデータを取り出す \n",
        "            inputs = Minn(x)['input_ids']\n",
        "            input_tensor = tensorFromIds(inputs)\n",
        "            target_tensor = tensorFromIds(inputs)\n",
        "            \n",
        "            #訓練の実施\n",
        "            loss = train(input_tensor, target_tensor, \n",
        "                         encoder, decoder, \n",
        "                         encoder_optimizer, decoder_optimizer, \n",
        "                         criterion)\n",
        "            epoch_loss += loss\n",
        "        \n",
        "        losses.append(epoch_loss/len(Minn.vocab))\n",
        "        print(colored(f'エポック:{epoch:2d} 損失:{epoch_loss/len(Minn.vocab):.2f}', 'cyan', attrs=['bold']),\n",
        "              f'{timeSince(start_time, (epoch+1) * len(Minn.vocab)/(epochs * len(Minn.vocab)))}')\n",
        "        \n",
        "        evaluateRandomly(encoder,decoder, n=n_sample)\n",
        "        \n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNXfBJNy2n88"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder:nn.Module, \n",
        "             decoder:nn.Module, \n",
        "             input_ids:list, \n",
        "             max_length:int=MAX_LENGTH)->(list,torch.LongTensor):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromIds(input_ids)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[Minn.vocab.index('<SOS>')]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == Minn.vocab.index('<EOS>'):\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(Minn.vocab[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9n5wDv02soE"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder:nn.Module, \n",
        "                     decoder:nn.Module, \n",
        "                     n:int=5)->None:\n",
        "    for x in np.random.randint(Minn.__len__(), size=n):\n",
        "        input_ids = Minn(x)['input_ids']\n",
        "        input_sent = \"\".join(Minn(x)['tokens'])\n",
        "        print(f'入力: {input_ids}: {input_sent}')\n",
        "        output_words, attentions = evaluate(encoder, decoder, input_ids)\n",
        "        output_sent = \"\".join(w for w in output_words)\n",
        "        print(f'出力: {[Minn.vocab.index(c) for c in output_words]}',\n",
        "              f': {output_sent}')\n",
        "        print('---')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zifpy2YC2iud"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "hidden_size = 256\n",
        "encoder = EncoderRNN(len(Minn.vocab), hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, len(Minn.vocab), dropout_p=0.1).to(device)\n",
        "\n",
        "losses = []\n",
        "losses = losses + fit(encoder, decoder, epochs=10, n_sample=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmrc13wKguol"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#losses = []\n",
        "losses = losses + fit(encoder, decoder, epochs=3, n_sample=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Llp_LTn2Z7U"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points:list)->None:\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base=0.2) # this locator puts ticks at regular intervals\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    \n",
        "showPlot(losses)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCbXunAm2vPi"
      },
      "outputs": [],
      "source": [
        "evaluateRandomly(encoder, decoder, n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3YdUpABguom"
      },
      "outputs": [],
      "source": [
        "def evaluate_free_input(encoder:nn.Module, \n",
        "                        decoder:nn.Module,\n",
        "                        inp=None,\n",
        "                       )->None:\n",
        "    if inp == None:\n",
        "        inp = input()\n",
        "    inp = jaconv.normalize(inp)\n",
        "    inputs = Minn.tokenize(inp, pad=False)\n",
        "    input_ids = inputs['input_ids']\n",
        "    input_sent = \"\".join(inputs['tokens'])\n",
        "    print(f'入力: {input_ids}: {input_sent}')\n",
        "    output_words, attentions = evaluate(encoder, decoder, input_ids)\n",
        "    output_sent = \"\".join(w for w in output_words)\n",
        "    print(f'出力: {[Minn.vocab.index(c) for c in output_words]}',\n",
        "          f': {output_sent}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRujOPGxguom"
      },
      "outputs": [],
      "source": [
        "evaluate_free_input(encoder,decoder, inp='おらは死んじまっただ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2neKJw7guom"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2022_0205minnichi_lm_generator.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}