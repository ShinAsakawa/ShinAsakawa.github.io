{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022_0529iwashita_yoshihara_demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMyAPdF/3Mr7mykFubYXlZ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0529iwashita_yoshihara_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "- data: 2022_0529\n",
        "- tile: `2022_0529iwashita_yoshihara_demo.ipynb'\n",
        "- author: 浅川伸一\n",
        "---\n",
        "\n",
        "# BERT のマスク化言語モデルを使った穴埋め問題のデモ"
      ],
      "metadata": {
        "id": "LHY90NGkOfr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install --upgrade openpyxl\n",
        "    !pip install --upgrade pandas\n",
        "    !pip install --upgrade fugashi[unidic-lite]\n",
        "    !pip install --upgrade ipadic\n",
        "    !python -m unidic download\n",
        "    !pip install transformers\n",
        "\n",
        "    !pip install --upgrade jaconv"
      ],
      "metadata": {
        "id": "EwKcisfSrrGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT の輸入"
      ],
      "metadata": {
        "id": "YjMpm_xIOa_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertJapaneseTokenizer\n",
        "from transformers import BertForMaskedLM\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "# stockmarket 本での事前訓練済データ\n",
        "# model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
        "# model_ja_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
        "model_ja_name = 'cl-tohoku/bert-base-japanese'  # 東北大学乾研による 日本語 BERT 実装\n",
        "# see https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2\n",
        "#model_ja_name = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # 東北大学乾研による 日本語 BERT 実装\n",
        "\n",
        "tknz = BertJapaneseTokenizer.from_pretrained(model_ja_name)\n",
        "bert_lm = BertForMaskedLM.from_pretrained(model_ja_name, return_dict = True)\n",
        "#model_orig = BertForMaskedLM.from_pretrained(model_ja_name, return_dict = True)\n",
        "\n",
        "# リソースの選択（CPU/GPU）\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "###################################################################################\n",
        "# import torch\n",
        "# from transformers import AutoModel, AutoTokenizer\n",
        "# bertjapanese = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese-char\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-char\")\n",
        "###################################################################################"
      ],
      "metadata": {
        "id": "mPwn0bXlKEzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# やさしい日本語のデータを取得する"
      ],
      "metadata": {
        "id": "POs_6kuRtTVF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmQoDkMUqNR5"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import requests\n",
        "import pandas as pd\n",
        "SNOWs={'T15': {'url':\"https://filedn.com/lit4DCIlHwxfS1gj9zcYuDJ/SNOW/T15-2020.1.7.xlsx\"},\n",
        "       'T23': {'url':\"https://filedn.com/lit4DCIlHwxfS1gj9zcYuDJ/SNOW/T23-2020.1.7.xlsx\"},}\n",
        "\n",
        "\n",
        "for corpus in SNOWs:\n",
        "    url = SNOWs[corpus]['url']\n",
        "    excel_fname = corpus + '-2020.1.7.xlsx'\n",
        "    if not os.path.exists(excel_fname):\n",
        "        r = requests.get(url)\n",
        "        with open(excel_fname, 'wb') as f:\n",
        "            total_length = int(r.headers.get('content-length'))\n",
        "            print(f'{excel_fname} をダウンロード中 {total_length} バイト')\n",
        "            f.write(r.content)\n",
        "\n",
        "    SNOWs[corpus]['df'] = pd.read_excel(excel_fname, engine='openpyxl')\n",
        "    SNOWs[corpus]['df'] = SNOWs[corpus]['df'].rename(columns={'#日本語(原文)': 'ja', \n",
        "                                                              '#やさしい日本語':'easy_ja',\n",
        "                                                              '#英語(原文)':'en'})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jaconv\n",
        "\n",
        "_snow_sents = SNOWs['T15']['df']['easy_ja'].to_list() + SNOWs['T23']['df']['easy_ja'].to_list()\n",
        "snow_sents = [jaconv.normalize(line, 'NFKC') for line in _snow_sents]\n",
        "print(snow_sents[:3])"
      ],
      "metadata": {
        "id": "wlq6iycBrYQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "\n",
        "texts = ['誰が一番に着くか私には分かりません。', '多くの動物が人間によって殺された。', '私はテニス部員です。']\n",
        "mask_token = tknz.special_tokens_map['mask_token']\n",
        "for text in texts:\n",
        "    print(tknz(text)['input_ids'])\n",
        "    print(tknz.convert_ids_to_tokens(tknz(text)['input_ids']))\n",
        "    token_to_be_masked = '分かり'\n",
        "    text_masked = text.replace(token_to_be_masked, mask_token)\n",
        "    print(text_masked)\n",
        "    print(text)\n",
        "#help(text.replace)"
      ],
      "metadata": {
        "id": "TGZ1eHtRKk4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tknz.special_tokens_map"
      ],
      "metadata": {
        "id": "1BOCpAsaMcqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = '誰が一番に着くか私には分かりません。'\n",
        "# _text = text.replace('誰が','[MASK]')\n",
        "# print(_text)"
      ],
      "metadata": {
        "id": "tJXCeE-LMD_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "texts = ['誰が一番に着くか私には分かりません。', '多くの動物が人間によって殺された。', '私はテニス部員です。']\n",
        "masked_texts = ['誰が一番に着くか私には[MASK]ません。', '多くの[MASK]が人間によって殺された。', '私はテニス部員[MASK]。']\n",
        "\n",
        "n_max = 5\n",
        "for i, text in enumerate(masked_texts):\n",
        "    print(colored(f'{i:3d} text:{text}', color='blue', attrs=['bold']))\n",
        "    inputs = tknz.encode_plus(text, return_tensors=\"pt\")\n",
        "    mask_index = torch.where(inputs['input_ids'][0] == tknz.mask_token_id)\n",
        "    outputs  = bert_lm(**inputs)\n",
        "    logits = outputs.logits\n",
        "    softmax = F.softmax(logits, dim=-1)\n",
        "    mask_word = softmax[0, mask_index, :]\n",
        "    topN = torch.topk(mask_word, n_max, dim=1)[1][0]\n",
        "\n",
        "    for i, token in enumerate(topN):\n",
        "        wrd = tknz.convert_ids_to_tokens([token])\n",
        "        sentence_replaced = text.replace(tknz.mask_token, wrd[0])\n",
        "        print(f'{i+1:2d}', colored(sentence_replaced, color='grey', attrs=['bold']))\n"
      ],
      "metadata": {
        "id": "otxMBypzuVdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# text =  '多くの動物が人間によって殺された。'\n",
        "# inputs = tknz.encode_plus(text, return_tensors='pt')\n",
        "# print(f'inputs_[\"input_ids\"]:{inputs[\"input_ids\"]}')\n",
        "# print(f'inputs[\"input_ids\"][0]:{inputs[\"input_ids\"][0]}')\n",
        "# input_ids_length = inputs['input_ids'].size()[1]\n",
        "# mask_idx = np.random.choice(input_ids_length)\n",
        "# print(f'mask_idx:{mask_idx}')\n",
        "# input_ids_masked = inputs['input_ids'].detach()\n",
        "# input_ids_masked[0][mask_idx] = tknz.mask_token_id\n",
        "# print(f'input_ids_masked:{input_ids_masked}')\n",
        "# print(tknz.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
        "\n",
        "# inputs['input_ids'] = input_ids_masked\n",
        "# bert_\n",
        "# #print(tknz.convert_ids_to_tokens(torch.LongTensor(input_ids_masked)))\n",
        "\n",
        "texts = ['誰が一番に着くか私には分かりません。', '多くの動物が人間によって殺された。', '私はテニス部員です。']\n",
        "\n",
        "n_max = 5\n",
        "for i, text in enumerate(texts):\n",
        "    print(colored(f'{i:3d} text:{text}', color='blue', attrs=['bold']))\n",
        "    inputs = tknz.encode_plus(text, return_tensors=\"pt\")\n",
        "    mask_pos = np.random.choice(inputs['input_ids'].size()[1]-2) + 1\n",
        "    mask_idx = inputs['input_ids'][0][mask_pos]\n",
        "    word_masked = tknz.convert_ids_to_tokens([mask_idx])\n",
        "    print(f'mask_pos:{mask_pos}', f'mask_idx:{mask_idx}',   f'mask_word:{word_masked}')\n",
        "    inputs['input_ids'][0][mask_index] = tknz.mask_token_id\n",
        "    outputs  = bert_lm(**inputs)\n",
        "    logits = outputs.logits\n",
        "    _softmax = F.softmax(logits, dim=-1)\n",
        "    _words_preded = _softmax[0, mask_index, :]\n",
        "    _topN_token = torch.topk(_words_preded, n_max)[1]  #, dim=1)\n",
        "\n",
        "    for j, token in enumerate(_topN_token):\n",
        "        word_pred = tknz.convert_ids_to_tokens([token])\n",
        "        #print(f'word_pred[0]:{word_pred[0]}')\n",
        "        sentence_replaced = text.replace(word_masked[0], word_pred[0])\n",
        "        print(f'{j+1:2d}', colored(sentence_replaced, color='grey', attrs=['bold']))\n"
      ],
      "metadata": {
        "id": "Pw55ZZ-DNnsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hdcSiCvkP2P0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}