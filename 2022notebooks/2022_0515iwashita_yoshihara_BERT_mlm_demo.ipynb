{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 2022_0515iwashita_yoshihara_BERT_mlm_demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNdzUHPWF8wdjlIYRohU9qE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0515iwashita_yoshihara_BERT_mlm_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 岩下，吉原勉強会資料\n",
        "- date: 2022_0515\n",
        "- filename: `2022_0515iwashita_yoshihara_BERT_mlm_demo.ipynb`\n",
        "- memo: BERT を用いたマスク化言語モデルによる穴埋め問題の回答や選択肢作成に向けて"
      ],
      "metadata": {
        "id": "Z05gJ9VwLtOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi98mDhtdAYD"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリのインストール\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install transformers   #transformers==4.5.0\n",
        "    !pip install fugashi        #fugashi==1.1.0\n",
        "    !pip install ipadic        #ipadic==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練済言語モデルの読み込み\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "\n",
        "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
        "#model_name = \"sonoisa/sentence-bert-base-ja-mean-tokens-v2\"  # <- v2です。\n",
        "#参照 https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2\n",
        "# とはいえ，旧バージョンの方が，納得できる結果を出す場合があるので，要検討である。\n",
        "\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
        "bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "EQb0QLUrdDFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークン化のテスト実施\n",
        "inp_texts = ['今日は[MASK]へ行く。', \n",
        "             'ジュースをお願いします', '[MASK]をお願いします',   # ミンニチテキストより\n",
        "             '宇宙ステーションはどこにあるんですか。',           # ミンニチテキストより\n",
        "             '宇宙は重力がありませんから、歩くことができないんです。', # ミンニチテキストより\n",
        "             '宇宙は[MASK]力がありませんから、歩くことができないんです。', # ミンニチテキストより\n",
        "             '今日は[MASK]へ行く。',\n",
        "]\n",
        "\n",
        "inp_text = '今日は[MASK]へ行く。'\n",
        "inp_tokens = tokenizer.tokenize(inp_text)\n",
        "print(inp_tokens)\n",
        "\n",
        "for inp_text in inp_texts:\n",
        "    inp_tokens = tokenizer.tokenize(inp_text)\n",
        "    print(inp_tokens)\n",
        "    print('---')\n"
      ],
      "metadata": {
        "id": "Km0AeiBpdE-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# `encode()` 関数に `inp_text` 文章を渡して，言語モデルによって符号化された系列 `input_ids` を得る\n",
        "input_ids = tokenizer.encode(inp_text, return_tensors='pt')\n",
        "\n",
        "# 系列長を揃える必要がないので，単に iput_ids のみを入力する。\n",
        "# 複数のテキストを処理させるときには max_length が必要となる\n",
        "with torch.no_grad():\n",
        "    output = bert_mlm(input_ids=input_ids)\n",
        "    scores = output.logits"
      ],
      "metadata": {
        "id": "kZypMORNeiYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ID 列で '[MASK]' (ID は 4) の位置を調べて mask_position に保存\n",
        "mask_position = input_ids[0].tolist().index(4) \n",
        "# index(4) という 魔法の数の招待は，直下行のと，次のセルを参照\n",
        "#mask_position = input_ids[0].tolist().index(tokenizer.convert_tokens_to_ids('[MASK]'))\n",
        "\n",
        "# 得点が最も良いトークンの ID を取り出してトークンに変換し，`id_best` に格納\n",
        "id_best = scores[0, mask_position].argmax(-1).item()   # `argmax()` 関数の最終項が最大値なので，その値を `is_best` に格納\n",
        "token_best = tokenizer.convert_ids_to_tokens(id_best)  # 直上行で計算された ID 番号 `is_best` (整数値) を tokenizer を使ってトークンに変換\n",
        "token_best = token_best.replace('##', '')              # BPE の断片を変換する\n",
        "\n",
        "# [MASK]を上で求めたトークンで置き換える。\n",
        "inp_text = inp_text.replace('[MASK]', token_best)\n",
        "print(inp_text)"
      ],
      "metadata": {
        "id": "rqd44MfOfz8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ちなみに BERT には次のような特殊トークンがあります\n",
        "print(tokenizer.special_tokens_map)\n",
        "print(f\"すなわち [MASK] の ID 番号は {tokenizer.convert_tokens_to_ids('[MASK]')} です\")"
      ],
      "metadata": {
        "id": "JNfjTbqzGxob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_mask_topk(text, tokenizer, bert_mlm, num_topk=4):\n",
        "    \"\"\"\n",
        "    文章中の最初の [MASK] を得点上位のトークンに置き換える。\n",
        "    上位何位まで使うかは num_topk で指定します。\n",
        "    出力は穴埋めされた文章のリストと，置き換えられたトークンのスコアのリスト。\n",
        "    \"\"\"\n",
        "    # 文章を符号化し BERT で分類得点を算出\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "    #input_ids = input_ids.cuda()\n",
        "    with torch.no_grad():\n",
        "        output = bert_mlm(input_ids=input_ids)\n",
        "    scores = output.logits\n",
        "\n",
        "    # 得点上位のトークンと対応する得点を求める。\n",
        "    mask_position = input_ids[0].tolist().index(4)  # [MASK] トークンの ID は 4\n",
        "    topk = scores[0, mask_position].topk(num_topk)\n",
        "    ids_topk = topk.indices # トークンのID\n",
        "    tokens_topk = tokenizer.convert_ids_to_tokens(ids_topk) # トークン\n",
        "    scores_topk = topk.values.cpu().numpy() # スコア\n",
        "\n",
        "    # 文章中の[MASK]を上で求めたトークンで置き換える。\n",
        "    text_topk = [] # 穴埋めされたテキストを追加する。\n",
        "    for token in tokens_topk:\n",
        "        token = token.replace('##', '')\n",
        "        text_topk.append(text.replace('[MASK]', token, 1))\n",
        "\n",
        "    return text_topk, scores_topk\n",
        "\n",
        "inp_texts = ['今日は[MASK]へ行く。', '[MASK]をお願いします', '宇宙[MASK]はどこにあるんですか。', '宇宙は[MASK]がありませんから、歩くことができないんです。']\n",
        "for inp_text in inp_texts:\n",
        "    text_topk, _ = predict_mask_topk(inp_text, tokenizer, bert_mlm, num_topk=5)\n",
        "    print(*text_topk, sep='\\n')\n",
        "    print('---')"
      ],
      "metadata": {
        "id": "fMphW6REgXrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_prediction(text, tokenizer, bert_mlm):\n",
        "    \"\"\"\n",
        "    [MASK] トークンを含む文章を入力として，貪欲法で穴埋めを行った文章を出力\n",
        "    \"\"\"\n",
        "    # 前から順に [MASK] を一つづつ得点の最も高いトークンに置き換える\n",
        "    for _ in range(text.count('[MASK]')):\n",
        "        text = predict_mask_topk(text, tokenizer, bert_mlm, 1)[0][0]\n",
        "    return text\n",
        "\n",
        "inp_texts = ['[MASK]をお願いし[MASK]', '宇宙[MASK]は[MASK]にあるんですか。', '宇宙は[MASK]がありませんから、歩くことができないんです。']\n",
        "\n",
        "for inp_text in inp_texts:\n",
        "    print(greedy_prediction(inp_text, tokenizer, bert_mlm))"
      ],
      "metadata": {
        "id": "h5Qfe_vsgjH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_texts = ['今日は[MASK][MASK][MASK][MASK][MASK]', '宇宙[MASK]は[MASK][MASK]あるんですか。', '宇宙は[MASK]がありませんから、[MASK][MASK]ができないん[MASK]。']\n",
        "\n",
        "for inp_text in inp_texts:\n",
        "    print(greedy_prediction(inp_text, tokenizer, bert_mlm))"
      ],
      "metadata": {
        "id": "mZnGhW3xgm_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(text, tokenizer, bert_mlm, num_topk=10):\n",
        "    \"\"\"ビームサーチで文章の穴埋めの候補項目を探索して表示\"\"\"\n",
        "    num_mask = text.count('[MASK]')\n",
        "    text_topk = [text]\n",
        "    scores_topk = np.array([0])\n",
        "    for _ in range(num_mask):\n",
        "        # 現在得られている、それぞれの文章に対して\n",
        "        # 最初の [MASK] をスコアが上位のトークンで空欄補充する。\n",
        "        # 問題作成の際に，上位項目を選べば，選択肢の半自動作成になるのではないだろうか。\n",
        "        text_candidates = []       # それぞれの文章を穴埋めした結果を追加する。\n",
        "        score_candidates = []      # 穴埋めに使ったトークンのスコアを追加する。\n",
        "        for text_mask, score in zip(text_topk, scores_topk):\n",
        "            text_topk_inner, scores_topk_inner = predict_mask_topk(\n",
        "                text_mask, tokenizer, bert_mlm, num_topk\n",
        "            )\n",
        "            text_candidates.extend(text_topk_inner)\n",
        "            score_candidates.append( score + scores_topk_inner )\n",
        "\n",
        "        # 穴埋めにより生成された文章の中から合計スコアの高い項目を選択\n",
        "        score_candidates = np.hstack(score_candidates)\n",
        "        idx_list = score_candidates.argsort()[::-1][:num_topk]\n",
        "        text_topk = [ text_candidates[idx] for idx in idx_list ]\n",
        "        scores_topk = score_candidates[idx_list]\n",
        "\n",
        "    return text_topk\n",
        "\n",
        "inp_texts = [\"今日は[MASK][MASK]へ行く。\", '宇宙は[MASK]がありませんから、歩くことができないんです。']\n",
        "for inp_text in inp_texts:\n",
        "    text_topk = beam_search(inp_text, tokenizer, bert_mlm, num_topk=4)\n",
        "    print(*text_topk, sep='\\n')\n",
        "    print('---')"
      ],
      "metadata": {
        "id": "JtqcWPFlgqAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_texts = ['今日は[MASK][MASK][MASK][MASK][MASK]', '宇宙は[MASK]がありませんから、[MASK]ことができないんです。']\n",
        "\n",
        "for inp_text in inp_texts:\n",
        "    text_topk = beam_search(inp_text, tokenizer, bert_mlm, 10)\n",
        "    print(*text_topk, sep='\\n')\n",
        "    print('---')"
      ],
      "metadata": {
        "id": "U-nCCaXAgss_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここから下は著作権の問題があリます。\n",
        "ですので，公表する場合には配慮が必要です。"
      ],
      "metadata": {
        "id": "CyWZeKxOUk3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GitHub からデータを持ってくる\n",
        "!git clone https://github.com/ShinAsakawa/ccap.git\n",
        "with open('ccap/2022_0515minnchi_sents.txt', 'r', encoding='utf-8') as f:\n",
        "    _lines = f.readlines()\n",
        "minnichi_sents = [line.strip() for line in _lines]\n",
        "print(minnichi_sents[:3])\n"
      ],
      "metadata": {
        "id": "V1TPPt30LYwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_minnichi(sentence_num=0, bert_mlm=bert_mlm, num_topk=4):\n",
        "    \n",
        "    # 第一引数がマイナスだったら，乱数を用いて文を一つ選ぶ\n",
        "    if sentence_num > len(minnichi_sents) or (sentence_num < 0):\n",
        "        sentence_num = np.random.choice(len(minnichi_sents))\n",
        "    inp_text = minnichi_sents[sentence_num]\n",
        "    inp_tokens = tokenizer.tokenize(inp_text)\n",
        "    print(f'オリジナル:{inp_tokens}')\n",
        "\n",
        "    target_pos = np.random.choice(len(inp_tokens))  # 乱数を用いて，ランダムな位置を選ぶ\n",
        "    target_token_id = inp_tokens[target_pos]        # 元の単語 ID を保存\n",
        "    inp_tokens[target_pos] = '[MASK]'               # 選んだ位置の単語を [MASK] で置き換え\n",
        "    print(f'単語置き換え後の文:{inp_tokens}')        # 置き換えた文を表示\n",
        "    input_ids = tokenizer.encode(inp_tokens, return_tensors='pt')\n",
        "    print(f'単語置き換え後の input_ids:{input_ids}')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = bert_mlm(input_ids=input_ids)\n",
        "    scores = output.logits\n",
        "\n",
        "    # 得点上位のトークンと対応する得点を求める。\n",
        "    topk = scores[0, target_pos].topk(num_topk)\n",
        "    ids_topk = topk.indices # トークン ID\n",
        "    tokens_topk = tokenizer.convert_ids_to_tokens(ids_topk) # トークン\n",
        "    scores_topk = topk.values.cpu().numpy() # スコア\n",
        "\n",
        "    # 文章中の [MASK] を上で求めたトークンで置き換える。\n",
        "    text_topk = [] # 穴埋めされたテキストを追加する。\n",
        "    for token in tokens_topk:\n",
        "        token = token.replace('##', '')\n",
        "        text_topk.append(inp_text.replace('[MASK]', token, 1))\n",
        "\n",
        "    return text_topk, target_pos, target_token_id, output, scores, \n",
        "\n",
        "masked_minnichi(sentence_num=-1)[0]\n"
      ],
      "metadata": {
        "id": "ubh5ezcyL4hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8GY6iRlWha_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}