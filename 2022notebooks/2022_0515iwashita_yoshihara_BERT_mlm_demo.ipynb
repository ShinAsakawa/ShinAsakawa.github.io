{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022_0515iwashita_yoshihara_BERT_mlm_demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPUZ1YHV0an5xy9kmjUjoVS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0515iwashita_yoshihara_BERT_mlm_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi98mDhtdAYD"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install transformers\n",
        "    #!pip install transformers==4.5.0\n",
        "    !pip install fugashi\n",
        "    #!pip install fugashi==1.1.0\n",
        "    !pip install ipadic\n",
        "    #!pip install ipadic==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "\n",
        "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
        "bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# if device == torch.device('cuda:0'):\n",
        "#     bert_mlm = bert_mlm.cuda()\n",
        "# else:\n",
        "#     bert_mlm = bert_mlm.cpu()"
      ],
      "metadata": {
        "id": "EQb0QLUrdDFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_text = '今日は[MASK]へ行く。'\n",
        "inp_text = 'ジュースをお願いします'\n",
        "inp_text = '[MASK]をお願いします'\n",
        "inp_text = '宇宙ステーションはどこにあるんですか。'\n",
        "inp_text = '宇宙は重力がありませんから、歩くことができないんです。'\n",
        "inp_tokens = tokenizer.tokenize(inp_text)\n",
        "print(inp_tokens)\n",
        "\n",
        "inp_text = '宇宙は[MASK]がありませんから、歩くことができないんです。'\n",
        "inp_tokens = tokenizer.tokenize(inp_text)\n",
        "print(inp_tokens)\n"
      ],
      "metadata": {
        "id": "Km0AeiBpdE-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 文章を符号化し、GPUに配置する。\n",
        "input_ids = tokenizer.encode(inp_text, return_tensors='pt')\n",
        "#input_ids = input_ids.device\n",
        "\n",
        "# BERTに入力し、分類スコアを得る。\n",
        "# 系列長を揃える必要がないので、単に iput_ids のみを入力します。\n",
        "with torch.no_grad():\n",
        "    output = bert_mlm(input_ids=input_ids)\n",
        "    scores = output.logits"
      ],
      "metadata": {
        "id": "kZypMORNeiYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ID 列で '[MASK]' (IDは4) の位置を調べる\n",
        "mask_position = input_ids[0].tolist().index(4) \n",
        "\n",
        "# スコアが最も良いトークンのIDを取り出し、トークンに変換する。\n",
        "id_best = scores[0, mask_position].argmax(-1).item()\n",
        "token_best = tokenizer.convert_ids_to_tokens(id_best)\n",
        "token_best = token_best.replace('##', '')\n",
        "\n",
        "# [MASK]を上で求めたトークンで置き換える。\n",
        "inp_text = inp_text.replace('[MASK]', token_best)\n",
        "\n",
        "print(inp_text)"
      ],
      "metadata": {
        "id": "rqd44MfOfz8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_mask_topk(text, tokenizer, bert_mlm, num_topk):\n",
        "    \"\"\"\n",
        "    文章中の最初の[MASK]をスコアの上位のトークンに置き換える。\n",
        "    上位何位まで使うかは、num_topkで指定。\n",
        "    出力は穴埋めされた文章のリストと、置き換えられたトークンのスコアのリスト。\n",
        "    \"\"\"\n",
        "    # 文章を符号化し、BERTで分類スコアを得る。\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "    #input_ids = input_ids.cuda()\n",
        "    with torch.no_grad():\n",
        "        output = bert_mlm(input_ids=input_ids)\n",
        "    scores = output.logits\n",
        "\n",
        "    # スコアが上位のトークンとスコアを求める。\n",
        "    mask_position = input_ids[0].tolist().index(4) \n",
        "    topk = scores[0, mask_position].topk(num_topk)\n",
        "    ids_topk = topk.indices # トークンのID\n",
        "    tokens_topk = tokenizer.convert_ids_to_tokens(ids_topk) # トークン\n",
        "    scores_topk = topk.values.cpu().numpy() # スコア\n",
        "\n",
        "    # 文章中の[MASK]を上で求めたトークンで置き換える。\n",
        "    text_topk = [] # 穴埋めされたテキストを追加する。\n",
        "    for token in tokens_topk:\n",
        "        token = token.replace('##', '')\n",
        "        text_topk.append(text.replace('[MASK]', token, 1))\n",
        "\n",
        "    return text_topk, scores_topk\n",
        "\n",
        "#text = '今日は[MASK]へ行く。'\n",
        "inp_text = '[MASK]をお願いします'\n",
        "inp_text = '宇宙[MASK]はどこにあるんですか。'\n",
        "inp_text = '宇宙は[MASK]がありませんから、歩くことができないんです。'\n",
        "text_topk, _ = predict_mask_topk(inp_text, tokenizer, bert_mlm, 10)\n",
        "print(*text_topk, sep='\\n')"
      ],
      "metadata": {
        "id": "fMphW6REgXrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_prediction(text, tokenizer, bert_mlm):\n",
        "    \"\"\"\n",
        "    [MASK]を含む文章を入力として、貪欲法で穴埋めを行った文章を出力する。\n",
        "    \"\"\"\n",
        "    # 前から順に[MASK]を一つづつ、スコアの最も高いトークンに置き換える。\n",
        "    for _ in range(text.count('[MASK]')):\n",
        "        text = predict_mask_topk(text, tokenizer, bert_mlm, 1)[0][0]\n",
        "    return text\n",
        "\n",
        "#text = '今日は[MASK][MASK]へ行く。'\n",
        "inp_text = '[MASK]をお願いし[MASK]'\n",
        "inp_text = '宇宙[MASK]は[MASK]にあるんですか。'\n",
        "inp_text = '宇宙は[MASK]がありませんから、歩くことができないんです。'\n",
        "\n",
        "greedy_prediction(inp_text, tokenizer, bert_mlm)"
      ],
      "metadata": {
        "id": "h5Qfe_vsgjH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-9\n",
        "inp_text = '今日は[MASK][MASK][MASK][MASK][MASK]'\n",
        "inp_text = '宇宙[MASK]は[MASK][MASK]あるんですか。'\n",
        "inp_text = '宇宙は[MASK]がありませんから、歩くことができないんです。'\n",
        "\n",
        "greedy_prediction(inp_text, tokenizer, bert_mlm)"
      ],
      "metadata": {
        "id": "mZnGhW3xgm_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(text, tokenizer, bert_mlm, num_topk):\n",
        "    \"\"\"\n",
        "    ビームサーチで文章の穴埋めを行う。\n",
        "    \"\"\"\n",
        "    num_mask = text.count('[MASK]')\n",
        "    text_topk = [text]\n",
        "    scores_topk = np.array([0])\n",
        "    for _ in range(num_mask):\n",
        "        # 現在得られている、それぞれの文章に対して、\n",
        "        # 最初の[MASK]をスコアが上位のトークンで穴埋めする。\n",
        "        text_candidates = [] # それぞれの文章を穴埋めした結果を追加する。\n",
        "        score_candidates = [] # 穴埋めに使ったトークンのスコアを追加する。\n",
        "        for text_mask, score in zip(text_topk, scores_topk):\n",
        "            text_topk_inner, scores_topk_inner = predict_mask_topk(\n",
        "                text_mask, tokenizer, bert_mlm, num_topk\n",
        "            )\n",
        "            text_candidates.extend(text_topk_inner)\n",
        "            score_candidates.append( score + scores_topk_inner )\n",
        "\n",
        "        # 穴埋めにより生成された文章の中から合計スコアの高いものを選ぶ。\n",
        "        score_candidates = np.hstack(score_candidates)\n",
        "        idx_list = score_candidates.argsort()[::-1][:num_topk]\n",
        "        text_topk = [ text_candidates[idx] for idx in idx_list ]\n",
        "        scores_topk = score_candidates[idx_list]\n",
        "\n",
        "    return text_topk\n",
        "\n",
        "inp_text = \"今日は[MASK][MASK]へ行く。\"\n",
        "inp_text = '宇宙は[MASK]がありませんから、歩くことができないんです。'\n",
        "text_topk = beam_search(inp_text, tokenizer, bert_mlm, 10)\n",
        "print(*text_topk, sep='\\n')"
      ],
      "metadata": {
        "id": "JtqcWPFlgqAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-11\n",
        "inp_text = '今日は[MASK][MASK][MASK][MASK][MASK]'\n",
        "inp_text = '宇宙は[MASK]がありませんから、[MASK]ことができないんです。'\n",
        "\n",
        "text_topk = beam_search(inp_text, tokenizer, bert_mlm, 10)\n",
        "print(*text_topk, sep='\\n')"
      ],
      "metadata": {
        "id": "U-nCCaXAgss_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "\n",
        "data_fname = 'ccap/2022_0205minnichi_data.json.gz'\n",
        "lines = {}\n",
        "with gzip.open(data_fname, 'rb') as fgz:\n",
        "    tmp = json.loads(fgz.read().decode('utf-8'))\n",
        "    for k in tmp.keys():\n",
        "        lines[k] = tmp[k]\n",
        "                    \n",
        "_max_length, max_length = 0, -1\n",
        "vocab, freq = ['<EOS>','<SOS>','<UNK>','<PAD>','<MASK>'], {}\n",
        "for i in range(len(lines)):\n",
        "    if _max_length < lines[str(i)]['n_token']:\n",
        "        _max_length = lines[str(i)]['n_token']\n",
        "\n",
        "    tokens = lines[str(i)]['tokens']\n",
        "    for token in tokens:\n",
        "        if not token in vocab:\n",
        "            vocab.append(token)\n",
        "            freq[token] = 1\n",
        "        else:\n",
        "            freq[token] += 1\n",
        "    freq = freq\n",
        "    if max_length == -1:\n",
        "        max_length = _max_length\n",
        "    else:\n",
        "        max_length = max_length\n",
        "    vocab = vocab\n"
      ],
      "metadata": {
        "id": "mo9xRdJsjkbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "id": "t_8jC7U2hNRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8GY6iRlWha_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}