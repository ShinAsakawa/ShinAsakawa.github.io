{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 2022_0515iwashita_yoshihara_BERT_mlm_demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMLa4W/g73dW4Q726UO1A9J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0515iwashita_yoshihara_BERT_mlm_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 岩下，吉原勉強会資料\n",
        "- date: 2022_0515\n",
        "- filename: `2022_0515iwashita_yoshihara_BERT_mlm_demo.ipynb`\n",
        "- memo: BERT を用いたマスク化言語モデルによる穴埋め問題の回答や選択肢作成に向けて"
      ],
      "metadata": {
        "id": "Z05gJ9VwLtOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi98mDhtdAYD"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install transformers   #transformers==4.5.0\n",
        "    !pip install fugashi        #fugashi==1.1.0\n",
        "    !pip install ipadic.        #ipadic==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "\n",
        "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
        "#model_name = \"sonoisa/sentence-bert-base-ja-mean-tokens-v2\"  # <- v2です。\n",
        "#参照 https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2\n",
        "\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
        "bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "EQb0QLUrdDFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_texts = ['今日は[MASK]へ行く。', \n",
        "             'ジュースをお願いします', '[MASK]をお願いします',   # ミンニチテキストより\n",
        "             '宇宙ステーションはどこにあるんですか。',           # ミンニチテキストより\n",
        "             '宇宙は重力がありませんから、歩くことができないんです。', # ミンニチテキストより\n",
        "             '宇宙は[MASK]力がありませんから、歩くことができないんです。', # ミンニチテキストより\n",
        "             '今日は[MASK]へ行く。',\n",
        "]\n",
        "\n",
        "inp_text = '今日は[MASK]へ行く。'\n",
        "inp_tokens = tokenizer.tokenize(inp_text)\n",
        "print(inp_tokens)\n",
        "\n",
        "for inp_text in inp_texts:\n",
        "    inp_tokens = tokenizer.tokenize(inp_text)\n",
        "    print(inp_tokens)\n",
        "    print('---')\n"
      ],
      "metadata": {
        "id": "Km0AeiBpdE-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# `encode()` 関数に `inp_text 文章を渡して，言語モデルによって符号化された系列 `input_ids` を得ます。\n",
        "input_ids = tokenizer.encode(inp_text, return_tensors='pt')\n",
        "\n",
        "# 系列長を揃える必要がないので，単に iput_ids のみを入力します。\n",
        "# 複数のテキストを処理させるときには max_length が必要になります\n",
        "with torch.no_grad():\n",
        "    output = bert_mlm(input_ids=input_ids)\n",
        "    scores = output.logits"
      ],
      "metadata": {
        "id": "kZypMORNeiYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ID 列で '[MASK]' (ID は 4) の位置を調べて mask_position に保存します\n",
        "mask_position = input_ids[0].tolist().index(4) \n",
        "#mask_position = input_ids[0].tolist().index(tokenizer.convert_tokens_to_ids('[MASK]'))\n",
        "\n",
        "# 得点が最も良いトークンの ID を取り出してトークンに変換します。\n",
        "id_best = scores[0, mask_position].argmax(-1).item()   # `argmax()` 関数の最終項が最大値なので，その値を `is_best` に格納\n",
        "token_best = tokenizer.convert_ids_to_tokens(id_best)  # 直上行で計算された ID 番号 `is_best` (整数値) を tokenizer を使ってトークンに変換\n",
        "token_best = token_best.replace('##', '')              # BPE の断片を変換する\n",
        "\n",
        "# [MASK]を上で求めたトークンで置き換える。\n",
        "inp_text = inp_text.replace('[MASK]', token_best)\n",
        "print(inp_text)"
      ],
      "metadata": {
        "id": "rqd44MfOfz8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ちなみに BERT には次のような特殊トークンがあります\n",
        "print(tokenizer.special_tokens_map)\n",
        "print(f\"すなわち [MASK] の ID 番号は {tokenizer.convert_tokens_to_ids('[MASK]')} です\")"
      ],
      "metadata": {
        "id": "JNfjTbqzGxob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_mask_topk(text, tokenizer, bert_mlm, num_topk=4):\n",
        "    \"\"\"\n",
        "    文章中の最初の [MASK] を得点上位のトークンに置き換える。\n",
        "    上位何位まで使うかは num_topk で指定します。\n",
        "    出力は穴埋めされた文章のリストと，置き換えられたトークンのスコアのリスト。\n",
        "    \"\"\"\n",
        "    # 文章を符号化し BERT で分類得点を算出します\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "    #input_ids = input_ids.cuda()\n",
        "    with torch.no_grad():\n",
        "        output = bert_mlm(input_ids=input_ids)\n",
        "    scores = output.logits\n",
        "\n",
        "    # 得点上位のトークンと対応する得点を求める。\n",
        "    mask_position = input_ids[0].tolist().index(4)  # [MASK] トークンの ID は 4\n",
        "    topk = scores[0, mask_position].topk(num_topk)\n",
        "    ids_topk = topk.indices # トークンのID\n",
        "    tokens_topk = tokenizer.convert_ids_to_tokens(ids_topk) # トークン\n",
        "    scores_topk = topk.values.cpu().numpy() # スコア\n",
        "\n",
        "    # 文章中の[MASK]を上で求めたトークンで置き換える。\n",
        "    text_topk = [] # 穴埋めされたテキストを追加する。\n",
        "    for token in tokens_topk:\n",
        "        token = token.replace('##', '')\n",
        "        text_topk.append(text.replace('[MASK]', token, 1))\n",
        "\n",
        "    return text_topk, scores_topk\n",
        "\n",
        "inp_texts = ['今日は[MASK]へ行く。', '[MASK]をお願いします', '宇宙[MASK]はどこにあるんですか。', '宇宙は[MASK]がありませんから、歩くことができないんです。']\n",
        "for inp_text in inp_texts:\n",
        "    text_topk, _ = predict_mask_topk(inp_text, tokenizer, bert_mlm, num_topk=4)\n",
        "    print(*text_topk, sep='\\n')\n",
        "    print('---')"
      ],
      "metadata": {
        "id": "fMphW6REgXrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_prediction(text, tokenizer, bert_mlm):\n",
        "    \"\"\"\n",
        "    [MASK] トークンを含む文章を入力として，貪欲法で穴埋めを行った文章を出力します\n",
        "    \"\"\"\n",
        "    # 前から順に [MASK] を一つづつ得点の最も高いトークンに置き換えます\n",
        "    for _ in range(text.count('[MASK]')):\n",
        "        text = predict_mask_topk(text, tokenizer, bert_mlm, 1)[0][0]\n",
        "    return text\n",
        "\n",
        "inp_texts = ['[MASK]をお願いし[MASK]', '宇宙[MASK]は[MASK]にあるんですか。', '宇宙は[MASK]がありませんから、歩くことができないんです。']\n",
        "\n",
        "for inp_text in inp_texts:\n",
        "    print(greedy_prediction(inp_text, tokenizer, bert_mlm))"
      ],
      "metadata": {
        "id": "h5Qfe_vsgjH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_texts = ['今日は[MASK][MASK][MASK][MASK][MASK]', '宇宙[MASK]は[MASK][MASK]あるんですか。', '宇宙は[MASK]がありませんから、[MASK][MASK]ができないん[MASK]。']\n",
        "\n",
        "for inp_text in inp_texts:\n",
        "    print(greedy_prediction(inp_text, tokenizer, bert_mlm))"
      ],
      "metadata": {
        "id": "mZnGhW3xgm_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(text, tokenizer, bert_mlm, num_topk=10):\n",
        "    \"\"\"ビームサーチで文章の穴埋めの候補項目を探索して表示\"\"\"\n",
        "    num_mask = text.count('[MASK]')\n",
        "    text_topk = [text]\n",
        "    scores_topk = np.array([0])\n",
        "    for _ in range(num_mask):\n",
        "        # 現在得られている、それぞれの文章に対して\n",
        "        # 最初の [MASK] をスコアが上位のトークンで空欄補充する。\n",
        "        # 問題作成の際に，上位項目を選べば，選択肢の半自動作成になるのではないだろうか。\n",
        "        text_candidates = []       # それぞれの文章を穴埋めした結果を追加する。\n",
        "        score_candidates = []      # 穴埋めに使ったトークンのスコアを追加する。\n",
        "        for text_mask, score in zip(text_topk, scores_topk):\n",
        "            text_topk_inner, scores_topk_inner = predict_mask_topk(\n",
        "                text_mask, tokenizer, bert_mlm, num_topk\n",
        "            )\n",
        "            text_candidates.extend(text_topk_inner)\n",
        "            score_candidates.append( score + scores_topk_inner )\n",
        "\n",
        "        # 穴埋めにより生成された文章の中から合計スコアの高い項目を選択\n",
        "        score_candidates = np.hstack(score_candidates)\n",
        "        idx_list = score_candidates.argsort()[::-1][:num_topk]\n",
        "        text_topk = [ text_candidates[idx] for idx in idx_list ]\n",
        "        scores_topk = score_candidates[idx_list]\n",
        "\n",
        "    return text_topk\n",
        "\n",
        "inp_texts = [\"今日は[MASK][MASK]へ行く。\", '宇宙は[MASK]がありませんから、歩くことができないんです。']\n",
        "for inp_text in inp_texts:\n",
        "    text_topk = beam_search(inp_text, tokenizer, bert_mlm, num_topk=4)\n",
        "    print(*text_topk, sep='\\n')\n",
        "    print('---')"
      ],
      "metadata": {
        "id": "JtqcWPFlgqAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_texts = ['今日は[MASK][MASK][MASK][MASK][MASK]', '宇宙は[MASK]がありませんから、[MASK]ことができないんです。']\n",
        "\n",
        "for inp_text in inp_texts:\n",
        "    text_topk = beam_search(inp_text, tokenizer, bert_mlm, 10)\n",
        "    print(*text_topk, sep='\\n')\n",
        "    print('---')"
      ],
      "metadata": {
        "id": "U-nCCaXAgss_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V1TPPt30LYwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ShinAsakawa/ccap.git\n",
        "import gzip\n",
        "import json\n",
        "\n",
        "data_fname = 'ccap/2022_0205minnichi_data.json.gz'\n",
        "lines = {}\n",
        "with gzip.open(data_fname, 'rb') as fgz:\n",
        "    tmp = json.loads(fgz.read().decode('utf-8'))\n",
        "    for k in tmp.keys():\n",
        "        lines[k] = tmp[k]\n",
        "                    \n",
        "_max_length, max_length = 0, -1\n",
        "vocab, freq = ['<EOS>','<SOS>','<UNK>','<PAD>','<MASK>'], {}\n",
        "for i in range(len(lines)):\n",
        "    if _max_length < lines[str(i)]['n_token']:\n",
        "        _max_length = lines[str(i)]['n_token']\n",
        "\n",
        "    tokens = lines[str(i)]['tokens']\n",
        "    for token in tokens:\n",
        "        if not token in vocab:\n",
        "            vocab.append(token)\n",
        "            freq[token] = 1\n",
        "        else:\n",
        "            freq[token] += 1\n",
        "    freq = freq\n",
        "    if max_length == -1:\n",
        "        max_length = _max_length\n",
        "    else:\n",
        "        max_length = max_length\n",
        "    vocab = vocab\n"
      ],
      "metadata": {
        "id": "mo9xRdJsjkbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "id": "t_8jC7U2hNRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8GY6iRlWha_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}