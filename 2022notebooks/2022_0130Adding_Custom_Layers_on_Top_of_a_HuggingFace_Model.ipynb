{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022_0130Adding_Custom_Layers_on_Top_of_a_HuggingFace_Model.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOkbYfG/IUJdTWdhFynr+BJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0130Adding_Custom_Layers_on_Top_of_a_HuggingFace_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- date: 2022_0130\n",
        "- author: asakawa\n",
        "- source: https://jovian.ai/rajbsangani/emotion-tuned-sarcasm/v/1?utm_source=embed#C11\n",
        "- blog: [Adding Custom Layers on Top of a Hugging Face Model](https://towardsdatascience.com/adding-custom-layers-on-top-of-a-hugging-face-model-f1ccdfc257bd)\n",
        "\n",
        "# Adding Custom Layers on Top of a Hugging Face Model\n",
        "\n",
        "Hugging Face モデルの本体から隠れ状態を抽出し，その上に課題固有の層を修正・追加し，PyTorch を使ってカスタムセットアップ全体をエンドツーエンドで訓練する方法を学びます。\n",
        "<!-- Learn how to extract the hidden states from a Hugging Face model body, modify/add task-specific layers on top of it and train the whole custom setup end-to-end using PyTorch -->"
      ],
      "metadata": {
        "id": "2DMLMK0-9dRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting, this post assumes basic familiarity with Hugging Face (using a model out-of-the-box ). \n",
        "Also, a huge shoutout to the folks at Hugging Face for setting up a beginner-friendly learning environment!\n",
        "\n",
        "## What will you learn from this blog?\n",
        "\n",
        "1. Use task-specific models from the Hugging Face Hub and make them adapt to your task at hand.\n",
        "2. De-coupling a Model’s head from its body and using the body to leverage domain-specific knowledge.\n",
        "3. Building a custom head and attaching it to the body of the HF model in PyTorch and training the system end-to-end.\n",
        "\n",
        "## The anatomy of a Hugging Face Model\n",
        "Here is what a typical HF model looks like\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1260/1*7JDSKluZfSSI0O1yRWUIOQ.png\"><br/>\n",
        "Image By Author\n",
        "</center>\n",
        "\n",
        "## Why will I need to use the head and body separately?\n",
        "\n",
        "Some models on Hugging Face are trained on downstream tasks like question-answering or text classification and contain knowledge about the data they were trained on in their weights.\n",
        "\n",
        "Sometimes, especially when our task at hand contains very little data or is domain-specific (such as medical or sports specific tasks), we can make use of other models on the hub trained on tasks (not necessarily the same task as our task at hand but falling within the same domain, such as sports or medicine) and make use of some of the pretrained knowledge these models to improve performance on our own task.\n",
        "\n",
        "1. A very simple example would be if say we have a small dataset about classifying whether some financial statements are positive or negative in terms of sentiment. However, we go onto the Hub and find that a lot of models have been trained for QA related to finance. We can use certain layers from these models for improving our own tasks.\n",
        "2. Another simple example is when a certain domain-specific model has learned to classify text into 5 categories from a huge dataset it was trained on. Say we have a similar classification task, a completely different dataset in the same domain and only want to classify the data into 2 categories instead of 5. We can again use a model’s body and add our own head in an attempt to augment domain-specific knowledge on our own task.\n",
        "\n",
        "Diagrammatically, this is what we are trying to do\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1286/1*5h3h7WtxAZpmmjfem3eoUQ.png\"><br/>\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1358/1*Zz_QpVlAPF0Jkgd02948Yg.png\"><br/>\n",
        "</center>\n",
        "\n",
        "## Jumping into the code!\n",
        "\n",
        "Our task is simple, sarcasm detection on this dataset from Kaggle.\n",
        "\n",
        "You can check out the full code [here](https://jovian.ai/rajbsangani/emotion-tuned-sarcasm). \n",
        "I have not included the preprocessing and some training details below in the interest of time so make sure to check out the notebook for the entire code.\n",
        "\n",
        "I will use a model with 5 classification outputs trained on a huge corpus of tweets to classify 5 different emotions, extract the body and add custom layers in PyTorch for our task (2 labels, sarcastic and not sarcastic) and train the new model end-to-end.\n",
        "\n",
        "Note: You can use any model in this example (not necessarily a model trained for classification) since we will only use that model’s body and leave the head.\n",
        "\n",
        "This is what our workflow looks like\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1126/1*vBXL8SiUl9lPLvZkUIoGUQ.png\"><br/>\n",
        "</center>\n",
        "\n",
        "I will be skipping the data-preprocessing steps and jumping straight to the main class, but you can check out the entire code in the link at the beginning of this section.\n",
        "\n",
        "## Tokenization and Dynamic Padding\n"
      ],
      "metadata": {
        "id": "3ks3vxoYCDa7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWPR15Mw9Xxv"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset,Dataset,DatasetDict\n",
        "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xtvCLr609lY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!echo '{\"username\":\"turingcomplete\",\"key\":\"a49cdd9a6452346d9fdacca035bde21a\"}' > kaggle.json\n",
        "#!ls -l kaggle.json"
      ],
      "metadata": {
        "id": "1reS7YWk_p2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/ \n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d rmisra/news-headlines-dataset-for-sarcasm-detection\n",
        "\n",
        "data=load_dataset(\"json\",data_files=\"/content/news-headlines-dataset-for-sarcasm-detection.zip\")\n",
        "data=data.rename_column(\"is_sarcastic\",\"label\")\n",
        "\n",
        "data=data.remove_columns(['article_link'])\n",
        "\n",
        "data.set_format('pandas')\n",
        "data=data['train'][:]\n",
        "\n",
        "data.drop_duplicates(subset=['headline'],inplace=True)\n",
        "data=data.reset_index()[['headline','label']]\n",
        "data=Dataset.from_pandas(data)\n",
        "\n",
        "# 80% train, 20% test + validation\n",
        "train_testvalid = data.train_test_split(test_size=0.2,seed=15)\n",
        "\n",
        "# Split the 10% test + valid in half test, half valid\n",
        "test_valid = train_testvalid['test'].train_test_split(test_size=0.5,seed=15)\n",
        "\n",
        "# gather everyone if you want to have a single DatasetDict\n",
        "data = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'valid': test_valid['train']})\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "lkjF0Htd9rU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.model_max_len=512"
      ],
      "metadata": {
        "id": "1AMQ24j0AcqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "  return tokenizer(batch[\"headline\"], truncation=True,max_length=512)\n",
        "\n",
        "tokenized_dataset = data.map(tokenize, batched=True)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "CTvuQ3wuAwvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "ytL6Su2QA0Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting the Body and adding our own layers\n"
      ],
      "metadata": {
        "id": "H-gY2EwSDgpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(nn.Module):\n",
        "  def __init__(self,checkpoint,num_labels): \n",
        "    super(CustomModel,self).__init__() \n",
        "    self.num_labels = num_labels \n",
        "\n",
        "    #Load Model with given checkpoint and extract its body\n",
        "    self.model = model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
        "    self.dropout = nn.Dropout(0.1) \n",
        "    self.classifier = nn.Linear(768,num_labels) # load and initialize weights\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
        "    #Extract outputs from the body\n",
        "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    #Add custom layers\n",
        "    sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
        "\n",
        "    logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses\n",
        "    \n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    \n",
        "    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
      ],
      "metadata": {
        "id": "NGgi7fNIA6g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=CustomModel(checkpoint=checkpoint,num_labels=2).to(device)"
      ],
      "metadata": {
        "id": "dbQc6fV8A8x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_dataset[\"train\"], shuffle=True, batch_size=32, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_dataset[\"valid\"], batch_size=32, collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "uqfg5y1tBAp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW,get_scheduler\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "print(num_training_steps)"
      ],
      "metadata": {
        "id": "WjlLfNoXBXg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "metric = load_metric(\"f1\")"
      ],
      "metadata": {
        "id": "Y7GtrH-vBbxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see we first subclass the nn Module from PyTorch, extract the model body using AutoModel (from transformers) and provide the checkpoint to the model whose body we want to use.\n",
        "\n",
        "Note that a TokenClassifierOutput (from the transformers library) is returned which makes sure that our output is in a similar format to that from a Hugging Face model on the hub.\n",
        "\n",
        "### Training the new model end-to-end"
      ],
      "metadata": {
        "id": "tzIXGpzyDvi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar_train = tqdm(range(num_training_steps))\n",
        "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  for batch in train_dataloader:\n",
        "      batch = {k: v.to(device) for k, v in batch.items()}\n",
        "      outputs = model(**batch)\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      lr_scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      progress_bar_train.update(1)\n",
        "\n",
        "  model.eval()\n",
        "  for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "    progress_bar_eval.update(1)\n",
        "    \n",
        "  print(metric.compute())\n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "48hPHiiDBfOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    tokenized_dataset[\"test\"], batch_size=32, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ],
      "metadata": {
        "id": "kXuXWu6dBic5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we achieve a decent performance using this method. \n",
        "**Keep in mind that the aim of this blog isn’t to analyze performance for this particular dataset but to learn how to use a pre-trained Body and add a Custom Head**.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "We saw how one can add custom layers to a pre-trained model’s body using the Hugging Face Hub.\n",
        "\n",
        "Some takeaways:\n",
        "\n",
        "1. This technique is particularly helpful in cases where we have small domain-specific datasets and want to leverage models trained on larger datasets in the same domain (task-agnostic) to augment performance on our small dataset.\n",
        "2. We can choose models that have been trained on downstream tasks different from our own task and still use the knowledge from that model’s body.\n",
        "3. This may not be necessary at all if your dataset is large enough and generic, in which case you can use AutoModelForSequenceClassification or whatever other task you have to solve using a BERT like checkpoint. In fact, if that is so, I would strongly recommend not building your own head.\n",
        "\n",
        "Check out my [GitHub](https://github.com/rajlm10) for some other projects. You can contact me [here](https://rajsangani.me/).\n",
        "Thank you for your time!\n",
        "\n",
        "If you liked this here are som more!\n",
        "\n",
        "- [Interpreting an LSTM through LIME](https://towardsdatascience.com/interpreting-an-lstm-through-lime-e294e6ed3a03)\n",
        "- [Powerful Text Augmentation Using NLPAUG](https://towardsdatascience.com/powerful-text-augmentation-using-nlpaug-5851099b4e97)"
      ],
      "metadata": {
        "id": "cfGcXaaVD5fN"
      }
    }
  ]
}