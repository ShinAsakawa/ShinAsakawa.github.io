{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0410iwa_yoshi_presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 岩下，吉原勉強会資料\n",
        "\n",
        "- 文責: 浅川伸一 <askaawa@ieee.org>\n",
        "- date: 2022_0410\n",
        "- filename: `2022_0410iwayoshi_ja_edu.ipynb`\n"
      ],
      "metadata": {
        "id": "Ho-nLXjjqFeV"
      },
      "id": "Ho-nLXjjqFeV"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # `20220324_minnichi_goilist_2202.xlsx` を指定してアップロードする"
      ],
      "metadata": {
        "id": "q9IrA4wmFEmc",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "5321b057-295f-4b3e-9715-8e89ee166b86"
      },
      "id": "q9IrA4wmFEmc",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-25fcfbea-48f7-4e75-9d1d-82c302f94573\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-25fcfbea-48f7-4e75-9d1d-82c302f94573\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 20220324_minnichi_goilist_2202.xlsx to 20220324_minnichi_goilist_2202.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import sys\n",
        "\n",
        "# 次行はローカルな実行環境とクラウド計算環境である google colab との差分を吸収するため\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install jaconv\n",
        "    !pip install japanize_matplotlib\n",
        "    !git clone https://github.com/ShinAsakawa/ccap.git > /dev/null 2>&1\n",
        "    !pip install 'konoha[mecab]'\n",
        "    \n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib"
      ],
      "metadata": {
        "id": "HrZ5-6D1Fyb0"
      },
      "id": "HrZ5-6D1Fyb0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "268614f7-bae0-41f7-b6f4-37f6abbeb170",
      "metadata": {
        "id": "268614f7-bae0-41f7-b6f4-37f6abbeb170"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import jaconv\n",
        "import unicodedata\n",
        "\n",
        "excel_filename = '20220324_minnichi_goilist_2202.xlsx'\n",
        "a = pd.read_excel(excel_filename)\n",
        "\n",
        "# 岩下先生からいただいたエクセルファイルの ['ことば'] 列の単語には，\n",
        "# 末尾に空白文字が入っているようなので  `\" \".join(word.split())` して除去\n",
        "min2022_0327 = [\" \".join(unicodedata.normalize('NFKC',word).split()) for word in sorted(list(a['ことば']))]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 上で読み込んだデータを minnichi 辞書として登録\n",
        "minnichi = {}\n",
        "minnichi_vocab = []\n",
        "for l, _w in zip(a.iterrows(), min2022_0327):\n",
        "    num = int(l[0])\n",
        "    _word = _w\n",
        "    _class = l[1][1]\n",
        "    _pos = l[1][2]\n",
        "    minnichi[_word] = {'num':num, \n",
        "                       #'word': _word,\n",
        "                       '課':_class, \n",
        "                       '品詞':_pos}\n",
        "    minnichi_vocab.append(_word)\n",
        "\n",
        "print(f'読み込んだデータ数:{len(minnichi)}')\n",
        "minnichi_vocab =  sorted(set(minnichi_vocab))\n",
        "print(f'でも単語数としては:{len(minnichi_vocab)} です。重複があるのかな？' )\n",
        "a"
      ],
      "metadata": {
        "id": "FnWP7zarEt9w"
      },
      "id": "FnWP7zarEt9w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "459a6322-5d49-4546-9d22-1ff5bd4f1b3b",
      "metadata": {
        "id": "459a6322-5d49-4546-9d22-1ff5bd4f1b3b"
      },
      "outputs": [],
      "source": [
        "# 前回お話した，ユニコードの正規化についてです。\n",
        "# 実際の動作とは関係ありません。\n",
        "# NFC, NFKC, NFD, NFKD と 4 種類があります。\n",
        "# それぞれの意味は，以下のとおりです\n",
        "# NF: Normalized Form\n",
        "# C: コンポーズド  飾り記号を分けて考えない\n",
        "# D: デコンポーズド 飾り記号を分けて考える\n",
        "# K: 互換性を保証する Comaptibility の意味。だが C が composed と競合するので K にしている\n",
        "import unicodedata\n",
        "help(unicodedata.normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ee9f5e4-60dc-4fb3-94bf-e44a85a021e2",
      "metadata": {
        "id": "3ee9f5e4-60dc-4fb3-94bf-e44a85a021e2"
      },
      "outputs": [],
      "source": [
        "print(minnichi_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b538736a-2167-4b6b-99ac-55ac169a9680",
      "metadata": {
        "id": "b538736a-2167-4b6b-99ac-55ac169a9680"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Google Colaboratory 上で実行する場合に，必要となるライブラリをインストールする\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install transformers > /dev/null 2>&1 \n",
        "\n",
        "    # MeCab, fugashi, ipadic のインストール\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1\n",
        "    !pip install mecab-python3 > /dev/null 2>&1\n",
        "    !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "    !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1\n",
        "    \n",
        "    import subprocess\n",
        "    cmd='echo `mecab-config --dicdir`\\\"/mecab-ipadic-neologd\\\"'\n",
        "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                                     shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "    !pip install 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    !python -m unidic download > /dev/null 2>&1\n",
        "    !pip install ipadic > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "61b1d64b-5594-4bdb-b9ae-4f7857882e41",
      "metadata": {
        "id": "61b1d64b-5594-4bdb-b9ae-4f7857882e41"
      },
      "outputs": [],
      "source": [
        "# 新しい minnichi を MeCab にしてみる。\n",
        "import MeCab\n",
        "mcb = MeCab.Tagger().parse\n",
        "\n",
        "# MeCab の品詞分類のデフォルト設定は [IPADIC 2.7](https://chasen.naist.jp/snapshot/ipadic/ipadic/doc/ipadic-ja.pdf) \n",
        "# に基づいている。\n",
        "# [IPADIC 大分類](https://hayashibe.jp/tr/mecab/dictionary/ipadic) は以下の通り\n",
        "pos_ipa = ['名詞', '接頭詞', '動詞', '形容詞', '副詞', '連体詞', '接続詞', '連体詞', \n",
        "           '接続詞', '助詞', '助動詞', '感動詞', '記号', 'フィラー', 'その他']\n",
        "\n",
        "for wrd in minnichi.keys():\n",
        "    wrd_splited = mcb(wrd).strip().splitlines()[:-1]\n",
        "    _mcb_data = []\n",
        "    for _wrd in wrd_splited:\n",
        "        x = _wrd.split('\\t')\n",
        "        _x = x[1].split(',')\n",
        "        __x = {'表層形':x[0], \n",
        "              '原形':_x[7],\n",
        "              #'品詞':_x[4],\n",
        "              '品詞':_x[4].split('-')[0],\n",
        "              '品詞1':_x[5],\n",
        "              '品詞2':_x[6]\n",
        "              }\n",
        "        _mcb_data.append(__x)\n",
        "    minnichi[wrd]['mecab'] = _mcb_data\n",
        "\n",
        "#minnichi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "import numpy as np\n",
        "\n",
        "for i in range(10):\n",
        "    wrd = minnichi_vocab[np.random.choice(len(minnichi))]\n",
        "    print(colored(wrd,'blue', attrs=['bold']), end=\": \") \n",
        "    #print(colored(minnichi[i]['word'],'blue', attrs=['bold']), end=\": \") \n",
        "    for _i in minnichi[wrd]['mecab']:\n",
        "        print('表層形',colored(_i['表層形'], 'green', attrs=['bold']),\n",
        "              '品詞',  colored(_i['品詞'],  'green', attrs=['bold','blink']), end=\" \") \n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgOMqDRbrrUH",
        "outputId": "b46b0d4f-5109-45be-9901-3871c7ddcc9b"
      },
      "id": "RgOMqDRbrrUH",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[34m習います\u001b[0m: 表層形 \u001b[1m\u001b[32m習い\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m五段\u001b[0m 表層形 \u001b[1m\u001b[32mます\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m助動詞\u001b[0m \n",
            "\u001b[1m\u001b[34m終わります\u001b[0m: 表層形 \u001b[1m\u001b[32m終わり\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m五段\u001b[0m 表層形 \u001b[1m\u001b[32mます\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m助動詞\u001b[0m \n",
            "\u001b[1m\u001b[34m銀行\u001b[0m: 表層形 \u001b[1m\u001b[32m銀行\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m \n",
            "\u001b[1m\u001b[34m邪魔\u001b[0m: 表層形 \u001b[1m\u001b[32m邪魔\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m \n",
            "\u001b[1m\u001b[34mにぎやか\u001b[0m: 表層形 \u001b[1m\u001b[32mにぎやか\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m \n",
            "\u001b[1m\u001b[34m失礼します\u001b[0m: 表層形 \u001b[1m\u001b[32m失礼\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m 表層形 \u001b[1m\u001b[32mし\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32mサ行変格\u001b[0m 表層形 \u001b[1m\u001b[32mます\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m助動詞\u001b[0m \n",
            "\u001b[1m\u001b[34m菓子\u001b[0m: 表層形 \u001b[1m\u001b[32m菓子\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m \n",
            "\u001b[1m\u001b[34mこんにちは\u001b[0m: 表層形 \u001b[1m\u001b[32mこんにちは\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m \n",
            "\u001b[1m\u001b[34m台所\u001b[0m: 表層形 \u001b[1m\u001b[32m台所\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m \n",
            "\u001b[1m\u001b[34m地震\u001b[0m: 表層形 \u001b[1m\u001b[32m地震\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 若干の老婆心\n"
      ],
      "metadata": {
        "id": "uMcW7sLZfc9k"
      },
      "id": "uMcW7sLZfc9k"
    },
    {
      "cell_type": "code",
      "source": [
        "# ここで MeCab の使い方を簡単に紹介する。\n",
        "# 入力文を分かち書きしたいだけならば，以下のように '-Owakati' オプションを指定して\n",
        "# MeCab を呼び出せばよい。\n",
        "print(MeCab.Tagger('-Owakati').parse('吾輩は猫である').strip())"
      ],
      "metadata": {
        "id": "xJaMB6wCWXpl"
      },
      "id": "xJaMB6wCWXpl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# あるいは入力文を変数に代入して実行する\n",
        "s = '名前はまだない'\n",
        "print(MeCab.Tagger('-Owakati').parse(s).strip())\n",
        "\n",
        "# 最後に付いている `strip()` は，文字列の最終要素，この場合は改行コード，を取り去るためである。"
      ],
      "metadata": {
        "id": "owR3F2v1futP"
      },
      "id": "owR3F2v1futP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 何度も分かち書きを繰り返して呼び出すのであれば，あらかじめ分かち書きを定義しておく\n",
        "wakati = MeCab.Tagger('-Owakati').parse\n",
        "s2 = 'どこで生れたか頓と見当がつかぬ。'\n",
        "print(wakati(s2).strip())"
      ],
      "metadata": {
        "id": "5JYjhDrgglFW"
      },
      "id": "5JYjhDrgglFW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 上記のようにして分かち書きした文は，空白で区切られているので，分割するには `split(' ')` を用いる\n",
        "# `split(' ')` で分割された文は，リストになっているので，表示の際はカギカッコで囲まれている。\n",
        "print(wakati(s2).strip().split(' '))"
      ],
      "metadata": {
        "id": "VniH5wQugm4k"
      },
      "id": "VniH5wQugm4k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ここで，みんなの日本語データが一行に一文で文字列からなるリストであると仮定しよう。\n",
        "# 以下のようにである:\n",
        "minnichi_sentences = [\n",
        "'ジュースをお願いします。',\n",
        "'いらっしゃいませ。メニューです。どうぞ。',\n",
        "'いくらですか。',\n",
        "'1.ホン:カレーとコーヒーをください。',\n",
        "'2.ジル:サンドイッチとジュースをお願いします。']\n",
        "\n",
        "# 上記のデータ `minnichi_sentences` を分かち書きさせてみよう。\n",
        "for s in minnichi_sentences:\n",
        "    print(wakati(s).strip().split(' '))"
      ],
      "metadata": {
        "id": "LqE1iCEhgvPk"
      },
      "id": "LqE1iCEhgvPk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ところで MeCab は入力文を構文解析する場合，解析結果をリストとして返す。\n",
        "mcb = MeCab.Tagger().parse  # mecab の定義。念のため再定義\n",
        "s = '何でも薄暗いじめじめした所でニヤーニヤー泣いて居た事丈は記憶して居る。'  # 入力文\n",
        "print(mcb(s))\n",
        "print(len(mcb(s).splitlines()[:-1]))\n"
      ],
      "metadata": {
        "id": "3dG51rPPZiHG"
      },
      "id": "3dG51rPPZiHG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 上記を行数を付番して表示してみる。\n",
        "# `enumerate` を使うと連番を得ることができる\n",
        "for i, x in enumerate(mcb(s).splitlines()[:-1]):\n",
        "    print(i, x)\n"
      ],
      "metadata": {
        "id": "lo9hW92Kg4rA"
      },
      "id": "lo9hW92Kg4rA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 分かち書きされた結果は更に，タブで表層形とその解析結果に分けられるので，分割して見よう。\n",
        "for i, x in enumerate(mcb(s).splitlines()[:-1]):\n",
        "    surface, content = x.split('\\t')\n",
        "    print(f'{i:2d}, {surface}: {content})')"
      ],
      "metadata": {
        "id": "Y02Skp2FhA8G"
      },
      "id": "Y02Skp2FhA8G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要な情報は，品詞1, 品詞2 および原形だけであるとしよう。\n",
        "# 上記の出力から，# 品詞1 は 0 番目，品詞2 は 1 番目，原形は 7 番目であることが分かるので\n",
        "# これを用いることにする\n",
        "for i, x in enumerate(mcb(s).splitlines()[:-1]):\n",
        "    surface, content = x.split('\\t')\n",
        "    _x = content.split(',')\n",
        "    pos1, pos2 = _x[0], _x[1] # それぞれ品詞1, 品詞2\n",
        "    original_form = surface if len(_x) <= 8 else _x[7]\n",
        "    print(f'{i:2d} 表層形:{surface}, 品詞1:{pos1} 品詞2:{pos2} 原形:{original_form}')\n"
      ],
      "metadata": {
        "id": "51aLGooShEEn"
      },
      "id": "51aLGooShEEn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, x in enumerate(mcb(s).splitlines()[:-1]):\n",
        "    surface, content = x.split('\\t')\n",
        "    _x = content.split(',')\n",
        "    pos1, pos2 = _x[0], _x[1] # それぞれ品詞1, 品詞2\n",
        "    original_form = surface if len(_x) <= 8 else _x[7]\n",
        "    # if len(_x) > 8:\n",
        "    #     original_form = _x[7]\n",
        "    # else:\n",
        "    #     original_form = \"\"\n",
        "\n",
        "    print(f'{i:2d} 表層形:{surface}, 品詞1:{pos1} 品詞2:{pos2} 原形:{original_form}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiKMwpvTezIt",
        "outputId": "a7772864-bd2a-453d-c618-6e1d922d86ef"
      },
      "id": "uiKMwpvTezIt",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0 表層形:何, 品詞1:代名詞 品詞2: 原形:何\n",
            " 1 表層形:で, 品詞1:助詞 品詞2:格助詞 原形:で\n",
            " 2 表層形:も, 品詞1:助詞 品詞2:係助詞 原形:も\n",
            " 3 表層形:薄暗い, 品詞1:形容詞 品詞2:一般 原形:薄暗い\n",
            " 4 表層形:じめじめ, 品詞1:副詞 品詞2: 原形:じめじめ\n",
            " 5 表層形:し, 品詞1:動詞 品詞2:非自立可能 原形:為る\n",
            " 6 表層形:た, 品詞1:助動詞 品詞2: 原形:た\n",
            " 7 表層形:所, 品詞1:名詞 品詞2:普通名詞 原形:所\n",
            " 8 表層形:で, 品詞1:助詞 品詞2:格助詞 原形:で\n",
            " 9 表層形:ニヤーニヤー, 品詞1:名詞 品詞2:普通名詞 原形:ニヤーニヤー\n",
            "10 表層形:泣い, 品詞1:動詞 品詞2:一般 原形:泣く\n",
            "11 表層形:て, 品詞1:助詞 品詞2:接続助詞 原形:て\n",
            "12 表層形:居, 品詞1:動詞 品詞2:非自立可能 原形:居る\n",
            "13 表層形:た, 品詞1:助動詞 品詞2: 原形:た\n",
            "14 表層形:事, 品詞1:名詞 品詞2:普通名詞 原形:事\n",
            "15 表層形:丈, 品詞1:名詞 品詞2:普通名詞 原形:丈\n",
            "16 表層形:は, 品詞1:助詞 品詞2:係助詞 原形:は\n",
            "17 表層形:記憶, 品詞1:名詞 品詞2:普通名詞 原形:記憶\n",
            "18 表層形:し, 品詞1:動詞 品詞2:非自立可能 原形:為る\n",
            "19 表層形:て, 品詞1:助詞 品詞2:接続助詞 原形:て\n",
            "20 表層形:居る, 品詞1:動詞 品詞2:非自立可能 原形:居る\n",
            "21 表層形:。, 品詞1:補助記号 品詞2:句点 原形:。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "import numpy as np\n",
        "\n",
        "for i in range(10):\n",
        "    wrd = minnichi_vocab[np.random.choice(len(minnichi))]\n",
        "    print(colored(wrd,'blue', attrs=['bold']), end=\": \") \n",
        "    #print(colored(minnichi[i]['word'],'blue', attrs=['bold']), end=\": \") \n",
        "    for _i in minnichi[wrd]['mecab']:\n",
        "        print('表層形',colored(_i['表層形'], 'green', attrs=['bold']),\n",
        "              '品詞',  colored(_i['品詞'],  'green', attrs=['bold','blink']), end=\" \") \n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HOxJAAnqDum",
        "outputId": "ea4518ca-849b-4fee-be42-5d692f5c3b8c"
      },
      "id": "5HOxJAAnqDum",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[34m父\u001b[0m: 表層形 \u001b[1m\u001b[32m父\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m \n",
            "\u001b[1m\u001b[34m港\u001b[0m: 表層形 \u001b[1m\u001b[32m港\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m \n",
            "\u001b[1m\u001b[34m脱ぎます\u001b[0m: 表層形 \u001b[1m\u001b[32m脱ぎ\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m五段\u001b[0m 表層形 \u001b[1m\u001b[32mます\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m助動詞\u001b[0m \n",
            "\u001b[1m\u001b[34mいす\u001b[0m: 表層形 \u001b[1m\u001b[32mいす\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m \n",
            "\u001b[1m\u001b[34m課長\u001b[0m: 表層形 \u001b[1m\u001b[32m課長\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m \n",
            "\u001b[1m\u001b[34m有名\u001b[0m: 表層形 \u001b[1m\u001b[32m有名\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m \n",
            "\u001b[1m\u001b[34m頼みます\u001b[0m: 表層形 \u001b[1m\u001b[32m頼み\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m五段\u001b[0m 表層形 \u001b[1m\u001b[32mます\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m助動詞\u001b[0m \n",
            "\u001b[1m\u001b[34m届けます\u001b[0m: 表層形 \u001b[1m\u001b[32m届け\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m下一段\u001b[0m 表層形 \u001b[1m\u001b[32mます\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m助動詞\u001b[0m \n",
            "\u001b[1m\u001b[34mそろそろ失礼します\u001b[0m: 表層形 \u001b[1m\u001b[32mそろそろ\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m 表層形 \u001b[1m\u001b[32m失礼\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m 表層形 \u001b[1m\u001b[32mし\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32mサ行変格\u001b[0m 表層形 \u001b[1m\u001b[32mます\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m助動詞\u001b[0m \n",
            "\u001b[1m\u001b[34m残業します\u001b[0m: 表層形 \u001b[1m\u001b[32m残業\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m\u001b[0m 表層形 \u001b[1m\u001b[32mし\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32mサ行変格\u001b[0m 表層形 \u001b[1m\u001b[32mます\u001b[0m 品詞 \u001b[5m\u001b[1m\u001b[32m助動詞\u001b[0m \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ccap import ccap_w2v\n",
        "w2v = ccap_w2v(is2017=True).w2v"
      ],
      "metadata": {
        "id": "9gXzeTXc0Zu-"
      },
      "id": "9gXzeTXc0Zu-",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#minn_vocab = [minnichi[x] for x in minnichi.keys()]\n",
        "minn_vocab = minnichi.keys()\n",
        "minnichi_not_w2v = []\n",
        "for w in minn_vocab:\n",
        "    if not w in w2v:\n",
        "        minnichi_not_w2v.append(w)\n",
        "    else:\n",
        "        ; \n",
        "        # print(w, end=\"\\t\")\n",
        "print(f'word2vec に存在しない minnichi 単語数:{len(minnichi_not_w2v)}')\n",
        "print(len(minnichi_not_w2v))\n",
        "print(f'最初の 3 語を表示: {minnichi_not_w2v[:3]}')\n",
        "print(f'最後の 3 語を表示: {minnichi_not_w2v[-3:]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPcdIaFoLiy-",
        "outputId": "3715513d-3c4e-4ca4-f1f4-663b402b7af8"
      },
      "id": "cPcdIaFoLiy-",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word2vec に存在しない minnichi 単語数:411\n",
            "411\n",
            "最初の 3 語を表示: ['あいます', 'あきらめます', 'あげます']\n",
            "最後の 3 語を表示: ['飼います', '飾ります', '騒ぎます']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word2vec に存在しないミンニチ語彙を MeCab によって分解し，各分解した語が word2vec に存在するか否かを調べる\n",
        "mcb = MeCab.Tagger().parse\n",
        "\n",
        "for wrd in minnichi_not_w2v:\n",
        "    surfaces = [ent.split('\\t')[0] for ent in mcb(wrd).splitlines()[:-1]]\n",
        "    for _s in surfaces:\n",
        "        color = 'red' if not _s in w2v else 'grey'\n",
        "        if not _s in w2v:\n",
        "            print(colored((wrd,_s), color, attrs=['bold']), end=\" \")    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cse28gnmH8on",
        "outputId": "05c29d55-3400-47ed-c024-dbdf331a51e3"
      },
      "id": "cse28gnmH8on",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[31m('おめでとうございます', 'めでとう')\u001b[0m \u001b[1m\u001b[31m('ごちそうさまでした', 'ちそう')\u001b[0m \u001b[1m\u001b[31m('初めまして', '初めまして')\u001b[0m \u001b[1m\u001b[31m('晩ごはん', '晩ごはん')\u001b[0m \u001b[1m\u001b[31m('片づきます', '片づき')\u001b[0m \u001b[1m\u001b[31m('雲ります', '雲り')\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrd = '本'\n",
        "topn=10\n",
        "wrd_list = [p[0] for p in w2v.most_similar(wrd,topn=topn)]\n",
        "print(wrd_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akpFP-8kPDn8",
        "outputId": "c3636ff2-6fcb-48e3-8fea-eac2e6b483db"
      },
      "id": "akpFP-8kPDn8",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['本書', '全', '上記', '同', '当', '本同', '文庫本', '一冊の本', '全巻', '抄出']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mecab_wakati(phrase:list):\n",
        "    \"\"\"mecab を使って分かち書きにする\n",
        "    colab のインストール状況によっては，分かち書きオプション -Owakati が存在しない。\n",
        "    根本的な解決は，定義ファイルを書けばよい。\n",
        "    だが，ここでは標準の MeCab 出力から等価な出力を実現してみた\n",
        "    \n",
        "    引数: \n",
        "        phrase: list[str]\n",
        "    \n",
        "    戻り値: list\n",
        "    \"\"\"\n",
        "    wakati = \" \".join(ent.split('\\t')[0] for ent in mcb(phrase).splitlines()[:-1])\n",
        "    return wakati\n",
        "\n",
        "print(mecab_wakati('これは，私が図書館で借りた本です。'))\n",
        "\n",
        "def mecab_pos(word:str):\n",
        "    \"\"\"mecab を使って単語の品詞情報を得る\n",
        "    引数: \n",
        "        word: str\n",
        "    戻り値:\n",
        "        list[str]\n",
        "    \"\"\"\n",
        "    # 次行はトリッキーに見えるかも知れないが，MeCab の出力 `mcb(word)` を\n",
        "    # 1. 行に区切り (`.splitlines()`)\n",
        "    # 2. 区切った各項目を更にタブ `('\\t')` で区切り\n",
        "    # 3. その 0 番目の要素である表層形と\n",
        "    # 4. '(',')` で区切った先頭要素を取り出して\n",
        "    # 5. タプルにして返す\n",
        "    # という操作を 1 行でしている\n",
        "    poses = [(ent.split('\\t')[0], ent.split('\\t')[1].split(',')[0]) for ent in mcb(word).splitlines()[:-1]]\n",
        "    return(poses)\n",
        "\n",
        "print(mecab_pos('これは，私が図書館で借りた本です。'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lni75hpwsfaS",
        "outputId": "8cf2e95d-ea53-407d-ddfa-5492b6b3ec61"
      },
      "id": "Lni75hpwsfaS",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "これ は ， 私 が 図書 館 で 借り た 本 です 。\n",
            "[('これ', '代名詞'), ('は', '助詞'), ('，', '補助記号'), ('私', '代名詞'), ('が', '助詞'), ('図書', '名詞'), ('館', '接尾辞'), ('で', '助詞'), ('借り', '動詞'), ('た', '助動詞'), ('本', '名詞'), ('です', '助動詞'), ('。', '補助記号')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3d14fb-8119-43b3-8d48-26afcd1eb31e",
      "metadata": {
        "id": "1d3d14fb-8119-43b3-8d48-26afcd1eb31e"
      },
      "source": [
        "# 2 「みんなの日本語」ファイルの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a525492-fa50-4898-b46d-ea7547481233",
      "metadata": {
        "id": "4a525492-fa50-4898-b46d-ea7547481233"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import unicodedata\n",
        "import glob\n",
        "import jaconv\n",
        "import sys\n",
        "from konoha import SentenceTokenizer\n",
        "splitter = SentenceTokenizer().tokenize\n",
        "\n",
        "if isColab:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # `2022_0410minnichi.txt` を指定してアップロードする    \n",
        "    with open('2022_0410minnichi.txt', 'r') as f:\n",
        "        minnichi_text = f.readlines()\n",
        "\n",
        "# コメントアウトしてあるのは，ローカル PC での実行のため，\n",
        "# # 岩下先生から頂いた「みんなの日本語」データの読み込み\n",
        "# minnichi_dir = '/Users/asakawa/study/2021jlpt'\n",
        "# minnichi_files = sorted(glob.glob(os.path.join(minnichi_dir, 'MINNICHI_*.txt')))\n",
        "\n",
        "# # みんなの日本語テキストを読み込み\n",
        "# minnichi_text = {}\n",
        "# for fname in minnichi_files:\n",
        "#     _fname = os.path.split(fname)[-1].split('.')[0]\n",
        "\n",
        "#     if not _fname in minnichi_text:\n",
        "#         minnichi_text[_fname] = []\n",
        "#     txt = []\n",
        "#     with open(fname,'r') as f:\n",
        "#         texts = f.readlines()\n",
        "        \n",
        "#         for txt in texts:\n",
        "#             txt = jaconv.normalize(txt.strip())\n",
        "#             if len(txt) > 0:\n",
        "#                 minnichi_text[_fname].append(txt)\n",
        "                \n",
        "\n",
        "# _minn_txt = []\n",
        "# for k, v in minnichi_text.items():\n",
        "#     for ll in minnichi_text[k]:\n",
        "#         for _ll in splitter(ll):\n",
        "#             _minn_txt.append(_ll)\n",
        "\n",
        "# minnichi_text = _minn_txt            \n",
        "# print(f'みんなの日本語テキストの総行数:{len(minnichi_text)}')\n",
        "\n",
        "# 結果を書き出す場合には，次行以下のコメントを削除する\n",
        "# with open('2022_0410minnichi.txt', 'w') as f:\n",
        "#     for l in minnichi_text:\n",
        "#         f.write(l+'\\n')\n",
        "\n",
        "minnichi_pos_dict = {}\n",
        "for l in minnichi_text:\n",
        "    for w, pos in mecab_pos(l):\n",
        "        if pos in minnichi_pos_dict:\n",
        "            minnichi_pos_dict[pos] += 1\n",
        "        else:\n",
        "            minnichi_pos_dict[pos] = 1\n",
        "        \n",
        "print(minnichi_pos_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "問題作成のための，同義語，反意語を探す工夫\n"
      ],
      "metadata": {
        "id": "S8hA99QP4_oQ"
      },
      "id": "S8hA99QP4_oQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# 次行はローカルな実行環境とクラウド計算環境である google colab との差分を吸収するため\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install --upgrade nltk    \n",
        "    import nltk\n",
        "    nltk.download('all')    \n",
        "    nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAMbO8hN5P0n",
        "outputId": "e0cbad30-5ccb-47e0-d014-c3cb4c9a3800"
      },
      "id": "IAMbO8hN5P0n",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.63.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.3.15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fd54fcc-302e-4dbb-aee7-b9f89215ee22",
      "metadata": {
        "id": "7fd54fcc-302e-4dbb-aee7-b9f89215ee22"
      },
      "outputs": [],
      "source": [
        "import typing\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def wordnet_synonym_antonym(word:str,\n",
        "                            lang:str='jpn'):\n",
        "    \"\"\"nltk の wordnet を使って，同義語と反意語の辞書を返す\n",
        "    引数として word: str をとる。\n",
        "    オプション lang: ['eng', 'jpn'] 英語か日本語かを指定\n",
        "    戻り値: dict\n",
        "        '同義語': list\n",
        "        '反意語': list\n",
        "    使用例:\n",
        "        wordnet_synonym_antonym('英語', lang='jpn')  \n",
        "        # オプション: lang='jpn' は省略可能\n",
        "        \n",
        "        word_synonym_antonym(word='data', lang='eng')\n",
        "    \"\"\"\n",
        "    synonyms, antonyms = [], []\n",
        "\n",
        "    for syn in wn.synsets(word, lang=lang):\n",
        "        for l in syn.lemmas(lang=lang):\n",
        "            synonyms.append(l.name())\n",
        "            if l.antonyms():\n",
        "                antonyms.append(l.antonyms()[0].name())\n",
        "                \n",
        "    return {'同義語':synonyms, '反意語':antonyms}\n",
        "\n",
        "\n",
        "s = 'これは，私が図書館で借りた本です。'\n",
        "s_wakati = mecab_wakati(s).split(' ')\n",
        "for wrd in s_wakati:\n",
        "    print(f'{wrd}: {wordnet_synonym_antonym(wrd)}')\n",
        "\n",
        "print('\\n---\\n')\n",
        "\n",
        "def wordnet_defs_examples(word:str,\n",
        "                          lang='jpn'):\n",
        "    syns = wn.synsets(wrd, lang=lang)\n",
        "    _def, _exmpl = [], []\n",
        "    for syn in wn.synsets(word, lang=lang):\n",
        "        if syn.definition():\n",
        "            _def.append(syn.definition())\n",
        "        if syn.examples():\n",
        "            _exmpl.append(syn.examples())\n",
        "    return {'定義':_def, '例文':_exmpl}\n",
        "                          \n",
        "\n",
        "#wordnet_defs_examples('本')\n",
        "s = 'これは，私が図書館で借りた本です。'\n",
        "s_wakati = mecab_wakati(s).split(' ')\n",
        "for wrd in s_wakati:\n",
        "    print(f'{wrd}: {wordnet_defs_examples(wrd)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syns = wn.synsets('図書', lang='jpn')\n",
        "\n",
        "for syn in syns:\n",
        "    print(f'syn:{syn}')\n",
        "    #print(syn.lemmas(lang='jpn'))\n",
        "    print(syn.definition())\n",
        "    print(syn.examples())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJNNoAP95LbJ",
        "outputId": "45eef653-9cf7-415c-996f-b752ea5d7683"
      },
      "id": "NJNNoAP95LbJ",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "syn:Synset('book.n.02')\n",
            "physical objects consisting of a number of pages bound together\n",
            "['he used a large book as a doorstop']\n",
            "syn:Synset('book.n.01')\n",
            "a written work or composition that has been published (printed on pages bound together)\n",
            "['I am reading a good book on economics']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3bQZpiyN9XNe"
      },
      "id": "3bQZpiyN9XNe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "2022_0410iwa_yoshi_presentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}