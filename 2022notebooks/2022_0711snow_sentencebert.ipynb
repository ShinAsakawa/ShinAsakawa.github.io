{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0711snow_sentencebert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2515f2f4-f6ab-4132-a66f-6e0e492cbf36",
      "metadata": {
        "id": "2515f2f4-f6ab-4132-a66f-6e0e492cbf36"
      },
      "source": [
        "# 長岡技術大学 やさしい日本語 SNOW コーパスのやさしい日本語部分を sentenceBERT により検討\n",
        "- date: 2022_0711\n",
        "- author: 浅川伸一"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d60fb2-7780-42f4-8043-0a87d74d7e00",
      "metadata": {
        "id": "a6d60fb2-7780-42f4-8043-0a87d74d7e00",
        "outputId": "fd855729-e8ea-4dfb-8e93-b21f2b62c7a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "日付: \u001b[1m\u001b[32m2022-07-12\u001b[0m\n",
            "HOSTNAME: \u001b[1m\u001b[32mLeda\u001b[0m\n",
            "ユーザ名: \u001b[1m\u001b[32masakawa\u001b[0m\n",
            "HOME: \u001b[1m\u001b[32m/Users/asakawa\u001b[0m\n",
            "ファイル名: \u001b[1m\u001b[32mstudy/2022jlpt/2022_0711snow_sentencebert.ipynb\u001b[0m\n",
            "torch.__version__: \u001b[1m\u001b[32m1.12.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "try:\n",
        "    import bit\n",
        "except ImportError:\n",
        "    !pip install ipynbname --upgrade > /dev/null 2>&1 \n",
        "    !git clone https://github.com/ShinAsakawa/bit.git\n",
        "import bit\n",
        "\n",
        "isColab = bit.isColab\n",
        "HOME = bit.HOME\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install --upgrade openpyxl\n",
        "    !pip install --upgrade pandas\n",
        "    !pip install --upgrade fugashi[unidic-lite]\n",
        "    !pip install --upgrade ipadic\n",
        "    !python -m unidic download\n",
        "    !pip install transformers\n",
        "    !pip install --upgrade jaconv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be66c3d2-4665-4f69-84c0-fa6e4e894359",
      "metadata": {
        "id": "be66c3d2-4665-4f69-84c0-fa6e4e894359"
      },
      "source": [
        "# やさしい日本語コーパスの取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd2f619-cab5-46f8-bd90-f9c27f600bdf",
      "metadata": {
        "id": "bdd2f619-cab5-46f8-bd90-f9c27f600bdf",
        "outputId": "1dfd68da-716e-48dd-8db4-e64585cce8b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['誰が一番に着くか私には分かりません。', '多くの動物が人間によって殺された。', '私はテニス部員です。']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import requests\n",
        "import pandas as pd\n",
        "import jaconv\n",
        "SNOWs={'T15': {'url':\"https://filedn.com/lit4DCIlHwxfS1gj9zcYuDJ/SNOW/T15-2020.1.7.xlsx\"},\n",
        "       'T23': {'url':\"https://filedn.com/lit4DCIlHwxfS1gj9zcYuDJ/SNOW/T23-2020.1.7.xlsx\"},}\n",
        "\n",
        "\n",
        "for corpus in SNOWs:\n",
        "    url = SNOWs[corpus]['url']\n",
        "    excel_fname = corpus + '-2020.1.7.xlsx'\n",
        "    if not os.path.exists(excel_fname):\n",
        "        r = requests.get(url)\n",
        "        with open(excel_fname, 'wb') as f:\n",
        "            total_length = int(r.headers.get('content-length'))\n",
        "            print(f'{excel_fname} をダウンロード中 {total_length} バイト')\n",
        "            f.write(r.content)\n",
        "\n",
        "    SNOWs[corpus]['df'] = pd.read_excel(excel_fname, engine='openpyxl')\n",
        "    SNOWs[corpus]['df'] = SNOWs[corpus]['df'].rename(columns={'#日本語(原文)': 'ja', \n",
        "                                                              '#やさしい日本語':'easy_ja',\n",
        "                                                              '#英語(原文)':'en'})\n",
        "\n",
        "_snow_sents = SNOWs['T15']['df']['easy_ja'].to_list() + SNOWs['T23']['df']['easy_ja'].to_list()\n",
        "snow_sents = [jaconv.normalize(line, 'NFKC') for line in _snow_sents]\n",
        "print(snow_sents[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bdcb2fa-d3be-463b-9d8c-6167974aa5cf",
      "metadata": {
        "id": "9bdcb2fa-d3be-463b-9d8c-6167974aa5cf"
      },
      "source": [
        "# センテンスBert の取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac8962a-a50c-402a-87a6-56191ec86843",
      "metadata": {
        "id": "cac8962a-a50c-402a-87a6-56191ec86843",
        "outputId": "7a5a297b-bb57-4f07-b09d-e9b70b56acc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'BertJapaneseTokenizer'.\n"
          ]
        }
      ],
      "source": [
        "import typing\n",
        "import torch\n",
        "from transformers import BertJapaneseTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "class SentenceBertJapanese:\n",
        "    def __init__(self, \n",
        "                 model_name_or_path:str, \n",
        "                 device:str=None, \n",
        "                ):\n",
        "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
        "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
        "        self.model.eval()\n",
        "\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = torch.device(device)\n",
        "        self.model.to(device)\n",
        "\n",
        "    def _mean_pooling(self, \n",
        "                      model_output:torch.Tensor, \n",
        "                      attention_mask:torch.Tensor):\n",
        "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode(self, \n",
        "               sentences:[str], \n",
        "               batch_size:int=8):\n",
        "        all_embeddings = []\n",
        "        iterator = range(0, len(sentences), batch_size)\n",
        "        for batch_idx in iterator:\n",
        "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
        "\n",
        "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\", \n",
        "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
        "            model_output = self.model(**encoded_input)\n",
        "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
        "\n",
        "            all_embeddings.extend(sentence_embeddings)\n",
        "\n",
        "        # return torch.stack(all_embeddings).numpy()\n",
        "        return torch.stack(all_embeddings)\n",
        "    \n",
        "sbert_model = SentenceBertJapanese(\"sonoisa/sentence-bert-base-ja-mean-tokens\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "528d2ac9-e394-472a-b03d-8b17a6f12a97",
      "metadata": {
        "id": "528d2ac9-e394-472a-b03d-8b17a6f12a97",
        "outputId": "60d01e65-acf4-406c-d8d6-dc7a6d7b548c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 30679228\n",
            "-rw-r--r--  1 asakawa staff       25441 Jul 12 19:04 2022_0711snow_sentencebert.ipynb\n",
            "-rw-r--r--  1 asakawa staff   258970347 Jul 12 09:23 2022_0712sbert_snow_vectros.pt\n",
            "-rw-r--r--  1 asakawa staff   442548785 Jul 12 09:15 2022_0712sbert_snow_model.pt\n",
            "-rw-r--r--  1 asakawa staff       35846 Jul 12 09:03 2022_0623BERT_SNOW_training.ipynb\n",
            "-rw-r--r--  1 asakawa staff       22966 Jul 11 15:43 2022_0703iwashita_yoshihara_demo.ipynb\n",
            "-rw-r--r--  1 asakawa staff      722985 Jul 10 07:24 2022_0703iwashita_yoshihara_demo.html\n",
            "-rw-r--r--  1 asakawa staff 10790400787 Jul  4 12:11 snow_embeddings.pt\n",
            "-rw-r--r--  1 asakawa staff       12880 Jul  3 09:23 2022_0529iwashita_yoshihara_demo.ipynb\n",
            "-rw-r--r--  1 asakawa staff        1346 Jul  3 08:11 Untitled4.ipynb\n"
          ]
        }
      ],
      "source": [
        "!gls -lt | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea0415c8-ee3a-4428-9c2f-23782dfa59b7",
      "metadata": {
        "id": "ea0415c8-ee3a-4428-9c2f-23782dfa59b7",
        "outputId": "cea0d3c8-7e32-4f6a-b596-3c45510ad529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1.2 ms, sys: 187 ms, total: 189 ms\n",
            "Wall time: 188 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "snow_vectors_pretrained_fname = '2022_0712sbert_snow_vectros.pt'\n",
        "if os.path.exists(snow_vectors_pretrained_fname):\n",
        "    snow_vectors = torch.load(snow_vectors_pretrained_fname)\n",
        "else:\n",
        "    snow_vectors = sbert_model.encode(snow_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0322b32-19fb-4cb3-b8c0-7fbb334fed92",
      "metadata": {
        "id": "a0322b32-19fb-4cb3-b8c0-7fbb334fed92"
      },
      "outputs": [],
      "source": [
        "from termcolor import colored\n",
        "import scipy.spatial\n",
        "\n",
        "def search_sim_sents(queries:list,\n",
        "                     answers:list,\n",
        "                     model:BertModel,\n",
        "                     vectors:torch.Tensor,\n",
        "                     top_n:int = 5,\n",
        "                     verbose:bool=False,\n",
        "                    ):\n",
        "    \n",
        "    if answers == None:\n",
        "        answers = queries\n",
        "    ret = {}\n",
        "    query_embeddings = model.encode(queries).numpy()\n",
        "    for query, query_embedding in zip(queries, query_embeddings):\n",
        "        distances = scipy.spatial.distance.cdist([query_embedding], \n",
        "                                                 vectors, \n",
        "                                                 metric=\"cosine\")[0]\n",
        "\n",
        "        results = zip(range(len(distances)), distances)\n",
        "        results = sorted(results, key=lambda x: x[1])\n",
        "        ret[query] = []\n",
        "        for idx, distance in results[1:top_n+1]:\n",
        "            print(f'{query}, {answers[idx]}, {1-distance/2:.3f}') if verbose else None\n",
        "            ret[query].append((answers[idx], 1 - distance/2))\n",
        "    return ret\n",
        "\n",
        "# 試みに最初の 3 文について\n",
        "ret = search_sim_sents(queries=snow_sents[:10],\n",
        "                       answers=snow_sents,\n",
        "                       model=sbert_model,\n",
        "                       vectors=snow_vectors,\n",
        "                       #verbose=True,\n",
        "                       top_n=10,\n",
        "                      )\n",
        "for i, (k, v) in enumerate(ret.items()):\n",
        "    print(colored(f'文番号 {i:3d} {k}', 'grey', attrs=['bold']))\n",
        "    for j, _v in enumerate(v):\n",
        "        print(f'\\t近接文{j:2d}:{_v[0]}({_v[1]:.3f})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575c479e-02ca-4ef9-9ceb-aadf3923f57b",
      "metadata": {
        "id": "575c479e-02ca-4ef9-9ceb-aadf3923f57b",
        "outputId": "4f6dbbdf-55c4-45a6-9195-8db6e471c3ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[2, 1325, 9, 6889, 12328, 2992, 3], [2, 6889, 12328, 228, 1325, 9, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n",
            "{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
            "4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.7037,  1.2595, -1.2334,  ...,  0.0934, -0.9106, -1.0724],\n",
              "         [-0.2396,  1.0861, -1.0015,  ..., -0.3351, -0.6798, -1.7550],\n",
              "         [-0.8514,  1.2397, -0.7524,  ..., -0.0784, -0.6006, -0.1424],\n",
              "         ...,\n",
              "         [-0.2357,  0.5510, -1.0350,  ...,  0.5522, -0.8738, -0.8861],\n",
              "         [-0.0384,  0.7335, -1.0317,  ..., -0.1277, -0.6316, -0.6313],\n",
              "         [-0.4820, -0.0082, -1.0848,  ..., -0.2211, -0.6802, -0.9180]],\n",
              "\n",
              "        [[-0.7876,  1.1099, -0.7971,  ..., -0.2486, -0.9013, -0.8675],\n",
              "         [-0.4757,  1.2360, -0.3378,  ..., -0.5264, -0.5743, -0.9001],\n",
              "         [-0.4064,  0.4635, -0.3317,  ...,  0.3605, -1.2550, -0.8625],\n",
              "         ...,\n",
              "         [-0.4165,  0.5617, -0.3629,  ..., -0.7836, -0.6884, -1.6008],\n",
              "         [-1.0472,  0.9636,  0.2279,  ..., -0.0137, -1.6680, -0.9403],\n",
              "         [-0.8573, -0.2707, -0.4982,  ..., -0.5501, -0.8519, -0.9521]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.5948, -0.3074,  0.1811,  ...,  0.2291, -0.3318, -0.3720],\n",
              "        [ 0.3298, -0.5058,  0.3227,  ...,  0.0891, -0.4465, -0.1800]],\n",
              "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(sbert_model.tokenizer(['私はテニス部員です', 'テニス部員，私は']))\n",
        "print(sbert_model.tokenizer.special_tokens_map)\n",
        "print(sbert_model.tokenizer.mask_token_id)\n",
        "\n",
        "inpX = {'input_ids': torch.LongTensor([[2, 1325, 9, 4, 12328, 2992, 3], [2, 4, 12328, 228, 1325, 9, 3]]), 'token_type_ids': torch.LongTensor([[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': torch.LongTensor([[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]])}\n",
        "\n",
        "sbert_model.model.eval()\n",
        "sbert_model.model. (**inpX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaff91af-9967-4956-b205-8e8374660450",
      "metadata": {
        "id": "aaff91af-9967-4956-b205-8e8374660450",
        "outputId": "e3ef2f30-ed33-443c-88f9-90c514977240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.         0.76833675 0.89350755 0.73074326]\n",
            " [0.76833675 1.         0.73862351 0.88788029]\n",
            " [0.89350755 0.73862351 1.         0.77682146]\n",
            " [0.73074326 0.88788029 0.77682146 1.        ]]\n",
            "['日', 'が', '射', '##し', 'たり', '光', 'が', 'とも', '##っ', 'て', '、', 'ぽ', 'っ']\n",
            "['あ', '##たた', '##かい', '日', 'ざ', '##し', 'が', '差し', 'こみ', '、', 'ぽ', '##か', '##ぽ', '##か']\n",
            "['暖', '##かい', '日', '##差し', 'が', '差し', 'こみ', '、', 'ぽ', '##か', '##ぽ', '##か']\n",
            "['組織', 'や', '機構', 'など', 'を', '動かし', ',', 'うまく', '機能', 'する', 'よう', 'に', 'する', 'こと', '、', '運営']\n",
            "['方針', 'を', '定め', ',', '組織', 'を', '整え', 'て', ',', '目的', 'を', '達成', 'する', 'よう', '持続', '的', 'に', '事', 'を', '行う', 'こと', '、', '経営']\n"
          ]
        }
      ],
      "source": [
        "# ans = sbert_model.encode(['組織や機構などを動かし，うまく機能するようにすること、運営', '運営', \n",
        "#                           '方針を定め，組織を整えて，目的を達成するよう持続的に事を行うこと、経営', '経営'])\n",
        "ans = sbert_model.encode(['組織や機構などを動かし，うまく機能するようにすること、運営', '運営', \n",
        "                           '方針を定め，組織を整えて，目的を達成するよう持続的に事を行うこと、経営', '経営'])\n",
        "\n",
        "ans1 = sbert_model.encode(['日が射したり光がともって、ぽっ', 'ぽっ', \n",
        "                          'あたたかい日ざしが差しこみ、ぽかぽか', 'ぽかぽか',\n",
        "                          #'水や日ざしが満ち満ちて、なんなん', 'なんなん'\n",
        "                         ])\n",
        "#ans = sbert_model.encode(['組織や機構などを動かし，うまく機能するようにすること', '方針を定め，組織を整えて，目的を達成するよう持続的に事を行うこと'])\n",
        "#type(ans)\n",
        "ans_ = ans.clone().numpy()\n",
        "#unei  = ans[0].clone().numpy()\n",
        "#keiei = ans[1].clone().numpy()\n",
        "\n",
        "ans__ = scipy.spatial.distance.cdist(ans_, ans_, metric=\"cosine\") # [0]\n",
        "ans___ = 1 - ans__/2\n",
        "#print(ans___) # [1.         0.89350759] 後ろありの結果\n",
        "print(ans___) # [1.         0.89259623] 後ろなしの結果\n",
        "\n",
        "print(sbert_model.tokenizer.tokenize('日が射したり光がともって、ぽっ'))\n",
        "print(sbert_model.tokenizer.tokenize('あたたかい日ざしが差しこみ、ぽかぽか'))\n",
        "print(sbert_model.tokenizer.tokenize('暖かい日差しが差しこみ、ぽかぽか'))\n",
        "print(sbert_model.tokenizer.tokenize('組織や機構などを動かし，うまく機能するようにすること、運営'))\n",
        "print(sbert_model.tokenizer.tokenize('方針を定め，組織を整えて，目的を達成するよう持続的に事を行うこと、経営'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e387df-d852-426f-b2f3-78473f055dfa",
      "metadata": {
        "id": "86e387df-d852-426f-b2f3-78473f055dfa",
        "outputId": "760830ed-9bb3-48ee-9658-93f591db9299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[30m文番号   0 組織や機構などを動かし，うまく機能するようにすること、運営\u001b[0m\n",
            "\t近接文 0:彼にはその仕事をする能力がある。(0.881)\n",
            "\t近接文 1:彼は働く人の生活を良くするのに力を使った。(0.877)\n",
            "\t近接文 2:彼には事業を経営するのに十分な能力がある。(0.872)\n",
            "\t近接文 3:この方法は、いろいろなところで使える。(0.871)\n",
            "\t近接文 4:しっかりした計画と努力のおかげです。(0.871)\n",
            "\t近接文 5:彼はその機会をうまく利用した。(0.870)\n",
            "\t近接文 6:行儀の良い行動が人を作る。(0.867)\n",
            "\t近接文 7:彼はその仕事を十分できる。(0.867)\n",
            "\t近接文 8:彼は自分の能力をうまく使う。(0.863)\n",
            "\t近接文 9:何かあった時役に立つ。(0.862)\n",
            "\u001b[1m\u001b[30m文番号   1 方針を定め，組織を整えて，目的を達成するよう持続的に事を行うこと、経営\u001b[0m\n",
            "\t近接文 0:彼は目的を達成するために頑張って働いた。(0.890)\n",
            "\t近接文 1:彼は人生の計画を立てるために頑張って働いている。(0.883)\n",
            "\t近接文 2:彼はその計画を実行するために力の限りを出した。(0.878)\n",
            "\t近接文 3:成功するためには、努力して働かなければならない。(0.874)\n",
            "\t近接文 4:彼には事業を経営するのに十分な能力がある。(0.873)\n",
            "\t近接文 5:成功するように頑張って働きなさい。(0.869)\n",
            "\t近接文 6:しっかりした計画と努力のおかげです。(0.868)\n",
            "\t近接文 7:彼はいい仕事をしようととても努力した。(0.867)\n",
            "\t近接文 8:私は、成功するために、頑張って働いた。(0.866)\n",
            "\t近接文 9:成功するように頑張って仕事をしろ。(0.865)\n"
          ]
        }
      ],
      "source": [
        "ret = search_sim_sents(queries=['組織や機構などを動かし，うまく機能するようにすること、運営', '方針を定め，組織を整えて，目的を達成するよう持続的に事を行うこと、経営'],\n",
        "#ret = search_sim_sents(queries=['私はテニス部員です', 'テニス部員，私は', '私は[MASK]部員です', '[MASK]部員，私は'],\n",
        "#                       answers=snow_sents+['私はテニス部員です', 'テニス部員，私は'],\n",
        "                       answers=snow_sents+['組織や機構などを動かし，うまく機能するようにすること、運営', '方針を定め，組織を整えて，目的を達成するよう持続的に事を行うこと、経営'],\n",
        "#ret = search_sim_sents(queries=['私はテニス部員です', 'テニス部員，私は', '私は[MASK]部員です', '[MASK]部員，私は'],\n",
        "#                       answers=snow_sents+['私はテニス部員です', 'テニス部員，私は'],\n",
        "                       model=sbert_model,\n",
        "                       vectors=snow_vectors,\n",
        "                       #verbose=True,\n",
        "                       top_n=10,\n",
        "                      )\n",
        "for i, (k, v) in enumerate(ret.items()):\n",
        "    print(colored(f'文番号 {i:3d} {k}', 'grey', attrs=['bold']))\n",
        "    for j, _v in enumerate(v):\n",
        "        print(f'\\t近接文{j:2d}:{_v[0]}({_v[1]:.3f})')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ba62bf-6d2f-46f9-ada1-b13eed601e04",
      "metadata": {
        "id": "69ba62bf-6d2f-46f9-ada1-b13eed601e04"
      },
      "source": [
        "# オノマトペの可視化 tSNE による散布図"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74f3685e-f441-4d2a-825a-aecb86b0bfca",
      "metadata": {
        "id": "74f3685e-f441-4d2a-825a-aecb86b0bfca"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from ccap import tsne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d18584-28b0-4f05-ab92-94e792ac4b0f",
      "metadata": {
        "id": "e3d18584-28b0-4f05-ab92-94e792ac4b0f"
      },
      "outputs": [],
      "source": [
        "def calc_and_draw_tsne(model:BertModel=sbert_model.model,\n",
        "                       vectors:snow_sentsTensor=snow_vectors,\n",
        "                       tag:list=snow_sents,\n",
        "                       figsize:tuple=(20,20),\n",
        "                       fontsize:int=5,\n",
        "                       title:str=None,\n",
        "                       fig_fname:str=None,\n",
        "                       excel_fname:str=None,\n",
        "                      ):\n",
        "    \n",
        "    X = tsne.tsne(vectors.clone().numpy())  # tSNE の実施\n",
        "    #X = tsne.tsne(vectors.clone().numpy())  # tSNE の実施\n",
        "    \n",
        "    plt.figure(figsize=figsize)             # 図のサイズ指定，単位インチ\n",
        "    plt.scatter(X[:,0], X[:,1], s=20)       # 散布図の描画\n",
        "    for i, txt in enumerate(tag):           # 図内にアノテーションを書き込む\n",
        "        plt.annotate(tag[i], (X[i,0], X[i,1]), \n",
        "                     alpha=0.7, \n",
        "                     ha='center', \n",
        "                     fontsize=fontsize)\n",
        "    plt.title(title) if title != None else None  # 図の表題\n",
        "    \n",
        "    if fig_fname != None:             # 図のファイル書き出し\n",
        "        if os.path.exists(fig_fname):\n",
        "            print(f'File: {fig_fname} exists.',end=\" \")\n",
        "            yn = input('delete[Y/n]')\n",
        "            if yn == 'Y':\n",
        "                os.remove(fig_fname)\n",
        "        plt.savefig(fig_fname)\n",
        "        \n",
        "    if excel_fname != None:           # 図のデータをエクセルファイルに書き出し       \n",
        "        if os.path.exists(excel_fname):\n",
        "            print(f'File: {excel_fname} exists.', end=\" \")\n",
        "            yn = input('delete[Y/n]')\n",
        "            if yn == 'Y':\n",
        "                os.remove(excel_fname)\n",
        "        _dict = {}\n",
        "        for i, (tag, _X) in enumerate(zip(tag, X)):\n",
        "            _dict[i] = {'tag':tag,\n",
        "                        'x': _X[0],\n",
        "                        'y': _X[1]\n",
        "                       }\n",
        "        df = pd.DataFrame(_dict).T\n",
        "        df.to_excel(excel_fname)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9d6671-95a1-487c-9181-e0ae69079721",
      "metadata": {
        "id": "6b9d6671-95a1-487c-9181-e0ae69079721"
      },
      "outputs": [],
      "source": [
        "print(type(snow_vectors), snow_vectors.size())\n",
        "N = 100\n",
        "_Xvects = snow_vectors[:N,:]\n",
        "_Xsents = snow_sents[:N]\n",
        "\n",
        "calc_and_draw_tsne(model=sbert_model.model,\n",
        "                   vectors=_Xvects,\n",
        "                   tag=_Xsents,\n",
        "                   fig_fname='2022_0711sbert_snow_tsne.pdf',\n",
        "                   excel_fname='2022_0711sbert_snow_tsne_values.xlsx'\n",
        "                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44979fd1-34e8-445f-9a31-56a662f067ec",
      "metadata": {
        "id": "44979fd1-34e8-445f-9a31-56a662f067ec"
      },
      "outputs": [],
      "source": [
        "calc_and_draw_tsne(model=sbert_model.model,\n",
        "                   vectors=snow_vectors,\n",
        "                   fig_fname='2022_0711sbert_snow_tsne.pdf',\n",
        "                   excel_fname='2022_0711sbert_snow_tsne_values.xlsx'\n",
        "                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6dc437-8cc0-4cca-a311-f5c9b298cf12",
      "metadata": {
        "id": "9b6dc437-8cc0-4cca-a311-f5c9b298cf12"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "2022_0711snow_sentencebert.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}