{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0623BERT_SNOW_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "685cf5c1-71dd-47ec-b65b-e8a2d66fa96f",
      "metadata": {
        "id": "685cf5c1-71dd-47ec-b65b-e8a2d66fa96f"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "try:\n",
        "    import bit\n",
        "except ImportError:\n",
        "    !pip install ipynbname\n",
        "    !git clone https://github.com/ShinAsakawa/bit.git\n",
        "import bit\n",
        "isColab = bit.isColab\n",
        "\n",
        "if isColab:\n",
        "    !pip install jaconv\n",
        "    !pip install transformers fugashi ipadic    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891af644-47fc-4966-b3d1-d1bdc03eedaf",
      "metadata": {
        "id": "891af644-47fc-4966-b3d1-d1bdc03eedaf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "from termcolor import colored\n",
        "import jaconv\n",
        "\n",
        "# やさしい日本語をダウンロード\n",
        "SNOWs={'T15': {'url':\"https://filedn.com/lit4DCIlHwxfS1gj9zcYuDJ/SNOW/T15-2020.1.7.xlsx\"},\n",
        "       'T23': {'url':\"https://filedn.com/lit4DCIlHwxfS1gj9zcYuDJ/SNOW/T23-2020.1.7.xlsx\"},\n",
        "      }\n",
        "print('エクセルファイル読込', end='...')\n",
        "for corpus in SNOWs:\n",
        "    url = SNOWs[corpus]['url']\n",
        "    excel_fname = corpus + '-2020.1.7.xlsx'\n",
        "    \n",
        "    if not os.path.exists(excel_fname):  # ファイルが存在しない場合ダウンロード\n",
        "        print(f'url:{url}')\n",
        "        r = requests.get(url)\n",
        "        with open(excel_fname, 'wb') as f:\n",
        "            total_length = int(r.headers.get('content-length'))\n",
        "            print(f'{excel_fname} をダウンロード中 {total_length} バイト')\n",
        "            f.write(r.content)\n",
        "\n",
        "    SNOWs[corpus]['df'] = pd.read_excel(excel_fname)\n",
        "    SNOWs[corpus]['df'] = SNOWs[corpus]['df'].rename(columns={'#日本語(原文)': 'ja', \n",
        "                                                              '#やさしい日本語':'easy_ja',\n",
        "                                                              '#英語(原文)':'en'})\n",
        "# 2 つのデータをあわせる    \n",
        "_snow = SNOWs['T15']['df']['easy_ja'].tolist() + SNOWs['T23']['df']['easy_ja'].tolist()\n",
        "snow = [jaconv.normalize(line, 'NFKC') for line in _snow] # 正規化"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6436543-dc1d-4bd5-935e-48c52bbb9f52",
      "metadata": {
        "id": "c6436543-dc1d-4bd5-935e-48c52bbb9f52"
      },
      "source": [
        "BERT の事前訓練には，マスク化言語モデル (**MLM**) と次文予測 (**NSP**) という 2 つの独自の学習アプローチがある。\n",
        "ここでは，マスク化言語モデルを用いて，モデルを微調整 fine-tuning する方法を試してみることとする。\n",
        "\n",
        "# 1. マスク化言語モデル MLM\n",
        "\n",
        "MLM とは BERT モデルに対して，文を入力として与え，BERT 内部の重みを最適化して，側に同じ文を出力することである。\n",
        "このとき，文中の任意の単語，または単語の断片を マスクトークンに置き換えることを行う。\n",
        "BERT はこの，マスクトークンの位置に正しいトークンを予測することが求められる。\n",
        "\n",
        "実際に BERT にその入力文を与える前に，トークンを定義するなどが，必要になる。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*phTLnQ8itb3ZX5_h9BWjWw.png\" width=\"600px\"><br/>\n",
        "本図では，トークン を BERT に渡す前に，リンカーン・トークンを [MASK] に置き換えてマスクしている。\n",
        "</center>\n",
        "\n",
        "すなわち，実際には不完全な文章を入力して，BERT に，その文章を完成させるように依頼している。\n",
        "国語の教科教育，あるいは外国語の習得おける，穴埋め問題とみなしうる。\n",
        "\n",
        "## 1.1 マスクを埋める\n",
        "\n",
        "例えば以下のような文章が与えられたとする:\n",
        "\n",
        "`秋 に は ， ___ が 木 から 落ちる 。`\n",
        "\n",
        "アンダーライン部分の単語を推定することを考えてみる。\n",
        "答えは，所与の文章かから，文脈を類推し予測していることになる。\n",
        "\n",
        "「落ちる」 と 「木」 という単語が出てきましたが，足りない単語は木から落ちるものだということがわかる。\n",
        "\n",
        "どんぐり，枝，葉など，木から落ちるものはたくさんある。\n",
        "だが，秋という別の条件があるので，秋に木から落ちる可能性が最も高いのは葉だということで，検索対象が絞られる。\n",
        "\n",
        "人間はは，一般的な世界の知識と言語的な理解を組み合わせて，その結論を導き出す。\n",
        "BERT の場合，この推測は，コーパスから，たくさんの文章を与えられることで，言語パターンを学んでいることから適切な単語を得ることとなる。\n",
        "\n",
        "BERT は，秋，木，葉が何であるかを知らないかもしれません。\n",
        "だが，言語パターンとこれらの単語の文脈から，答えが葉である可能性が最も高いことを知ることとなる。\n",
        "\n",
        "この処理の結果，BERT にとっては，使用されている言語のスタイルの理解度が向上する。\n",
        "\n",
        "\n",
        "## 1.2 実際の処理\n",
        "\n",
        "MLM が何をしているかは理解できたと思うが，実際にはどのように機能するのだろうか?\n",
        "コード上で必要となる論理的なステップは何だろうか？\n",
        "以下のように考えることができよう:\n",
        "\n",
        "1. テキストをトークン化する\n",
        "通常の変換機と同じように，まずテキストをトークン化する。\n",
        "BERT の標準的な手続きでは，トークン化により，3 つの異なるテンソルが得られる。\n",
        "\n",
        "* input_ids\n",
        "* token_type_ids\n",
        "* attention_mask\n",
        "\n",
        "MLM には `token_type_ids` は必要ない。\n",
        "本例では `attention_mask` はそれほど重要ではない。\n",
        "\n",
        "この問題設定では `input_ids` テンソルが重要である。\n",
        "`input_ids` はトークン化済みの文表現であり，これを修正していくこととする。\n",
        "\n",
        "2. `labels tensor` を作成\n",
        "\n",
        "ここではモデルを訓練しているので，損失を計算して最適化するためのラベルテンソルが必要である。\n",
        "実際には `labels tensor` は単純に `input_ids` なので，これをコピーするだけである。\n",
        "\n",
        "3. `input_ids` のトークンをマスクする\n",
        "\n",
        "label 用の `input_ids` のコピーを作成した後，トークン系列内のランダムな位置のトークンを選択し，マスクする。\n",
        "\n",
        "\n",
        "BERT 論文では，モデルの事前訓練中に，いくつかの追加ルールを用いて，各トークンを 15％ の確率でマスク化している。\n",
        "ここではこれを簡略化して，各単語を 15％ の確率でマスク化することとする。\n",
        "\n",
        "\n",
        "4. 損失を計算する\n",
        "\n",
        "`input_ids` と `labels` のテンソルを BERT モデルで処理し，両者の間の損失を計算する。\n",
        "この損失値を用いて，BERT による必要な勾配変化を計算し，モデルの重みを最適化する。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*0KvOrY6rY055m9oq36HRkg.png\" width=\"600px\"><br/>\n",
        "<div style=\"text-align:left; width:66%; background-color:cornsilk\">\n",
        "\n",
        "512 個のトークンはすべて，モデルの語彙サイズに等しいベクトル長を持つ。\n",
        "このベクトルを BERT に与えることにより，最終的な出力埋め込みベクトルであるロジット(確率比) が生成される。\n",
        "予測されたトークン ID は，ソフトマックスと argmax 変換を用いて，このロジットから抽出される。\n",
        "</div>    \n",
        "</center>    \n",
        "\n",
        "損失は，各「トークン」の出力確率分布と，真のワンホット符号化ラベルとの差として計算される。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "677aa827-b7ef-4824-ab70-7d71665330d1",
      "metadata": {
        "id": "677aa827-b7ef-4824-ab70-7d71665330d1"
      },
      "source": [
        "# 2. マスク化言語モデル MLM の実装\n",
        "\n",
        "HuggingFace のトランスフォーマーと PyTorch を用いて実装を検討する。\n",
        "標的言語が英語の場合には `bert-base-uncased` モデルを使用する。\n",
        "日本語の場合には，複数の訓練済モデルが提案されている。\n",
        "ここでは，東北大学版の訓練済モデル `cl-tohoku/bert-base-japanese` を用いた実装を試みる。\n",
        "\n",
        "まず全てをインポートして初期化する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d40c979f-9cfd-4905-ae03-45c9036cb262",
      "metadata": {
        "id": "d40c979f-9cfd-4905-ae03-45c9036cb262"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers    \n",
        "from transformers import BertJapaneseTokenizer\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "# BERT 訓練済モデルをダウロード\n",
        "model_name_ja = 'cl-tohoku/bert-base-japanese'  # 東北大学乾研による 日本語 BERT 実装\n",
        "# see https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2\n",
        "# model_name_ja = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # 東北大学乾研による 日本語 BERT 実装\n",
        "\n",
        "tknz = BertJapaneseTokenizer.from_pretrained(model_name_ja)\n",
        "bert_model = BertForMaskedLM.from_pretrained(model_name_ja, return_dict = True)\n",
        "\n",
        "# リソースの選択（CPU/GPU）\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4f8d7b0-d0e5-4b86-ac6f-aee979d7c70f",
      "metadata": {
        "id": "d4f8d7b0-d0e5-4b86-ac6f-aee979d7c70f"
      },
      "source": [
        "最大トークン長 `max_length` を設定する必要があるため，SNOW コーパスの最大長を求めておく。\n",
        "BPE (Byte-per-encoding) では，任意の文章のトークン数が，その文章の文字数以上になることはあり得ないことから，\n",
        "SNOW コーパスに現れる全文から，最大文字数の文章を探して，その文章の文字数を最大値として利用することとしてみる。\n",
        "\n",
        "BPE の詳細については，ここでは触れない。\n",
        "簡単に説明すると，どのような言語でも，文字数 <= 単語数 という関係が成り立つ。\n",
        "そのため，入力ベクトル表現の次元数は，文字ベースではれば小さくなり，単語ベースであれば，総語彙分の次元を用意しなければならない。\n",
        "一方，単語ベースでは任意の単語の持つ意味表現を，表現できてはいない。\n",
        "そこで，単語ベースの次元数の少なさと，単語ベースの意味表現の長所の折衷案を考える。\n",
        "まず単語ベースの表現を考えて全文を走査する。\n",
        "そして，頻出する文字の並びを，新たなトークンとみなして，新たなトークンを作成する。\n",
        "望むトークン数の上限に達するまで，この操作を繰り返す。\n",
        "これにより，単語ベースの入力ベクトル次元数の小ささと，\n",
        "頻出語彙は登録されているため，意味情報を反映できるという，両者の長所を捉えた表現が可能である。\n",
        "これが BPE の本質である。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e1a3b017-2dfa-433a-9a33-128a75b82212",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1a3b017-2dfa-433a-9a33-128a75b82212",
        "outputId": "24f93ae0-e681-4263-cc96-53957588bd0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "やさしい日本語における一文の最長文字数:99\n",
            "torch.Size([84300, 100])\n"
          ]
        }
      ],
      "source": [
        "max_length = 0\n",
        "for line in snow:\n",
        "    max_length = len(line) if len(line) > max_length else max_length\n",
        "print(f'やさしい日本語における一文の最長文字数:{max_length}')\n",
        "inputs = tknz(snow, return_tensors='pt', max_length=max_length+1, truncation=True, padding='max_length')\n",
        "# トークンの分割数が文字数以上になることはないので `max_length=max_length+1`とした\n",
        "print(inputs.input_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe96e8d-8674-41c2-9ffc-70a82ee7fafa",
      "metadata": {
        "id": "6fe96e8d-8674-41c2-9ffc-70a82ee7fafa"
      },
      "outputs": [],
      "source": [
        "#type(tknz.vocab)  # collections.OrderedDict\n",
        "#len(tknz.vocab)    # 3200\n",
        "\n",
        "# BERT のトークン化器に登録されている語彙のうち，最後の 30 トークンを表示させてみる\n",
        "print(list(tknz.vocab.keys())[-30:])\n",
        "\n",
        "# やさしい日本語コーパスの最初の文のトークン化を表示させてみる\n",
        "print(tknz.tokenize(snow[0]))\n",
        "\n",
        "# 直上の，やさしい日本語コーパスの最初の文に対応するトークン ID を表示させてみる\n",
        "print(tknz(snow[0])['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ca177ef-e158-4cb2-a244-7905a619cce4",
      "metadata": {
        "id": "7ca177ef-e158-4cb2-a244-7905a619cce4"
      },
      "source": [
        "## 2.1 文章のトークン化\n",
        "\n",
        "試みに，最初の 3 文だけトークン化器にとおして，戻り値を観察して理解を深めておきたい。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d685d8f9-7c84-4b1d-9c64-9d7a1b720251",
      "metadata": {
        "id": "d685d8f9-7c84-4b1d-9c64-9d7a1b720251"
      },
      "outputs": [],
      "source": [
        "print(snow[:3])  # やさしい日本語コーパスの最初の 3 文\n",
        "\n",
        "# やさしい日本語コーパスの最初の 3 文をトークナイズして `inputs` に代入\n",
        "inputs = tknz(snow[:3], return_tensors='pt', padding=True)\n",
        "\n",
        "# 代入した結果が `dict` として返ってくるので，その `dict` の key を表示\n",
        "print(inputs.keys())\n",
        "\n",
        "# 返ってきた `dict` の内容を表示\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dcd7602-cda9-4dfb-aafc-2e729821acf1",
      "metadata": {
        "id": "9dcd7602-cda9-4dfb-aafc-2e729821acf1"
      },
      "source": [
        "## ラベル作成\n",
        "\n",
        "`input_ids` テンソルを新しい labels テンソルに複製 `clone()` して，結果を `inputs` 変数に格納する。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91588018-5aa5-49b3-8e96-955cced3bbd2",
      "metadata": {
        "id": "91588018-5aa5-49b3-8e96-955cced3bbd2"
      },
      "outputs": [],
      "source": [
        "inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f370fc20-6d86-40d4-ab80-824f80de077c",
      "metadata": {
        "id": "f370fc20-6d86-40d4-ab80-824f80de077c"
      },
      "source": [
        "## 2.2 マスクの作成\n",
        "\n",
        "BERT の原著論文では，マスク化率が 15% のときに，BERT の性能が高くなるとの記述がある。\n",
        "そこで，この 15% の割合で，ランダムなマスクを作成する。\n",
        "\n",
        "トークンを 15% の確率でマスク化するためには `torch.rand` を用いて乱数を発生させ，その発生した値と $<0.15$ を比較することで，マスク化すべきトークン位置を決定することとする。\n",
        "このときの，マスクを示す配列を `mask_arr` とする。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a6be4a8-e2e6-41a7-bcdc-a0615034baf5",
      "metadata": {
        "id": "1a6be4a8-e2e6-41a7-bcdc-a0615034baf5"
      },
      "outputs": [],
      "source": [
        "# input_idsと同じ次元の浮動小数点数のランダムな配列を作成\n",
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "\n",
        "# 乱数配列が 0.15 より小さい場合は `true` を設定\n",
        "mask_arr = rand < 0.15\n",
        "\n",
        "# 結果としてできあがったマスク化配列を印字\n",
        "print(mask_arr) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cb3290f-f18d-49c7-a13c-635ee3addda5",
      "metadata": {
        "id": "6cb3290f-f18d-49c7-a13c-635ee3addda5"
      },
      "source": [
        "- [MASK] トークンを配置する場所を選ぶために mask_arr を用いる。\n",
        "- 駄菓子菓子，[CLS] トークンや [SEP] トークンなどの特殊トークン (それぞれ `tknz.cls_token_id` と `tknz.sep_token_id` で取得可能) の上に MASK トークンを配置したくはない。\n",
        "\n",
        "そこで，さらに条件を追加する必要がある。\n",
        "トークン ID `tknz.cls_token_id` または `tknz.sep_token_id` を含む位置をチェックしてみることとする。\n",
        "\n",
        "その前に，どの特殊トークンが，どのトークン ID を持つのかを調べてみよう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff179178-beed-47f0-b91e-5f4a7e3efdd2",
      "metadata": {
        "id": "ff179178-beed-47f0-b91e-5f4a7e3efdd2"
      },
      "outputs": [],
      "source": [
        "print(tknz.vocab['[MASK]'], '=', tknz.mask_token_id)\n",
        "print(tknz.vocab['[CLS]'], '=', tknz.cls_token_id)\n",
        "print(tknz.vocab['[SEP]'], '=', tknz.sep_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d09f0b0-7a48-4776-8bd5-4e54da94ab4c",
      "metadata": {
        "id": "6d09f0b0-7a48-4776-8bd5-4e54da94ab4c"
      },
      "source": [
        "上記のように，マスクトークンは `.mask_token_id`, 文トークンは `.cls_token_id`, 2 文の分離トークンは `sep_token_id` で参照される。\n",
        "これら特殊トークンの語彙辞書内のトークン番号は， `.vocab[トークン]` で参照可能である。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30967253-51f4-448f-bf7b-13aecf0462c9",
      "metadata": {
        "id": "30967253-51f4-448f-bf7b-13aecf0462c9"
      },
      "outputs": [],
      "source": [
        "# 特殊トークンを表示\n",
        "print(tknz.special_tokens_map)\n",
        "\n",
        "# 各特殊トークンが，どのようなトークン番号を持つかを表示\n",
        "print([(value,tknz.vocab[value]) for token, value in tknz.special_tokens_map.items()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9fe24f8-046c-4664-bcd2-81b74c828049",
      "metadata": {
        "id": "e9fe24f8-046c-4664-bcd2-81b74c828049"
      },
      "outputs": [],
      "source": [
        "# 入力文 `input_ids` 内のトークンが [CLS] ではなく，かつ，分離トークン [SEP] でもない 場合には `True` となる\n",
        "print((inputs.input_ids != tknz.cls_token_id) * (inputs.input_ids != tknz.sep_token_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "698e0ab6-d47a-4fc3-a147-4cfb5930f3b1",
      "metadata": {
        "id": "698e0ab6-d47a-4fc3-a147-4cfb5930f3b1"
      },
      "outputs": [],
      "source": [
        "# 入力文 `input_ids` 内のトークンが [CLS] ではなく，かつ，分離トークン [SEP] でもなく\n",
        "# かつ，15% のマスク配列が `True` である場合には `True` となる\n",
        "mask_arr = (rand < 0.15) * (inputs.input_ids != tknz.cls_token_id) * (inputs.input_ids != tknz.sep_token_id)\n",
        "print(mask_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6206fb1c-6b0c-4468-985f-67e22a7d998b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6206fb1c-6b0c-4468-985f-67e22a7d998b",
        "outputId": "79f42f0b-3fe9-4a5d-c539-ea5b626c857d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 1, 4, 1, 6, 1, 8, 1, 9]\n"
          ]
        }
      ],
      "source": [
        "# 直上セルで作成した mask_arr から selection を計算して作成\n",
        "selection = torch.flatten((mask_arr).nonzero()).tolist()\n",
        "print(selection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb059596-8700-4828-a4d4-992fd414f71b",
      "metadata": {
        "id": "bb059596-8700-4828-a4d4-992fd414f71b"
      },
      "outputs": [],
      "source": [
        "# selection で表現された文中の位置情報を `inputs.input_ids` へ適用し MASK トークンに置き換える\n",
        "inputs.input_ids[0, selection] = tknz.vocab[tknz.mask_token]\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205ce6d9-aa35-4ed3-960e-d15cc76d7072",
      "metadata": {
        "id": "205ce6d9-aa35-4ed3-960e-d15cc76d7072"
      },
      "source": [
        "上の結果から `input_ids` テンソル内に MASK トークン `4` が存在することを確認せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "820861da-f8a9-4c81-a39e-662bc2f1e101",
      "metadata": {
        "id": "820861da-f8a9-4c81-a39e-662bc2f1e101"
      },
      "source": [
        "## 2.3 損失の計算\n",
        "\n",
        "最後のステップは，一般的なモデルの訓練処理と変わりない。\n",
        "`input_ids` テンソルと `labels` テンソルが `input` に入っているので，これをモデルに渡してモデルの損失を返すことができる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ece89f0e-36cc-4b58-a825-5c7d5cef1c63",
      "metadata": {
        "id": "ece89f0e-36cc-4b58-a825-5c7d5cef1c63"
      },
      "outputs": [],
      "source": [
        "outputs = bert_model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "0cb7e526-8854-49cc-9d51-99dd7a451fda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cb7e526-8854-49cc-9d51-99dd7a451fda",
        "outputId": "08cb069d-3111-4466-e932-c8e44b16b0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['loss', 'logits'])\n"
          ]
        }
      ],
      "source": [
        "# 戻り値 `outputs` は `dict` であり，この `output` 辞書のキーには `loss` (損失値) と `logits` (確率) とがある。\n",
        "print(outputs.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b776b1e-19a4-4180-9217-e2341258fed7",
      "metadata": {
        "id": "4b776b1e-19a4-4180-9217-e2341258fed7"
      },
      "outputs": [],
      "source": [
        "# 損失値を印字\n",
        "print(outputs.loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9912fb70-d3f1-4bca-97cd-e249294f0898",
      "metadata": {
        "id": "9912fb70-d3f1-4bca-97cd-e249294f0898"
      },
      "source": [
        "# 3. 学習\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572e5702-67f5-4351-89ed-8fbb219e2066",
      "metadata": {
        "id": "572e5702-67f5-4351-89ed-8fbb219e2066"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# やさしい日本語コーパス SNOW 全文をトークン化器に与え，結果を `inputs` に代入する\n",
        "inputs = tknz(snow, return_tensors='pt', max_length=100, truncation=True, padding='max_length')\n",
        "\n",
        "# 結果を表示\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a622617c-59a6-4818-9d17-5275653a3ba4",
      "metadata": {
        "id": "a622617c-59a6-4818-9d17-5275653a3ba4"
      },
      "outputs": [],
      "source": [
        "print(f\"inputs['input_ids'].size() 戻り値 inputs の大きさ:{inputs['input_ids'].size()}\")\n",
        "print(f'type(inputs.input_ids.detach()) 戻り値 inputs の型:{type(inputs.input_ids.detach())}')\n",
        "print(f'type(inputs.input_ids.detach().numpy()) 戻り値を numpy 行列に変換した際の型:{type(inputs.input_ids.detach().numpy())}')\n",
        "print(f'inputs.input_ids.detach().numpy().shape 戻り値を numpy 行列に変換した際の行列サイズ:{inputs.input_ids.detach().numpy().shape}')\n",
        "print(f'inputs.input_ids.detach().numpy()[:2] numpy 変換した際の最初の 2 文:{inputs.input_ids.detach().numpy()[:2]}')\n",
        "print(f'type(snow) snow の型:{type(snow)}, len(snow) snow のデータ長:{len(snow)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77b3889e-8636-49cf-9edb-ad9347d517f4",
      "metadata": {
        "id": "77b3889e-8636-49cf-9edb-ad9347d517f4"
      },
      "outputs": [],
      "source": [
        "# 今一度，各トークン番号が，どのようなトークンを表しているのかを\n",
        "# トークナイザの `convert_ids_to_tokens()` 関数を使ってトークンに再変換して印字\n",
        "for line in inputs.input_ids[:2].detach().numpy():\n",
        "    print(tknz.convert_ids_to_tokens(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ea49d76-70b5-4fa4-aa22-bd0c52d8c5ff",
      "metadata": {
        "id": "1ea49d76-70b5-4fa4-aa22-bd0c52d8c5ff"
      },
      "source": [
        "次に `input_id` をクローンして，ラベルテンソルを作成します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "43f498c4-e7a9-4c0a-955e-842cc8e6f411",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43f498c4-e7a9-4c0a-955e-842cc8e6f411",
        "outputId": "7580a92e-f758-422e-94a5-b8f267c82b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "print(inputs.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8587cd1c-a89a-4e2f-ac51-5fc79c824e51",
      "metadata": {
        "id": "8587cd1c-a89a-4e2f-ac51-5fc79c824e51"
      },
      "source": [
        "次に，マスクのコードですが，マスクに PAD トークンを含めてはいけない (CLS や SEP では以前のとおりにあつかう)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ec5cc58b-6135-441d-a0b4-db0dc2aac745",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec5cc58b-6135-441d-a0b4-db0dc2aac745",
        "outputId": "b899b59c-10b4-43b3-ea3f-3dba751ee22a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tknz.pad_token_id:0\n",
            "tknz.mask_token_id:4\n",
            "tknz.sep_token_id:3\n"
          ]
        }
      ],
      "source": [
        "print(f'tknz.pad_token_id:{tknz.pad_token_id}')\n",
        "print(f'tknz.mask_token_id:{tknz.mask_token_id}')\n",
        "print(f'tknz.sep_token_id:{tknz.sep_token_id}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef68b05a-19b0-443d-987e-7a7b3ff16f99",
      "metadata": {
        "id": "ef68b05a-19b0-443d-987e-7a7b3ff16f99"
      },
      "outputs": [],
      "source": [
        "# input_ids テンソルと同じ次元の乱数配列を作成 \n",
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "\n",
        "# mask 配列の作成 \n",
        "mask_arr = (rand < 0.15) * (inputs.input_ids != tknz.cls_token_id) * \\\n",
        "           (inputs.input_ids != tknz.sep_token_id) * (inputs.input_ids != tknz.pad_token_id)\n",
        "print(mask_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "db28a1e3-2144-41eb-9416-5e0e483b99f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db28a1e3-2144-41eb-9416-5e0e483b99f2",
        "outputId": "4fd92afe-fce0-4c4d-ef9e-b2a0fa08c52a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7], [2], [3, 4], [1], [7]]\n"
          ]
        }
      ],
      "source": [
        "selection = []\n",
        "\n",
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    selection.append(\n",
        "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "    )\n",
        "print(selection[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d507f86d-53a8-4cfe-b004-07dd45114374",
      "metadata": {
        "id": "d507f86d-53a8-4cfe-b004-07dd45114374"
      },
      "source": [
        "次に，これらのインデックスを `input_ids` の各行に適用し，これらのインデックスの値をそれぞれ `tknz.mask_token_id` として割り当てる。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f36c7a-5a19-443d-add2-5b228438f2e3",
      "metadata": {
        "id": "d2f36c7a-5a19-443d-add2-5b228438f2e3"
      },
      "outputs": [],
      "source": [
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    inputs.input_ids[i, selection[i]] = tknz.mask_token_id\n",
        "\n",
        "print(tknz.mask_token_id)\n",
        "print(inputs.input_ids[:3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a9dc3ca-6d4f-47c0-b880-036e6895b5d8",
      "metadata": {
        "id": "6a9dc3ca-6d4f-47c0-b880-036e6895b5d8"
      },
      "source": [
        "`mask_arr` テンソルの `True` 値と同じ位置に `tknz.mask_token_id` が割り当てられている\n",
        "\n",
        "これで入力テンソルの準備が整い，学習時にモデルに入力するための設定を始めることができる。\n",
        "\n",
        "学習時には PyTorch の `DataLoader` を使ってデータを読み込みます。\n",
        "これを使うには，データを PyTorch の `Dataset` オブジェクトにフォーマットする必要がある\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "6c85628d-40a2-4299-8233-0ebab62a9a43",
      "metadata": {
        "id": "6c85628d-40a2-4299-8233-0ebab62a9a43"
      },
      "outputs": [],
      "source": [
        "class snowDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "    \n",
        "dataset = snowDataset(inputs) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9af04dd4-fc0f-49d5-b3c6-aeae8af2f761",
      "metadata": {
        "id": "9af04dd4-fc0f-49d5-b3c6-aeae8af2f761"
      },
      "source": [
        "`dataloader` を初期化する。\n",
        "`dataloader` は，訓練時にモデルにデータを読み込むために使用。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "ef55b9f9-6ec9-4c57-9cdd-c76e15763128",
      "metadata": {
        "id": "ef55b9f9-6ec9-4c57-9cdd-c76e15763128"
      },
      "outputs": [],
      "source": [
        "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8971d04-ef07-4fe0-b362-a1182fe83672",
      "metadata": {
        "id": "e8971d04-ef07-4fe0-b362-a1182fe83672"
      },
      "source": [
        "これで，反復訓練に入る準備が整った。\n",
        "反復学習を始める前に，以下の 3 つを設定する必要がある:\n",
        "\n",
        "1. モデルを GPU/CPU (GPU が利用可能あれば) に移動\n",
        "2. モデルの訓練モードを有効にする\n",
        "3. 重み付けされた重み崩壊付き最適化 `AdamW` を初期化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "4cdd99cc-2f4b-43de-87f8-9a5f21a7c690",
      "metadata": {
        "id": "4cdd99cc-2f4b-43de-87f8-9a5f21a7c690"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "bert_model.to(device) # モデルを選択したデバイスに移動\n",
        "bert_model.train();   # 訓練モードに設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "85265c5b-3f38-4fef-8ac8-473c4e8b60f3",
      "metadata": {
        "id": "85265c5b-3f38-4fef-8ac8-473c4e8b60f3"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "optim = AdamW(bert_model.parameters(), lr=5e-5)  # 最適化関数を初期化"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d2a12c2-7a9d-45da-bc1a-2ff26624827e",
      "metadata": {
        "id": "5d2a12c2-7a9d-45da-bc1a-2ff26624827e"
      },
      "source": [
        "これでようやくセットアップが完了し，訓練を開始することができる。\n",
        "ここでは PyTorch の典型的な訓練ループを導入してみる。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10b71e4c-5801-45f9-a464-4e012ce1c008",
      "metadata": {
        "id": "10b71e4c-5801-45f9-a464-4e012ce1c008"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from tqdm.notebook import tqdm\n",
        "#from tqdm import tqdm  # for our progress bar\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch in loop:\n",
        "        \n",
        "        optim.zero_grad()\n",
        "        \n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        loop.set_description(f'エポック {epoch}')\n",
        "        loop.set_postfix(OrderedDict(loss=loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec1d00f-cf7e-4135-bc44-b9475443e57c",
      "metadata": {
        "id": "dec1d00f-cf7e-4135-bc44-b9475443e57c"
      },
      "outputs": [],
      "source": [
        "class snowDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "dataset = snowDataset(inputs)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "for data in dataloader:\n",
        "    print(data.keys(), type(data))\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "2022_0623BERT_SNOW_training.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}