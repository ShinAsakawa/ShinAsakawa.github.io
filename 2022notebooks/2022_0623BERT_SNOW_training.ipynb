{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0623BERT_SNOW_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 「やさしい日本語」コーパスを使って BERT を訓練する実習\n",
        "\n",
        "- 浅川伸一\n",
        "- filename: 2022_0623BERT_SNOW_trainign.ipynb"
      ],
      "metadata": {
        "id": "ngigDPlnX6nj"
      },
      "id": "ngigDPlnX6nj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "685cf5c1-71dd-47ec-b65b-e8a2d66fa96f",
      "metadata": {
        "id": "685cf5c1-71dd-47ec-b65b-e8a2d66fa96f"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "\n",
        "from termcolor import colored\n",
        "import platform\n",
        "HOSTNAME = platform.node().split('.')[0]\n",
        "\n",
        "import os\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "try:\n",
        "    import ipynbname\n",
        "except ImportError:\n",
        "    !pip install ipynbname > /dev/null\n",
        "import ipynbname\n",
        "FILEPATH = str(ipynbname.path()).replace(HOME+'/','')\n",
        "\n",
        "import pwd\n",
        "USER=pwd.getpwuid(os.geteuid())[0]\n",
        "\n",
        "from datetime import date\n",
        "TODAY=date.today()\n",
        "\n",
        "import torch\n",
        "TORCH_VERSION = torch.__version__\n",
        "\n",
        "color = 'green'\n",
        "print('日付:',colored(f'{TODAY}', color=color, attrs=['bold']))\n",
        "print('HOSTNAME:',colored(f'{HOSTNAME}', color=color, attrs=['bold']))\n",
        "print('ユーザ名:',colored(f'{USER}', color=color, attrs=['bold']))\n",
        "print('HOME:',colored(f'{HOME}', color=color,attrs=['bold']))\n",
        "print('ファイル名:',colored(f'{FILEPATH}', color=color, attrs=['bold']))\n",
        "print('torch.__version__:',colored(f'{TORCH_VERSION}', color=color, attrs=['bold']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891af644-47fc-4966-b3d1-d1bdc03eedaf",
      "metadata": {
        "id": "891af644-47fc-4966-b3d1-d1bdc03eedaf"
      },
      "outputs": [],
      "source": [
        "if isColab:\n",
        "    !pip install jaconv\n",
        "    !pip install transformers fugashi ipadic\n",
        "    \n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "from termcolor import colored\n",
        "import jaconv\n",
        "\n",
        "# やさしい日本語をダウンロード\n",
        "SNOWs={'T15': {'url':\"https://filedn.com/lit4DCIlHwxfS1gj9zcYuDJ/SNOW/T15-2020.1.7.xlsx\"},\n",
        "       'T23': {'url':\"https://filedn.com/lit4DCIlHwxfS1gj9zcYuDJ/SNOW/T23-2020.1.7.xlsx\"},\n",
        "      }\n",
        "print('エクセルファイル読込', end='...')\n",
        "for corpus in SNOWs:\n",
        "    url = SNOWs[corpus]['url']\n",
        "    excel_fname = corpus + '-2020.1.7.xlsx'\n",
        "    if not os.path.exists(excel_fname):  # ファイルが存在しない場合，ダウンロードする\n",
        "        print(f'url:{url}')\n",
        "        r = requests.get(url)\n",
        "        with open(excel_fname, 'wb') as f:\n",
        "            total_length = int(r.headers.get('content-length'))\n",
        "            print(f'{excel_fname} をダウンロード中 {total_length} バイト')\n",
        "            f.write(r.content)\n",
        "\n",
        "    SNOWs[corpus]['df'] = pd.read_excel(excel_fname)\n",
        "    SNOWs[corpus]['df'] = SNOWs[corpus]['df'].rename(columns={'#日本語(原文)': 'ja', \n",
        "                                                              '#やさしい日本語':'easy_ja',\n",
        "                                                              '#英語(原文)':'en'})\n",
        "# 2 つのデータをあわせる    \n",
        "_snow = SNOWs['T15']['df']['easy_ja'].tolist() + SNOWs['T23']['df']['easy_ja'].tolist()\n",
        "snow = [jaconv.normalize(line, 'NFKC') for line in _snow] # 正規化"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ade4ea5-cada-4bda-85f0-6829ee9b2dd2",
      "metadata": {
        "id": "9ade4ea5-cada-4bda-85f0-6829ee9b2dd2"
      },
      "source": [
        "BERT は マスク化言語モデル (**MLM**) と次文予測 (**NSP**) という 2 つの独自の学習アプローチがある。\n",
        "\n",
        "モデルを微調整 fine-tuning する必要がある\n",
        "\n",
        "# 1. マスク化言語モデル MLM\n",
        "\n",
        "MLM は BERT に文を与え，BERT 内部の重みを最適化して，相手側に同じ文を出力することで構成されています。\n",
        "すなわち，文を入力し，BERT が同じ文を出力するように要求します。\n",
        "\n",
        "しかし，実際に BERT にその入力文を与える前に，いくつかのトークンを必要とします。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*phTLnQ8itb3ZX5_h9BWjWw.png\" width=\"600px\"><br/>\n",
        "この画像では，トークン を BERT に渡す前に，リンカーン・トークンを [MASK] に置き換えてマスクしています。\n",
        "</center>\n",
        "\n",
        "つまり，実際には不完全な文章を入力して，BERT に完成させるように依頼しているのです。\n",
        "\n",
        "## 1.1 間隙を埋める\n",
        "\n",
        "これはどのような効果があるのでしょうか？\n",
        "それは、多くの人が学校で与えられた問題のようなものです。つまり、文章が与えられたら、その穴を埋めなければなりません。\n",
        "\n",
        "`秋 に は ， ___ が 木 から 落ちる 。`\n",
        "\n",
        "答えがわかりますか？\n",
        "ほとんど、あなたは分かっているでしょう。\n",
        "それは，文脈を考慮したからです。\n",
        "\n",
        "\n",
        "「落ちる」 と 「木」 という単語が出てきましたが，足りない単語は木から落ちるものだということがわかります。\n",
        "\n",
        "どんぐり，枝，葉など，木から落ちるものはたくさんありますが，秋という別の条件があるので，秋に木から落ちる可能性が最も高いのは葉だということで，検索対象が絞られます。\n",
        "\n",
        "人間として，私たちは一般的な世界の知識と言語的な理解を組み合わせて，その結論を導き出します。\n",
        "BERT の場合，この推測は，たくさんの本を読み，言語パターンを非常によく学んでいることから得られます。\n",
        "\n",
        "BERT は，秋，木，葉が何であるかを知らないかもしれませんが，言語パターンとこれらの単語の文脈から，答えが葉である可能性が最も高いことを知っています。\n",
        "\n",
        "この処理の結果，BERT にとっては，使用されている言語のスタイルの理解度が向上します。\n",
        "\n",
        "\n",
        "## 1.2 実際の処理\n",
        "\n",
        "MLM が何をしているかは理解できましたが，実際にはどのように機能するのでしょうか？\n",
        "コード上で必要となる論理的なステップは何でしょうか？\n",
        "\n",
        "1.  テキストをトークン化します。\n",
        "通常の変換機と同じように，まずテキストのトークン化を行います。\n",
        "トークン化からは 3 つの異なるテンソルが得られます。\n",
        "\n",
        "* input_ids\n",
        "* token_type_ids\n",
        "* attention_mask\n",
        "\n",
        "MLM には token_type_ids は必要ないし，この例では attention_mask はそれほど重要ではありません。\n",
        "\n",
        "我々にとっては input_ids テンソルが最も重要です。\n",
        "ここには，トークン化されたテキスト表現があり，これを今後修正していくことになるでしょう。\n",
        "\n",
        "2. `labels tensor` を作成する。\n",
        "ここではモデルを訓練しているので，損失を計算して最適化するためのラベルテンソルが必要です。\n",
        "`labels tensor` は単純に `input_ids` なので，これをコピーするだけです。\n",
        "\n",
        "\n",
        "3. `input_ids` のトークンをマスクする。\n",
        "label 用の `input_ids` のコピーを作成したので，先にトークンのランダムな選択をマスクすることができます。\n",
        "\n",
        "\n",
        "BERT 論文では，モデルの事前訓練中に，いくつかの追加ルールを用いて，各トークンを 15％ の確率でマスク化していますが，ここではこれを簡略化して，各単語を 15％ の確率でマスク化することにします。\n",
        "\n",
        "\n",
        "4. 損失を計算します。\n",
        "`input_ids` と `labels` のテンソルを BERT モデルで処理し，両者の間の損失を計算します。\n",
        "この損失を用いて，BERT による必要な勾配変化を計算し，モデルの重みを最適化します。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*0KvOrY6rY055m9oq36HRkg.png\" width=\"600px\"><br/>\n",
        "<div style=\"text-align:left; width:66%; background-color:cornsilk\">\n",
        "\n",
        "512 個のトークンはすべて，モデルの語彙サイズに等しいベクトル長を持ちます。\n",
        "最終的な出力埋め込みベクトルであるロジット(確率比) を生成します。\n",
        "予測されたトークン ID は，lソフトマックスと argmax 変換を用いて，このロジットから抽出されます。\n",
        "</div>    \n",
        "</center>    \n",
        "\n",
        "損失は，各出力「トークン」の出力確率分布と，真のワンホット符号化ラベルとの差として計算されます。\n",
        "\n",
        "\n",
        "# 2. マスク化言語モデル MLM のコード\n",
        "\n",
        "MLM をコードで実証するにはどうすればよいでしょうか？\n",
        "\n",
        "HuggingFace のトランスフォーマーと PyTorch そして bert-base-uncased モデルを使用します。\n",
        "では，まず全てをインポートして初期化しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d40c979f-9cfd-4905-ae03-45c9036cb262",
      "metadata": {
        "id": "d40c979f-9cfd-4905-ae03-45c9036cb262"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import BertJapaneseTokenizer\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "# BERT 訓練済モデルをダウロード\n",
        "# model_name_ja = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
        "model_name_ja = 'cl-tohoku/bert-base-japanese'  # 東北大学乾研による 日本語 BERT 実装\n",
        "# see https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2\n",
        "# model_name_ja = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # 東北大学乾研による 日本語 BERT 実装\n",
        "\n",
        "tknz = BertJapaneseTokenizer.from_pretrained(model_name_ja)\n",
        "bert_model = BertForMaskedLM.from_pretrained(model_name_ja, return_dict = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a3b017-2dfa-433a-9a33-128a75b82212",
      "metadata": {
        "id": "e1a3b017-2dfa-433a-9a33-128a75b82212"
      },
      "outputs": [],
      "source": [
        "max_length = 0\n",
        "for line in snow:\n",
        "    max_length = len(line) if len(line) > max_length else max_length\n",
        "print(f'やさしい日本語における一文の最長文字数:{max_length}')\n",
        "inputs = tknz(snow, return_tensors='pt', max_length=max_length+1, truncation=True, padding='max_length')\n",
        "# トークンの分割数が文字数以上になることはないので `max_length=max_length+1`とした\n",
        "print(inputs.input_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe96e8d-8674-41c2-9ffc-70a82ee7fafa",
      "metadata": {
        "id": "6fe96e8d-8674-41c2-9ffc-70a82ee7fafa"
      },
      "outputs": [],
      "source": [
        "#type(tknz.vocab)  # collections.OrderedDict\n",
        "#len(tknz.vocab)    # 3200\n",
        "print(list(tknz.vocab.keys())[-30:])\n",
        "print(tknz.tokenize(snow[0]))\n",
        "print(tknz(snow[0])['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ca177ef-e158-4cb2-a244-7905a619cce4",
      "metadata": {
        "id": "7ca177ef-e158-4cb2-a244-7905a619cce4"
      },
      "source": [
        "## 文章のトークン化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d685d8f9-7c84-4b1d-9c64-9d7a1b720251",
      "metadata": {
        "id": "d685d8f9-7c84-4b1d-9c64-9d7a1b720251"
      },
      "outputs": [],
      "source": [
        "print(snow[:3])\n",
        "inputs = tknz(snow[:3], return_tensors='pt', padding=True)\n",
        "print(inputs.keys())\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dcd7602-cda9-4dfb-aafc-2e729821acf1",
      "metadata": {
        "id": "9dcd7602-cda9-4dfb-aafc-2e729821acf1"
      },
      "source": [
        "## ラベル作成\n",
        "\n",
        "`input_ids` テンソルを新しい labels テンソルにクローンして，\n",
        "結果を `inputs` 変数に格納します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91588018-5aa5-49b3-8e96-955cced3bbd2",
      "metadata": {
        "id": "91588018-5aa5-49b3-8e96-955cced3bbd2"
      },
      "outputs": [],
      "source": [
        "inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f370fc20-6d86-40d4-ab80-824f80de077c",
      "metadata": {
        "id": "f370fc20-6d86-40d4-ab80-824f80de077c"
      },
      "source": [
        "## マスクの作成\n",
        "\n",
        "BERT の原著論文では，マスク化率が 15% なので，この割合でランダムなマスクを作成します。\n",
        "\n",
        "トークンを 15% の確率でマスク化するためには `torch.rand` と各値 $<0.15$ という条件を用い，\n",
        "マスク配列 `mask_arr` を生成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a6be4a8-e2e6-41a7-bcdc-a0615034baf5",
      "metadata": {
        "id": "1a6be4a8-e2e6-41a7-bcdc-a0615034baf5"
      },
      "outputs": [],
      "source": [
        "# input_idsと同じ次元の浮動小数点数のランダムな配列を作る\n",
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "\n",
        "# 乱数配列が 0.15 より小さい場合は true を設定\n",
        "mask_arr = rand < 0.15\n",
        "print(mask_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cb3290f-f18d-49c7-a13c-635ee3addda5",
      "metadata": {
        "id": "6cb3290f-f18d-49c7-a13c-635ee3addda5"
      },
      "source": [
        "[MASK] トークンを配置する場所を選ぶために mask_arr を使いますが，[CLS] トークンや [SEP] トークンなどの他の特殊トークン (それぞれ `tknz.cls_token_id` と `tknz.sep_token_id` で取得可能) の上に MASK トークンを配置したくはありません。\n",
        "\n",
        "そこで，さらに条件を追加する必要があります。\n",
        "トークン ID `tknz.cls_token_id` または `tknz.sep_token_id` を含む位置のチェック。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff179178-beed-47f0-b91e-5f4a7e3efdd2",
      "metadata": {
        "id": "ff179178-beed-47f0-b91e-5f4a7e3efdd2"
      },
      "outputs": [],
      "source": [
        "print(tknz.vocab['[MASK]'], '=', tknz.mask_token_id)\n",
        "print(tknz.vocab['[CLS]'], '=', tknz.cls_token_id)\n",
        "print(tknz.vocab['[SEP]'], '=', tknz.sep_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30967253-51f4-448f-bf7b-13aecf0462c9",
      "metadata": {
        "id": "30967253-51f4-448f-bf7b-13aecf0462c9"
      },
      "outputs": [],
      "source": [
        "print(tknz.special_tokens_map)\n",
        "print([(value,tknz.vocab[value]) for token, value in tknz.special_tokens_map.items()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9fe24f8-046c-4664-bcd2-81b74c828049",
      "metadata": {
        "id": "e9fe24f8-046c-4664-bcd2-81b74c828049"
      },
      "outputs": [],
      "source": [
        "print((inputs.input_ids != tknz.cls_token_id) * (inputs.input_ids != tknz.sep_token_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "698e0ab6-d47a-4fc3-a147-4cfb5930f3b1",
      "metadata": {
        "id": "698e0ab6-d47a-4fc3-a147-4cfb5930f3b1"
      },
      "outputs": [],
      "source": [
        "mask_arr = (rand < 0.15) * (inputs.input_ids != tknz.cls_token_id) * (inputs.input_ids != tknz.sep_token_id)\n",
        "print(mask_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6206fb1c-6b0c-4468-985f-67e22a7d998b",
      "metadata": {
        "id": "6206fb1c-6b0c-4468-985f-67e22a7d998b"
      },
      "outputs": [],
      "source": [
        "# mask_arr から selection を作成\n",
        "selection = torch.flatten((mask_arr[0]).nonzero()).tolist()\n",
        "print(selection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb059596-8700-4828-a4d4-992fd414f71b",
      "metadata": {
        "id": "bb059596-8700-4828-a4d4-992fd414f71b"
      },
      "outputs": [],
      "source": [
        "# selection インデックスを inputs.input_ids へ適用し MASK トークンを追加 \n",
        "inputs.input_ids[0, selection] = tknz.vocab[tknz.mask_token]\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f681f186-8bb2-4f5a-9d42-1215661033d9",
      "metadata": {
        "id": "f681f186-8bb2-4f5a-9d42-1215661033d9"
      },
      "source": [
        "これで，上の `input_ids` テンソルで  103 で表された MASK トークンを見ることができます。\n",
        "\n",
        "## 損失の計算\n",
        "\n",
        "最後のステップは，一般的なモデルの訓練処理と変わりません。\n",
        "`input_ids` テンソルと `labels` テンソルが `input dictionary` に入っているので，これをモデルに渡してモデルの損失を返すことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ece89f0e-36cc-4b58-a825-5c7d5cef1c63",
      "metadata": {
        "id": "ece89f0e-36cc-4b58-a825-5c7d5cef1c63"
      },
      "outputs": [],
      "source": [
        "outputs = bert_model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cb7e526-8854-49cc-9d51-99dd7a451fda",
      "metadata": {
        "id": "0cb7e526-8854-49cc-9d51-99dd7a451fda"
      },
      "outputs": [],
      "source": [
        "# outputs には loss と logits とがあります。\n",
        "print(outputs.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b776b1e-19a4-4180-9217-e2341258fed7",
      "metadata": {
        "id": "4b776b1e-19a4-4180-9217-e2341258fed7"
      },
      "outputs": [],
      "source": [
        "print(outputs.loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9912fb70-d3f1-4bca-97cd-e249294f0898",
      "metadata": {
        "id": "9912fb70-d3f1-4bca-97cd-e249294f0898"
      },
      "source": [
        "# 学習の実装\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572e5702-67f5-4351-89ed-8fbb219e2066",
      "metadata": {
        "id": "572e5702-67f5-4351-89ed-8fbb219e2066"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "inputs = tknz(snow, return_tensors='pt', max_length=100, truncation=True, padding='max_length')\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a622617c-59a6-4818-9d17-5275653a3ba4",
      "metadata": {
        "id": "a622617c-59a6-4818-9d17-5275653a3ba4"
      },
      "outputs": [],
      "source": [
        "print(inputs['input_ids'].size())\n",
        "print(inputs.input_ids.size())\n",
        "print(type(inputs.input_ids.detach()))\n",
        "print(type(inputs.input_ids.detach().numpy()))\n",
        "print(inputs.input_ids.detach().numpy().shape)\n",
        "print(inputs.input_ids.detach().numpy()[:2])\n",
        "print(type(snow), len(snow))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ea49d76-70b5-4fa4-aa22-bd0c52d8c5ff",
      "metadata": {
        "id": "1ea49d76-70b5-4fa4-aa22-bd0c52d8c5ff"
      },
      "source": [
        "次に `input_id` をクローンして，ラベルテンソルを作成します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f498c4-e7a9-4c0a-955e-842cc8e6f411",
      "metadata": {
        "id": "43f498c4-e7a9-4c0a-955e-842cc8e6f411"
      },
      "outputs": [],
      "source": [
        "inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "print(inputs.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8587cd1c-a89a-4e2f-ac51-5fc79c824e51",
      "metadata": {
        "id": "8587cd1c-a89a-4e2f-ac51-5fc79c824e51"
      },
      "source": [
        "次に，マスクのコードですが，マスクに PAD トークンを含めてはいけない (CLS や SEP では以前のとおりにあつかう)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5cc58b-6135-441d-a0b4-db0dc2aac745",
      "metadata": {
        "id": "ec5cc58b-6135-441d-a0b4-db0dc2aac745"
      },
      "outputs": [],
      "source": [
        "print(tknz.pad_token_id)\n",
        "print(tknz.mask_token_id)\n",
        "print(tknz.sep_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef68b05a-19b0-443d-987e-7a7b3ff16f99",
      "metadata": {
        "id": "ef68b05a-19b0-443d-987e-7a7b3ff16f99"
      },
      "outputs": [],
      "source": [
        "# input_ids テンソルと同じ次元の浮動小数点数のランダムな配列を作成 \n",
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "\n",
        "# mask 配列の作成 \n",
        "mask_arr = (rand < 0.15) * (inputs.input_ids != tknz.cls_token_id) * \\\n",
        "           (inputs.input_ids != tknz.sep_token_id) * (inputs.input_ids != tknz.pad_token_id)\n",
        "print(mask_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db28a1e3-2144-41eb-9416-5e0e483b99f2",
      "metadata": {
        "id": "db28a1e3-2144-41eb-9416-5e0e483b99f2"
      },
      "outputs": [],
      "source": [
        "selection = []\n",
        "\n",
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    selection.append(\n",
        "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "    )\n",
        "print(selection[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d507f86d-53a8-4cfe-b004-07dd45114374",
      "metadata": {
        "id": "d507f86d-53a8-4cfe-b004-07dd45114374"
      },
      "source": [
        "次に，これらのインデックスを `input_ids` の各行に適用し，これらのインデックスの値をそれぞれ `tknz.mask_token_id` として割り当てます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f36c7a-5a19-443d-add2-5b228438f2e3",
      "metadata": {
        "id": "d2f36c7a-5a19-443d-add2-5b228438f2e3"
      },
      "outputs": [],
      "source": [
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    inputs.input_ids[i, selection[i]] = tknz.mask_token_id\n",
        "\n",
        "print(tknz.mask_token_id)\n",
        "print(inputs.input_ids[:3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a9dc3ca-6d4f-47c0-b880-036e6895b5d8",
      "metadata": {
        "id": "6a9dc3ca-6d4f-47c0-b880-036e6895b5d8"
      },
      "source": [
        "`mask_arr` テンソルの `True` 値と同じ位置に `tknz.mask_token_id` が割り当てられている\n",
        "\n",
        "これで入力テンソルの準備が整い，学習時にモデルに入力するための設定を始めることができる。\n",
        "\n",
        "学習時には PyTorch の `DataLoader` を使ってデータを読み込みます。\n",
        "これを使うには，データを PyTorch の `Dataset` オブジェクトにフォーマットする必要がある\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c85628d-40a2-4299-8233-0ebab62a9a43",
      "metadata": {
        "id": "6c85628d-40a2-4299-8233-0ebab62a9a43"
      },
      "outputs": [],
      "source": [
        "class snowDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "    \n",
        "dataset = snowDataset(inputs) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9af04dd4-fc0f-49d5-b3c6-aeae8af2f761",
      "metadata": {
        "id": "9af04dd4-fc0f-49d5-b3c6-aeae8af2f761"
      },
      "source": [
        "`dataloader` を初期化する。\n",
        "`dataloader` は，訓練時にモデルにデータを読み込むために使用。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef55b9f9-6ec9-4c57-9cdd-c76e15763128",
      "metadata": {
        "id": "ef55b9f9-6ec9-4c57-9cdd-c76e15763128"
      },
      "outputs": [],
      "source": [
        "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8971d04-ef07-4fe0-b362-a1182fe83672",
      "metadata": {
        "id": "e8971d04-ef07-4fe0-b362-a1182fe83672"
      },
      "source": [
        "これで，反復訓練に入る準備が整った。\n",
        "反復学習を始める前に，3 つのことを設定する必要がある。\n",
        "\n",
        "1. モデルを GPU/CPU (GPU が利用可能あれば) に移動させる\n",
        "2. モデルの訓練モードを有効にする\n",
        "3. 重み付けされた重み崩壊付き最適化 `AdamW` を初期化する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cdd99cc-2f4b-43de-87f8-9a5f21a7c690",
      "metadata": {
        "id": "4cdd99cc-2f4b-43de-87f8-9a5f21a7c690"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "bert_model.to(device) # モデルを選択したデバイスに移動\n",
        "bert_model.train();   # 訓練モードに設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85265c5b-3f38-4fef-8ac8-473c4e8b60f3",
      "metadata": {
        "id": "85265c5b-3f38-4fef-8ac8-473c4e8b60f3"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optim = AdamW(bert_model.parameters(), lr=5e-5) #最適化関数を初期化"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d2a12c2-7a9d-45da-bc1a-2ff26624827e",
      "metadata": {
        "id": "5d2a12c2-7a9d-45da-bc1a-2ff26624827e"
      },
      "source": [
        "これでようやくセットアップが完了し，訓練を開始することができる。\n",
        "ここでは PyTorch の典型的な訓練ループを導入する。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10b71e4c-5801-45f9-a464-4e012ce1c008",
      "metadata": {
        "id": "10b71e4c-5801-45f9-a464-4e012ce1c008"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm  # for our progress bar\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch in loop:\n",
        "        \n",
        "        optim.zero_grad()\n",
        "        \n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        loop.set_description(f'エポック {epoch}')\n",
        "        #loop.set_postfix(f'損失 {loss.item()}')\n",
        "        loop.set_postfix(loss=loss.item())        "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "2022_0623BERT_SNOW_training.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}