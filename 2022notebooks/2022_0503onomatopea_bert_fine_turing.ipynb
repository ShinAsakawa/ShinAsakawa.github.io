{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0503onomatopea_bert_fine_turing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41f11cc3-a7a0-408b-be54-c99e95f9ca41",
      "metadata": {
        "id": "41f11cc3-a7a0-408b-be54-c99e95f9ca41"
      },
      "source": [
        "- filename: 2021_1222onomatopea_bert_fine_tuing.ipynb\n",
        "- memo: 2022年01月22日現在，\n",
        "- author: 浅川伸一 asakawa@ieee.org\n",
        "- lincense: MIT\n",
        "\n",
        "* transformers は M1 Mac では動作しない。具体的には，最適化関数 Adamw を呼び出すと halt する。\n",
        "Intel Mac such as pasiphae では動作する。おそらく，最適化関数の下請けとして呼び出している C で書かれた関数が intel cpu や arm に特化したコードを使っている，あるいは，intel cpu や arm cpu でコンパイル済のライブラリを呼び出しているためではないかと思う。\n",
        "M1 Mac は arm cpu なのだが，やや特殊なようで，libffi.dylib などのダイナミックライブラリなどでは，ターゲット cpu を arm64e としないといけないように見える。\n",
        "詳細は未確認である。2022_0124\n",
        "\n",
        "# このコードの概要，ねらい\n",
        "\n",
        "- huggingface が提供する `transformers` から BERT を呼び出す。\n",
        "`transformers` に登録されているモデルのうち，東北大学乾研提供の 日本語化 BERT モデルを微調整 fine-tuning する。\n",
        "モデル名としては `cl-tohoku/bert-base-japanese` である。\n",
        "\n",
        "- この日本語化された BERT モデル (BERT-MLM) を `BertForMaskedLM` (マスク化言語モデルに特化した BERT) として呼び出し，オノマトペ予測課題としみなして訓練を行うことである。\n",
        "\n",
        "## 本コードの具体的な手順\n",
        "\n",
        "1. 必要なライブラリを輸入 import \n",
        "2. 小野編 「オノマトペ辞典4500」の読み込み\n",
        "3. 訓練済 日本語 BERT モデルの読み込み\n",
        "4. 日本語 BERT モデルで提供されているトークナイザに，小野編「オノマトペ辞典」を登録\n",
        "5. 訓練テキストデータ (original.csv) の読み込み\n",
        "6. 小野版オノマトペ辞典の，各オノマトペ記述文に出てくるオノマトペを [MASK] で置換する\n",
        "7. PyTorch の流儀に従って Dataset, DataLoader を定義する\n",
        "8. 最適化関数を定義する\n",
        "9. 訓練関数を定義して訓練を行う\n",
        "10. 結果の損失関数の減衰曲線を描画する\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1.  必要なライブラリを輸入 import "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6640bdc1-716e-4c76-a12c-829914444287",
      "metadata": {
        "id": "6640bdc1-716e-4c76-a12c-829914444287"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "from termcolor import colored\n",
        "\n",
        "# 本ファイルを Google Colaboratory 上で実行する場合に，必要となるライブラリをインストールする\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install transformers > /dev/null 2>&1 \n",
        "\n",
        "    # MeCab, fugashi, ipadic のインストール\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1\n",
        "    !pip install mecab-python3 > /dev/null 2>&1\n",
        "    !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "    !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1\n",
        "    \n",
        "    import subprocess\n",
        "    cmd='echo `mecab-config --dicdir`\\\"/mecab-ipadic-neologd\\\"'\n",
        "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                                     shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "    !pip install 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    !python -m unidic download > /dev/null 2>&1\n",
        "    !pip install ipadic > /dev/null 2>&1\n",
        "    !pip install jaconv > /dev/null 2>&1\n",
        "    !pip install japanize_matplotlib > /dev/null 2>&1    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16567480-c352-4378-a4e5-85fca1cfa061",
      "metadata": {
        "id": "16567480-c352-4378-a4e5-85fca1cfa061"
      },
      "outputs": [],
      "source": [
        "# PyTorch の seed の設定関連 再現性確保のため\n",
        "# https://qiita.com/takubb/items/7d45ae701390912c7629\n",
        "# https://qiita.com/si1242/items/d2f9195c08826d87d6ad\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# リソースの選択（CPU/GPU）\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 乱数シード固定（再現性の担保）\n",
        "def fix_seed(seed):\n",
        "    # random\n",
        "    random.seed(seed)\n",
        "    # numpy\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # pytorch\\n\",\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed = 42\n",
        "fix_seed(seed)\n",
        "# データローダーのサブプロセスの乱数のseedが固定\n",
        "def worker_init_fn(worker_id):\n",
        "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
        "    print(worker_init_fn(1))\n",
        "    \n",
        "# # データローダーの作成\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "#                                            batch_size=16,  # バッチサイズ\n",
        "#                                            shuffle=True,  # データシャッフル\n",
        "#                                            num_workers=2,  # 高速化\n",
        "#                                            pin_memory=True,  # 高速化\n",
        "#                                            worker_init_fn=worker_init_fn\n",
        "#                                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "182d190a-d984-45e8-bf76-7c18248131d7",
      "metadata": {
        "id": "182d190a-d984-45e8-bf76-7c18248131d7"
      },
      "source": [
        "## 2. 小野編 「オノマトペ辞典4500」の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a514e28d-3aaa-4c58-b519-6d7940e663ac",
      "metadata": {
        "id": "a514e28d-3aaa-4c58-b519-6d7940e663ac",
        "outputId": "90e80bcd-610a-4254-b803-6473c7d1520f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "データファイル名: /Users/asakawa/study/2021ccap/notebooks/2021-0325日本語オノマトペ辞典4500より.xls\n",
            " オノマトペ単語総数: len(onomatopea):1741\n"
          ]
        }
      ],
      "source": [
        "# 2021/Jan 近藤先生からいただいたオノマトペ辞典のデータの読み込み\n",
        "\n",
        "#'日本語オノマトペ辞典4500より.xls' は著作権の問題があり，公にできません。\n",
        "# そのため Google Colab での解法，ローカルファイルよりアップロードしてください\n",
        "if isColab:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # ここで `日本語オノマトペ辞典4500より.xls` を指定してアップロードする\n",
        "    data_dir = '.'\n",
        "else:\n",
        "    data_dir = '/Users/asakawa/study/2021ccap/notebooks'\n",
        "\n",
        "import pandas as pd\n",
        "import jaconv\n",
        "\n",
        "onomatopea_excel = '2021-0325日本語オノマトペ辞典4500より.xls'\n",
        "onmtp2761 = pd.read_excel(os.path.join(data_dir, onomatopea_excel), sheet_name='2761語')\n",
        "\n",
        "#すべてカタカナ表記にしてデータとして利用する場合\n",
        "#`日本語オノマトペ辞典4500` はすべてひらがな表記だが，一般にオノマトペはカタカナ表記されることが多いはず\n",
        "#onomatopea = list(sorted(set([jaconv.hira2kata(o) for o in onmtp2761['オノマトペ']])))\n",
        "\n",
        "# Mac と Windows の表記の相違を吸収\n",
        "onomatopea = list(sorted(set([jaconv.normalize(o) for o in onmtp2761['オノマトペ']])))\n",
        "print(f'データファイル名: {os.path.join(data_dir, onomatopea_excel)}\\n',\n",
        "      f'オノマトペ単語総数: len(onomatopea):{len(onomatopea)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48955785-8fae-4c3f-a5c5-9af0d55e700a",
      "metadata": {
        "id": "48955785-8fae-4c3f-a5c5-9af0d55e700a"
      },
      "source": [
        "## 3. 訓練済 日本語 BERT モデルの読み込みと，小野編「オノマトペ辞典」のトークナイザへの登録"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f44ca863-d272-4e74-9183-20936642c9f3",
      "metadata": {
        "tags": [],
        "id": "f44ca863-d272-4e74-9183-20936642c9f3",
        "outputId": "eccbae32-8af0-4ad0-c84f-669d6175c362"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForMaskedLM were not initialized from the model checkpoint at sonoisa/sentence-bert-base-ja-mean-tokens-v2 and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# transformers, huggingface 版の BERT 実装の読み込み\n",
        "import torch\n",
        "from transformers import BertConfig\n",
        "from transformers import BertForPreTraining\n",
        "from transformers import BertJapaneseTokenizer\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "#model_ja_name = 'cl-tohoku/bert-base-japanese'  # 東北大学乾研による 日本語 BERT 実装\n",
        "\n",
        "# see https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2\n",
        "model_ja_name = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # 東北大学乾研による 日本語 BERT 実装\n",
        "model = BertForMaskedLM.from_pretrained(model_ja_name) # マスク化言語モデルを指定\n",
        "config = BertConfig.from_pretrained(model_ja_name)\n",
        "\n",
        "# GPU が利用可能であれば利用する\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "tknz1 = BertJapaneseTokenizer.from_pretrained(model_ja_name)\n",
        "# BPE (or sentencepiece) による下位単語分割あり"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b44fb71e-0fe6-474f-ab3a-f6e435816083",
      "metadata": {
        "id": "b44fb71e-0fe6-474f-ab3a-f6e435816083"
      },
      "source": [
        "## 4. 日本語 BERT モデルで提供されているトークナイザに，小野編「オノマトペ辞典」を登録\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33fb87ee-2dff-47d7-a652-c615fe0e038f",
      "metadata": {
        "tags": [],
        "id": "33fb87ee-2dff-47d7-a652-c615fe0e038f",
        "outputId": "9bcb3004-74de-4f01-bbb2-deb428f9fd8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "追加されたトークン数:1711/オノマトペ数:1741\n",
            " len(tknz1):33711\n",
            " len(tknz1.vocab):32000\n",
            " tknz1.vocab_size:32000\n",
            "# 確認用\n",
            "単語:わんわ(id:33706) -> token:わんわ\n",
            "単語:わんわん(id:33707) -> token:わんわん\n",
            "単語:わーっ(id:33708) -> token:わーっ\n",
            "単語:わーわー(id:33709) -> token:わーわー\n",
            "単語:わーん(id:33710) -> token:わーん\n"
          ]
        }
      ],
      "source": [
        "# トークナイザ の修正，実際には onomatopea 単語リストを引数に指定して `add_tokens()` を呼び出すだけ\n",
        "# ただし，語彙数 tknz.vocab は変更されない。追加された語彙，本コードの場合はオノマトペは，\n",
        "# `tknz1.added_tokens_encoder` と `tknz1.added_tokens_decoder` に反映されているためである\n",
        "num_added = tknz1.add_tokens(onomatopea)\n",
        "print(f'追加されたトークン数:{num_added}/オノマトペ数:{len(onomatopea)}') \n",
        "model.resize_token_embeddings(len(tknz1))\n",
        "\n",
        "print(f' len(tknz1):{len(tknz1)}\\n', \n",
        "      f'len(tknz1.vocab):{len(tknz1.vocab)}\\n',  # 一見すると，この数字からオノマトペが追加されていないように見える。\n",
        "      f'tknz1.vocab_size:{tknz1.vocab_size}')    # 駄菓子菓子，下で見るように，正しく動作しているように見受けられる\n",
        "\n",
        "print('# 確認用')\n",
        "for w in onomatopea[-5:]:\n",
        "    idx = tknz1.convert_tokens_to_ids(w)\n",
        "    w_ = tknz1.convert_ids_to_tokens(idx)\n",
        "    print(f'単語:{w}(id:{idx}) -> token:{w_}')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edfae422-37a3-439d-8bdc-e4814be6fc36",
      "metadata": {
        "id": "edfae422-37a3-439d-8bdc-e4814be6fc36"
      },
      "source": [
        "## 5. 訓練テキストデータ (original.csv) の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "685260e5-527b-4a09-8129-43a25cad554e",
      "metadata": {
        "id": "685260e5-527b-4a09-8129-43a25cad554e",
        "outputId": "654ec49a-765a-46e0-957b-559c8e17ded9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2469 has been read\n"
          ]
        }
      ],
      "source": [
        "# 近藤先生 (2021年12月22日） から送っていただいた，オノマトペ文章データ 'original.csv' を読み込む\n",
        "import jaconv\n",
        "\n",
        "if isColab:\n",
        "    uploaded = files.upload()  # original.csv をアップロード\n",
        "    data_dir = '.'\n",
        "else:\n",
        "    data_dir = '/Users/asakawa/study/2021kondo_project'\n",
        "\n",
        "original = []\n",
        "n = 0\n",
        "with open(os.path.join(data_dir,'original.csv'), 'r', encoding='utf8') as f:\n",
        "    s = f.read()\n",
        "    for s_ in s.split('\\n'):\n",
        "        if n == 0:\n",
        "            n += 1\n",
        "            continue\n",
        "        idx, sent = s_.split(',')\n",
        "        \n",
        "        # Mac と Windows との unicode 符号化の差分を吸収する\n",
        "        # jaconv.normalize は内部で unicodedata.normalize('NFKC') を呼び出しているので\n",
        "        # 差異 between Mac and Windows を吸収できる\n",
        "        sent = ''.join(jaconv.normalize(x) for x in sent)\n",
        "        original.append(sent)\n",
        "        #original[int(idx)] = sent\n",
        "\n",
        "print(f'{len(original)} has been read')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd09930d-d6ac-49d8-af31-54d88f0aa6e2",
      "metadata": {
        "id": "bd09930d-d6ac-49d8-af31-54d88f0aa6e2"
      },
      "source": [
        "## 6. 小野版オノマトペ辞典の，各オノマトペ記述文に出てくるオノマトペを [MASK] で置換する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128263c1-29e0-4ad4-8687-26150d3d7cec",
      "metadata": {
        "id": "128263c1-29e0-4ad4-8687-26150d3d7cec",
        "outputId": "96858d47-023e-46f3-f44e-7930e874d3db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_token_len:23\n"
          ]
        }
      ],
      "source": [
        "max_token_len = np.array([len(tknz1(s).input_ids) for s in original]).max()\n",
        "max_token_len += 2  # 保険のため 2 くらい加えておく\n",
        "print(f'max_token_len:{max_token_len}')\n",
        "\n",
        "# トークナイザにかけて出力を得る。`max_length` のデフォルトは 512 だが，今回は長文である必要がないと考えられる。\n",
        "# ここでは `max_token_len = 23` にしている。512 でも動作するが，学習に要する時間が増える\n",
        "text = tuple(original)  # 全文をタプルに変換\n",
        "inputs = tknz1(text, \n",
        "               return_tensors='pt', \n",
        "               max_length=max_token_len, \n",
        "               truncation=True, \n",
        "               padding='max_length')\n",
        "\n",
        "#`labels` キーを追加する。実際には inputs_ids なのでラベルではなくトークンID の系列\n",
        "inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "\n",
        "# この時点で inputs には以下が含まれている\n",
        "# ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n",
        "# `input_ids`: 入力トークン ID のリスト\n",
        "# `token_type_ids':\n",
        "# `attention_mask`:\n",
        "# `labels`: \n",
        "\n",
        "#トークン ID を走査して，オノマトペ単語であれば，[MASK] トークンに置き換える。\n",
        "# この操作が，このコードの最大のポイントと言える。\n",
        "l_ = []\n",
        "for l in inputs['labels']:\n",
        "    # 各入力行 l に対して，l の各 ID をトークンに変換し，そのトークンが，`onomatopea_vocab` であれば，`[MASK]' に置き換える\n",
        "    # そうでなければ，そのままの ID を出力する\n",
        "    l_.append([tknz1.mask_token_id if w in onomatopea else tknz1.convert_tokens_to_ids(w) for w in tknz1.convert_ids_to_tokens(l)])\n",
        "\n",
        "inputs['input_ids'] = torch.LongTensor(l_)\n",
        "#print(inputs['input_ids'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a282a5e-07d3-4749-9e38-b5ad8fd0f30b",
      "metadata": {
        "id": "5a282a5e-07d3-4749-9e38-b5ad8fd0f30b"
      },
      "source": [
        "## 7. PyTorch の流儀に従って Dataset, DataLoader を定義する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d68e713d-9e71-43ea-a540-b47d01067f85",
      "metadata": {
        "tags": [],
        "id": "d68e713d-9e71-43ea-a540-b47d01067f85",
        "outputId": "910fa935-b298-4bea-d021-504b3de61c29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(33711, 768)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=33711, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#データセットのためのクラスを定義\n",
        "class onmtpDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encoder):\n",
        "        self.encoder = encoder\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return {key:val[idx].clone().detach() for key, val in self.encoder.items()}\n",
        "        #return {key:torch.tensor(val[idx]) for key, val in self.encoder.items()}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encoder.input_ids)\n",
        "    \n",
        "dataset = onmtpDataset(inputs)\n",
        "\n",
        "#データローダを準備\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# GPU/CPU 使用を設定し，モデルの訓練モードを起動 #Setup GPU/CPU usage and activate the training mode of our model.\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device) # モデルを選択したデバイスに移動 # and move our model over to the selected device\n",
        "#model.train()  # 訓練モードに設定 #activate training mode\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34198d50-8e60-4156-ae52-890bdd4cdabb",
      "metadata": {
        "id": "34198d50-8e60-4156-ae52-890bdd4cdabb"
      },
      "source": [
        "### 7.1 訓練データセットを，訓練，検証，テストデータセットの 3 つに分割する\n",
        "<!-- # Split train dataset into train, validation and test sets -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97e4a082-28ba-4beb-82f7-9b65e0c0da09",
      "metadata": {
        "id": "97e4a082-28ba-4beb-82f7-9b65e0c0da09"
      },
      "outputs": [],
      "source": [
        "#データセットを 7:1.5:1.5 に分割して 訓練データセット，検証データセット，テストデータセットに分割\n",
        "train_size = int(dataset.__len__() * 0.70)\n",
        "valid_size = int(dataset.__len__() * 0.15)\n",
        "# 残りの 0.15 を検証データセットとする\n",
        "test_size = dataset.__len__() - train_size - valid_size\n",
        "\n",
        "train_dataset, \\\n",
        "valid_dataset, \\\n",
        "test_dataset = torch.utils.data.random_split(dataset, \n",
        "                                             lengths=[train_size, test_size, valid_size], \n",
        "                                             generator=torch.Generator().manual_seed(seed))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e35d01d-54fc-4872-8f81-7293819cb298",
      "metadata": {
        "id": "2e35d01d-54fc-4872-8f81-7293819cb298"
      },
      "source": [
        "## 8. 最適化関数を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc58f11-075b-4a13-bcbd-bbfce8488092",
      "metadata": {
        "id": "abc58f11-075b-4a13-bcbd-bbfce8488092"
      },
      "outputs": [],
      "source": [
        "import socket   \n",
        "if not 'Sinope' in socket.gethostname():\n",
        "    # 以下だと M1 mac では halt する。\n",
        "    #最適化関数を初期化 (AdamW は重み付き崩壊で，過学習の可能性を減らす) \n",
        "    #Initialize our optimizer (Adam with weighted decay - reduces chance of overfitting).\n",
        "    from transformers import AdamW\n",
        "else:\n",
        "    # なので，transformers.AdamW ではなく，PyTorch の標準関数である Adam で代用する\n",
        "    from torch.optim import AdamW\n",
        "\n",
        "#最適化関数を初期化 # initialize optimizer\n",
        "optim = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0ee9a31-40d2-40f9-918d-9ac485c965d3",
      "metadata": {
        "id": "c0ee9a31-40d2-40f9-918d-9ac485c965d3"
      },
      "source": [
        "## 9. 訓練関数を定義して訓練を行う"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7614371-42ec-44ad-9b86-37cdc7a25831",
      "metadata": {
        "id": "f7614371-42ec-44ad-9b86-37cdc7a25831"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import typing\n",
        "import transformers\n",
        "\n",
        "n_batch_size = 128\n",
        "traindataset_loader = torch.utils.data.DataLoader(train_dataset, batch_size=n_batch_size, shuffle=True)\n",
        "testdataset_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=n_batch_size, shuffle=False)\n",
        "validdataset_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=n_batch_size, shuffle=False)\n",
        "\n",
        "def forward(data:transformers.tokenization_utils_base.BatchEncoding, \n",
        "        model:transformers.models.bert.modeling_bert.BertForMaskedLM=model) -> transformers.modeling_outputs.MaskedLMOutput:\n",
        "    _input_ids = data['input_ids'].clone().detach().to(device)  # ミニバッチサイズだけデータを取得\n",
        "    _attention_mask = data['attention_mask'].clone().detach().to(device)\n",
        "    _labels = data['labels'].clone().detach().to(device)\n",
        "    _out = model(_input_ids,\n",
        "                 attention_mask=_attention_mask,\n",
        "                 labels=_labels)\n",
        "    return _out\n",
        "\n",
        "\n",
        "def eval(data:transformers.tokenization_utils_base.BatchEncoding, \n",
        "         model:transformers.models.bert.modeling_bert.BertForMaskedLM=model) -> transformers.modeling_outputs.MaskedLMOutput:\n",
        "    model.eval()\n",
        "    _input_ids = data['input_ids'].clone().detach().to(device)  # ミニバッチサイズだけデータを取得\n",
        "    _attention_mask = data['attention_mask'].clone().detach().to(device)\n",
        "    _labels = data['labels'].clone().detach().to(device)\n",
        "    _out = model(_input_ids,\n",
        "                 attention_mask=_attention_mask,\n",
        "                 labels=_labels)\n",
        "    return _out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1f5c4dc-7587-4bd4-a6b8-4743f14abe65",
      "metadata": {
        "id": "b1f5c4dc-7587-4bd4-a6b8-4743f14abe65"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "train_losses, valid_losses = [], []\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    # 検証，検証を先にしないと 0 段階での成績がわからんので。\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    valid_loop = tqdm(validdataset_loader)\n",
        "    for data in valid_loop:\n",
        "        _out = eval(data)\n",
        "        _loss = _out.loss\n",
        "        valid_loss += _loss.item()\n",
        "        valid_loop.set_description(f'Epoch {epoch}') # 進行状況の表示\n",
        "        valid_loop.set_postfix(loss=_loss.item())\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    # 訓練\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    train_loop = tqdm(traindataset_loader, leave=True)\n",
        "    for data in train_loop:\n",
        "        optim.zero_grad()   # 勾配情報の 0 クリア\n",
        "        _out = forward(data)\n",
        "        _loss = _out.loss   # 損失値を取得\n",
        "        _loss.backward()    # 取得した損失値に基づいて BERT のパラメータを逆伝播\n",
        "        optim.step()        # BERT パラメータの更新 すなわち学習\n",
        "        train_loop.set_description(f'Epoch {epoch}') # 進行状況の表示\n",
        "        train_loop.set_postfix(loss=_loss.item())\n",
        "        train_loss += _loss.item()\n",
        "    train_losses.append(train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6738cedc-3d91-4e7e-aef6-28a6e8520199",
      "metadata": {
        "id": "6738cedc-3d91-4e7e-aef6-28a6e8520199",
        "outputId": "d11a100e-9f32-48ff-b795-68d6f67723d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([    2,  6223, 28457,   120,    11,     4,    13,    52,   559,  3269,\n",
            "        11145,  1755,     3,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0])\n"
          ]
        }
      ],
      "source": [
        "#for data in validdataset_loader:\n",
        "#for data in testdataset_loader:\n",
        "#    print(type(data), data.keys(), tknz1.convert_ids_to_tokens(data['input_ids'][0])) # inputs_ids[0])\n",
        "input_ids = test_dataset.__getitem__(0)['input_ids']\n",
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a7c4b43-397b-4c44-8a6a-3bb297f5761f",
      "metadata": {
        "id": "6a7c4b43-397b-4c44-8a6a-3bb297f5761f"
      },
      "outputs": [],
      "source": [
        "len(train_losses)\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "\n",
        "plt.plot(train_losses, color='red') # [:100])\n",
        "plt.plot(valid_losses, color='blue') # [:100])\n",
        "plt.xlabel('訓練時間')\n",
        "plt.ylabel('損失値')\n",
        "plt.title('オノマトペ微調整における学習の推移 (損失値) の減少')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec40b7be-141b-48ff-91fd-3d0d4e555be9",
      "metadata": {
        "id": "ec40b7be-141b-48ff-91fd-3d0d4e555be9"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm  # 進捗状況の可視化のため\n",
        "epochs = 10  # 学習回数を指定\n",
        "losses = []\n",
        "for epoch in range(epochs):\n",
        "    # setup loop with TQDM and dataloader\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch in loop:\n",
        "\n",
        "        optim.zero_grad()  # 学習に用いる BERT パラメータの勾配を 0 で初期化\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)  # ミニバッチサイズだけデータを取得\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # BERT を呼び出して結果を得る\n",
        "        outputs = model(input_ids, \n",
        "                        attention_mask=attention_mask,\n",
        "                        labels=labels)\n",
        "\n",
        "        loss = outputs.loss  # 損失値を取得\n",
        "        loss.backward()      # 取得した損失値に基づいて BERT のパラメータを逆伝播\n",
        "        optim.step()         # BERT パラメータの更新 すなわち学習\n",
        "        \n",
        "        loop.set_description(f'Epoch {epoch}') # 進行状況の表示\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "        losses.append(loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a443034c-2739-4bab-87b0-8e69626fabe7",
      "metadata": {
        "id": "a443034c-2739-4bab-87b0-8e69626fabe7"
      },
      "source": [
        "## 10. 結果の損失関数の減衰曲線を描画する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "949c8343-2940-413d-99ab-0baf710b24e5",
      "metadata": {
        "id": "949c8343-2940-413d-99ab-0baf710b24e5"
      },
      "outputs": [],
      "source": [
        "#print(len(losses))\n",
        "\n",
        "plt.plot(train_losses, color='red') # [:100])\n",
        "plt.plot(valid_losses, color='blue') # [:100])\n",
        "plt.xlabel('訓練時間')\n",
        "plt.ylabel('損失値')\n",
        "plt.title('オノマトペ微調整における学習の推移 (損失値) の減少')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea301d8c-2f15-4e29-9414-c342de50dcaa",
      "metadata": {
        "id": "ea301d8c-2f15-4e29-9414-c342de50dcaa",
        "outputId": "29095bc8-755f-43f5-9fd5-7e31fb37854e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    2,  6223, 28457,   120,    11,     4,    13,    52,   559,  3269,\n",
            "         11145,  1755,     3,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0]]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) tensor([[    2,  6223, 28457,   120,    11, 32126,    13,    52,   559,  3269,\n",
            "         11145,  1755,     3,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0]])\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids,attention_mask,labels)\n\u001b[0;32m----> 6\u001b[0m _out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1343\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1341\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1343\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1357\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1358\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:989\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    987\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 989\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    997\u001b[0m     embedding_output,\n\u001b[1;32m    998\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1007\u001b[0m )\n\u001b[1;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:215\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings(input_ids)\n\u001b[0;32m--> 215\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_type_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:2043\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2037\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2038\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2040\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "a = test_dataset.__getitem__(0)\n",
        "input_ids = a['input_ids'].unsqueeze(0).to(device)\n",
        "attention_mask = a['attention_mask'].unsqueeze(0).to(device)\n",
        "labels = a['labels'].unsqueeze(0).to(device)\n",
        "print(input_ids,attention_mask,labels)\n",
        "_out = model(input_ids, attention_mask, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592b9da1-b8fc-4b1e-9cec-d99de088f3d4",
      "metadata": {
        "id": "592b9da1-b8fc-4b1e-9cec-d99de088f3d4"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def eval_an_output(N, original=original, tknz1=tknz1, inputs=inputs, print_flag=True):\n",
        "    \"\"\"\n",
        "    引数として 数字を 1 つ入力すると (N)，`original.csv` の N 行目のデータを読み込んで，\n",
        "    その文のオノマトペを [MASK] に置き換えて，マスク化言語モデルで [MASK] を予測する。\n",
        "    結果を表示する場合には 引数 `print_flag=True` として呼び出す\n",
        "    \"\"\"\n",
        "    if N >= len(original) or (not isinstance(N, int)):\n",
        "        return\n",
        "\n",
        "    _out = model(inputs.input_ids[N].unsqueeze(0).to(device), attention_mask=inputs.attention_mask[N].unsqueeze(0).to(device), labels=inputs.labels[N].to(device))\n",
        "    _x = _out.logits.detach()\n",
        "    __x = _x.squeeze(0).detach().clone()\n",
        "    _pred_idx  = torch.argmax(__x, dim=1, keepdim=True)\n",
        "    _pred_s    = \"/\".join(tknz1.convert_ids_to_tokens(_pred_idx)).replace('/[PAD]','')\n",
        "    \n",
        "    _orig      = original[N] # 原文\n",
        "    _inp_idx   = tknz1.convert_ids_to_tokens(inputs.input_ids[N]) # 入力トークンID\n",
        "    _inp_s     = \"/\".join(_inp_idx).replace('/[PAD]','')          # 入力文\n",
        "    _teach_idx = tknz1.convert_ids_to_tokens(inputs.labels[N])    # 教師信号トークンID\n",
        "    _teach_s   = \"/\".join(_teach_idx).replace('/[PAD]','')        # 教師信号文\n",
        "    \n",
        "    _mask_pos = np.where(inputs.input_ids[N].detach().numpy() == tknz1.mask_token_id)\n",
        "    _teach_tokens = inputs.labels[N][_mask_pos].detach().squeeze().numpy()\n",
        "    _pred_tokens  = _pred_idx[_mask_pos].detach().squeeze().cpu().numpy()\n",
        "\n",
        "    _n_hit = np.array([_teach_tokens == _pred_tokens]).sum()       # 正解したか否か\n",
        "    if print_flag:\n",
        "        color = 'grey' if _n_hit > 0 else 'red'\n",
        "        print(f'{N:5,d}   原文:{_orig}')\n",
        "        print(f'\\t入力:{_inp_s}')\n",
        "        print(f'\\t正解:{_teach_s}')\n",
        "        print(colored(f'\\t出力:{_pred_s}',color))\n",
        "        print(f'\\tmask 位置:{_mask_pos}')\n",
        "        print(f'\\t正解トークン:{_teach_tokens}', f'予測トークン:{_pred_tokens}', \n",
        "              f'{np.array([_teach_tokens == _pred_tokens]).sum() > 0}')\n",
        "        print(f'\\t_out.loss:{_out.loss:.3f}')\n",
        "    \n",
        "    return _out.loss, _n_hit\n",
        "\n",
        "total_hit = 0\n",
        "#for i in range(len(original)):\n",
        "for i in range(300):\n",
        "    _, hit = eval_an_output(i, print_flag=True) \n",
        "    # print_flag = True にすると推論結果を表示します. 逆に False にすれば正解率だけ計算します\n",
        "    total_hit += hit\n",
        "\n",
        "print(f'正解数:{total_hit}/{i}= {total_hit/i * 100:.3f} %')    \n",
        "#print(f'正解数:{total_hit}/{len(original)}= {total_hit/len(original) * 100:.3f} %')    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e89f6a66-e284-46cb-8edf-cdc9e1b0045e",
      "metadata": {
        "id": "e89f6a66-e284-46cb-8edf-cdc9e1b0045e",
        "outputId": "8a7db57c-f3f1-428e-cea3-87e9b971e60c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!gls -lt *.pt\n",
        "#model.load_state_dict(torch.load('2022_0120onomatopea.pt'))\n",
        "model.load_state_dict(torch.load('2022_0125onomatopea.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "776d45c7-205f-4d13-9918-d9254030b2fd",
      "metadata": {
        "id": "776d45c7-205f-4d13-9918-d9254030b2fd",
        "outputId": "a6f2981a-0087-4d0a-bdf4-a6b58f764b00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'かた',\n",
              " '##い',\n",
              " 'もの',\n",
              " 'を',\n",
              " 'かりっ',\n",
              " 'と',\n",
              " '一',\n",
              " '度',\n",
              " '歯',\n",
              " '##切れ',\n",
              " 'よく',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tknz1.convert_ids_to_tokens(test_dataset.__getitem__(0)['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b6e06c0-2668-4faa-9e40-7192d84ad3fb",
      "metadata": {
        "id": "2b6e06c0-2668-4faa-9e40-7192d84ad3fb",
        "outputId": "0f303849-3550-4892-e7b0-1310ed8727ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS]/日/が/射/##し/たり/光/が/[MASK]/と/とも/##る/[SEP]\n",
            "[CLS]/かた/##い/もの/を/かりっ/と/一/度/歯/##切れ/よく/[SEP]\n",
            "[CLS]/日/が/射/##し/たり/光/が/ぽっ/と/とも/##る/[SEP]\n",
            "[CLS]/あ/##たた/##かい/日/ざ/##し/が/[MASK]/(/と/)/差/し/##こ/##む/[SEP]\n",
            "[CLS]/三味線/を/[UNK]/(/と/)/つま/##び/##く/[SEP]\n",
            "[CLS]/あ/##たた/##かい/日/ざ/##し/が/ぽかぽか/(/と/)/差/し/##こ/##む/[SEP]\n",
            "[CLS]/水/や/日/ざ/##し/が/[MASK]/(/と/)/満ち/満ち/##る/[SEP]\n",
            "[CLS]/靴/が/床/に/当たる/(/と/)/何/度/も/当たる/[SEP]\n",
            "[CLS]/水/や/日/ざ/##し/が/なんなん/(/と/)/満ち/満ち/##る/[SEP]\n",
            "[CLS]/[MASK]/かり/日/が/[MASK]/(/と/)/暮れ/##る/[SEP]\n",
            "[CLS]/ばっ/て/ぬらりくらり/(/と/)/つか/##まえ/にくく/[SEP]\n",
            "[CLS]/すっ/かり/日/が/とっぷり/(/と/)/暮れ/##る/[SEP]\n",
            "[CLS]/日光/が/[MASK]/(/と/)/照/##り/##輝/##く/[SEP]\n",
            "[CLS]/表面/の/粗/##い/もの/が/ざらざら/(/と/)/こ/##す/##れる/[SEP]\n",
            "[CLS]/日光/が/てかてか/(/と/)/照/##り/##輝/##く/[SEP]\n",
            "[CLS]/雲/ひとつ/なく/[MASK]/と/晴れ/##上がる/[SEP]\n",
            "[CLS]/体/や/骨/##組み/など/ががっ/しり/(/と/)/頑丈/[SEP]\n",
            "[CLS]/雲/ひとつ/なく/すかっ/と/晴れ/##上がる/[SEP]\n",
            "[CLS]/太陽/が/[MASK]/(/と/)/焼け/つく/よう/に/照/##る/[SEP]\n",
            "[CLS]/涙/など/が/ほろり/と/一/##滴/こぼ/##れ/##落ち/##る/[SEP]\n",
            "[CLS]/太陽/が/じりじり/(/と/)/焼け/つく/よう/に/照/##る/[SEP]\n",
            "[CLS]/日/ざ/##し/が/[MASK]/(/[UNK]/)/と/いっぱい/に/差し/##込む/[SEP]\n",
            "[CLS]/心配/や/あ/##せ/##り/で/やきもき/(/と/)/落ち/##着/##か/ない/[SEP]\n",
            "[CLS]/日/ざ/##し/が/さんさん/(/[UNK]/)/と/いっぱい/に/差し/##込む/[SEP]\n",
            "[CLS]/曇/##っ/て/い/た/空/が/[MASK]/と/明るく/晴れ/##る/[SEP]\n",
            "[CLS]/もの/が/ぽつん/と/小さく/目/に/見える/[SEP]\n",
            "[CLS]/曇/##っ/て/い/た/空/が/けろり/と/明るく/晴れ/##る/[SEP]\n",
            "[CLS]/真夏/の/太陽/が/[MASK]/(/と/)/強く/照/つける/[SEP]\n",
            "[CLS]/強い/摩擦/によって/きゅー/と/音/を/立てる/[SEP]\n",
            "[CLS]/真夏/の/太陽/が/ぎらぎら/(/と/)/強く/照/つける/[SEP]\n"
          ]
        }
      ],
      "source": [
        "#inputs.input_ids[0]\n",
        "#print(inputs.input_ids.size())         # torch.Size([2469, 23])\n",
        "#print(inputs.attention_mask.size())    # torch.Size([2469, 23])\n",
        "#print(inputs.attention_mask[0].size())  # torch.Size([23])\n",
        "#print(inputs.attention_mask[0])         # tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "for N in range(10):\n",
        "    _out = model(inputs.input_ids[N].unsqueeze(0).to(device), attention_mask=inputs.attention_mask[N].unsqueeze(0).to(device), labels=inputs.labels[N].to(device))\n",
        "    \n",
        "    inp = test_dataset.__getitem__(N)\n",
        "    _inp = inp['input_ids'].unsqueeze(0).to(device)\n",
        "    _attention_mask = inp['attention_mask'].unsqueeze(0).to(device)\n",
        "    _labels = inp['labels'].unsqueeze(0).to(device)\n",
        "    _out = model(_inp, attention_mask=_attention_mask, labels=_labels)\n",
        "\n",
        "    _x = _out.logits.detach()\n",
        "    __x = _x.squeeze(0).detach().clone()\n",
        "    _pred_idx  = torch.argmax(__x, dim=1, keepdim=True)\n",
        "    _pred_s    = \"/\".join(tknz1.convert_ids_to_tokens(_pred_idx)).replace('/[PAD]','')\n",
        "    \n",
        "    _orig      = original[N] # 原文\n",
        "    _inp_idx   = tknz1.convert_ids_to_tokens(inputs.input_ids[N]) # 入力トークンID\n",
        "    _inp_s     = \"/\".join(_inp_idx).replace('/[PAD]','')          # 入力文\n",
        "    _teach_idx = tknz1.convert_ids_to_tokens(inputs.labels[N])    # 教師信号トークンID\n",
        "    _teach_s   = \"/\".join(_teach_idx).replace('/[PAD]','')        # 教師信号文\n",
        "    \n",
        "    _mask_pos = np.where(inputs.input_ids[N].detach().numpy() == tknz1.mask_token_id)\n",
        "    _teach_tokens = inputs.labels[N][_mask_pos].detach().squeeze().numpy()\n",
        "    _pred_tokens  = _pred_idx[_mask_pos].detach().squeeze().cpu().numpy()\n",
        "\n",
        "    _n_hit = np.array([_teach_tokens == _pred_tokens]).sum()       # 正解したか否か\n",
        "    print(_inp_s)\n",
        "    print(_pred_s)\n",
        "    print(_teach_s)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d2e614-95e1-44ea-b2a3-317ff47ccf13",
      "metadata": {
        "id": "01d2e614-95e1-44ea-b2a3-317ff47ccf13"
      },
      "outputs": [],
      "source": [
        "#print(inputs.input_ids[N][:13])\n",
        "#print(_pred_idx[:13])\n",
        "print(_inp_s)\n",
        "print(_pred_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c664678-e8c9-488e-a14b-b22fc57052d9",
      "metadata": {
        "id": "3c664678-e8c9-488e-a14b-b22fc57052d9"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '2022_0120onomatopea.pt')\n",
        "#model = BertForMaskedLM.from_pretrained(model_ja_name)\n",
        "model.load_state_dict(torch.load('2022_0120onomatopea.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf4a732-7667-41b4-865b-6cd6d3e2574c",
      "metadata": {
        "id": "bdf4a732-7667-41b4-865b-6cd6d3e2574c"
      },
      "outputs": [],
      "source": [
        "#type(tknz1)\n",
        "type(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b9f681-aa84-4f9a-8f55-8772c4b01729",
      "metadata": {
        "id": "60b9f681-aa84-4f9a-8f55-8772c4b01729"
      },
      "outputs": [],
      "source": [
        "import typing\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def eval_an_output(N:int, \n",
        "                   sent:list, \n",
        "                   tknz1:transformers.models.bert_japanese.tokenization_bert_japanese.BertJapaneseTokenizer=tknz1, \n",
        "                   inputs:transformers.tokenization_utils_base.BatchEncoding =inputs, print_flag=True):\n",
        "    \"\"\"\n",
        "    引数として 数字を 1 つ入力すると (N)，`original.csv` の N 行目のデータを読み込んで，\n",
        "    その文のオノマトペを [MASK] に置き換えて，マスク化言語モデルで [MASK] を予測する。\n",
        "    結果を表示する場合には 引数 `print_flag=True` として呼び出す\n",
        "    \"\"\"\n",
        "    if N >= len(original) or (not isinstance(N, int)):\n",
        "        return\n",
        "\n",
        "    _out = model(inputs.input_ids[N].unsqueeze(0).to(device), attention_mask=inputs.attention_mask[N].unsqueeze(0).to(device), labels=inputs.labels[N].to(device))\n",
        "    _x = _out.logits.detach()\n",
        "    __x = _x.squeeze(0).detach().clone()\n",
        "    _pred_idx  = torch.argmax(__x, dim=1, keepdim=True)\n",
        "    _pred_s    = \"/\".join(tknz1.convert_ids_to_tokens(_pred_idx)).replace('/[PAD]','')\n",
        "    \n",
        "    _orig      = original[N] # 原文\n",
        "    _inp_idx   = tknz1.convert_ids_to_tokens(inputs.input_ids[N]) # 入力トークンID\n",
        "    _inp_s     = \"/\".join(_inp_idx).replace('/[PAD]','')          # 入力文\n",
        "    _teach_idx = tknz1.convert_ids_to_tokens(inputs.labels[N])    # 教師信号トークンID\n",
        "    _teach_s   = \"/\".join(_teach_idx).replace('/[PAD]','')        # 教師信号文\n",
        "    \n",
        "    _mask_pos = np.where(inputs.input_ids[N].detach().numpy() == tknz1.mask_token_id)\n",
        "    _teach_tokens = inputs.labels[N][_mask_pos].detach().squeeze().numpy()\n",
        "    _pred_tokens  = _pred_idx[_mask_pos].detach().squeeze().cpu().numpy()\n",
        "\n",
        "    _n_hit = np.array([_teach_tokens == _pred_tokens]).sum()       # 正解したか否か\n",
        "    if print_flag:\n",
        "        color = 'grey' if _n_hit > 0 else 'red'\n",
        "        print(f'{N:5,d}   原文:{_orig}')\n",
        "        print(f'\\t入力:{_inp_s}')\n",
        "        print(f'\\t正解:{_teach_s}')\n",
        "        print(colored(f'\\t出力:{_pred_s}',color))\n",
        "        print(f'\\tmask 位置:{_mask_pos}')\n",
        "        print(f'\\t正解トークン:{_teach_tokens}', f'予測トークン:{_pred_tokens}', \n",
        "              f'{np.array([_teach_tokens == _pred_tokens]).sum() > 0}')\n",
        "        print(f'\\t_out.loss:{_out.loss:.3f}')\n",
        "    \n",
        "    return _out.loss, _n_hit\n",
        "\n",
        "total_hit = 0\n",
        "for i in range(len(original)):\n",
        "    _, hit = eval_an_output(i, print_flag=True) # print_flag = True にすると推論結果を表示します. 逆に False にすれば正解率だけ計算します\n",
        "    total_hit += hit\n",
        "\n",
        "print(f'正解数:{total_hit}/{len(original)}= {total_hit/len(original) * 100:.3f} %')    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d93805-6b9f-4106-bd02-f698c6b5862b",
      "metadata": {
        "id": "64d93805-6b9f-4106-bd02-f698c6b5862b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "2022_0503onomatopea_bert_fine_turing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}