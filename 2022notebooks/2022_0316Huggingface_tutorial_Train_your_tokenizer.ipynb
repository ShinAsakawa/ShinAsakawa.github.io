{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0316Huggingface_tutorial_Train_your_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "colabでこのノートを開く場合，おそらく🤗 Transformers と 🤗 Datasets のインストールが必要です。\n",
        "以下のセルのコメントを解除して実行します。\n",
        "<!-- If you're opening this Notebook on colab, you will probably need to install 🤗 Transformers and 🤗 Datasets. Uncomment the following cell and execute it: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFASsisvIrIb"
      },
      "source": [
        "このノートブックをローカルで開いている場合，Datasets の最終版からのインストールと Transformers のソースインストール環境であることを確認してください。\n",
        "<!-- If you're opening this notebook locally, make sure your environment has an install from the last version of Datasets and a source install of Transformers. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvVlwzZlwXLM"
      },
      "source": [
        "# ゼロから独自トークン化器を訓練\n",
        "<!-- # Training your own tokenizer from scratch -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVcbUt6TwXLM"
      },
      "source": [
        "このノートブックでは，与えられたコーパスを用いてゼロから独自のトークン化器を学習し，それを使ってゼロから言語モデルを学習する方法をいくつか紹介します。\n",
        "<!-- In this notebook, we will see several ways to train your own tokenizer from scratch on a given corpus, so you can then use it to train a language model from scratch.-->\n",
        "\n",
        "なぜトークン化器を訓練する必要があるのでしょうか？\n",
        "それは Transformer のモデルは下位単語型トークン化アルゴリズムを使うことが非常に多く，あなたが使っているコーパスによく含まれる単語の部分を識別するように訓練する必要があるからです。\n",
        "Hugging Face コースの [tokenization chapter](https://huggingface.co/course/chapter2/4?fw=pt) でトークン化器の一般的な紹介を [tokenizers summary](https://huggingface.co/transformers/tokenizer_summary.html) で下位単語型トークン化アルゴリズムの違いをご覧になることをお勧めします。\n",
        "<!-- Why would you need to *train* a tokenizer? That's because Transformer models very often use subword tokenization algorithms, and they need to be trained to identify the parts of words that are often present in the corpus you are using. \n",
        "We recommend you take a look at the [tokenization chapter](https://huggingface.co/course/chapter2/4?fw=pt) of the Hugging Face course for a general introduction on tokenizers, and at the [tokenizers summary](https://huggingface.co/transformers/tokenizer_summary.html) for a look at the differences between the subword tokenization algorithms. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iMJuYOewXLM"
      },
      "source": [
        "## コーパスの取得\n",
        "<!-- ## Getting a corpus -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbCYGOY4wXLM"
      },
      "source": [
        "トークン化器を学習させるためにテキストが必要です。\n",
        "テキストデータをダウンロードするには [🤗 Datasets](https://github.com/huggingface/datasets) ライブラリを使用します。\n",
        "これは `load_dataset` 関数で簡単に行うことができます。\n",
        "<!-- We will need texts to train our tokenizer. \n",
        "We will use the [🤗 Datasets](https://github.com/huggingface/datasets) library to download our text data, which can be easily done with the `load_dataset` function: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdcewTzhwXLN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoEP_BZTwXLN"
      },
      "source": [
        "この例では Wikitext-2 (4.5MB の文を含むので，この例では学習が速い) を使いますが，好きなデータセットを使うことができます (英語以外なら，言語も問いません)。\n",
        "<!-- For this example, we will use Wikitext-2 (which contains 4.5MB of texts so training goes fast for our example) but you can use any dataset you want (and in any language, just not English). -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbPW_1yDwXLO"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KKAaXD0wXLO"
      },
      "source": [
        "36,718 文からなるデータセットを見ましょう\n",
        "<!-- We can have a look at the dataset, which as 36,718 texts: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMVRcF23wXLP"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCZZ1kR1wXLP"
      },
      "source": [
        "任意の要素にアクセスするには，そのインデックスを指定するだけです。\n",
        "<!-- To access an element, we just have to provide its index: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "846fOOFPwXLP"
      },
      "outputs": [],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a5y7WWAwXLP"
      },
      "source": [
        "スライスに直接アクセスすることもできます。\n",
        "その場合は，キーが  `text` で値が  text のリストである辞書を取得します。\n",
        "<!-- We can also access a slice directly, in which case we get a dictionary with the key `\"text\"` and a list of texts as value: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AnuopwpwXLP"
      },
      "outputs": [],
      "source": [
        "dataset[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpbOu2hLwXLQ"
      },
      "source": [
        "トークナイザーを学習させるための API は，文のバッチの反復器 (iterator)，例えば文のリストのリストを必要とします。\n",
        "<!-- The API to train our tokenizer will require an iterator of batch of texts, for instance a list of list of texts: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz_c2UtQwXLQ"
      },
      "outputs": [],
      "source": [
        "batch_size = 1000\n",
        "all_texts = [dataset[i : i + batch_size][\"text\"] for i in range(0, len(dataset), batch_size)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZgc9K_VwXLQ"
      },
      "source": [
        "すべてをメモリに読み込むことを避けるため (Datasets ライブラリは要素をディスクに保持し，要求されたときだけメモリに読み込むため)，Python の反復器を定義しています。\n",
        "これは特に巨大なデータセットを持っている場合に有効です。\n",
        "<!-- To avoid loading everything into memory (since the Datasets library keeps the element on disk and only load them in memory when requested), we define a Python iterator. \n",
        "This is particularly useful if you have a huge dataset: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGyeaoTtwXLQ"
      },
      "outputs": [],
      "source": [
        "def batch_iterator():\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        yield dataset[i : i + batch_size][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFr5z9KTwXLQ"
      },
      "source": [
        "では，このコーパスを用いて，どのように新しいトークン化器を学習させるか見てみましょう。\n",
        "最初の API は，既存のトークン化器を使って，1 行のコードであなたのコーパスに新しいバージョンのトークン化器を学習させるもので，もう一つは，実際にブロックごとにトークン化器を構築し，すべてのステップをカスタマイズするものです\n",
        "<!-- Now let's see how we can use this corpus to train a new tokenizer! \n",
        "There are two APIs to do this: the first one uses an existing tokenizer and will train a new version of it on your corpus in one line of code, the second is to actually build your tokenizer block by block, so lets you customize every step! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_jT4wiSwXLQ"
      },
      "source": [
        "## 既存のトークン化器を利用する\n",
        "<!-- ## Using an existing tokenizer -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eL8Jy2MwXLQ"
      },
      "source": [
        "既存のトークン化器と全く同じアルゴリズムとパラメータでトークン化器を学習させたい場合は  `train_new_from_iterator` API を使用すればよいです。\n",
        "例えば Wikitext-2 で GPT-2 tokenzier の新しいバージョンを同じアルゴリズムで学習してみよう。\n",
        "<!-- If you want to train a tokenizer with the exact same algorithms and parameters as an existing one, you can just use the `train_new_from_iterator` API. \n",
        "For instance, let's train a new version of the GPT-2 tokenzier on Wikitext-2 using the same tokenization algorithm.-->\n",
        "\n",
        "まず，モデルとして使いたいトークン化器をロードする必要があります。\n",
        "<!-- First we need to load the tokenizer we want to use as a model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f0xooENwXLR"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8A36l9pwXLR"
      },
      "source": [
        "選んだトークナイザーが *fast* 版であることを確認してください (🤗 Tokenizers ライブラリによってバックアップされます)。\n",
        "そうでなければノートブックの残りの部分は実行されません。\n",
        "<!-- Make sure that the tokenizer you picked as a *fast* version (backed by the 🤗 Tokenizers library) otherwise the rest of the notebook will not run: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9Hd2wKnwXLR"
      },
      "outputs": [],
      "source": [
        "tokenizer.is_fast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg4ne-2dwXLR"
      },
      "source": [
        "次に，学習用コーパス (リストのリスト，または先ほど定義した反復器) を `train_new_from_iterator` メソッドに渡します。\n",
        "また，使用する語彙規模も指定する必要があります:\n",
        "<!-- Then we feed the training corpus (either the list of list or the iterator we defined earlier) to the `train_new_from_iterator` method. \n",
        "We also have to specify the vocabulary size we want to use: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX2fNIdbwXLR"
      },
      "outputs": [],
      "source": [
        "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=25000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeKMHu-2wXLR"
      },
      "source": [
        "と，これだけです。\n",
        "Rust に支えられた🤗 Tokenizers ライブラリのおかげで，学習はとても速く進みます。\n",
        "<!-- And that's all there is to it! The training goes very fast thanks to the 🤗 Tokenizers library, backed by Rust.-->\n",
        "\n",
        "これで新しいトークン化器がデータの前処理と言語モデルの学習を行う準備が整いました。\n",
        "いつものように入力文を与えてください。\n",
        "<!-- You now have a new tokenizer ready to preprocess your data and train a language model. You can feed it input texts as usual: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B_QziVfwXLR",
        "outputId": "3e5e53c8-3b63-4fe4-a735-5191a163dc99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[], [238, 8576, 9441, 2987, 238, 252], [], [4657, 74, 4762, 826, 8576, 428, 466, 609, 6881, 412, 204, 9441, 311, 2746, 466, 10816, 168, 99, 150, 192, 112, 14328, 3983, 112, 4446, 94, 18288, 4446, 193, 3983, 98, 3983, 22171, 95, 19, 201, 6374, 209, 8576, 218, 198, 3455, 1972, 428, 310, 201, 5099, 3242, 227, 281, 8576, 9441, 2987, 2553, 1759, 201, 301, 196, 13996, 1496, 277, 2330, 1464, 674, 1898, 307, 742, 3541, 225, 7514, 14, 54, 719, 274, 198, 4777, 15522, 209, 19895, 221, 1341, 1633, 221, 1759, 201, 322, 301, 198, 1368, 674, 221, 198, 8576, 843, 209, 2468, 1795, 223, 198, 1049, 9595, 218, 13996, 225, 1563, 277, 582, 6493, 281, 457, 14371, 201, 198, 1422, 3373, 7452, 227, 198, 455, 674, 225, 4687, 198, 239, 21976, 239, 201, 196, 21657, 1680, 3773, 5591, 198, 4196, 218, 4679, 427, 661, 198, 3518, 1288, 220, 1051, 516, 889, 3947, 1922, 2500, 225, 390, 2065, 744, 872, 198, 7592, 3773, 239, 1975, 251, 208, 89, 22351, 239, 209, 252], [261, 674, 959, 1921, 221, 1462, 201, 7600, 547, 196, 1178, 4753, 218, 198, 630, 3591, 263, 8576, 9441, 1180, 209, 1831, 322, 7568, 198, 3621, 2240, 218, 198, 843, 201, 322, 471, 9575, 5291, 16591, 967, 201, 781, 281, 1815, 198, 674, 604, 10344, 1252, 274, 843, 664, 3147, 320, 209, 13290, 8751, 8124, 2528, 6023, 74, 235, 225, 7445, 10040, 17384, 241, 11487, 8950, 857, 1835, 340, 1382, 22582, 201, 1008, 296, 8576, 9441, 1180, 2436, 21134, 5337, 19463, 5161, 209, 240, 1178, 927, 218, 3776, 8650, 198, 3355, 209, 261, 674, 268, 83, 2511, 3472, 258, 8288, 307, 1010, 268, 78, 209, 252]], 'attention_mask': [[], [1, 1, 1, 1, 1, 1], [], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "new_tokenizer(dataset[:5][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFcw2OvPwXLS"
      },
      "source": [
        "保存は `save_pretrained` メソッドでローカルに行うことができます。\n",
        "<!-- You can save it locally with the `save_pretrained` method: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpKf_bI8wXLS"
      },
      "outputs": [],
      "source": [
        "new_tokenizer.save_pretrained(\"my-new-tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESL6fgpqwXLS"
      },
      "source": [
        "あるいは [Hugging Face Hub](https://huggingface.co/models) にプッシュして，どこからでもその新しいトークン化器を使えるようにすることも可能です。\n",
        "ターミナルで `huggingface-cli login` を実行するか，以下のセルを実行して，認証トークンが保存されていることを確認するだけです。\n",
        "<!-- Or even push it to the [Hugging Face Hub](https://huggingface.co/models) to use that new tokenzier from anywhere. \n",
        "Just make sure you have your authentication token stored by executing `huggingface-cli login` in a terminal or executing the following cell: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FiFFK_NwXLS"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Go8CWrVwXLS"
      },
      "source": [
        "`git lfs` がインストールされていることが必要です。\n",
        "以下のセルをアンコメントすれば，このノートブックから直接行うことができます。\n",
        "<!-- We are almost there, it is also necessary that you have `git lfs` installed. \n",
        "You can do it directly from this notebook by uncommenting the following cells: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vpo0MM-awXLS"
      },
      "outputs": [],
      "source": [
        "#!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRsyW8I5wXLS"
      },
      "outputs": [],
      "source": [
        "new_tokenizer.push_to_hub(\"my-new-shiny-tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWpBOttowXLS"
      },
      "source": [
        "これで，このマシンでトークン化器を再読み込みできるようになりました。\n",
        "<!-- The tokenizer can now be reloaded on this machine with: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eENhUkRRwXLS"
      },
      "outputs": [],
      "source": [
        "tok = new_tokenizer.from_pretrained(\"my-new-tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OANh2DctwXLT"
      },
      "source": [
        "また，どこからでも，自分の名前空間の後にスラッシュを付けて `push_to_hub` メソッドで指定した名前のリポ ID を使うことができます。\n",
        "例えば，以下のようになります。\n",
        "<!-- Or from anywhere using the repo ID, which is your namespace followed by a slash an the name you gave in the `push_to_hub` method, so for instance: -->\n",
        "\n",
        "```python\n",
        "tok = new_tokenizer.from_pretrained(\"sgugger/my-new-shiny-tokenizer\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9l3pUVIwXLT"
      },
      "source": [
        "さて，もし既存のものとは似て非なる新しいトークン化器を作って訓練したい場合は，🤗 Tokenizers library を使って一から構築する必要があります。\n",
        "<!-- Now if you want to create and a train a new tokenizer that doesn't look like anything in existence, you will need to build it from scratch using the 🤗 Tokenizers library. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei74bV3qwXLT"
      },
      "source": [
        "## ゼロからトークン化器を構築する\n",
        "<!-- ## Building your tokenizer from scratch -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrxonz_ywXLT"
      },
      "source": [
        "トークン化器をゼロから構築する方法を理解するために，🤗 Tokenizers ライブラリーとトークン化パイプラインにもう少し深潜する必要があります。\n",
        "このパイプラインはいくつかの段階を踏みます:\n",
        "<!-- To understand how to build your tokenizer from scratch, we have to dive a little bit more in the 🤗 Tokenizers library and the tokenization pipeline. This pipeline takes several steps:-->\n",
        "\n",
        "1. **正規化**: 最初の入力文字列に対して，すべての初期変換を実行します。\n",
        "たとえば，文を小文字にしたり，削除したり，あるいは一般的な Unicode 正規化処理を適用する必要がある場合，Normalizer を追加します。\n",
        "2.  **事前トークン化器**:  最初の入力文字列の分割を担当します。これは，元の文字列をどこでどのように事前分割するかを決定する要素です。\n",
        "最も単純な例は，単にスペースで分割することです。\n",
        "3. **モデル**: すべての下位トークンの発見と生成を担当します。\n",
        "この部分は学習可能で，入力データに大きく依存します。\n",
        "4. **事後処理**:  Transformers ベースの SoTA モデルの一部と互換性があるように，高度な構築機能を提供します。\n",
        "例えば BERT では，トークン化された文を [CLS] と [SEP] トークンで囲みます。\n",
        "\n",
        "<!--\n",
        "- **Normalization**: Executes all the initial transformations over the initial input string. For example when you need to lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer.\n",
        "- **Pre-tokenization**: In charge of splitting the initial input string. That's the component that decides where and how to pre-segment the origin string. The simplest example would be to simply split on spaces.\n",
        "- **Model**: Handles all the sub-token discovery and generation, this is the part that is trainable and really dependent of your input data.\n",
        "- **Post-Processing**: Provides advanced construction features to be compatible with some of the Transformers-based SoTA models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens. -->\n",
        "\n",
        "そして，その逆方向:\n",
        "<!-- And to go in the other direction:-->\n",
        "\n",
        "5. **復号化**:  トークン化された入力を元の文字列に逆写像します。符号化器は通常，前回使用した `PreTokenizer` に従って選択されます。\n",
        "<!-- - **Decoding**: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according to the `PreTokenizer` we used previously. -->\n",
        "\n",
        "モデルの学習のために 🤗 Tokenizers ライブラリは `Trainer` クラスを提供しており，これを利用することになります。\n",
        "<!-- For the training of the model, the 🤗 Tokenizers library provides a `Trainer` class that we will use. -->\n",
        "\n",
        "これらの構成要素をすべて組み合わせて，実用的なトークン化パイプラインを作成することができます。\n",
        "ここでは GPT-2，BERT, T5 (BPE, WordPiece, Unigram トークン化器の例)の 3 つのパイプラインの例を紹介します。\n",
        "<!-- All of these building blocks can be combined to create working tokenization pipelines. \n",
        "To give you some examples, we will show three full pipelines here: how to replicate GPT-2, BERT and T5 (which will give you an example of BPE, WordPiece and Unigram tokenizer). -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2PAX2bkwXLU"
      },
      "source": [
        "### BERT のような WordPiece モデル\n",
        "<!-- ### WordPiece model like BERT -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IttAHXlfwXLU"
      },
      "source": [
        "BERT の訓練に使用されるような WordPiece トークン化器を作成する方法を見てみましょう。\n",
        "最初の段階は，空の `WordPiece` モデルで `Tokenizer` を作成することです。\n",
        "<!-- Let's have a look at how we can create a WordPiece tokenizer like the one used for training BERT. \n",
        "The first step is to create a `Tokenizer` with an empty `WordPiece` model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYiW8zrLwXLU"
      },
      "outputs": [],
      "source": [
        "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(models.WordPiece(unl_token=\"[UNK]\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLa1Uz1HwXLU"
      },
      "source": [
        "この `tokenizer` はまだ学習する準備ができていません。\n",
        "正規化  (これはオプション) と 事前トークン化器  (入力を単語と呼ぶチャンクに分割する) という前処理の段階を追加する必要があるのです。\n",
        "トークンはこれらの単語の一部となる (ただし，それ以上大きくはできない)。\n",
        "<!-- This `tokenizer` is not ready for training yet. \n",
        "We have to add some preprocessing steps: the normalization (which is optional) and the pre-tokenizer, which will split inputs into the chunks we will call words. \n",
        "The tokens will then be part of those words (but can't be larger than that).-->\n",
        "\n",
        "BERT の場合，正規化は小文字になります。\n",
        "BERT は非常に人気のあるモデルであるため，独自の正規化器を備えています。\n",
        "<!-- In the case of BERT, the normalization is lowercasing. Since BERT is such a popular model, it has its own normalizer: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0cnU2XuwXLU"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKfLniMRwXLU"
      },
      "source": [
        "例えば，ここでは小文字にし，NFD の正規化を行い，アクセント記号を除去しています。\n",
        "<!-- If you want to customize it, you can use the existing blocks and compose them in a sequence: here for instance we lower case, apply NFD normalization and strip the accents: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-eFgH0LwXLU"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDMVpf8OwXLU"
      },
      "source": [
        "また，直接利用できる `BertPreTokenizer` もある。\n",
        "これは，空白と句読点を用いて事前トークン化を行います。\n",
        "<!-- There is also a `BertPreTokenizer` we can use directly. \n",
        "It pre-tokenizes using white space and punctuation: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcdPbnuowXLU"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QLahNgvwXLU"
      },
      "source": [
        "正則化器の場合と同様，いくつかの事前トークン化器を `Sequence` にまとめることができます。\n",
        "入力の前処理を簡単に確認したい場合は  `pre_tokenize_str` メソッドを呼び出すことができます。\n",
        "<!-- Like for the normalizer, we can combine several pre-tokenizers in a `Sequence`. \n",
        "If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-hmTg-QwXLU",
        "outputId": "eda46eff-d36c-4d0f-bf0b-9d34cef422f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This', (0, 4)),\n",
              " ('is', (5, 7)),\n",
              " ('an', (8, 10)),\n",
              " ('example', (11, 18)),\n",
              " ('!', (18, 19))]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uy0YRbIwXLU"
      },
      "source": [
        "事前トークン化器は文を単語に分割するだけでなく，オフセット，つまり元の文内の各単語の開始と終了を保持していることに注意してください。\n",
        "これにより，最終的なトークン化器は，各トークンを文のどの部分に由来するのかを照合できるようになります (質問応答やトークン分類課題で使用する機能です)。\n",
        "<!-- Note that the pre-tokenizer not only split the text into words but keeps the offsets, that is the beginning and start of each of those words inside the original text. \n",
        "This is what will allow the final tokenizer to be able to match each token to the part of the text that it comes from (a feature we use for question answering or token classification tasks).-->\n",
        "\n",
        "ここでトークン化器を学習します。 (パイプラインはまだ完成していませんが，事後処理器を構築するために学習済みのトークン化器が必要になります) このために `WordPieceTrainer` を使用します。\n",
        "ここで重要なのは，特殊なトークンはコーパスにはないので，トレーナーに渡すということです。\n",
        "<!-- We can now train our tokenizer (the pipeline is not entirely finished but we will need a trained tokenizer to build the post-processor), we use a `WordPieceTrainer` for that. \n",
        "The key thing to remember is to pass along the special tokens to the trainer, as they won't be seen in the corpus. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsZPUvZLwXLU"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWt3G1yEwXLU"
      },
      "source": [
        "実際にトークン化器を学習させるために，このメソッドは以前使用したものと同じように，いくつかのテキストファイルか，文のバッチの反復器 (イテレータ) を渡すことができます。\n",
        "<!-- To actually train the tokenizer, the method looks like what we used before: we can either pass some text files, or an iterator of batches of texts: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuPvBVoEwXLV"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aso-jqA3wXLV"
      },
      "source": [
        "トークン化器の学習が完了したので，次に事後処理器を定義します。\n",
        "CLS トークンを先頭に，SEP トークンを末尾に (単一文の場合)，または複数の SEP トークン (文の対の場合) を追加する必要がありま す。\n",
        "これを行うには [`TemplateProcessing`](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.processors.TemplateProcessing) を使います。\n",
        "これは CLS と SEP トークンの ID を知っている必要があります (これが学習を待った理由です)。\n",
        "<!-- Now that the tokenizer is trained, we can define the post-processor: \n",
        "we need to add the CLS token at the beginning and the SEP token at the end (for single sentences) or several SEP tokens (for pairs of sentences). \n",
        "We use a [`TemplateProcessing`](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.processors.TemplateProcessing) to do this, which requires to know the IDs of the CLS and SEP token (which is why we waited for the training).-->\n",
        "\n",
        "そこで，まず 2 つの特殊なトークンの ID を取得しましょう。\n",
        "<!-- So let's first grab the ids of the two special tokens: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xcmr0URYwXLV",
        "outputId": "1fc1a981-90c3-4819-fcd9-240314dbdf30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None None\n"
          ]
        }
      ],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "print(cls_token_id, sep_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNdV8oyDwXLV"
      },
      "source": [
        "そして，事後処理器の作り方は以下のとおりです。\n",
        "特殊なトークンを 1 つの文  (`$A`) または 2 つの文  (`$A` と `$B`) で構成する方法をテンプレートに記述する必要があります。\n",
        "また、`:` の後に続く数字は，各パートに付与するトークン・タイプの ID を示します。\n",
        "<!-- And here is how we can build our post processor. \n",
        "We have to indicate in the template how to organize the special tokens with one sentence (`$A`) or two sentences (`$A` and `$B`). The `:` followed by a number indicates the token type ID to give to each part. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsVMCm-IwXLV",
        "outputId": "afcc8250-d2d2-419d-ac7e-d8ec1dacd9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-ac4da1f8e850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     special_tokens=[\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"[CLS]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"[SEP]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     ],\n\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected Union[Tuple[str, int], Tuple[int, str], dict]"
          ]
        }
      ],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", cls_token_id),\n",
        "        (\"[SEP]\", sep_token_id),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08tUn6suwXLV"
      },
      "source": [
        "例えば，文の対を符号化することで，期待通りの結果が得られることを確認することができます。\n",
        "<!-- We can check we get the expected results by encoding a pair of sentences for instance: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3f0FpwmwXLV"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer.encode(\"This is one sentence.\", \"With this one we have a pair.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZcoAbsAwXLV"
      },
      "source": [
        "トークンを見て，特殊トークンが正しい位置に挿入されていることを確認することができるのです。\n",
        "<!-- We can look at the tokens to check the special tokens have been inserted in the right places: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irQC8wcBwXLV",
        "outputId": "189f663b-252d-46f7-9f06-867e5cafa9f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-24c405529c20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'encoding' is not defined"
          ]
        }
      ],
      "source": [
        "encoding.tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANYsD59QwXLV"
      },
      "source": [
        "And we can check the token type ids are correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WydA2_jlwXLV",
        "outputId": "3a466998-ef46-43ec-8525-aca5b9a86fac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding.type_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNHL1XlUwXLV"
      },
      "source": [
        "The last piece in this tokenizer is the decoder, we use a `WordPiece` decoder and indicate the special prefix `##`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f_MCpOvwXLV"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxKH5VzBwXLV"
      },
      "source": [
        "Now that our tokenizer is finished, we need to wrap it inside a Transformers object to be able to use it with the Transformers library. More specifically, we have to put it inside the class of tokenizer fast corresponding to the model we want to use, here a `BertTokenizerFast`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzZRCwzRwXLV"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "new_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN-qsrbuwXLW"
      },
      "source": [
        "And like before, we can use this tokenizer as a normal Transformers tokenizer, and use the `save_pretrained` or `push_to_hub` methods.\n",
        "\n",
        "If the tokenizer you are building does not match any class in Transformers because it's really special, you can wrap it in `PreTrainedTokenizerFast`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIKOzluJwXLW"
      },
      "source": [
        "### BPE model like GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScuPI1lYwXLW"
      },
      "source": [
        "Let's now have a look at how we can create a BPE tokenizer like the one used for training GPT-2. The first step is to create a `Tokenizer` with an empty `BPE` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aS-PiHnwXLW"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJAqY_RDwXLW"
      },
      "source": [
        "Like before, we have to add the optional normalization (not used in the case of GPT-2) and we need to specify a pre-tokenizer before training. In the case of GPT-2, the pre-tokenizer used is a byte level pre-tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA-PRus2wXLW"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDH3UzpswXLW"
      },
      "source": [
        "If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tmYGXGowXLW",
        "outputId": "bbd6f53b-4d2a-41f4-ef93-ce16f361a438"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('This', (0, 4)),\n",
              " ('Ġis', (4, 7)),\n",
              " ('Ġan', (7, 10)),\n",
              " ('Ġexample', (10, 18)),\n",
              " ('!', (18, 19))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiNEg6qKwXLW"
      },
      "source": [
        "We used the same default as for GPT-2 for the prefix space, so you can see that each word gets an initial `'Ġ'` added at the beginning, except the first one.\n",
        "\n",
        "We can now train our tokenizer! This time we use a `BpeTrainer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWg2LLDowXLX"
      },
      "outputs": [],
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9K7Bkn8wXLX"
      },
      "source": [
        "To finish the whole pipeline, we have to include the post-processor and decoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSrkuo4iwXLX"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
        "tokenizer.decoder = decoders.ByteLevel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF0duBCEwXLX"
      },
      "source": [
        "And like before, we finish by wrapping this in a Transformers tokenizer object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHdOm2ObwXLX"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "new_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fLEBH2NwXLX"
      },
      "source": [
        "### Unigram model like Albert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LgicDQhwXLX"
      },
      "source": [
        "Let's now have a look at how we can create a Unigram tokenizer like the one used for training T5. The first step is to create a `Tokenizer` with an empty `Unigram` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj5fBEsowXLX"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.Unigram())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKD9bu-GwXLX"
      },
      "source": [
        "Like before, we have to add the optional normalization (here some replaces and lower-casing) and we need to specify a pre-tokenizer before training. The pre-tokenizer used is a `Metaspace` pre-tokenizer: it replaces all spaces by a special character (defaulting to ▁) and then splits on that character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYgec1_lwXLX"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.Replace(\"``\", '\"'), normalizers.Replace(\"''\", '\"'), normalizers.Lowercase()]\n",
        ")\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q21dbMF4wXLX"
      },
      "source": [
        "If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwgzLV6jwXLX",
        "outputId": "3c49a1ff-8ed7-4c44-a667-9b75ba60ae37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('▁This', (0, 4)), ('▁is', (4, 7)), ('▁an', (7, 10)), ('▁example!', (10, 19))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SXp7g7vwXLX"
      },
      "source": [
        "You can see that each word gets an initial `▁` added at the beginning, as is usually done by sentencepiece.\n",
        "\n",
        "We can now train our tokenizer! This time we use a `UnigramTrainer`.\"We have to explicitely set the unknown token in this trainer otherwise it will forget it afterward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51hSnSjBwXLX"
      },
      "outputs": [],
      "source": [
        "trainer = trainers.UnigramTrainer(vocab_size=25000, special_tokens=[\"[CLS]\", \"[SEP]\", \"<unk>\", \"<pad>\", \"[MASK]\"], unk_token=\"<unk>\")\n",
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZrhiWoXwXLX"
      },
      "source": [
        "To finish the whole pipeline, we have to include the post-processor and decoder. The post-processor is very similar to what we saw with BERT, the decoder is just `Metaspace`, like for the pre-tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1iYTpwUwXLX"
      },
      "outputs": [],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dgz-40M_wXLX"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", cls_token_id),\n",
        "        (\"[SEP]\", sep_token_id),\n",
        "    ],\n",
        ")\n",
        "tokenizer.decoder = decoders.Metaspace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSI3c4SwwXLX"
      },
      "source": [
        "And like before, we finish by wrapping this in a Transformers tokenizer object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK17csMAwXLX"
      },
      "outputs": [],
      "source": [
        "from transformers import AlbertTokenizerFast\n",
        "\n",
        "new_tokenizer = AlbertTokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK_TOkUWwXLX"
      },
      "source": [
        "## Use your new tokenizer to train a language model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHR_iwx2wXLY"
      },
      "source": [
        "You can either use your new tokenizer in the language modeling from scratch notebook [Link to come] or use the `--tokenizer_name` argument in the [language modeling scripts](https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling) to use it there to train a model from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnKm4DbFwXLY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2022_0316Huggingface_tutorial_Train_your_tokenizer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}