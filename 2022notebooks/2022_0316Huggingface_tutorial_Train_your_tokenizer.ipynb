{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0316Huggingface_tutorial_Train_your_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "colabã§ã“ã®ãƒãƒ¼ãƒˆã‚’é–‹ãå ´åˆï¼ŒãŠãã‚‰ãğŸ¤— Transformers ã¨ ğŸ¤— Datasets ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚\n",
        "ä»¥ä¸‹ã®ã‚»ãƒ«ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’è§£é™¤ã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚\n",
        "<!-- If you're opening this Notebook on colab, you will probably need to install ğŸ¤— Transformers and ğŸ¤— Datasets. Uncomment the following cell and execute it: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFASsisvIrIb"
      },
      "source": [
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§é–‹ã„ã¦ã„ã‚‹å ´åˆï¼ŒDatasets ã®æœ€çµ‚ç‰ˆã‹ã‚‰ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ Transformers ã®ã‚½ãƒ¼ã‚¹ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç’°å¢ƒã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
        "<!-- If you're opening this notebook locally, make sure your environment has an install from the last version of Datasets and a source install of Transformers. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvVlwzZlwXLM"
      },
      "source": [
        "# ã‚¼ãƒ­ã‹ã‚‰ç‹¬è‡ªãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’è¨“ç·´\n",
        "<!-- # Training your own tokenizer from scratch -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVcbUt6TwXLM"
      },
      "source": [
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ï¼Œä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç”¨ã„ã¦ã‚¼ãƒ­ã‹ã‚‰ç‹¬è‡ªã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’å­¦ç¿’ã—ï¼Œãã‚Œã‚’ä½¿ã£ã¦ã‚¼ãƒ­ã‹ã‚‰è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹æ–¹æ³•ã‚’ã„ãã¤ã‹ç´¹ä»‹ã—ã¾ã™ã€‚\n",
        "<!-- In this notebook, we will see several ways to train your own tokenizer from scratch on a given corpus, so you can then use it to train a language model from scratch.-->\n",
        "\n",
        "ãªãœãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’è¨“ç·´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
        "ãã‚Œã¯ Transformer ã®ãƒ¢ãƒ‡ãƒ«ã¯ä¸‹ä½å˜èªå‹ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ã†ã“ã¨ãŒéå¸¸ã«å¤šãï¼Œã‚ãªãŸãŒä½¿ã£ã¦ã„ã‚‹ã‚³ãƒ¼ãƒ‘ã‚¹ã«ã‚ˆãå«ã¾ã‚Œã‚‹å˜èªã®éƒ¨åˆ†ã‚’è­˜åˆ¥ã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‰ã§ã™ã€‚\n",
        "Hugging Face ã‚³ãƒ¼ã‚¹ã® [tokenization chapter](https://huggingface.co/course/chapter2/4?fw=pt) ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã®ä¸€èˆ¬çš„ãªç´¹ä»‹ã‚’ [tokenizers summary](https://huggingface.co/transformers/tokenizer_summary.html) ã§ä¸‹ä½å˜èªå‹ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é•ã„ã‚’ã”è¦§ã«ãªã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n",
        "<!-- Why would you need to *train* a tokenizer? That's because Transformer models very often use subword tokenization algorithms, and they need to be trained to identify the parts of words that are often present in the corpus you are using. \n",
        "We recommend you take a look at the [tokenization chapter](https://huggingface.co/course/chapter2/4?fw=pt) of the Hugging Face course for a general introduction on tokenizers, and at the [tokenizers summary](https://huggingface.co/transformers/tokenizer_summary.html) for a look at the differences between the subword tokenization algorithms. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iMJuYOewXLM"
      },
      "source": [
        "## ã‚³ãƒ¼ãƒ‘ã‚¹ã®å–å¾—\n",
        "<!-- ## Getting a corpus -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbCYGOY4wXLM"
      },
      "source": [
        "ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’å­¦ç¿’ã•ã›ã‚‹ãŸã‚ã«ãƒ†ã‚­ã‚¹ãƒˆãŒå¿…è¦ã§ã™ã€‚\n",
        "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ [ğŸ¤— Datasets](https://github.com/huggingface/datasets) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "ã“ã‚Œã¯ `load_dataset` é–¢æ•°ã§ç°¡å˜ã«è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- We will need texts to train our tokenizer. \n",
        "We will use the [ğŸ¤— Datasets](https://github.com/huggingface/datasets) library to download our text data, which can be easily done with the `load_dataset` function: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdcewTzhwXLN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoEP_BZTwXLN"
      },
      "source": [
        "ã“ã®ä¾‹ã§ã¯ Wikitext-2 (4.5MB ã®æ–‡ã‚’å«ã‚€ã®ã§ï¼Œã“ã®ä¾‹ã§ã¯å­¦ç¿’ãŒé€Ÿã„) ã‚’ä½¿ã„ã¾ã™ãŒï¼Œå¥½ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã†ã“ã¨ãŒã§ãã¾ã™ (è‹±èªä»¥å¤–ãªã‚‰ï¼Œè¨€èªã‚‚å•ã„ã¾ã›ã‚“)ã€‚\n",
        "<!-- For this example, we will use Wikitext-2 (which contains 4.5MB of texts so training goes fast for our example) but you can use any dataset you want (and in any language, just not English). -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbPW_1yDwXLO"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KKAaXD0wXLO"
      },
      "source": [
        "36,718 æ–‡ã‹ã‚‰ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¦‹ã¾ã—ã‚‡ã†\n",
        "<!-- We can have a look at the dataset, which as 36,718 texts: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMVRcF23wXLP"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCZZ1kR1wXLP"
      },
      "source": [
        "ä»»æ„ã®è¦ç´ ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã«ã¯ï¼Œãã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ‡å®šã™ã‚‹ã ã‘ã§ã™ã€‚\n",
        "<!-- To access an element, we just have to provide its index: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "846fOOFPwXLP"
      },
      "outputs": [],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a5y7WWAwXLP"
      },
      "source": [
        "ã‚¹ãƒ©ã‚¤ã‚¹ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n",
        "ãã®å ´åˆã¯ï¼Œã‚­ãƒ¼ãŒ  `text` ã§å€¤ãŒ  text ã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹è¾æ›¸ã‚’å–å¾—ã—ã¾ã™ã€‚\n",
        "<!-- We can also access a slice directly, in which case we get a dictionary with the key `\"text\"` and a list of texts as value: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AnuopwpwXLP"
      },
      "outputs": [],
      "source": [
        "dataset[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpbOu2hLwXLQ"
      },
      "source": [
        "ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å­¦ç¿’ã•ã›ã‚‹ãŸã‚ã® API ã¯ï¼Œæ–‡ã®ãƒãƒƒãƒã®åå¾©å™¨ (iterator)ï¼Œä¾‹ãˆã°æ–‡ã®ãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚\n",
        "<!-- The API to train our tokenizer will require an iterator of batch of texts, for instance a list of list of texts: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz_c2UtQwXLQ"
      },
      "outputs": [],
      "source": [
        "batch_size = 1000\n",
        "all_texts = [dataset[i : i + batch_size][\"text\"] for i in range(0, len(dataset), batch_size)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZgc9K_VwXLQ"
      },
      "source": [
        "ã™ã¹ã¦ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚€ã“ã¨ã‚’é¿ã‘ã‚‹ãŸã‚ (Datasets ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯è¦ç´ ã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿æŒã—ï¼Œè¦æ±‚ã•ã‚ŒãŸã¨ãã ã‘ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚€ãŸã‚)ï¼ŒPython ã®åå¾©å™¨ã‚’å®šç¾©ã—ã¦ã„ã¾ã™ã€‚\n",
        "ã“ã‚Œã¯ç‰¹ã«å·¨å¤§ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æŒã£ã¦ã„ã‚‹å ´åˆã«æœ‰åŠ¹ã§ã™ã€‚\n",
        "<!-- To avoid loading everything into memory (since the Datasets library keeps the element on disk and only load them in memory when requested), we define a Python iterator. \n",
        "This is particularly useful if you have a huge dataset: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGyeaoTtwXLQ"
      },
      "outputs": [],
      "source": [
        "def batch_iterator():\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        yield dataset[i : i + batch_size][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFr5z9KTwXLQ"
      },
      "source": [
        "ã§ã¯ï¼Œã“ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç”¨ã„ã¦ï¼Œã©ã®ã‚ˆã†ã«æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’å­¦ç¿’ã•ã›ã‚‹ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "æœ€åˆã® API ã¯ï¼Œæ—¢å­˜ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ä½¿ã£ã¦ï¼Œ1 è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ã‚ãªãŸã®ã‚³ãƒ¼ãƒ‘ã‚¹ã«æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’å­¦ç¿’ã•ã›ã‚‹ã‚‚ã®ã§ï¼Œã‚‚ã†ä¸€ã¤ã¯ï¼Œå®Ÿéš›ã«ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’æ§‹ç¯‰ã—ï¼Œã™ã¹ã¦ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ã‚‚ã®ã§ã™\n",
        "<!-- Now let's see how we can use this corpus to train a new tokenizer! \n",
        "There are two APIs to do this: the first one uses an existing tokenizer and will train a new version of it on your corpus in one line of code, the second is to actually build your tokenizer block by block, so lets you customize every step! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_jT4wiSwXLQ"
      },
      "source": [
        "## æ—¢å­˜ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’åˆ©ç”¨ã™ã‚‹\n",
        "<!-- ## Using an existing tokenizer -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eL8Jy2MwXLQ"
      },
      "source": [
        "æ—¢å­˜ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã¨å…¨ãåŒã˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’å­¦ç¿’ã•ã›ãŸã„å ´åˆã¯  `train_new_from_iterator` API ã‚’ä½¿ç”¨ã™ã‚Œã°ã‚ˆã„ã§ã™ã€‚\n",
        "ä¾‹ãˆã° Wikitext-2 ã§ GPT-2 tokenzier ã®æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’åŒã˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å­¦ç¿’ã—ã¦ã¿ã‚ˆã†ã€‚\n",
        "<!-- If you want to train a tokenizer with the exact same algorithms and parameters as an existing one, you can just use the `train_new_from_iterator` API. \n",
        "For instance, let's train a new version of the GPT-2 tokenzier on Wikitext-2 using the same tokenization algorithm.-->\n",
        "\n",
        "ã¾ãšï¼Œãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä½¿ã„ãŸã„ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "<!-- First we need to load the tokenizer we want to use as a model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f0xooENwXLR"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8A36l9pwXLR"
      },
      "source": [
        "é¸ã‚“ã ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒ *fast* ç‰ˆã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ (ğŸ¤— Tokenizers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã‚ˆã£ã¦ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã•ã‚Œã¾ã™)ã€‚\n",
        "ãã†ã§ãªã‘ã‚Œã°ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®æ®‹ã‚Šã®éƒ¨åˆ†ã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã€‚\n",
        "<!-- Make sure that the tokenizer you picked as a *fast* version (backed by the ğŸ¤— Tokenizers library) otherwise the rest of the notebook will not run: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9Hd2wKnwXLR"
      },
      "outputs": [],
      "source": [
        "tokenizer.is_fast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg4ne-2dwXLR"
      },
      "source": [
        "æ¬¡ã«ï¼Œå­¦ç¿’ç”¨ã‚³ãƒ¼ãƒ‘ã‚¹ (ãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼Œã¾ãŸã¯å…ˆã»ã©å®šç¾©ã—ãŸåå¾©å™¨) ã‚’ `train_new_from_iterator` ãƒ¡ã‚½ãƒƒãƒ‰ã«æ¸¡ã—ã¾ã™ã€‚\n",
        "ã¾ãŸï¼Œä½¿ç”¨ã™ã‚‹èªå½™è¦æ¨¡ã‚‚æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™:\n",
        "<!-- Then we feed the training corpus (either the list of list or the iterator we defined earlier) to the `train_new_from_iterator` method. \n",
        "We also have to specify the vocabulary size we want to use: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX2fNIdbwXLR"
      },
      "outputs": [],
      "source": [
        "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=25000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeKMHu-2wXLR"
      },
      "source": [
        "ã¨ï¼Œã“ã‚Œã ã‘ã§ã™ã€‚\n",
        "Rust ã«æ”¯ãˆã‚‰ã‚ŒãŸğŸ¤— Tokenizers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãŠã‹ã’ã§ï¼Œå­¦ç¿’ã¯ã¨ã¦ã‚‚é€Ÿãé€²ã¿ã¾ã™ã€‚\n",
        "<!-- And that's all there is to it! The training goes very fast thanks to the ğŸ¤— Tokenizers library, backed by Rust.-->\n",
        "\n",
        "ã“ã‚Œã§æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ãŒãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’è¡Œã†æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚\n",
        "ã„ã¤ã‚‚ã®ã‚ˆã†ã«å…¥åŠ›æ–‡ã‚’ä¸ãˆã¦ãã ã•ã„ã€‚\n",
        "<!-- You now have a new tokenizer ready to preprocess your data and train a language model. You can feed it input texts as usual: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B_QziVfwXLR",
        "outputId": "3e5e53c8-3b63-4fe4-a735-5191a163dc99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[], [238, 8576, 9441, 2987, 238, 252], [], [4657, 74, 4762, 826, 8576, 428, 466, 609, 6881, 412, 204, 9441, 311, 2746, 466, 10816, 168, 99, 150, 192, 112, 14328, 3983, 112, 4446, 94, 18288, 4446, 193, 3983, 98, 3983, 22171, 95, 19, 201, 6374, 209, 8576, 218, 198, 3455, 1972, 428, 310, 201, 5099, 3242, 227, 281, 8576, 9441, 2987, 2553, 1759, 201, 301, 196, 13996, 1496, 277, 2330, 1464, 674, 1898, 307, 742, 3541, 225, 7514, 14, 54, 719, 274, 198, 4777, 15522, 209, 19895, 221, 1341, 1633, 221, 1759, 201, 322, 301, 198, 1368, 674, 221, 198, 8576, 843, 209, 2468, 1795, 223, 198, 1049, 9595, 218, 13996, 225, 1563, 277, 582, 6493, 281, 457, 14371, 201, 198, 1422, 3373, 7452, 227, 198, 455, 674, 225, 4687, 198, 239, 21976, 239, 201, 196, 21657, 1680, 3773, 5591, 198, 4196, 218, 4679, 427, 661, 198, 3518, 1288, 220, 1051, 516, 889, 3947, 1922, 2500, 225, 390, 2065, 744, 872, 198, 7592, 3773, 239, 1975, 251, 208, 89, 22351, 239, 209, 252], [261, 674, 959, 1921, 221, 1462, 201, 7600, 547, 196, 1178, 4753, 218, 198, 630, 3591, 263, 8576, 9441, 1180, 209, 1831, 322, 7568, 198, 3621, 2240, 218, 198, 843, 201, 322, 471, 9575, 5291, 16591, 967, 201, 781, 281, 1815, 198, 674, 604, 10344, 1252, 274, 843, 664, 3147, 320, 209, 13290, 8751, 8124, 2528, 6023, 74, 235, 225, 7445, 10040, 17384, 241, 11487, 8950, 857, 1835, 340, 1382, 22582, 201, 1008, 296, 8576, 9441, 1180, 2436, 21134, 5337, 19463, 5161, 209, 240, 1178, 927, 218, 3776, 8650, 198, 3355, 209, 261, 674, 268, 83, 2511, 3472, 258, 8288, 307, 1010, 268, 78, 209, 252]], 'attention_mask': [[], [1, 1, 1, 1, 1, 1], [], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "new_tokenizer(dataset[:5][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFcw2OvPwXLS"
      },
      "source": [
        "ä¿å­˜ã¯ `save_pretrained` ãƒ¡ã‚½ãƒƒãƒ‰ã§ãƒ­ãƒ¼ã‚«ãƒ«ã«è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- You can save it locally with the `save_pretrained` method: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpKf_bI8wXLS"
      },
      "outputs": [],
      "source": [
        "new_tokenizer.save_pretrained(\"my-new-tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESL6fgpqwXLS"
      },
      "source": [
        "ã‚ã‚‹ã„ã¯ [Hugging Face Hub](https://huggingface.co/models) ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¦ï¼Œã©ã“ã‹ã‚‰ã§ã‚‚ãã®æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚\n",
        "ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ `huggingface-cli login` ã‚’å®Ÿè¡Œã™ã‚‹ã‹ï¼Œä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ï¼Œèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã ã‘ã§ã™ã€‚\n",
        "<!-- Or even push it to the [Hugging Face Hub](https://huggingface.co/models) to use that new tokenzier from anywhere. \n",
        "Just make sure you have your authentication token stored by executing `huggingface-cli login` in a terminal or executing the following cell: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FiFFK_NwXLS"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Go8CWrVwXLS"
      },
      "source": [
        "`git lfs` ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå¿…è¦ã§ã™ã€‚\n",
        "ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’ã‚¢ãƒ³ã‚³ãƒ¡ãƒ³ãƒˆã™ã‚Œã°ï¼Œã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‹ã‚‰ç›´æ¥è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- We are almost there, it is also necessary that you have `git lfs` installed. \n",
        "You can do it directly from this notebook by uncommenting the following cells: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vpo0MM-awXLS"
      },
      "outputs": [],
      "source": [
        "#!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRsyW8I5wXLS"
      },
      "outputs": [],
      "source": [
        "new_tokenizer.push_to_hub(\"my-new-shiny-tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWpBOttowXLS"
      },
      "source": [
        "ã“ã‚Œã§ï¼Œã“ã®ãƒã‚·ãƒ³ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’å†èª­ã¿è¾¼ã¿ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n",
        "<!-- The tokenizer can now be reloaded on this machine with: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eENhUkRRwXLS"
      },
      "outputs": [],
      "source": [
        "tok = new_tokenizer.from_pretrained(\"my-new-tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OANh2DctwXLT"
      },
      "source": [
        "ã¾ãŸï¼Œã©ã“ã‹ã‚‰ã§ã‚‚ï¼Œè‡ªåˆ†ã®åå‰ç©ºé–“ã®å¾Œã«ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ä»˜ã‘ã¦ `push_to_hub` ãƒ¡ã‚½ãƒƒãƒ‰ã§æŒ‡å®šã—ãŸåå‰ã®ãƒªãƒ ID ã‚’ä½¿ã†ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ä¾‹ãˆã°ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n",
        "<!-- Or from anywhere using the repo ID, which is your namespace followed by a slash an the name you gave in the `push_to_hub` method, so for instance: -->\n",
        "\n",
        "```python\n",
        "tok = new_tokenizer.from_pretrained(\"sgugger/my-new-shiny-tokenizer\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9l3pUVIwXLT"
      },
      "source": [
        "ã•ã¦ï¼Œã‚‚ã—æ—¢å­˜ã®ã‚‚ã®ã¨ã¯ä¼¼ã¦éãªã‚‹æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ä½œã£ã¦è¨“ç·´ã—ãŸã„å ´åˆã¯ï¼ŒğŸ¤— Tokenizers library ã‚’ä½¿ã£ã¦ä¸€ã‹ã‚‰æ§‹ç¯‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "<!-- Now if you want to create and a train a new tokenizer that doesn't look like anything in existence, you will need to build it from scratch using the ğŸ¤— Tokenizers library. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei74bV3qwXLT"
      },
      "source": [
        "## ã‚¼ãƒ­ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’æ§‹ç¯‰ã™ã‚‹\n",
        "<!-- ## Building your tokenizer from scratch -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrxonz_ywXLT"
      },
      "source": [
        "ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ã‚¼ãƒ­ã‹ã‚‰æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ï¼ŒğŸ¤— Tokenizers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ¼ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ã‚‚ã†å°‘ã—æ·±æ½œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "ã“ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã„ãã¤ã‹ã®æ®µéšã‚’è¸ã¿ã¾ã™:\n",
        "<!-- To understand how to build your tokenizer from scratch, we have to dive a little bit more in the ğŸ¤— Tokenizers library and the tokenization pipeline. This pipeline takes several steps:-->\n",
        "\n",
        "1. **æ­£è¦åŒ–**: æœ€åˆã®å…¥åŠ›æ–‡å­—åˆ—ã«å¯¾ã—ã¦ï¼Œã™ã¹ã¦ã®åˆæœŸå¤‰æ›ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
        "ãŸã¨ãˆã°ï¼Œæ–‡ã‚’å°æ–‡å­—ã«ã—ãŸã‚Šï¼Œå‰Šé™¤ã—ãŸã‚Šï¼Œã‚ã‚‹ã„ã¯ä¸€èˆ¬çš„ãª Unicode æ­£è¦åŒ–å‡¦ç†ã‚’é©ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆï¼ŒNormalizer ã‚’è¿½åŠ ã—ã¾ã™ã€‚\n",
        "2.  **äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨**:  æœ€åˆã®å…¥åŠ›æ–‡å­—åˆ—ã®åˆ†å‰²ã‚’æ‹…å½“ã—ã¾ã™ã€‚ã“ã‚Œã¯ï¼Œå…ƒã®æ–‡å­—åˆ—ã‚’ã©ã“ã§ã©ã®ã‚ˆã†ã«äº‹å‰åˆ†å‰²ã™ã‚‹ã‹ã‚’æ±ºå®šã™ã‚‹è¦ç´ ã§ã™ã€‚\n",
        "æœ€ã‚‚å˜ç´”ãªä¾‹ã¯ï¼Œå˜ã«ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²ã™ã‚‹ã“ã¨ã§ã™ã€‚\n",
        "3. **ãƒ¢ãƒ‡ãƒ«**: ã™ã¹ã¦ã®ä¸‹ä½ãƒˆãƒ¼ã‚¯ãƒ³ã®ç™ºè¦‹ã¨ç”Ÿæˆã‚’æ‹…å½“ã—ã¾ã™ã€‚\n",
        "ã“ã®éƒ¨åˆ†ã¯å­¦ç¿’å¯èƒ½ã§ï¼Œå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«å¤§ããä¾å­˜ã—ã¾ã™ã€‚\n",
        "4. **äº‹å¾Œå‡¦ç†**:  Transformers ãƒ™ãƒ¼ã‚¹ã® SoTA ãƒ¢ãƒ‡ãƒ«ã®ä¸€éƒ¨ã¨äº’æ›æ€§ãŒã‚ã‚‹ã‚ˆã†ã«ï¼Œé«˜åº¦ãªæ§‹ç¯‰æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚\n",
        "ä¾‹ãˆã° BERT ã§ã¯ï¼Œãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸæ–‡ã‚’ [CLS] ã¨ [SEP] ãƒˆãƒ¼ã‚¯ãƒ³ã§å›²ã¿ã¾ã™ã€‚\n",
        "\n",
        "<!--\n",
        "- **Normalization**: Executes all the initial transformations over the initial input string. For example when you need to lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer.\n",
        "- **Pre-tokenization**: In charge of splitting the initial input string. That's the component that decides where and how to pre-segment the origin string. The simplest example would be to simply split on spaces.\n",
        "- **Model**: Handles all the sub-token discovery and generation, this is the part that is trainable and really dependent of your input data.\n",
        "- **Post-Processing**: Provides advanced construction features to be compatible with some of the Transformers-based SoTA models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens. -->\n",
        "\n",
        "ãã—ã¦ï¼Œãã®é€†æ–¹å‘:\n",
        "<!-- And to go in the other direction:-->\n",
        "\n",
        "5. **å¾©å·åŒ–**:  ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸå…¥åŠ›ã‚’å…ƒã®æ–‡å­—åˆ—ã«é€†å†™åƒã—ã¾ã™ã€‚ç¬¦å·åŒ–å™¨ã¯é€šå¸¸ï¼Œå‰å›ä½¿ç”¨ã—ãŸ `PreTokenizer` ã«å¾“ã£ã¦é¸æŠã•ã‚Œã¾ã™ã€‚\n",
        "<!-- - **Decoding**: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according to the `PreTokenizer` we used previously. -->\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã®ãŸã‚ã« ğŸ¤— Tokenizers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ `Trainer` ã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã¦ãŠã‚Šï¼Œã“ã‚Œã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚\n",
        "<!-- For the training of the model, the ğŸ¤— Tokenizers library provides a `Trainer` class that we will use. -->\n",
        "\n",
        "ã“ã‚Œã‚‰ã®æ§‹æˆè¦ç´ ã‚’ã™ã¹ã¦çµ„ã¿åˆã‚ã›ã¦ï¼Œå®Ÿç”¨çš„ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ã“ã“ã§ã¯ GPT-2ï¼ŒBERT, T5 (BPE, WordPiece, Unigram ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã®ä¾‹)ã® 3 ã¤ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¾‹ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚\n",
        "<!-- All of these building blocks can be combined to create working tokenization pipelines. \n",
        "To give you some examples, we will show three full pipelines here: how to replicate GPT-2, BERT and T5 (which will give you an example of BPE, WordPiece and Unigram tokenizer). -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2PAX2bkwXLU"
      },
      "source": [
        "### BERT ã®ã‚ˆã†ãª WordPiece ãƒ¢ãƒ‡ãƒ«\n",
        "<!-- ### WordPiece model like BERT -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IttAHXlfwXLU"
      },
      "source": [
        "BERT ã®è¨“ç·´ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚ˆã†ãª WordPiece ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "æœ€åˆã®æ®µéšã¯ï¼Œç©ºã® `WordPiece` ãƒ¢ãƒ‡ãƒ«ã§ `Tokenizer` ã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã™ã€‚\n",
        "<!-- Let's have a look at how we can create a WordPiece tokenizer like the one used for training BERT. \n",
        "The first step is to create a `Tokenizer` with an empty `WordPiece` model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYiW8zrLwXLU"
      },
      "outputs": [],
      "source": [
        "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(models.WordPiece(unl_token=\"[UNK]\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLa1Uz1HwXLU"
      },
      "source": [
        "ã“ã® `tokenizer` ã¯ã¾ã å­¦ç¿’ã™ã‚‹æº–å‚™ãŒã§ãã¦ã„ã¾ã›ã‚“ã€‚\n",
        "æ­£è¦åŒ–  (ã“ã‚Œã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³) ã¨ äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨  (å…¥åŠ›ã‚’å˜èªã¨å‘¼ã¶ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹) ã¨ã„ã†å‰å‡¦ç†ã®æ®µéšã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã®ã§ã™ã€‚\n",
        "ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã“ã‚Œã‚‰ã®å˜èªã®ä¸€éƒ¨ã¨ãªã‚‹ (ãŸã ã—ï¼Œãã‚Œä»¥ä¸Šå¤§ããã¯ã§ããªã„)ã€‚\n",
        "<!-- This `tokenizer` is not ready for training yet. \n",
        "We have to add some preprocessing steps: the normalization (which is optional) and the pre-tokenizer, which will split inputs into the chunks we will call words. \n",
        "The tokens will then be part of those words (but can't be larger than that).-->\n",
        "\n",
        "BERT ã®å ´åˆï¼Œæ­£è¦åŒ–ã¯å°æ–‡å­—ã«ãªã‚Šã¾ã™ã€‚\n",
        "BERT ã¯éå¸¸ã«äººæ°—ã®ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ãŸã‚ï¼Œç‹¬è‡ªã®æ­£è¦åŒ–å™¨ã‚’å‚™ãˆã¦ã„ã¾ã™ã€‚\n",
        "<!-- In the case of BERT, the normalization is lowercasing. Since BERT is such a popular model, it has its own normalizer: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0cnU2XuwXLU"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKfLniMRwXLU"
      },
      "source": [
        "ä¾‹ãˆã°ï¼Œã“ã“ã§ã¯å°æ–‡å­—ã«ã—ï¼ŒNFD ã®æ­£è¦åŒ–ã‚’è¡Œã„ï¼Œã‚¢ã‚¯ã‚»ãƒ³ãƒˆè¨˜å·ã‚’é™¤å»ã—ã¦ã„ã¾ã™ã€‚\n",
        "<!-- If you want to customize it, you can use the existing blocks and compose them in a sequence: here for instance we lower case, apply NFD normalization and strip the accents: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-eFgH0LwXLU"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDMVpf8OwXLU"
      },
      "source": [
        "ã¾ãŸï¼Œç›´æ¥åˆ©ç”¨ã§ãã‚‹ `BertPreTokenizer` ã‚‚ã‚ã‚‹ã€‚\n",
        "ã“ã‚Œã¯ï¼Œç©ºç™½ã¨å¥èª­ç‚¹ã‚’ç”¨ã„ã¦äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’è¡Œã„ã¾ã™ã€‚\n",
        "<!-- There is also a `BertPreTokenizer` we can use directly. \n",
        "It pre-tokenizes using white space and punctuation: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcdPbnuowXLU"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QLahNgvwXLU"
      },
      "source": [
        "æ­£å‰‡åŒ–å™¨ã®å ´åˆã¨åŒæ§˜ï¼Œã„ãã¤ã‹ã®äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ `Sequence` ã«ã¾ã¨ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "å…¥åŠ›ã®å‰å‡¦ç†ã‚’ç°¡å˜ã«ç¢ºèªã—ãŸã„å ´åˆã¯  `pre_tokenize_str` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- Like for the normalizer, we can combine several pre-tokenizers in a `Sequence`. \n",
        "If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-hmTg-QwXLU",
        "outputId": "eda46eff-d36c-4d0f-bf0b-9d34cef422f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This', (0, 4)),\n",
              " ('is', (5, 7)),\n",
              " ('an', (8, 10)),\n",
              " ('example', (11, 18)),\n",
              " ('!', (18, 19))]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uy0YRbIwXLU"
      },
      "source": [
        "äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã¯æ–‡ã‚’å˜èªã«åˆ†å‰²ã™ã‚‹ã ã‘ã§ãªãï¼Œã‚ªãƒ•ã‚»ãƒƒãƒˆï¼Œã¤ã¾ã‚Šå…ƒã®æ–‡å†…ã®å„å˜èªã®é–‹å§‹ã¨çµ‚äº†ã‚’ä¿æŒã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n",
        "ã“ã‚Œã«ã‚ˆã‚Šï¼Œæœ€çµ‚çš„ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã¯ï¼Œå„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ–‡ã®ã©ã®éƒ¨åˆ†ã«ç”±æ¥ã™ã‚‹ã®ã‹ã‚’ç…§åˆã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ (è³ªå•å¿œç­”ã‚„ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡èª²é¡Œã§ä½¿ç”¨ã™ã‚‹æ©Ÿèƒ½ã§ã™)ã€‚\n",
        "<!-- Note that the pre-tokenizer not only split the text into words but keeps the offsets, that is the beginning and start of each of those words inside the original text. \n",
        "This is what will allow the final tokenizer to be able to match each token to the part of the text that it comes from (a feature we use for question answering or token classification tasks).-->\n",
        "\n",
        "ã“ã“ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’å­¦ç¿’ã—ã¾ã™ã€‚ (ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã¾ã å®Œæˆã—ã¦ã„ã¾ã›ã‚“ãŒï¼Œäº‹å¾Œå‡¦ç†å™¨ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã«å­¦ç¿’æ¸ˆã¿ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ãŒå¿…è¦ã«ãªã‚Šã¾ã™) ã“ã®ãŸã‚ã« `WordPieceTrainer` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "ã“ã“ã§é‡è¦ãªã®ã¯ï¼Œç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ã¯ã‚³ãƒ¼ãƒ‘ã‚¹ã«ã¯ãªã„ã®ã§ï¼Œãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã«æ¸¡ã™ã¨ã„ã†ã“ã¨ã§ã™ã€‚\n",
        "<!-- We can now train our tokenizer (the pipeline is not entirely finished but we will need a trained tokenizer to build the post-processor), we use a `WordPieceTrainer` for that. \n",
        "The key thing to remember is to pass along the special tokens to the trainer, as they won't be seen in the corpus. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsZPUvZLwXLU"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWt3G1yEwXLU"
      },
      "source": [
        "å®Ÿéš›ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’å­¦ç¿’ã•ã›ã‚‹ãŸã‚ã«ï¼Œã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ä»¥å‰ä½¿ç”¨ã—ãŸã‚‚ã®ã¨åŒã˜ã‚ˆã†ã«ï¼Œã„ãã¤ã‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ï¼Œæ–‡ã®ãƒãƒƒãƒã®åå¾©å™¨ (ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿) ã‚’æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- To actually train the tokenizer, the method looks like what we used before: we can either pass some text files, or an iterator of batches of texts: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuPvBVoEwXLV"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aso-jqA3wXLV"
      },
      "source": [
        "ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã®å­¦ç¿’ãŒå®Œäº†ã—ãŸã®ã§ï¼Œæ¬¡ã«äº‹å¾Œå‡¦ç†å™¨ã‚’å®šç¾©ã—ã¾ã™ã€‚\n",
        "CLS ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…ˆé ­ã«ï¼ŒSEP ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æœ«å°¾ã« (å˜ä¸€æ–‡ã®å ´åˆ)ï¼Œã¾ãŸã¯è¤‡æ•°ã® SEP ãƒˆãƒ¼ã‚¯ãƒ³ (æ–‡ã®å¯¾ã®å ´åˆ) ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ ã™ã€‚\n",
        "ã“ã‚Œã‚’è¡Œã†ã«ã¯ [`TemplateProcessing`](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.processors.TemplateProcessing) ã‚’ä½¿ã„ã¾ã™ã€‚\n",
        "ã“ã‚Œã¯ CLS ã¨ SEP ãƒˆãƒ¼ã‚¯ãƒ³ã® ID ã‚’çŸ¥ã£ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ (ã“ã‚ŒãŒå­¦ç¿’ã‚’å¾…ã£ãŸç†ç”±ã§ã™)ã€‚\n",
        "<!-- Now that the tokenizer is trained, we can define the post-processor: \n",
        "we need to add the CLS token at the beginning and the SEP token at the end (for single sentences) or several SEP tokens (for pairs of sentences). \n",
        "We use a [`TemplateProcessing`](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.processors.TemplateProcessing) to do this, which requires to know the IDs of the CLS and SEP token (which is why we waited for the training).-->\n",
        "\n",
        "ãã“ã§ï¼Œã¾ãš 2 ã¤ã®ç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ã® ID ã‚’å–å¾—ã—ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- So let's first grab the ids of the two special tokens: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xcmr0URYwXLV",
        "outputId": "1fc1a981-90c3-4819-fcd9-240314dbdf30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None None\n"
          ]
        }
      ],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "print(cls_token_id, sep_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNdV8oyDwXLV"
      },
      "source": [
        "ãã—ã¦ï¼Œäº‹å¾Œå‡¦ç†å™¨ã®ä½œã‚Šæ–¹ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã§ã™ã€‚\n",
        "ç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’ 1 ã¤ã®æ–‡  (`$A`) ã¾ãŸã¯ 2 ã¤ã®æ–‡  (`$A` ã¨ `$B`) ã§æ§‹æˆã™ã‚‹æ–¹æ³•ã‚’ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«è¨˜è¿°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "ã¾ãŸã€`:` ã®å¾Œã«ç¶šãæ•°å­—ã¯ï¼Œå„ãƒ‘ãƒ¼ãƒˆã«ä»˜ä¸ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãƒ»ã‚¿ã‚¤ãƒ—ã® ID ã‚’ç¤ºã—ã¾ã™ã€‚\n",
        "<!-- And here is how we can build our post processor. \n",
        "We have to indicate in the template how to organize the special tokens with one sentence (`$A`) or two sentences (`$A` and `$B`). The `:` followed by a number indicates the token type ID to give to each part. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsVMCm-IwXLV",
        "outputId": "afcc8250-d2d2-419d-ac7e-d8ec1dacd9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-ac4da1f8e850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     special_tokens=[\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"[CLS]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"[SEP]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     ],\n\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected Union[Tuple[str, int], Tuple[int, str], dict]"
          ]
        }
      ],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", cls_token_id),\n",
        "        (\"[SEP]\", sep_token_id),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08tUn6suwXLV"
      },
      "source": [
        "ä¾‹ãˆã°ï¼Œæ–‡ã®å¯¾ã‚’ç¬¦å·åŒ–ã™ã‚‹ã“ã¨ã§ï¼ŒæœŸå¾…é€šã‚Šã®çµæœãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- We can check we get the expected results by encoding a pair of sentences for instance: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3f0FpwmwXLV"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer.encode(\"This is one sentence.\", \"With this one we have a pair.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZcoAbsAwXLV"
      },
      "source": [
        "ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ã¦ï¼Œç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ãŒæ­£ã—ã„ä½ç½®ã«æŒ¿å…¥ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã®ã§ã™ã€‚\n",
        "<!-- We can look at the tokens to check the special tokens have been inserted in the right places: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irQC8wcBwXLV",
        "outputId": "189f663b-252d-46f7-9f06-867e5cafa9f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-24c405529c20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'encoding' is not defined"
          ]
        }
      ],
      "source": [
        "encoding.tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANYsD59QwXLV"
      },
      "source": [
        "And we can check the token type ids are correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WydA2_jlwXLV",
        "outputId": "3a466998-ef46-43ec-8525-aca5b9a86fac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding.type_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNHL1XlUwXLV"
      },
      "source": [
        "The last piece in this tokenizer is the decoder, we use a `WordPiece` decoder and indicate the special prefix `##`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f_MCpOvwXLV"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxKH5VzBwXLV"
      },
      "source": [
        "Now that our tokenizer is finished, we need to wrap it inside a Transformers object to be able to use it with the Transformers library. More specifically, we have to put it inside the class of tokenizer fast corresponding to the model we want to use, here a `BertTokenizerFast`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzZRCwzRwXLV"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "new_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN-qsrbuwXLW"
      },
      "source": [
        "And like before, we can use this tokenizer as a normal Transformers tokenizer, and use the `save_pretrained` or `push_to_hub` methods.\n",
        "\n",
        "If the tokenizer you are building does not match any class in Transformers because it's really special, you can wrap it in `PreTrainedTokenizerFast`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIKOzluJwXLW"
      },
      "source": [
        "### BPE model like GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScuPI1lYwXLW"
      },
      "source": [
        "Let's now have a look at how we can create a BPE tokenizer like the one used for training GPT-2. The first step is to create a `Tokenizer` with an empty `BPE` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aS-PiHnwXLW"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJAqY_RDwXLW"
      },
      "source": [
        "Like before, we have to add the optional normalization (not used in the case of GPT-2) and we need to specify a pre-tokenizer before training. In the case of GPT-2, the pre-tokenizer used is a byte level pre-tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA-PRus2wXLW"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDH3UzpswXLW"
      },
      "source": [
        "If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tmYGXGowXLW",
        "outputId": "bbd6f53b-4d2a-41f4-ef93-ce16f361a438"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('This', (0, 4)),\n",
              " ('Ä is', (4, 7)),\n",
              " ('Ä an', (7, 10)),\n",
              " ('Ä example', (10, 18)),\n",
              " ('!', (18, 19))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiNEg6qKwXLW"
      },
      "source": [
        "We used the same default as for GPT-2 for the prefix space, so you can see that each word gets an initial `'Ä '` added at the beginning, except the first one.\n",
        "\n",
        "We can now train our tokenizer! This time we use a `BpeTrainer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWg2LLDowXLX"
      },
      "outputs": [],
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9K7Bkn8wXLX"
      },
      "source": [
        "To finish the whole pipeline, we have to include the post-processor and decoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSrkuo4iwXLX"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
        "tokenizer.decoder = decoders.ByteLevel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF0duBCEwXLX"
      },
      "source": [
        "And like before, we finish by wrapping this in a Transformers tokenizer object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHdOm2ObwXLX"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "new_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fLEBH2NwXLX"
      },
      "source": [
        "### Unigram model like Albert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LgicDQhwXLX"
      },
      "source": [
        "Let's now have a look at how we can create a Unigram tokenizer like the one used for training T5. The first step is to create a `Tokenizer` with an empty `Unigram` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj5fBEsowXLX"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.Unigram())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKD9bu-GwXLX"
      },
      "source": [
        "Like before, we have to add the optional normalization (here some replaces and lower-casing) and we need to specify a pre-tokenizer before training. The pre-tokenizer used is a `Metaspace` pre-tokenizer: it replaces all spaces by a special character (defaulting to â–) and then splits on that character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYgec1_lwXLX"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.Replace(\"``\", '\"'), normalizers.Replace(\"''\", '\"'), normalizers.Lowercase()]\n",
        ")\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q21dbMF4wXLX"
      },
      "source": [
        "If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwgzLV6jwXLX",
        "outputId": "3c49a1ff-8ed7-4c44-a667-9b75ba60ae37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('â–This', (0, 4)), ('â–is', (4, 7)), ('â–an', (7, 10)), ('â–example!', (10, 19))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SXp7g7vwXLX"
      },
      "source": [
        "You can see that each word gets an initial `â–` added at the beginning, as is usually done by sentencepiece.\n",
        "\n",
        "We can now train our tokenizer! This time we use a `UnigramTrainer`.\"We have to explicitely set the unknown token in this trainer otherwise it will forget it afterward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51hSnSjBwXLX"
      },
      "outputs": [],
      "source": [
        "trainer = trainers.UnigramTrainer(vocab_size=25000, special_tokens=[\"[CLS]\", \"[SEP]\", \"<unk>\", \"<pad>\", \"[MASK]\"], unk_token=\"<unk>\")\n",
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZrhiWoXwXLX"
      },
      "source": [
        "To finish the whole pipeline, we have to include the post-processor and decoder. The post-processor is very similar to what we saw with BERT, the decoder is just `Metaspace`, like for the pre-tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1iYTpwUwXLX"
      },
      "outputs": [],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dgz-40M_wXLX"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", cls_token_id),\n",
        "        (\"[SEP]\", sep_token_id),\n",
        "    ],\n",
        ")\n",
        "tokenizer.decoder = decoders.Metaspace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSI3c4SwwXLX"
      },
      "source": [
        "And like before, we finish by wrapping this in a Transformers tokenizer object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK17csMAwXLX"
      },
      "outputs": [],
      "source": [
        "from transformers import AlbertTokenizerFast\n",
        "\n",
        "new_tokenizer = AlbertTokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK_TOkUWwXLX"
      },
      "source": [
        "## Use your new tokenizer to train a language model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHR_iwx2wXLY"
      },
      "source": [
        "You can either use your new tokenizer in the language modeling from scratch notebook [Link to come] or use the `--tokenizer_name` argument in the [language modeling scripts](https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling) to use it there to train a model from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnKm4DbFwXLY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2022_0316Huggingface_tutorial_Train_your_tokenizer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}