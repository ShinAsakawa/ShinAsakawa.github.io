{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0627sala_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8086cbc-98d1-4ac8-8442-c3cc22584949",
      "metadata": {
        "id": "e8086cbc-98d1-4ac8-8442-c3cc22584949"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "\n",
        "    import nltk\n",
        "    nltk.download('wordnet')    \n",
        "    nltk.download('omw-1.4')    \n",
        "\n",
        "    import os\n",
        "    if os.path.exists('ccap'):\n",
        "        import shutil\n",
        "        shutil.rmtree('ccap')\n",
        "    !git clone https://github.com/project-ccap/ccap.git\n",
        "\n",
        "   \n",
        "# try:    \n",
        "#     import japanize_matplotlib\n",
        "# except ImportError:\n",
        "#     !pip install japanize_matplotlib\n",
        "    \n",
        "from ccap import salaDataset\n",
        "sala = salaDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1541b875-79c5-42aa-9225-a99b92b8b2ee",
      "metadata": {
        "id": "1541b875-79c5-42aa-9225-a99b92b8b2ee"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25512a0e-b8d2-4c23-a1c7-278b5333806e",
      "metadata": {
        "id": "25512a0e-b8d2-4c23-a1c7-278b5333806e"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "_image_size = 224\n",
        "_mean = [0.485, 0.456, 0.406]\n",
        "_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "train_trans = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomCrop(_image_size),\n",
        "    #transforms.RandomRotation(degrees=(-10,10))\n",
        "    transforms.RandomAffine(degrees=(-15,+15), scale=(0.6,1.4)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(.3, .3, .3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(_mean, _std),\n",
        "])\n",
        "\n",
        "val_trans = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(_image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(_mean, _std),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99d04937-069c-4933-9ed0-05ea909773a4",
      "metadata": {
        "id": "99d04937-069c-4933-9ed0-05ea909773a4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a354a9-ec9f-4dc2-aef4-78e83b6b276c",
      "metadata": {
        "id": "b3a354a9-ec9f-4dc2-aef4-78e83b6b276c"
      },
      "outputs": [],
      "source": [
        "sala(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "597dca3b-fbe9-4e12-890a-acde5346e336",
      "metadata": {
        "id": "597dca3b-fbe9-4e12-890a-acde5346e336"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import PIL\n",
        "\n",
        "#from torchvision.datasets.folder import ImageFolder, default_loader\n",
        "#from torchvision.datasets.utils import download_url, check_integrity\n",
        "\n",
        "class SALADataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    SALA の画像データ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sala=salaDataset(),\n",
        "        #root_path:str='./ccap/data',  #/sala_imgs',\n",
        "        transform=train_trans,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        data = {}\n",
        "        for idx in range(sala.__len__()):\n",
        "            img_fname, label = sala(idx)\n",
        "            data[idx] = {'fname': img_fname,\n",
        "                         'label': label,\n",
        "                        }\n",
        "        self.data = data\n",
        "        \n",
        "        self.idx2name = list(data.keys())\n",
        "        self.name2idx = {x:i for i, x in enumerate(self.idx2name)}\n",
        "        self.transform = transform\n",
        "            \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, x):\n",
        "        name = self.idx2name[x]\n",
        "        img_fname = self.data[name]['fname']\n",
        "        img = PIL.Image.open(img_fname)\n",
        "        _img = train_trans(img)\n",
        "        return _img, x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "094eee4a-766e-4e91-8e40-035d3cca9554",
      "metadata": {
        "id": "094eee4a-766e-4e91-8e40-035d3cca9554"
      },
      "outputs": [],
      "source": [
        "train_dataset = SALADataset()\n",
        "val_dataset = SALADataset(transform=val_trans)\n",
        "print(train_dataset.__getitem__(0)[0].size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1553371c-7b7c-41c9-ae37-13bab366dc9a",
      "metadata": {
        "id": "1553371c-7b7c-41c9-ae37-13bab366dc9a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html#sphx-glr-auto-examples-plot-visualization-utils-py\n",
        "\n",
        "N = np.random.choice(train_dataset.__len__())\n",
        "img = train_dataset.__getitem__(N)[0]\n",
        "_img = img.permute(1,2,0).clone()\n",
        "_img = img.permute(1,2,0).clone().numpy()\n",
        "print(f'_img.shape:{_img.shape}', \n",
        "      f'_img.max():{_img.max():.2f}'\n",
        "      f' _img.min():{ _img.min():.2f}')\n",
        "#_img = torchvision.transforms.functional.to_pil_image(_img)\n",
        "\n",
        "plt.imshow(_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18616340-7f88-4f30-9eb4-a52288dcbd33",
      "metadata": {
        "id": "18616340-7f88-4f30-9eb4-a52288dcbd33"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 32\n",
        "\n",
        "train_dl = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "val_dl = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f71d9a74-6b4b-4160-a4ef-7dfbfdbbf107",
      "metadata": {
        "id": "f71d9a74-6b4b-4160-a4ef-7dfbfdbbf107"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "\n",
        "model = models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b4aa51-3343-4137-821c-415e5533e91d",
      "metadata": {
        "id": "b2b4aa51-3343-4137-821c-415e5533e91d"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import torchsummary\n",
        "except ImportError:\n",
        "    !pip install torhcsummary\n",
        "# import torchsummary\n",
        "# torchsummary.summary(model, (3, 224, 224), device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49308a71-5f75-476c-ba1e-a452927a5dca",
      "metadata": {
        "id": "49308a71-5f75-476c-ba1e-a452927a5dca"
      },
      "outputs": [],
      "source": [
        "# %load my_train_helper.py\n",
        "def get_trainable(model_params):\n",
        "    return (p for p in model_params if p.requires_grad)\n",
        "\n",
        "\n",
        "def get_frozen(model_params):\n",
        "    return (p for p in model_params if not p.requires_grad)\n",
        "\n",
        "\n",
        "def all_trainable(model_params):\n",
        "    return all(p.requires_grad for p in model_params)\n",
        "\n",
        "\n",
        "def all_frozen(model_params):\n",
        "    return all(not p.requires_grad for p in model_params)\n",
        "\n",
        "\n",
        "def freeze_all(model_params):\n",
        "    for param in model_params:\n",
        "        param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef4b83ac-82ff-4420-bb08-81d2e791b3cb",
      "metadata": {
        "id": "ef4b83ac-82ff-4420-bb08-81d2e791b3cb"
      },
      "outputs": [],
      "source": [
        "# Freeze all parameters manually\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# Or use our convenient functions from before\n",
        "freeze_all(model.parameters())\n",
        "assert all_frozen(model.parameters())    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ed4bd11-58f1-40c8-ac43-0e77faeae9e2",
      "metadata": {
        "id": "2ed4bd11-58f1-40c8-ac43-0e77faeae9e2"
      },
      "outputs": [],
      "source": [
        "#help(transforms.RandomRotation)\n",
        "#help(transforms.RandomAffine)\n",
        "#transforms.RandomAffine(degrees=[-15,-5,5,10], scale=(0.8,1.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f207dce8-a2de-4751-b040-c75edeb3c971",
      "metadata": {
        "id": "f207dce8-a2de-4751-b040-c75edeb3c971"
      },
      "source": [
        "最終直下層を入れ替えて， `requires_grad=True` に設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2232d08f-c4dd-4399-b105-40839ab5f8f3",
      "metadata": {
        "id": "2232d08f-c4dd-4399-b105-40839ab5f8f3"
      },
      "outputs": [],
      "source": [
        "n_classes = train_dataset.__len__()\n",
        "model.fc = nn.Linear(512, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65492cc3-957f-465d-a31e-a2db6695b836",
      "metadata": {
        "id": "65492cc3-957f-465d-a31e-a2db6695b836"
      },
      "outputs": [],
      "source": [
        "def get_model(n_classes=n_classes):\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    freeze_all(model.parameters())\n",
        "    model.fc = nn.Linear(512, n_classes)\n",
        "    model = model.to(device)\n",
        "    return model\n",
        "\n",
        "model = get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33595cd9-d0dd-4244-850f-a327511e0d20",
      "metadata": {
        "id": "33595cd9-d0dd-4244-850f-a327511e0d20"
      },
      "outputs": [],
      "source": [
        "#model;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "001e61d2-dbcc-4548-8f3e-ebf73af9671f",
      "metadata": {
        "id": "001e61d2-dbcc-4548-8f3e-ebf73af9671f"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f3ffdeb-d548-4740-9130-e64132eaea25",
      "metadata": {
        "id": "9f3ffdeb-d548-4740-9130-e64132eaea25"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    get_trainable(model.parameters()),\n",
        "    lr=0.001,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a00929-033d-4cf8-8201-0263b0c05287",
      "metadata": {
        "id": "11a00929-033d-4cf8-8201-0263b0c05287"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "N_EPOCHS = 10\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    total_loss, n_correct, n_samples = 0.0, 0, 0\n",
        "    for batch_i, (X, y) in enumerate(train_dl):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        y_ = model(X)\n",
        "        loss = criterion(y_, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        _, y_label_ = torch.max(y_, 1)\n",
        "        n_correct += (y_label_ == y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "        n_samples += X.shape[0]\n",
        "    \n",
        "    print(\n",
        "        f\"エポック {epoch+1:2d}/{N_EPOCHS:2d} \"\n",
        "        f\"訓練損失: {total_loss / n_samples:.3f} \"\n",
        "        f\"訓練精度: {n_correct / n_samples * 100:.2f}%\"\n",
        "    )\n",
        "    \n",
        "    \n",
        "    model.eval()\n",
        "    total_loss, n_correct, n_samples = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_dl:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_ = model(X)\n",
        "            \n",
        "            _, y_label_ = torch.max(y_, 1)\n",
        "            n_correct += (y_label_ == y).sum().item()\n",
        "            loss = criterion(y_, y)\n",
        "            total_loss += loss.item() * X.shape[0]\n",
        "            n_samples += X.shape[0]\n",
        "\n",
        "    print(\n",
        "        f\"エポック {epoch+1:2d}/{N_EPOCHS:2d} \"\n",
        "        f\"検証損失: {total_loss / n_samples:.3f} \"\n",
        "        f\"検証精度: {n_correct / n_samples * 100:.2f}%\"\n",
        "    )\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zY2-iQgMmdva"
      },
      "id": "zY2-iQgMmdva",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "2022_0627sala_demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}