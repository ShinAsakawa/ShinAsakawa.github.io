{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMAzne1JNdTVsJMsi6eAbBk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1002T5_demo_filling_blank_question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 「はごろも」をもちいた T5 による文章穴埋め問題作成の試み\n",
        "- datafile: `hagoromo-data-20180918.xlsx` \n",
        "- date: 2022_1002\n",
        "- author: 浅川伸一\n",
        "- filename: `2022_1002T5_demo_filling_blank_question.ipynb`\n"
      ],
      "metadata": {
        "id": "lrQT_-JqI-kE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6ZPdkMOluJt"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "try:\n",
        "    import bit\n",
        "except ImportError:\n",
        "    !pip install --upgrade 'fugashi[ipadic]' > /dev/null 2>&1\n",
        "    !pip install --upgrade 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    !python -m unidic download\n",
        "    !pip install --upgrade ipadic > /dev/null 2>&1\n",
        "    !pip install --upgrade sentencepiece > /dev/null 2>&1\n",
        "    !pip install --upgrade transformers > /dev/null 2>&1\n",
        "\n",
        "    !pip install 'konoha[all]' > /dev/null 2>&1\n",
        "    !pip install --upgrade termcolor > /dev/null 2>&1\n",
        "    !pip install --upgrade jaconv  > /dev/null 2>&1      \n",
        "    !pip install ipynbname --upgrade > /dev/null 2>&1\n",
        "    !git clone https://github.com/ShinAsakawa/bit.git\n",
        "    import bit\n",
        "\n",
        "isColab = bit.isColab\n",
        "HOME = bit.HOME"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if isColab:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # ここで `hagoromo-data-20180918.xlsx` を指定してアップロード\n",
        "    data_dir = '.'\n",
        "else:\n",
        "    data_dir = os.path.join(HOME, 'study/2022jlpt')\n"
      ],
      "metadata": {
        "id": "Ao4aSp9_mbDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import jaconv  \n",
        "hagoromo_fname = 'hagoromo-data-20180918.xlsx'\n",
        "hag = pd.read_excel(hagoromo_fname)\n",
        "hag"
      ],
      "metadata": {
        "id": "wUeTyLgBnPi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "import jaconv\n",
        "\n",
        "from konoha import SentenceTokenizer\n",
        "splitter = SentenceTokenizer()\n",
        "\n",
        "_hag = hag['例文'].to_list()\n",
        "\n",
        "hag_sentences = []\n",
        "max_len = 0\n",
        "min_len = 30000\n",
        "for i, l in enumerate(_hag):\n",
        "    if isinstance(l, str):\n",
        "        \n",
        "        _l = jaconv.normalize(l)\n",
        "        \n",
        "        # hagoromo データには '/' と '／' とが混在して用いられているようだ\n",
        "        # もしかしたら意味があるのかも知れないが，現時点では問い合わせていない。\n",
        "        # そのためデータ前処理として '/' を '／' に置換して文の区切りとして用いる\n",
        "        _l = _l.replace('/','／') \n",
        "        \n",
        "        #_l = _l.replace('\\n','')\n",
        "        \n",
        "        # 上と同じく ',' と '、' とが混在して用いられているようなので '、' に統一\n",
        "        _l = _l.replace(',','、')  \n",
        "        \n",
        "        _l2 = _l.split('／') \n",
        "        _l3_ = []\n",
        "        for _l4 in _l2:\n",
        "            for _l5 in splitter.tokenize(_l4):\n",
        "                _l3_.append(_l5)\n",
        "            \n",
        "        # 一つのエクセルセル内に複数の文が登録されていので分割\n",
        "        #_l2 = splitter.tokenize(_l)\n",
        "        for _l3 in _l3_:\n",
        "            \n",
        "            len_l = len(_l3)\n",
        "            \n",
        "            if len_l >= 1:\n",
        "                hag_sentences.append(_l3)\n",
        "                if len_l > max_len:\n",
        "                    max_len = len_l\n",
        "                    print(f'line:{i:5d}:{_l3} max_len:{max_len}')\n",
        "                if len_l < min_len:\n",
        "                    min_len = len_l\n",
        "                    print(f'line:{i:5d}:{_l3} min_len:{min_len}')\n",
        "    elif isinstance(l, float):\n",
        "        continue\n",
        "    else:\n",
        "        print(colored(f'({i},{l},{type(l)}',\"red\",attrs=['bold']))"
      ],
      "metadata": {
        "id": "0WF5xwEwnh35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in hag_sentences[:5]:\n",
        "    print(x)"
      ],
      "metadata": {
        "id": "LOatdNmTpIgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, RobertaForMaskedLM\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-roberta-base\")\n",
        "tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n",
        "\n",
        "# load model\n",
        "model = RobertaForMaskedLM.from_pretrained(\"rinna/japanese-roberta-base\")\n",
        "model = model.eval()\n"
      ],
      "metadata": {
        "id": "7IY8au6X3Mc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用上の注意\n",
        "\n",
        "* `[CLS]` を使用すること。`[CLS]` はモデルの学習時に使用されるため，マスクされたトークンを予測するには，必ず文の前に `[CLS]` トークンを追加して，モデルが正しく符号化できるようにする。\n",
        "* トークン化した後に `[MASK]` を使用する。入力文字列に直接 [MASK] を入力するのと，トークン化した後に `[MASK]` で置き換えるのとでは、トークンの並びが異なるので，予測結果も異なる。\n",
        "トークン化後に `[MASK]` を使用する方がよい。\n",
        "これは，モデルがどのように事前学習されたかと一致するためである。\n",
        "しかし Huggingface Inference API は入力文字列に `[MASK]` をタイプすることしかサポートしておらず，よりロバストな予測を生成することができない。\n",
        "* `position_ids` を明示的に引数として与える。\n",
        "`Roberta` モデルでは，モデルに対して `position_ids` が与えられない場合 Huggingface 版の transformers は自動的に `position_ids` を構築する。だが， 0 ではなく `padding_idx` から作成する。これでは `rinna/japanese-roberta-base` では期待通りに動作しない。\n",
        "なぜなら対応するトークナイザーの `padding_idx` が 0 ではないからである。\n",
        "そのため，必ず自分で `position_ids` を `constrcut` して，位置 ID 0 から開始するようにすること。"
      ],
      "metadata": {
        "id": "k5Zadosc3KNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_predict(\n",
        "    full_text:str, \n",
        "    blank:str,\n",
        "    top_k:int=10,\n",
        "    model=model,\n",
        "    ):\n",
        "    model = model.eval()                \n",
        "    \n",
        "    text = '[CLS]' + full_text\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    \n",
        "    # convert to ids\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    \n",
        "    masked_tokens = tokenizer.tokenize(blank)[1:]\n",
        "    masked_pos = []\n",
        "    for masked_token in masked_tokens:\n",
        "        #print(f'masked_token:{masked_token}')\n",
        "        try:\n",
        "            if tokens.index(masked_token):\n",
        "                pos = tokens.index(masked_token)\n",
        "        except ValueError:\n",
        "            pos = -1\n",
        "        if pos != -1:\n",
        "            token_ids[pos] = tokenizer.mask_token_id\n",
        "            masked_pos.append(pos)\n",
        "\n",
        "    print(f'トークン ID をもう一度トークンに戻す:{tokenizer.convert_ids_to_tokens(token_ids)}')\n",
        "            \n",
        "    # convert to tensor\n",
        "    token_tensor = torch.LongTensor([token_ids])\n",
        "\n",
        "    position_ids = list(range(0, token_tensor.size(1)))\n",
        "    position_id_tensor = torch.LongTensor([position_ids])\n",
        "    \n",
        "    # マスクに対応する上位 top_k 候補を得る\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=token_tensor, \n",
        "                        position_ids=position_id_tensor)\n",
        "    for _masked_pos in masked_pos:\n",
        "        predictions = outputs[0][0, _masked_pos].topk(10)\n",
        "        print(f'_masked_pos:{_masked_pos}')\n",
        "        for i, index_t in enumerate(predictions.indices):\n",
        "            index = index_t.item()\n",
        "            token = tokenizer.convert_ids_to_tokens([index])[0]\n",
        "            print(f'\\t{i}:{token}', end=\" \")\n",
        "        print()                            \n",
        "    #print(tokens)\n",
        "\n",
        "gpt_predict(\n",
        "    full_text='このパソコンは誰でも使えますが，コピーは有料です', \n",
        "    blank='有料',\n",
        "    model=model)\n",
        "\n",
        "gpt_predict(\n",
        "    full_text='次々に新しいゲームが作られる。', \n",
        "    blank='次々に',\n",
        "    model=model)\n"
      ],
      "metadata": {
        "id": "PQ5JoiUyE6XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JPcm9PiRE_cG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}