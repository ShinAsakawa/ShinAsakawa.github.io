{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022_0607bert_classify.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOcVGlyIkFRtul9p7VMMCUI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0607bert_classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 認知科学会用 BERT 再検討ファイル\n",
        "\n",
        "- date: 2022_0607\n",
        "- author: 浅川伸一"
      ],
      "metadata": {
        "id": "uFCn0FHYM8uG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s5JN4k6Evs8"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "hostname = platform.node().split('.')[0]\n",
        "if hostname == 'Sinope':\n",
        "    HOME = '/Users/_asakawa'\n",
        "else:\n",
        "    HOME = '/Users/asakawa'\n",
        "\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install --upgrade xlrd\n",
        "\n",
        "    !pip install unidic-lite\n",
        "    !pip install --upgrade 'fugashi[unidic-lite]'\n",
        "    #!pip install --upgrade ipadic\n",
        "    #!pip install --upgrade 'fugashi[ipadic]'\n",
        "    !pip install --upgrade 'fugashi[unidic]'\n",
        "    !python -m unidic download\n",
        "    !pip install --upgrade transformers\n",
        "    \n",
        "    !pip install --upgrade termcolor\n",
        "    !pip install --upgrade jaconv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from transformers import BertJapaneseTokenizer\n",
        "#from transformers import BertForMaskedLM\n",
        "from transformers import BertForNextSentencePrediction\n",
        "\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
        "model_nsp = BertForNextSentencePrediction.from_pretrained('cl-tohoku/bert-base-japanese')\n",
        "\n",
        "# GPU が利用可能であれば利用する\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model_nsp.to(device)\n",
        "\n",
        "prompt = \"行く川の流れ絶えずして\"\n",
        "#next_sentence = \"しかも元の水にあらず。\"\n",
        "next_sentence = \"吾輩は猫である。\"\n",
        "\n",
        "encoding = tokenizer.encode_plus(prompt, next_sentence, return_tensors='pt')\n",
        "outputs = model_nsp(**encoding)[0]\n",
        "softmax = F.softmax(outputs, dim=1)\n",
        "# このソフトマックスは，2 つの変数間でなされる。\n",
        "# すなわち，次の文章であると予測すると 0 番目の確率が高くなり，反対に 1 番目の確率が低くなる\n",
        "for p in softmax.detach().tolist()[0]:\n",
        "    print(f'{p:.4f}')"
      ],
      "metadata": {
        "id": "KvV3dgVzE2Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 次文予測とは，与えられた文に対して，どの程度の文が次の文になるかを予測する作業である。\n",
        "- この場合 \"The child came home from school.\" が与えられた文であり \"He played soccer after school.\" が次の文であるかどうかを予測しようとしている。\n",
        "- そのために BERT トークナイザが自動的に文の間に [SEP] トークンを挿入し，2 文の区切りを表し，特定の Bert For Next Sentence Prediction モデルが，その文が次の文であるかどうかの 2 値を予測する。\n",
        "- Bert は 2 つの値をテンソルで返す。\n",
        "- 最初の値は 2 番目の文が最初の文の続きであるかどうかを表し，2 番目の値は 2 番目の文がランダムな並びか最初の文の続きでないかを表す。\n",
        "- 言語モデリングとは異なり BERT の語彙のソフトマックスを計算しようとしているわけではないので，ロジットを取得することはない。\n",
        "- 我々は，次の文予測のための BERT が返す 2 つの値のソフトマックスを計算しようとしているだけで，どちらの値が最も高い確率値を持っているかを見ることができ，これは第 2 文が第 1 文にとって良い次の文であるかを表すことになる。\n",
        "- ソフトマックスの値を得たら，それをプリントアウトすることで簡単にテンソルを見ることができる。"
      ],
      "metadata": {
        "id": "VD4Xsm9sMHDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face は事前に学習されたモデルを持つ課題については，その特定のモデルをダウンロード/インポートする必要があるように設定されている。\n",
        "この場合，私たちは Bert For Masked Language Modeling モデルをダウンロードしなければなりませんが，トークナイザは前節で述べたように，すべての異なるモデルに対して同じものである。\n",
        "\n",
        "- マスク言語モデリングは，マスクトークンを任意の位置に挿入し，その位置に入る最適な単語候補を予測することで機能する。\n",
        "- マスクトークンは，上記のように，入力の希望する位置に連結して挿入すればよい。\n",
        "- Bert Model for Masked Language Modeling は，その単語を置き換える最適な単語/トークンを語彙として予測する。\n",
        "- logits は BERT の出力にソフトマックス活性化関数が適用される前の BERT モデルの出力である。\n",
        "- ロジットを取得するためには，モデルを初期化するときにパラメータで `return_dict = True` を指定する必要があり，そうしないと上記のコードはコンパイルエラーになる。\n",
        "- BERT Model に入力エンコーディングを渡した後，テンソルを返す \n",
        "- `output.logits` を指定するだけで `logits` を得ることができ，この後ようやく `logits` にソフトマックス活性化関数を適用することができる。 \n",
        "- BERT の出力にソフトマックスを適用することで BERT の語彙の各単語の確率分布を得ることができる。\n",
        "- より高い確率値を持つ単語はマスクトークンのより良い置換候補となる。\n",
        "- マスクトークンを置き換えるための BERT の語彙内のすべての単語のソフトマックス値のテンソルを取得するために `torch.where()` を使用して取得したマスクトークンのインデックスを指定することができる。\n",
        "- この例ではマスクトークンの置換候補単語の上位 10 個を取得しているので (パラメータを適宜調整すれば 10 個以上取得できる)，与えられたテンソル中の上位 $k$ 個の値を取得できる\n",
        "- `torch.topk()` 関数を使い，その上位 $k$ 個の値を含むテンソルを返している。\n",
        "- この後はテンソルを繰り返し処理し，文中のマスクトークンを候補トークンに置き換えるだけなので，比較的簡単な処理となる。"
      ],
      "metadata": {
        "id": "aKSPn2jiLl4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from transformers import BertJapaneseTokenizer\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "#model_name = 'cl-tohoku/bert-base-japanese'      # 東北大学乾研による 日本語 BERT 実装\n",
        "model_name = 'cl-tohoku/bert-base-japanese-v2'\n",
        "\n",
        "tknz = BertJapaneseTokenizer.from_pretrained(model_name, mecab_dic='unidic')\n",
        "mlm  = BertForMaskedLM.from_pretrained(model_name, return_dict=True)\n",
        "\n",
        "# # GPU が利用可能であれば利用する\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "mlm.to(device)\n",
        "\n",
        "text = \"吾輩は猫である。\" + tknz.mask_token + \"はまだない。\"\n",
        "_input = tknz.encode_plus(text, return_tensors=\"pt\")\n",
        "mask_index = torch.where(_input[\"input_ids\"][0] == tknz.mask_token_id)\n",
        "output = mlm(**_input)\n",
        "logits = output.logits\n",
        "softmax = F.softmax(logits, dim = -1)\n",
        "mask_word = softmax[0, mask_index, :]\n",
        "topN = 10\n",
        "tokens = torch.topk(mask_word, topN, dim=1)[1][0]\n",
        "for token in tokens:\n",
        "    word = tknz.convert_ids_to_tokens(token.detach().item())\n",
        "    sent_pred = text.replace(tknz.mask_token, word)\n",
        "    print(sent_pred)\n"
      ],
      "metadata": {
        "id": "yocU9ZW8E4ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "classify_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1742)\n",
        "print(classify_model.classifier)\n",
        "# Linear(in_features=768, out_features=1742, bias=True)\n",
        "\n",
        "# `from_pretrained` 時に `num_labels` を指定する。\n",
        "# これにより，任意のクラス数の分類器にできる。\n",
        "# デフォルトでは 2 クラス分類器\n",
        "# tokenizerと同様キャッシュダウンロードになる。\n",
        "# なので保存したい場合は下記のようにする。\n",
        "model_saved_fname = '2022_0607bert_classify_model.pt'\n",
        "classify_model.save_pretrained(model_saved_fname) # save\n",
        "classify_model2 = BertForSequenceClassification.from_pretrained(model_saved_fname) # load"
      ],
      "metadata": {
        "id": "ysp6dfVaE7Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2021/Jan 近藤先生からいただいたオノマトペ辞典のデータの読み込み\n",
        "#'日本語オノマトペ辞典4500より.xls' は著作権の問題があり，公にできません。\n",
        "# そのため Google Colab での解法，ローカルファイルよりアップロードしてください\n",
        "import os\n",
        "import pandas as pd\n",
        "import jaconv\n",
        "\n",
        "if isColab:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # ここで `日本語オノマトペ辞典4500より.xls` を指定してアップロードする\n",
        "    data_dir = '.'\n",
        "else:\n",
        "    data_dir = os.path.join(HOME, 'study/2021ccap/notebooks')\n",
        "\n",
        "\n",
        "onomatopea_excel = '2021-0325日本語オノマトペ辞典4500より.xls'\n",
        "onmtp2761 = pd.read_excel(os.path.join(data_dir, onomatopea_excel), sheet_name='2761語')\n",
        "\n",
        "#すべてカタカナ表記にしてデータとして利用する場合\n",
        "#`日本語オノマトペ辞典4500` はすべてひらがな表記だが，一般にオノマトペはカタカナ表記されることが多いはず\n",
        "#onomatopea = list(sorted(set([jaconv.hira2kata(o) for o in onmtp2761['オノマトペ']])))\n",
        "\n",
        "# Mac と Windows の表記の相違を吸収\n",
        "onomatopea = list(sorted(set([jaconv.normalize(o) for o in onmtp2761['オノマトペ']])))\n",
        "print(f'データファイル名: {os.path.join(data_dir, onomatopea_excel)}\\n',\n",
        "      f'オノマトペ単語総数: len(onomatopea):{len(onomatopea)}')\n",
        "\n",
        "# トークナイザ の修正，実際には onomatopea 単語リストを引数に指定して `add_tokens()` を呼び出すだけ\n",
        "# ただし，語彙数 tknz.vocab は変更されない。追加された語彙，本コードの場合はオノマトペは，\n",
        "# `tknz.added_tokens_encoder` と `tknz1.added_tokens_decoder` に反映されているためである\n",
        "num_added = tknz.add_tokens(onomatopea)\n",
        "print(f'追加されたトークン数:{num_added}/オノマトペ数:{len(onomatopea)}') \n",
        "#model_onmt.resize_token_embeddings(len(tknz))\n",
        "#model_orig.resize_token_embeddings(len(tknz))\n",
        "classify_model.resize_token_embeddings(len(tknz))\n",
        "classify_model2.resize_token_embeddings(len(tknz))\n",
        "\n",
        "print(f' len(tknz):{len(tknz)}\\n', \n",
        "      f'len(tknz.vocab):{len(tknz.vocab)}\\n',  # 一見すると，この数字からオノマトペが追加されていないように見える。\n",
        "      f'tknz.vocab_size:{tknz.vocab_size}')    # 駄菓子菓子，下で見るように，正しく動作しているように見受けられる\n",
        "\n",
        "print('# 確認用')\n",
        "for w in onomatopea[-5:]:\n",
        "    idx = tknz.convert_tokens_to_ids(w)\n",
        "    w_ = tknz.convert_ids_to_tokens(idx)\n",
        "    print(f'単語:{w}(id:{idx}) -> token:{w_}')"
      ],
      "metadata": {
        "id": "hpB9xE39E9hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import jaconv\n",
        "\n",
        "if isColab:\n",
        "    uploaded = files.upload()  # ここで `2022_0531onomatope11.xlsx` をアップロードする\n",
        "    data_dir = '.'\n",
        "else:\n",
        "    data_dir = '..'\n",
        "\n",
        "df = pandas.read_excel(os.path.join(data_dir, '2022_0531onomatope11.xlsx'))\n",
        "# 若干の入力ミスと思われる部分を修正した。オリジナルは onomatope11(1).xlsx\n",
        "bunn = df['bunn'].to_list()\n",
        "onmtp = df['onomatope'].to_list()\n",
        "_bunn = [jaconv.normalize(line, 'NFKC') for line in bunn]\n",
        "_onmtp = [jaconv.normalize(line, 'NFKC') for line in onmtp]\n",
        "bunn = _bunn\n",
        "onmtp = _onmtp\n",
        "\n",
        "ono2sen = {}\n",
        "sen2ono = {}\n",
        "for __bunn, __onmtp in zip(bunn, onmtp):\n",
        "    if not __onmtp in ono2sen:\n",
        "        ono2sen[__onmtp] = []\n",
        "    ono2sen[__onmtp].append(__bunn)\n",
        "    \n",
        "    if not __bunn in sen2ono:\n",
        "        sen2ono[__bunn] = []\n",
        "    sen2ono[__bunn].append(__onmtp)\n",
        "\n",
        "    \n",
        "max_cand = 0\n",
        "for k, v in ono2sen.items():\n",
        "    n = len(ono2sen[k])\n",
        "    if n > max_cand:\n",
        "        max_cand = n\n",
        "        max_ono = k\n",
        "\n",
        "print(max_cand, max_ono)\n",
        "print(ono2sen[max_ono])\n",
        "\n",
        "\n",
        "max_cand = 0\n",
        "for k, v in sen2ono.items():\n",
        "    n = len(sen2ono[k])\n",
        "    if n > max_cand:\n",
        "        max_cand = n\n",
        "        max_sen = k\n",
        "print(max_cand, max_sen)\n",
        "print(sen2ono[max_sen])\n",
        "\n",
        "onomatopea_ids = [tknz.convert_tokens_to_ids(w) for w in onomatopea]\n",
        "onomatopea == [tknz.convert_ids_to_tokens(tknz.convert_tokens_to_ids(w)) for w in onomatopea]\n",
        "print(len(onomatopea_ids))"
      ],
      "metadata": {
        "id": "Oo3ZChuIFAvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BOsXc5F5KufE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}