{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0424timbmg_Sentence_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 変分オートエンコーダのデモ\n",
        "\n",
        "- date: 2022_0424\n",
        "- GitHub directory: `ShinAsakawa/ShinAsakawa.github.io/2022notebooks/`\n",
        "- filename: `2022_0424timbmg_Sentence-VAE.ipynb`\n",
        "- source: https://github.com/timbmg/Sentence-VAE の train.py と inferece.py\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/timbmg/Sentence-VAE/raw/master/figs/model.png\"><br/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "FBbTeiPlfU9O"
      },
      "id": "FBbTeiPlfU9O"
    },
    {
      "cell_type": "code",
      "source": [
        "# このセルは実行しないで良いです。ハーバード大大学院生たちの ELBO についてのおふざけ動画です\n",
        "# もちろん，肘を表す英単語 elbow と，変分下限 ELBO: Evidence Lower BOund とは同じ発音なので\n",
        "from IPython.display import YouTubeVideo, display\n",
        "youtube_id = 'jugUBL4rEIM'\n",
        "display(YouTubeVideo(youtube_id, width=600, height=480))"
      ],
      "metadata": {
        "id": "HTPumGsHlYWs"
      },
      "id": "HTPumGsHlYWs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    # download.sh の内容\n",
        "    !mkdir data\n",
        "    !wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
        "    !tar -xf  simple-examples.tgz\n",
        "    !mv simple-examples/data/ptb.train.txt data/\n",
        "    !mv simple-examples/data/ptb.valid.txt data/\n",
        "    !mv simple-examples/data/ptb.test.txt data/\n",
        "    !rm -rf simple_examples"
      ],
      "metadata": {
        "id": "gyaue9syN0Wp"
      },
      "id": "gyaue9syN0Wp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# ptb.py, utils.py, model.py をアップロードする必要があります\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "gwPkge23RdTy"
      },
      "id": "gwPkge23RdTy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30434c2-8ec9-4ef9-800a-d28a3689fd43",
      "metadata": {
        "id": "e30434c2-8ec9-4ef9-800a-d28a3689fd43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "#import argparse\n",
        "import numpy as np\n",
        "from multiprocessing import cpu_count\n",
        "#from tensorboardX import SummaryWriter\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "import tensorboard\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import OrderedDict, defaultdict\n",
        "\n",
        "from ptb import PTB\n",
        "from model import SentenceVAE\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from utils import to_var, idx2word, expierment_name\n",
        "def to_var(x):\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return x\n",
        "\n",
        "\n",
        "def idx2word(idx, i2w, pad_idx):\n",
        "    sent_str = [str()]*len(idx)\n",
        "    for i, sent in enumerate(idx):\n",
        "        for word_id in sent:\n",
        "            if word_id == pad_idx:\n",
        "                break\n",
        "            sent_str[i] += i2w[str(word_id.item())] + \" \"\n",
        "        sent_str[i] = sent_str[i].strip()\n",
        "    return sent_str\n",
        "\n",
        "\n",
        "def interpolate(start, end, steps):\n",
        "\n",
        "    interpolation = np.zeros((start.shape[0], steps + 2))\n",
        "\n",
        "    for dim, (s, e) in enumerate(zip(start, end)):\n",
        "        interpolation[dim] = np.linspace(s, e, steps+2)\n",
        "\n",
        "    return interpolation.T\n",
        "\n",
        "def expierment_name(args, ts):\n",
        "    exp_name = str()\n",
        "    exp_name += \"BS=%i_\" % args.batch_size\n",
        "    exp_name += \"LR={}_\".format(args.learning_rate)\n",
        "    exp_name += \"EB=%i_\" % args.embedding_size\n",
        "    exp_name += \"%s_\" % args.rnn_type.upper()\n",
        "    exp_name += \"HS=%i_\" % args.hidden_size\n",
        "    exp_name += \"L=%i_\" % args.num_layers\n",
        "    exp_name += \"BI=%i_\" % args.bidirectional\n",
        "    exp_name += \"LS=%i_\" % args.latent_size\n",
        "    exp_name += \"WD={}_\".format(args.word_dropout)\n",
        "    exp_name += \"ANN=%s_\" % args.anneal_function.upper()\n",
        "    exp_name += \"K={}_\".format(args.k)\n",
        "    exp_name += \"X0=%i_\" % args.x0\n",
        "    exp_name += \"TS=%s\" % ts\n"
      ],
      "metadata": {
        "id": "0t0pMFt91XY_"
      },
      "id": "0t0pMFt91XY_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# argparse の処理を代替する\n",
        "class _args():\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.data_dir = 'data'\n",
        "        self.create_data = True\n",
        "        self.max_sequence_length = 60\n",
        "        self.min_occ = 1\n",
        "        self.test = True\n",
        "        \n",
        "        self.epochs = 10\n",
        "        self.batch_size = 32\n",
        "        self.learning_rate = 0.001\n",
        "        \n",
        "        self.embedding_size = 300\n",
        "        self.rnn_type = 'gru'\n",
        "        self.hidden_size = 256\n",
        "        self.num_layers = 1\n",
        "        self.bidirectional = True\n",
        "        self.latent_size = 16\n",
        "        self.word_dropout = 0\n",
        "        self.embedding_dropout = 0.5\n",
        "\n",
        "        self.anneal_function= 'logistic'\n",
        "        self.k=0.0025\n",
        "        self.x0 = 2500\n",
        "\n",
        "        self.print_every = 50\n",
        "        self.tensorboard_logging = True\n",
        "        self.logdir = 'logs'\n",
        "        self.save_model_path ='bin'\n",
        "        self.load_checkpoint = './bin/2022-0424ccap/E9.pytorch'\n",
        "        self.num_samples = 10\n",
        "\n",
        "\n",
        "args = _args()\n",
        "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
        "assert args.anneal_function in ['logistic', 'linear']\n",
        "assert 0 <= args.word_dropout <= 1        \n",
        "#main(args)"
      ],
      "metadata": {
        "id": "0y4-eUJHZwYl"
      },
      "id": "0y4-eUJHZwYl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts = '2022_0424ccap'\n",
        "#ts = time.strftime('%Y-%b-%d-%H:%M:%S', time.gmtime())\n",
        "splits = ['train', 'valid'] + (['test'] if args.test else [])\n",
        "\n",
        "datasets = OrderedDict()\n",
        "for split in splits:\n",
        "    datasets[split] = PTB(\n",
        "        data_dir=args.data_dir,\n",
        "        split=split,\n",
        "        create_data=args.create_data,\n",
        "        max_sequence_length=args.max_sequence_length,\n",
        "        min_occ=args.min_occ\n",
        "    )\n",
        "\n",
        "params = dict(\n",
        "    vocab_size=datasets['train'].vocab_size,\n",
        "    sos_idx=datasets['train'].sos_idx,\n",
        "    eos_idx=datasets['train'].eos_idx,\n",
        "    pad_idx=datasets['train'].pad_idx,\n",
        "    unk_idx=datasets['train'].unk_idx,\n",
        "    max_sequence_length=args.max_sequence_length,\n",
        "    embedding_size=args.embedding_size,\n",
        "    rnn_type=args.rnn_type,\n",
        "    hidden_size=args.hidden_size,\n",
        "    word_dropout=args.word_dropout,\n",
        "    embedding_dropout=args.embedding_dropout,\n",
        "    latent_size=args.latent_size,\n",
        "    num_layers=args.num_layers,\n",
        "    bidirectional=args.bidirectional\n",
        ")\n",
        "model = SentenceVAE(**params)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "print(model)\n",
        "\n",
        "if args.tensorboard_logging:\n",
        "    writer = SummaryWriter(os.path.join(args.logdir, expierment_name(args, ts)))\n",
        "    writer.add_text(\"model\", str(model))\n",
        "    writer.add_text(\"args\", str(args))\n",
        "    writer.add_text(\"ts\", ts)\n",
        "\n",
        "save_model_path = os.path.join(args.save_model_path, ts)\n",
        "os.makedirs(save_model_path)\n",
        "\n",
        "with open(os.path.join(save_model_path, 'model_params.json'), 'w') as f:\n",
        "    json.dump(params, f, indent=4)\n",
        "\n",
        "def kl_anneal_function(anneal_function, step, k, x0):\n",
        "    if anneal_function == 'logistic':\n",
        "        return float(1/(1+np.exp(-k*(step-x0))))\n",
        "    elif anneal_function == 'linear':\n",
        "        return min(1, step/x0)\n",
        "\n",
        "NLL = torch.nn.NLLLoss(ignore_index=datasets['train'].pad_idx, reduction='sum')\n",
        "def loss_fn(logp, target, length, mean, logv, anneal_function, step, k, x0):\n",
        "\n",
        "    # cut-off unnecessary padding from target, and flatten\n",
        "    target = target[:, :torch.max(length).item()].contiguous().view(-1)\n",
        "    logp = logp.view(-1, logp.size(2))\n",
        "\n",
        "    # Negative Log Likelihood\n",
        "    NLL_loss = NLL(logp, target)\n",
        "\n",
        "    # KL Divergence\n",
        "    KL_loss = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp())\n",
        "    KL_weight = kl_anneal_function(anneal_function, step, k, x0)\n",
        "\n",
        "    return NLL_loss, KL_loss, KL_weight\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n"
      ],
      "metadata": {
        "id": "JFaiLfHTbpJh"
      },
      "id": "JFaiLfHTbpJh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = 0\n",
        "for epoch in range(args.epochs):\n",
        "\n",
        "    for split in splits:\n",
        "        data_loader = DataLoader(\n",
        "            dataset=datasets[split],\n",
        "            batch_size=args.batch_size,\n",
        "            shuffle=split=='train',\n",
        "            num_workers=cpu_count(),\n",
        "            pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "\n",
        "        tracker = defaultdict(tensor)\n",
        "\n",
        "        # Enable/Disable Dropout\n",
        "        if split == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        for iteration, batch in enumerate(data_loader):\n",
        "\n",
        "            batch_size = batch['input'].size(0)\n",
        "\n",
        "            for k, v in batch.items():\n",
        "                if torch.is_tensor(v):\n",
        "                    batch[k] = to_var(v)\n",
        "\n",
        "            # Forward pass\n",
        "            logp, mean, logv, z = model(batch['input'], batch['length'])\n",
        "\n",
        "            # loss calculation\n",
        "            NLL_loss, KL_loss, KL_weight = loss_fn(logp, batch['target'],\n",
        "                batch['length'], mean, logv, args.anneal_function, step, args.k, args.x0)\n",
        "\n",
        "            loss = (NLL_loss + KL_weight * KL_loss) / batch_size\n",
        "\n",
        "            # backward + optimization\n",
        "            if split == 'train':\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                step += 1\n",
        "\n",
        "            # bookkeepeing\n",
        "            tracker['ELBO'] = torch.cat((tracker['ELBO'], loss.data.view(1, -1)), dim=0)\n",
        "\n",
        "            if args.tensorboard_logging:\n",
        "                writer.add_scalar(\"%s/ELBO\" % split.upper(), loss.item(), epoch*len(data_loader) + iteration)\n",
        "                writer.add_scalar(\"%s/NLL Loss\" % split.upper(), NLL_loss.item() / batch_size,\n",
        "                                    epoch*len(data_loader) + iteration)\n",
        "                writer.add_scalar(\"%s/KL Loss\" % split.upper(), KL_loss.item() / batch_size,\n",
        "                                    epoch*len(data_loader) + iteration)\n",
        "                writer.add_scalar(\"%s/KL Weight\" % split.upper(), KL_weight,\n",
        "                                    epoch*len(data_loader) + iteration)\n",
        "\n",
        "            if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
        "                print(\"%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"\n",
        "                        % (split.upper(), iteration, len(data_loader)-1, loss.item(), NLL_loss.item()/batch_size,\n",
        "                        KL_loss.item()/batch_size, KL_weight))\n",
        "\n",
        "            if split == 'valid':\n",
        "                if 'target_sents' not in tracker:\n",
        "                    tracker['target_sents'] = list()\n",
        "                tracker['target_sents'] += idx2word(batch['target'].data, i2w=datasets['train'].get_i2w(),\n",
        "                                                    pad_idx=datasets['train'].pad_idx)\n",
        "                tracker['z'] = torch.cat((tracker['z'], z.data), dim=0)\n",
        "\n",
        "        print(\"%s Epoch %02d/%i, Mean ELBO %9.4f\" % (split.upper(), epoch, args.epochs, tracker['ELBO'].mean()))\n",
        "\n",
        "        if args.tensorboard_logging:\n",
        "            writer.add_scalar(\"%s-Epoch/ELBO\" % split.upper(), torch.mean(tracker['ELBO']), epoch)\n",
        "\n",
        "        # save a dump of all sentences and the encoded latent space\n",
        "        if split == 'valid':\n",
        "            dump = {'target_sents': tracker['target_sents'], 'z': tracker['z'].tolist()}\n",
        "            if not os.path.exists(os.path.join('dumps', ts)):\n",
        "                os.makedirs('dumps/'+ts)\n",
        "            with open(os.path.join('dumps/'+ts+'/valid_E%i.json' % epoch), 'w') as dump_file:\n",
        "                json.dump(dump,dump_file)\n",
        "\n",
        "        # save checkpoint\n",
        "        if split == 'train':\n",
        "            checkpoint_path = os.path.join(save_model_path, \"E%i.pytorch\" % epoch)\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(\"Model saved at %s\" % checkpoint_path)"
      ],
      "metadata": {
        "id": "OUOZ4WMMcSOu"
      },
      "id": "OUOZ4WMMcSOu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lat bin/2022"
      ],
      "metadata": {
        "id": "yyeCaB0_Yo71"
      },
      "id": "yyeCaB0_Yo71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs"
      ],
      "metadata": {
        "id": "9hniRSaFUxI7"
      },
      "id": "9hniRSaFUxI7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model import SentenceVAE\n",
        "from utils import to_var, idx2word, interpolate"
      ],
      "metadata": {
        "id": "ETzkXBM1WuR-"
      },
      "id": "ETzkXBM1WuR-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(args.data_dir+'/ptb.vocab.json', 'r') as file:\n",
        "    vocab = json.load(file)\n",
        "\n",
        "w2i, i2w = vocab['w2i'], vocab['i2w']\n",
        "_model = SentenceVAE(\n",
        "    vocab_size=len(w2i),\n",
        "    sos_idx=w2i['<sos>'],\n",
        "    eos_idx=w2i['<eos>'],\n",
        "    pad_idx=w2i['<pad>'],\n",
        "    unk_idx=w2i['<unk>'],\n",
        "    max_sequence_length=args.max_sequence_length,\n",
        "    embedding_size=args.embedding_size,\n",
        "    rnn_type=args.rnn_type,\n",
        "    hidden_size=args.hidden_size,\n",
        "    word_dropout=args.word_dropout,\n",
        "    embedding_dropout=args.embedding_dropout,\n",
        "    latent_size=args.latent_size,\n",
        "    num_layers=args.num_layers,\n",
        "    bidirectional=args.bidirectional\n",
        ")\n",
        "\n",
        "# この行は，`bin/なんちゃらと直接書き換えないと動かないだろうな\n",
        "if not os.path.exists(args.load_checkpoint):\n",
        "    raise FileNotFoundError(args.load_checkpoint)\n",
        "\n",
        "model.load_state_dict(torch.load(args.load_checkpoint))\n",
        "print(\"Model loaded from %s\" % args.load_checkpoint)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    \n",
        "model.eval()\n",
        "samples, z = model.inference(n=args.num_samples)\n",
        "print('----------SAMPLES----------')\n",
        "print(*idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>']), sep='\\n')\n",
        "\n",
        "z1 = torch.randn([args.latent_size]).numpy()\n",
        "z2 = torch.randn([args.latent_size]).numpy()\n",
        "z = to_var(torch.from_numpy(interpolate(start=z1, end=z2, steps=8)).float())\n",
        "samples, _ = model.inference(z=z)\n",
        "print('-------INTERPOLATION-------')\n",
        "print(*idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>']), sep='\\n')"
      ],
      "metadata": {
        "id": "qqrjZqMqW_Su"
      },
      "id": "qqrjZqMqW_Su",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ywhd2U6MXCWs"
      },
      "id": "Ywhd2U6MXCWs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "2022_0424timbmg_Sentence-VAE.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}