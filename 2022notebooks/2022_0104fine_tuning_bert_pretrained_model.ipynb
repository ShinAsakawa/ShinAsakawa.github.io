{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0104fine_tuning_bert_pretrained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- filename: `2022_0104fine_tuning_bert_pretrained_model.ipynb`\n",
        "- date; 2022_0104\n",
        "- source: https://huggingface.co/docs/transformers/master/en/training#preparing-the-datasets\n",
        "\n",
        "# è¨“ç·´æ¸ˆãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ PyTorch ç‰ˆ\n",
        "<!-- # Fine-tuning a pretrained model-->\n"
      ],
      "metadata": {
        "id": "MKAre5x3JWUL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T74maVQCJIGc"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6VwvyUmJIGe"
      },
      "source": [
        "# Fine-tuning a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCjuXKF1JIGf"
      },
      "source": [
        "ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ï¼ŒTransformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰äº‹å‰è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚\n",
        "TensorFlow ã§ã¯ Keras ã¨`fit` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥è¨“ç·´ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "PyTorch ã§ã¯ï¼Œæ±ç”¨çš„ãªè¨“ç·´ãƒ«ãƒ¼ãƒ—ãŒãªã„ãŸã‚ï¼ŒğŸ¤— Transformer sãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã¯ [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) ã¨ã„ã†ã‚¯ãƒ©ã‚¹ã§ API ã‚’æä¾›ã—ã¦ãŠã‚Šï¼Œãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã‚„è¨“ç·´ã‚’ç°¡å˜ã«è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚ \n",
        "æ¬¡ã« PyTorch ã§è¨“ç·´ãƒ«ãƒ¼ãƒ—å…¨ä½“ã‚’è¨˜è¿°ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚\n",
        "<!-- In this tutorial, we will show you how to fine-tune a pretrained model from the Transformers library. \n",
        "In TensorFlow, models can be directly trained using Keras and the `fit` method. \n",
        "In PyTorch, there is no generic training loop so the ğŸ¤— Transformers library provides an API with the class [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) to let you fine-tune or train a model from scratch easily. \n",
        "Then we will show you how to alternatively write the whole training loop in PyTorch. -->\n",
        "\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹å‰ã«ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå¿…è¦ã§ã™ã€‚\n",
        "ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ [IMDB dataset](https://www.imdb.com/interfaces/) ã§ BERT ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚\n",
        "èª²é¡Œã¯ï¼Œæ˜ ç”»ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒè‚¯å®šçš„ã‹å¦å®šçš„ã‹ã‚’åˆ†é¡ã™ã‚‹ã“ã¨ã§ã™ã€‚\n",
        "ä»–ã®èª²é¡Œä¾‹ã«ã¤ã„ã¦ã¯ [additional-resources](#additional-resources) ç¯€ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
        "<!-- Before we can fine-tune a model, we need a dataset. \n",
        "In this tutorial, we will show you how to fine-tune BERT on the [IMDB dataset](https://www.imdb.com/interfaces/): the task is to classify whether movie reviews are positive or negative. \n",
        "For examples of other tasks, refer to the [additional-resources](#additional-resources) section! -->\n",
        "\n",
        "<a id='data-processing'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zcNH-yeJIGg"
      },
      "source": [
        "## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ <!--## Preparing the datasets-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": true,
        "id": "YkejdvTdJIGh",
        "outputId": "9d403a0d-e8b5-4604-c4c2-1b7e5a08a1eb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_BZearw7f0w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_BZearw7f0w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj2KgIQrJIGh"
      },
      "source": [
        "ã“ã“ã§ã¯ [ğŸ¤— Datasets](https://github.com/huggingface/datasets/) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ã¦ï¼ŒIMDB ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ï¼Œå‰å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚\n",
        "ã“ã®éƒ¨åˆ†ã¯ç°¡å˜ã«èª¬æ˜ã—ã¾ã™ã€‚\n",
        "ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã®ç„¦ç‚¹ã¯è¨“ç·´ãªã®ã§ï¼Œè©³ç´°ã¯ğŸ¤— Datasets [documentation](https://huggingface.co/docs/datasets/) ã‚„ [preprocessing](https://huggingface.co/docs/transformers/master/en/preprocessing) ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
        "<!-- We will use the [ğŸ¤— Datasets](https://github.com/huggingface/datasets/) library to download and preprocess the IMDB datasets. \n",
        "We will go over this part pretty quickly. Since the focus of this tutorial is on training, you should refer to the ğŸ¤— Datasets [documentation](https://huggingface.co/docs/datasets/) or the [preprocessing](https://huggingface.co/docs/transformers/master/en/preprocessing) tutorial for more information. -->\n",
        "\n",
        "\n",
        "ã¾ãš `load_dataset` é–¢æ•°ã‚’ä½¿ã£ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚\n",
        "<!-- First, we can use the `load_dataset` function to download and cache the dataset: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPtI4JULJIGi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICIUsuh_JIGi"
      },
      "source": [
        "ã“ã‚Œã¯ï¼Œãƒ¢ãƒ‡ãƒ«ã‚„ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã§è¦‹ãŸ `from_pretrained` ãƒ¡ã‚½ãƒƒãƒ‰ã¨åŒã˜ã‚ˆã†ã«å‹•ä½œã—ã¾ã™  (ãŸã ã—ï¼Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§  _~/.cache/huggingface/dataset_  ã«ãªã£ã¦ã„ã¾ã™)ã€‚\n",
        "<!-- This works like the `from_pretrained` method we saw for the models and tokenizers (except the cache directory is _~/.cache/huggingface/dataset_ by default). -->\n",
        "\n",
        "`raw_datasets` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ï¼Œ3 ã¤ã®ã‚­ãƒ¼ã‚’æŒã¤è¾æ›¸ã§ã™ã€‚\n",
        "`\"train\"`, `\"test\"`, `\"unsupervisedâ€` ã® 3 ã¤ã®ã‚­ãƒ¼ã‚’æŒã¤è¾æ›¸ã§ã™ (ã“ã‚Œã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® 3 ã¤ã®åˆ†å‰²ã«å¯¾å¿œã—ã¦ã„ã¾ã™)ã€‚\n",
        "`\"trainâ€` ã®åˆ†å‰²ã¯è¨“ç·´ã«ï¼Œ`\"testâ€` ã®åˆ†å‰²ã¯æ¤œè¨¼ã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "<!-- The `raw_datasets` object is a dictionary with three keys: `\"train\"`, `\"test\"` and `\"unsupervised\"` (which correspond to the three splits of that dataset). \n",
        "We will use the `\"train\"` split for training and the `\"test\"` split for validation. -->\n",
        "\n",
        "ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã™ã‚‹ãŸã‚ã«ã¯ï¼Œãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãŒå¿…è¦ã§ã™ã€‚\n",
        "<!-- To preprocess our data, we will need a tokenizer: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PodWI6A-JIGj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6veK0rEJIGk"
      },
      "source": [
        "[å‰å‡¦ç†](https://huggingface.co/docs/transformers/master/en/preprocessing) ã§è¦‹ãŸã‚ˆã†ã«ï¼Œä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚’æº–å‚™ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ (ã“ã‚Œã¯ä¾‹ã§ã‚ã£ã¦ï¼Œå®Ÿè¡Œã§ãã‚‹ã‚³ãƒãƒ³ãƒ‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“)ã€‚\n",
        "<!-- As we saw in [preprocessing](https://huggingface.co/docs/transformers/master/en/preprocessing), we can prepare the text inputs for the model with the following command (this is an example, not a command you can execute): -->"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = 'This is a pen.'"
      ],
      "metadata": {
        "id": "kgtzXYTMLxXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMMjOzm3JIGk"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(sentences, padding=\"max_length\", truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy6rEf1xJIGk"
      },
      "source": [
        "ã“ã‚Œã«ã‚ˆã‚Šï¼Œã™ã¹ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ã¯ï¼Œãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯åˆ‡ã‚Šè©°ã‚ã«ã‚ˆã£ã¦ï¼Œãƒ¢ãƒ‡ãƒ«ãŒå—ã‘å…¥ã‚Œã‚‰ã‚Œã‚‹æœ€å¤§ã®é•·ã• (ã“ã“ã§ã¯512) ã«ãªã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã—ã‹ã—ï¼Œä»£ã‚ã‚Šã«ï¼Œ`map` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã£ã¦ï¼Œã“ã‚Œã‚‰ã®å‰å‡¦ç†ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã™ã¹ã¦ã®åˆ†å‰²ã«ä¸€åº¦ã«é©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- This will make all the samples have the maximum length the model can accept (here 512), either by padding or truncating them.\n",
        "\n",
        "However, we can instead apply these preprocessing steps to all the splits of our dataset at once by using the `map` method: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JZE_xzMJIGk"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nQPni31JIGl"
      },
      "source": [
        "ãƒãƒƒãƒ—æ³•ã‚„ï¼Œãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã™ã‚‹ä»–ã®æ–¹æ³•ã«ã¤ã„ã¦ã¯ï¼ŒğŸ¤— Datasets [document](https://huggingface.co/docs/datasets/) ã§è©³ã—ãèª¬æ˜ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "æ¬¡ã«ï¼Œè¨“ç·´ã‚»ãƒƒãƒˆã¨æ¤œè¨¼ã‚»ãƒƒãƒˆã®å°ã•ãªã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã—ã¦ï¼Œè¨“ç·´ã®é«˜é€ŸåŒ–ã‚’å›³ã‚Šã¾ã™ã€‚\n",
        "<!-- You can learn more about the map method or the other ways to preprocess the data in the ğŸ¤— Datasets [documentation](https://huggingface.co/docs/datasets/).\n",
        "\n",
        "Next we will generate a small subset of the training and validation set, to enable faster training: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1BlVGVJJIGl"
      },
      "outputs": [],
      "source": [
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "full_train_dataset = tokenized_datasets[\"train\"]\n",
        "full_eval_dataset = tokenized_datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XTxbFk3JIGl"
      },
      "source": [
        "ä»¥ä¸‹ã®ã™ã¹ã¦ã®ä¾‹ã§ã¯ï¼Œå¸¸ã« `small_train_dataset` ã¨ `small_eval_dataset` ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\n",
        "å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã‚„è©•ä¾¡ã‚’è¡Œã†ã«ã¯ï¼Œã“ã‚Œã‚‰ã‚’ _full_ ã«ç½®ãæ›ãˆã‚‹ã ã‘ã§ã™ã€‚\n",
        "<!-- In all the examples below, we will always use `small_train_dataset` and `small_eval_dataset`. Just replace them by their _full_ equivalent to train or evaluate on the full dataset. -->\n",
        "\n",
        "<a id='trainer'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Ln_eQ8JIGl"
      },
      "source": [
        "## Trainer API ã«ã‚ˆã‚‹ PyTorch ã®å¾®èª¿æ•´\n",
        "<!-- ## Fine-tuning in PyTorch with the Trainer API -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": true,
        "id": "JqXM3lL_JIGm",
        "outputId": "1ff3a192-0074-460f-8c32-2387e6830935"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nvBXf7s7vTI?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nvBXf7s7vTI?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKoYDjBWJIGm"
      },
      "source": [
        "PyTorch ã¯è¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’æä¾›ã—ã¦ã„ãªã„ã®ã§ ğŸ¤— Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ï¼ŒğŸ¤— Transformers ãƒ¢ãƒ‡ãƒ«ã«æœ€é©åŒ–ã•ã‚ŒãŸ [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) ã¨ã„ã† API ã‚’æä¾›ã—ã¦ãŠã‚Šï¼Œå¹…åºƒã„è¨“ç·´ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ï¼Œãƒ­ã‚®ãƒ³ã‚°ï¼Œå‹¾é…ç´¯ç©ï¼Œæ··åˆç²¾åº¦ãªã©ã®æ©Ÿèƒ½ãŒçµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "ã¾ãšï¼Œãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã—ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Since PyTorch does not provide a training loop, the ğŸ¤— Transformers library provides a [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) API that is optimized for ğŸ¤— Transformers models, with a wide range of training options and with built-in features like logging, gradient accumulation, and mixed precision.\n",
        "\n",
        "First, let's define our model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm_HSteiJIGm"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIYkCVERJIGm"
      },
      "source": [
        "ã“ã‚Œã«ã‚ˆã‚Šï¼Œäº‹å‰å­¦ç¿’ã•ã‚ŒãŸé‡ã¿ã®ä¸€éƒ¨ãŒä½¿ç”¨ã•ã‚Œãšï¼Œä¸€éƒ¨ã®é‡ã¿ãŒãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚Œã‚‹ã¨ã„ã†è­¦å‘ŠãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚\n",
        "ã“ã‚Œã¯ï¼ŒBERT ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ãƒ˜ãƒƒãƒ‰ã‚’ç ´æ£„ã—ã¦ï¼Œãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚ŒãŸåˆ†é¡ãƒ˜ãƒƒãƒ‰ã«ç½®ãæ›ãˆã¦ã„ã‚‹ãŸã‚ã§ã™ã€‚\n",
        "æˆ‘ã€…ã¯ï¼Œäº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’ç§»è¡Œã—ãªãŒã‚‰ï¼Œèª²é¡Œä¸Šã§ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ (ã“ã‚ŒãŒï¼Œè»¢ç§»å­¦ç¿’ã¨å‘¼ã°ã‚Œã‚‹ç†ç”±ã§ã™)ã€‚\n",
        "<!-- This will issue a warning about some of the pretrained weights not being used and some weights being randomly initialized. \n",
        "That's because we are throwing away the pretraining head of the BERT model to replace it with a classification head which is randomly initialized. \n",
        "We will fine-tune this model on our task, transferring the knowledge of the pretrained model to it (which is why doing this is called transfer learning).-->\n",
        "\n",
        "æ¬¡ã« [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer)ã‚’å®šç¾©ã™ã‚‹ãŸã‚ã«ï¼Œ [TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments) ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "ã“ã®ã‚¯ãƒ©ã‚¹ã«ã¯ï¼Œ[Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) ã§èª¿æ•´ã§ãã‚‹ã™ã¹ã¦ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„ï¼Œã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã•ã¾ã–ã¾ãªå­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹ã«ã™ã‚‹ãƒ•ãƒ©ã‚°ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã™ã‚‹ã ã‘ã§ã™ã€‚\n",
        "<!-- Then, to define our [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer), we will need to instantiate a [TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments). \n",
        "This class contains all the hyperparameters we can tune for the [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) or the flags to activate the different training options it supports. \n",
        "Let's begin by using all the defaults, the only thing we then have to provide is a directory in which the checkpoints will be saved: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CvKhr17JIGm"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"test_trainer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBWQNSqaJIGn"
      },
      "source": [
        "ãã‚Œã§ã¯ï¼Œ[Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) ã‚’æ¬¡ã®ã‚ˆã†ã«å®Ÿä½“åŒ–ã—ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Then we can instantiate a [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) like this: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiiMw_h4JIGn"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIe_osrGJIGn"
      },
      "source": [
        "ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ãŸã‚ã«ã¯ï¼ŒãŸã å‘¼ã³å‡ºã™ã ã‘ã§ã™ã€‚\n",
        "<!-- To fine-tune our model, we just need to call -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlSoweJJJIGn"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sueYX7-wJIGn"
      },
      "source": [
        "ã‚’å®Ÿè¡Œã™ã‚‹ã¨ï¼Œãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã§ç¢ºèªã§ãã‚‹è¨“ç·´ãŒé–‹å§‹ã•ã‚Œï¼Œæ•°åˆ†ã§å®Œäº†ã—ã¾ã™ (GPU ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹é™ã‚Š)ã€‚\n",
        "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ï¼Œè¨“ç·´ä¸­ã«è©•ä¾¡ã¯è¡Œã‚ã‚Œãšï¼Œ[Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) ã«æŒ‡æ¨™ã‚’è¨ˆç®—ã™ã‚‹ã‚ˆã†ã«æŒ‡ç¤ºã—ã¦ã„ãªã„ã®ã§ï¼Œãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒã©ã®ç¨‹åº¦ã‹ã«ã¤ã„ã¦ã¯ï¼Œå®Ÿéš›ã«ã¯ä½•ã‚‚ã‚ã‹ã‚Šã¾ã›ã‚“ã€‚\n",
        "ã§ã¯ï¼Œãã®æ–¹æ³•ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- which will start a training that you can follow with a progress bar, which should take a couple of minutes to complete (as long as you have access to a GPU). \n",
        "It won't actually tell you anything useful about how well (or badly) your model is performing however as by default, there is no evaluation during training, and we didn't tell the [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) to compute any metrics. Let's have a look on how to do that now!-->\n",
        "\n",
        "[Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) ãŒãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã¦å ±å‘Šã™ã‚‹ã«ã¯ï¼Œäºˆæ¸¬å€¤ã¨ãƒ©ãƒ™ãƒ«ï¼ˆ[EvalPrediction](https://huggingface.co/docs/transformers/master/en/internal/trainer_utils#transformers.EvalPrediction) ã¨ã„ã†åå‰ã®ã¤ã„ãŸã‚¿ãƒ—ãƒ«ã«ã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã‚‹) ã‚’å—ã‘å–ã‚Šï¼Œæ–‡å­—åˆ—ã‚¢ã‚¤ãƒ†ãƒ  (ãƒ¡ãƒˆãƒªã‚¯ã‚¹å) ã¨æµ®å‹•å°æ•°ç‚¹å€¤ (ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤) ã‚’å«ã‚€è¾æ›¸ã‚’è¿”ã™ `compute_metrics` é–¢æ•°ã‚’ä¸ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "<!-- To have the [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) compute and report metrics, we need to give it a `compute_metrics` function that takes predictions and labels (grouped in a namedtuple called [EvalPrediction](https://huggingface.co/docs/transformers/master/en/internal/trainer_utils#transformers.EvalPrediction)) and return a dictionary with string items (the metric names) and float values (the metric values). -->\n",
        "\n",
        "ğŸ¤— Datasets ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã¯ï¼ŒNLP  ã§ä½¿ã‚ã‚Œã‚‹ä¸€èˆ¬çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ `load_metric` é–¢æ•°ã§ç°¡å˜ã«å–å¾—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ã“ã“ã§ã¯ï¼Œå˜ç´”ã« `accuracy` ã‚’ä½¿ã„ã¾ã™ã€‚\n",
        "æ¬¡ã«ï¼Œå¯¾æ•°ã‚’äºˆæ¸¬å€¤ã«å¤‰æ›ã™ã‚‹ã ã‘ã® `compute_metrics` é–¢æ•°ã‚’å®šç¾©ã— (ã™ã¹ã¦ã®ğŸ¤— Transformers ãƒ¢ãƒ‡ãƒ«ã¯å¯¾æ•°ã‚’è¿”ã™ã“ã¨ã‚’è¦šãˆã¦ãŠã„ã¦ãã ã•ã„)ï¼Œ ãã‚Œã‚’ã“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®  `compute`  ãƒ¡ã‚½ãƒƒãƒ‰ã«ä¸ãˆã¾ã™ã€‚\n",
        "<!-- The ğŸ¤— Datasets library provides an easy way to get the common metrics used in NLP with the `load_metric` function. \n",
        "here we simply use accuracy. \n",
        "Then we define the `compute_metrics` function that just convert logits to predictions (remember that all ğŸ¤— Transformers models return the logits) and feed them to `compute` method of this metric. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVQcZY1LJIGn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUOZl_PzJIGn"
      },
      "source": [
        "`compute` é–¢æ•°ã¯ï¼Œ(ãƒ­ã‚¸ãƒƒãƒˆã¨ãƒ©ãƒ™ãƒ«ã‚’å«ã‚€) ã‚¿ãƒ—ãƒ«ã‚’å—ã‘å–ã‚Šï¼Œæ–‡å­—åˆ—ã‚­ãƒ¼ (ãƒ¡ãƒˆãƒªãƒƒã‚¯å) ã¨æµ®å‹•å°æ•°ç‚¹å€¤ã‚’å«ã‚€è¾æ›¸ã‚’è¿”ã•ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚\n",
        "ã“ã®é–¢æ•°ã¯ï¼Œå„è©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚ºã®çµ‚ã‚ã‚Šã«ï¼Œäºˆæ¸¬å€¤ã‚„ãƒ©ãƒ™ãƒ«ã®é…åˆ—å…¨ä½“ã«å¯¾ã—ã¦å‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚\n",
        "<!-- The compute function needs to receive a tuple (with logits and labels) and has to return a dictionary with string keys (the name of the metric) and float values. \n",
        "It will be called at the end of each evaluation phase on the whole arrays of predictions/labels.-->\n",
        "\n",
        "å®Ÿéš›ã«å‹•ä½œã™ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ï¼Œå¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§æ–°ã—ã„ [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) ã‚’ä½œæˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- To check if this works on practice, let's create a new [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) with our fine-tuned model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRbGXp-gJIGo"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2eTm3imJIGo"
      },
      "source": [
        "ã§ 87.5%ã®ç²¾åº¦ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚\n",
        "<!-- which showed an accuracy of 87.5% in our case. -->\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ï¼Œè©•ä¾¡æŒ‡æ¨™ã‚’å®šæœŸçš„ã« (ä¾‹ãˆã°ï¼Œå„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«) å ±å‘Šã—ãŸã„å ´åˆï¼Œè¨“ç·´å¼•æ•°ã‚’ã©ã®ã‚ˆã†ã«å®šç¾©ã™ã¹ãã‹ã‚’ç¤ºã—ã¾ã™ã€‚\n",
        "<!-- If you want to fine-tune your model and regularly report the evaluation metrics (for instance at the end of each epoch), here is how you should define your training arguments: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuf2SZmeJIGo"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"test_trainer\", evaluation_strategy=\"epoch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMKW4aQpJIGo"
      },
      "source": [
        "ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ã¯ [TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments) ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
        "<!-- See the documentation of [TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments) for more options. -->\n",
        "\n",
        "\n",
        "<a id='keras'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VIzWScpJIGo"
      },
      "source": [
        "## Keras ã«ã‚ˆã‚‹å¾®èª¿æ•´\n",
        "<!-- ## Fine-tuning with Keras -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "Jbwz-9e-JIGo",
        "outputId": "64fc7ba5-58aa-445a-bd96-b82add92eb82"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rnTGBy2ax1c?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rnTGBy2ax1c?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g0KbyCsJIGo"
      },
      "source": [
        "ãƒ¢ãƒ‡ãƒ«ã¯ï¼ŒKeras API ã‚’ä½¿ã£ã¦ TensorFlow ã§ãƒã‚¤ãƒ†ã‚£ãƒ–ã«è¨“ç·´ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã¾ãšï¼Œãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã—ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Models can also be trained natively in TensorFlow using the Keras API. First, let's define our model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYJ39_dYJIGp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiJpTP9JIGp"
      },
      "source": [
        "æ¬¡ã«ï¼Œå…ˆã»ã©ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¨™æº–ã® `tf.data.Dataset` ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "Shape ãŒå›ºå®šã•ã‚Œã¦ã„ã‚‹ã®ã§ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«ç°¡å˜ã«è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ã¾ãšï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ _\"textâ€_ åˆ—ã‚’å‰Šé™¤ã—ï¼ŒTensorFlow ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«è¨­å®šã—ã¾ã™ã€‚\n",
        "<!-- Then we will need to convert our datasets from before in standard `tf.data.Dataset`. \n",
        "Since we have fixed shapes, it can easily be done like this. First we remove the _\"text\"_ column from our datasets and set them in TensorFlow format: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k5Yx8auJIGp"
      },
      "outputs": [],
      "source": [
        "tf_train_dataset = small_train_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
        "tf_eval_dataset = small_eval_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCIFznT-JIGp"
      },
      "source": [
        "ãã—ã¦ï¼Œã™ã¹ã¦ã‚’å¤§ããªãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ï¼Œ`tf.data.Dataset.from_tensor_slices` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "<!-- Then we convert everything in big tensors and use the `tf.data.Dataset.from_tensor_slices` method: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeH4SLZFJIGp"
      },
      "outputs": [],
      "source": [
        "train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names}\n",
        "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[\"label\"]))\n",
        "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(8)\n",
        "\n",
        "eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names}\n",
        "eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset[\"label\"]))\n",
        "eval_tf_dataset = eval_tf_dataset.batch(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iVfNrmRJIGp"
      },
      "source": [
        "ã“ã®ã‚ˆã†ã«ã—ã¦ä½œæˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ï¼Œä»–ã® Keras ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- With this done, the model can then be compiled and trained as any Keras model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpGN1awkJIGp"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "\n",
        "model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTjX2uUfJIGp"
      },
      "source": [
        "TensorFlow ãƒ¢ãƒ‡ãƒ«ã¨ PyTorch ãƒ¢ãƒ‡ãƒ«ã®é–“ã®ç·Šå¯†ãªç›¸äº’é‹ç”¨æ€§ã«ã‚ˆã‚Šï¼Œãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ã‹ã‚‰ PyTorch ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦å†èª­ã¿è¾¼ã¿ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ (ã¾ãŸã¯ãã®é€†)ã€‚\n",
        "<!-- With the tight interoperability between TensorFlow and PyTorch models, you can even save the model and then reload it as a PyTorch model (or vice-versa): -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruW_HO9RJIGq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model.save_pretrained(\"my_imdb_model\")\n",
        "pytorch_model = AutoModelForSequenceClassification.from_pretrained(\"my_imdb_model\", from_tf=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0lV8tGFJIGq"
      },
      "source": [
        "<a id='pytorch_native'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X9MQqjoJIGq"
      },
      "source": [
        "## ç”Ÿã® PyTorch ã‚’ä½¿ã£ãŸå¾®èª¿æ•´\n",
        "<!-- ## Fine-tuning in native PyTorch -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "DD_5Ei8HJIGq",
        "outputId": "b7861cfd-4018-4de6-83a2-24f24ee920eb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Dh9CL8fyG80?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Dh9CL8fyG80?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLKgNo11JIGq"
      },
      "source": [
        "ã“ã®æ®µéšã§ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å†èµ·å‹•ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã™ã‚‹ã‹ï¼Œæ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚\n",
        "<!-- You might need to restart your notebook at this stage to free some memory, or execute the following code: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3OuovFbJIGq",
        "outputId": "fa1db2de-2f1b-42fc-c4bb-fe1bd278114c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0e7a9cdd293c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mpytorch_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pytorch_model' is not defined"
          ]
        }
      ],
      "source": [
        "del model\n",
        "del pytorch_model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28Mqg8S_JIGq"
      },
      "source": [
        "ãã‚Œã§ã¯ [trainer section](#trainer) ã¨åŒã˜çµæœã‚’ PyTorch ã§å®Ÿç¾ã™ã‚‹æ–¹æ³•ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "ã¾ãšæœ€åˆã«ï¼Œãƒãƒƒãƒã‚’åå¾©å‡¦ç†ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "ãã®å‰ã« `tokenized_datasets` ã«ã¡ã‚‡ã£ã¨ã—ãŸå¾Œå‡¦ç†ã‚’æ–½ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "<!-- Let's now see how to achieve the same results as in [trainer section](#trainer) in PyTorch. \n",
        "First we need to define the dataloaders, which we will use to iterate over batches. We just need to apply a bit of post-processing to our `tokenized_datasets` before doing that to: -->\n",
        "\n",
        "- ãƒ¢ãƒ‡ãƒ«ãŒæƒ³å®šã—ã¦ã„ãªã„å€¤ã«å¯¾å¿œã™ã‚‹ã‚«ãƒ©ãƒ ã‚’å‰Šé™¤ã™ã‚‹ (ã“ã“ã§ã¯ `\"text\"` ã‚«ãƒ©ãƒ )ã€‚\n",
        "- â€œlabel\" ã‚«ãƒ©ãƒ ã®åå‰ã‚’ `\"labels\"` ã«å¤‰æ›´ã™ã‚‹ (ãƒ¢ãƒ‡ãƒ«ã¯å¼•æ•°ã« `labels` ã¨ã„ã†åå‰ãŒã¤ãã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã‚‹ãŸã‚)ã€‚\n",
        "- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è¨­å®šã—ï¼Œãƒªã‚¹ãƒˆã§ã¯ãªã PyTorch ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã™ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
        "\n",
        "<!-- - remove the columns corresponding to values the model does not expect (here the `\"text\"` column)\n",
        "- rename the column `\"label\"` to `\"labels\"` (because the model expect the argument to be named `labels`)\n",
        "- set the format of the datasets so they return PyTorch Tensors instead of lists. -->\n",
        "\n",
        "\n",
        "æˆ‘ã€…ã® _tokenized_datasets_ ã¯ï¼Œã“ã‚Œã‚‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã« 1 ã¤ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŒã£ã¦ã„ã¾ã™ã€‚\n",
        "<!-- Our _tokenized_datasets_ has one method for each of those steps: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR3wlu2wJIGq"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-L_uU8rJIGr"
      },
      "source": [
        "ã“ã‚Œã§ï¼Œãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã‚’ç°¡å˜ã«å®šç¾©ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n",
        "<!-- Now that this is done, we can easily define our dataloaders: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjORecmeJIGr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
        "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMGM1H96JIGr"
      },
      "source": [
        "æ¬¡ã«ï¼Œãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã—ã¾ã™ï¼š\n",
        "<!-- Next, we define our model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_B1km59JIGr",
        "outputId": "94d52fa8-782f-468e-a97a-12c633cfa950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8D12Jq7JIGr"
      },
      "source": [
        "ã‚ã¨ã¯ï¼Œæœ€é©åŒ–ã¨å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã® 2 ã¤ã ã‘ã§ã™ã€‚\n",
        "[Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æœ€é©åŒ–ã¯ [AdamW](https://huggingface.co/docs/transformers/master/en/main_classes/optimizer_schedules#transformers.AdamW) ã§ã™ã€‚\n",
        "<!-- We are almost ready to write our training loop, the only two things are missing are an optimizer and a learning rate scheduler. \n",
        "The default optimizer used by the [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) is [AdamW](https://huggingface.co/docs/transformers/master/en/main_classes/optimizer_schedules#transformers.AdamW): -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap42FVCcJIGr"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cZBD-DTJIGr"
      },
      "source": [
        "æœ€å¾Œã«ï¼Œãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¯ï¼Œæœ€å¤§å€¤ (ã“ã“ã§ã¯5e-5) ã‹ã‚‰ 0 ã¾ã§ã®ç›´ç·šçš„ãªæ¸›è¡°ã ã‘ã§ã™ã€‚\n",
        "<!-- Finally, the learning rate scheduler used by default is just a linear decay from the maximum value (5e-5 here) to 0: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dTcKfNvJIGr"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ8at8DYJIGr"
      },
      "source": [
        "æœ€å¾Œã«ï¼ŒGPU ãŒåˆ©ç”¨ã§ãã‚‹å ´åˆã¯ï¼ŒGPU ã‚’ä½¿ç”¨ã—ãŸã„ã¨æ€ã„ã¾ã™ (ãã†ã—ãªã„ã¨ï¼Œè¨“ç·´ãŒæ•°åˆ†ã§ã¯ãªãï¼Œæ•°æ™‚é–“ã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™)ã€‚\n",
        "ãã®ãŸã‚ã«ã¯ï¼Œãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒƒãƒã‚’é…ç½®ã™ã‚‹ã€Œãƒ‡ãƒã‚¤ã‚¹ã€ã‚’å®šç¾©ã—ã¾ã™ã€‚\n",
        "<!-- One last thing, we will want to use the GPU if we have access to one (otherwise training might take several hours instead of a couple of minutes). \n",
        "To do this, we define a `device` we will put our model and our batches on. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbxKo8-cJIGs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#device"
      ],
      "metadata": {
        "id": "WP4vS2aiSudY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3HXkjKnJIGs"
      },
      "source": [
        "ã“ã‚Œã§è¨“ç·´ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚\n",
        "è¨“ç·´ãŒã„ã¤çµ‚äº†ã™ã‚‹ã‹ã‚’çŸ¥ã‚‹ãŸã‚ã« _tqdm_ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ã¦ï¼Œè¨“ç·´ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’ç¤ºã™ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚\n",
        "<!-- We now are ready to train! To get some sense of when it will be finished, we add a progress bar over our number of training steps, using the _tqdm_ library. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPqMg4tNJIGs",
        "outputId": "0f06555d-4095-4c62-c92a-79bf94744b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d881b3ba3b6940e186fdf4db688914a7",
            "040448002bc549f48325ef8b78f343d2",
            "56f7b6dd2b4943908f11058babb89ead",
            "5688fb89c765434098921c99ebe2efd8",
            "e28550d40df74780a7ca6ba6629f18e0",
            "365297c296ae4ac9aab65426f8112302",
            "35dbcc9ccda34eda91921918a3671a53",
            "68c1e7d8f2f84b9a9e748f22bbb3e0e2",
            "03e69c2181184dddb0ca9f701065c87f",
            "6e71d95070704a91bca41d5f83be9c21",
            "cc24003af9414d4fb2025277d19df335"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d881b3ba3b6940e186fdf4db688914a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/375 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIMwZDlKJIGs"
      },
      "source": [
        "ã‚‚ã—ã‚ãªãŸãŒ (ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®ã‚ˆã†ã«) äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒœãƒ‡ã‚£ã‚’å‡çµã•ã›ã‚‹ã“ã¨ã«æ…£ã‚Œã¦ã„ã‚‹ã®ã§ã‚ã‚Œã°ï¼Œä½•ã®äºˆé˜²æªç½®ã‚‚å–ã‚‰ãšã«ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’ç›´æ¥å¾®èª¿æ•´ã—ã¦ã„ã‚‹ãŸã‚ï¼Œä¸Šè¨˜ã¯å°‘ã—å¥‡å¦™ã«è¦‹ãˆã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n",
        "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã¯ï¼Œå®Ÿéš›ã«ã“ã®æ–¹æ³•ã§ã†ã¾ãæ©Ÿèƒ½ã—ã¦ã„ã¾ã™ (ãªã®ã§ï¼Œã“ã‚Œã¯ç§ãŸã¡å´ã®è¦‹è½ã¨ã—ã§ã¯ã‚ã‚Šã¾ã›ã‚“)ã€‚\n",
        "ãƒ¢ãƒ‡ãƒ«ã®ã€Œãƒœãƒ‡ã‚£ã®å‡çµã€ãŒä½•ã‚’æ„å‘³ã™ã‚‹ã®ã‹ã‚ã‹ã‚‰ãªã„æ–¹ã¯ï¼Œã“ã®æ®µè½ã‚’èª­ã‚€ã®ã‚’å¿˜ã‚Œã¦ãã ã•ã„ã€‚\n",
        "<!-- Note that if you are used to freezing the body of your pretrained model (like in computer vision) the above may seem a bit strange, as we are directly fine-tuning the whole model without taking any precaution. \n",
        "It actually works better this way for Transformers model (so this is not an oversight on our side). \n",
        "If you're not familiar with what \"freezing the body\" of the model means, forget you read this paragraph.-->\n",
        "\n",
        "ã•ã¦ï¼Œçµæœã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã¯ï¼Œè©•ä¾¡ãƒ«ãƒ¼ãƒ—ã‚’æ›¸ã‹ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚\n",
        "[ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³](#trainer) ã®ã‚ˆã†ã«ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æŒ‡æ¨™ã‚’ä½¿ã„ã¾ã™ã€‚\n",
        "ã“ã“ã§ã¯ï¼Œå„ãƒãƒƒãƒã”ã¨ã«äºˆæ¸¬å€¤ã‚’è“„ç©ã—ï¼Œãƒ«ãƒ¼ãƒ—ãŒçµ‚äº†ã—ãŸã¨ãã«æœ€çµ‚çµæœã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
        "<!--Now to check the results, we need to write the evaluation loop. Like in the [trainer section](#trainer) we will use a metric from the datasets library. \n",
        "Here we accumulate the predictions at each batch before computing the final result when the loop is finished. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWLFI3fDJIGs"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"accuracy\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr6esigVJIGs"
      },
      "source": [
        "<a id='additional-resources'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UOMav8DJIGs"
      },
      "source": [
        "## è¿½åŠ è³‡æº\n",
        "<!-- ## Additional resources -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt_Ew23lJIGs"
      },
      "source": [
        "ã‚ˆã‚Šè©³ç´°ãªèª¿æ•´ä¾‹ã‚’è¦‹ã‚‹ã«ã¯ï¼Œä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
        "<!-- To look at more fine-tuning examples you can refer to:-->\n",
        "\n",
        "- [ğŸ¤— Transformers Examples](https://github.com/huggingface/transformers/tree/master/examples) ã«ã¯ PyTorch ã¨TensorFlow ã§ä¸€èˆ¬çš„ãª NLP èª²é¡Œã‚’å­¦ç¿’ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "- [ğŸ¤— Transformers Notebooks](https://huggingface.co/docs/transformers/master/en/notebooks) ã«ã¯æ§˜ã€…ãªãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãŒå«ã¾ã‚Œã¦ãŠã‚Šï¼Œç‰¹ã«èª²é¡Œã”ã¨ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãŒã‚ã‚Šã¾ã™ (_how to finetune a model on xxx_ ã‚’æ¢ã—ã¦ã¿ã¦ãã ã•ã„)ã€‚\n",
        "\n",
        "<!-- \n",
        "- [ğŸ¤— Transformers Examples](https://github.com/huggingface/transformers/tree/master/examples) which includes scripts to train on all common NLP tasks in PyTorch and TensorFlow.\n",
        "\n",
        "- [ğŸ¤— Transformers Notebooks](https://huggingface.co/docs/transformers/master/en/notebooks) which contains various notebooks and in particular one per task (look for the _how to finetune a model on xxx_). -->"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qay2sKxyT7dA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "2022_0104fine_tuning_bert_pretrained_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d881b3ba3b6940e186fdf4db688914a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_040448002bc549f48325ef8b78f343d2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_56f7b6dd2b4943908f11058babb89ead",
              "IPY_MODEL_5688fb89c765434098921c99ebe2efd8",
              "IPY_MODEL_e28550d40df74780a7ca6ba6629f18e0"
            ]
          }
        },
        "040448002bc549f48325ef8b78f343d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56f7b6dd2b4943908f11058babb89ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_365297c296ae4ac9aab65426f8112302",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  2%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_35dbcc9ccda34eda91921918a3671a53"
          }
        },
        "5688fb89c765434098921c99ebe2efd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_68c1e7d8f2f84b9a9e748f22bbb3e0e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 375,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_03e69c2181184dddb0ca9f701065c87f"
          }
        },
        "e28550d40df74780a7ca6ba6629f18e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6e71d95070704a91bca41d5f83be9c21",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9/375 [06:28&lt;4:17:17, 42.18s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc24003af9414d4fb2025277d19df335"
          }
        },
        "365297c296ae4ac9aab65426f8112302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "35dbcc9ccda34eda91921918a3671a53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68c1e7d8f2f84b9a9e748f22bbb3e0e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "03e69c2181184dddb0ca9f701065c87f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6e71d95070704a91bca41d5f83be9c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc24003af9414d4fb2025277d19df335": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}