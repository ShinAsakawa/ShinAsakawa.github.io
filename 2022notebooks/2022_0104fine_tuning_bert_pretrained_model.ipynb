{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0104fine_tuning_bert_pretrained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- filename: `2022_0104fine_tuning_bert_pretrained_model.ipynb`\n",
        "- date; 2022_0104\n",
        "- source: https://huggingface.co/docs/transformers/master/en/training#preparing-the-datasets\n",
        "\n",
        "# 訓練済モデルの微調整 PyTorch 版\n",
        "<!-- # Fine-tuning a pretrained model-->\n"
      ],
      "metadata": {
        "id": "MKAre5x3JWUL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T74maVQCJIGc"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6VwvyUmJIGe"
      },
      "source": [
        "# Fine-tuning a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCjuXKF1JIGf"
      },
      "source": [
        "このチュートリアルでは，Transformers ライブラリから事前訓練されたモデルを微調整する方法を紹介します。\n",
        "TensorFlow では Keras と`fit` メソッドを使ってモデルを直接訓練することができます。\n",
        "PyTorch では，汎用的な訓練ループがないため，🤗 Transformer sライブラリでは [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) というクラスで API を提供しており，モデルの微調整や訓練を簡単に行うことができます。 \n",
        "次に PyTorch で訓練ループ全体を記述する方法を紹介します。\n",
        "<!-- In this tutorial, we will show you how to fine-tune a pretrained model from the Transformers library. \n",
        "In TensorFlow, models can be directly trained using Keras and the `fit` method. \n",
        "In PyTorch, there is no generic training loop so the 🤗 Transformers library provides an API with the class [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) to let you fine-tune or train a model from scratch easily. \n",
        "Then we will show you how to alternatively write the whole training loop in PyTorch. -->\n",
        "\n",
        "\n",
        "モデルを微調整する前に，データセットが必要です。\n",
        "このチュートリアルでは [IMDB dataset](https://www.imdb.com/interfaces/) で BERT を微調整する方法を紹介します。\n",
        "課題は，映画のレビューが肯定的か否定的かを分類することです。\n",
        "他の課題例については [additional-resources](#additional-resources) 節を参照してください。\n",
        "<!-- Before we can fine-tune a model, we need a dataset. \n",
        "In this tutorial, we will show you how to fine-tune BERT on the [IMDB dataset](https://www.imdb.com/interfaces/): the task is to classify whether movie reviews are positive or negative. \n",
        "For examples of other tasks, refer to the [additional-resources](#additional-resources) section! -->\n",
        "\n",
        "<a id='data-processing'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zcNH-yeJIGg"
      },
      "source": [
        "## データセットの準備 <!--## Preparing the datasets-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": true,
        "id": "YkejdvTdJIGh",
        "outputId": "9d403a0d-e8b5-4604-c4c2-1b7e5a08a1eb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_BZearw7f0w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_BZearw7f0w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj2KgIQrJIGh"
      },
      "source": [
        "ここでは [🤗 Datasets](https://github.com/huggingface/datasets/) ライブラリを使って，IMDB データセットをダウンロードし，前処理を行います。\n",
        "この部分は簡単に説明します。\n",
        "このチュートリアルの焦点は訓練なので，詳細は🤗 Datasets [documentation](https://huggingface.co/docs/datasets/) や [preprocessing](https://huggingface.co/docs/transformers/master/en/preprocessing) のチュートリアルを参照してください。\n",
        "<!-- We will use the [🤗 Datasets](https://github.com/huggingface/datasets/) library to download and preprocess the IMDB datasets. \n",
        "We will go over this part pretty quickly. Since the focus of this tutorial is on training, you should refer to the 🤗 Datasets [documentation](https://huggingface.co/docs/datasets/) or the [preprocessing](https://huggingface.co/docs/transformers/master/en/preprocessing) tutorial for more information. -->\n",
        "\n",
        "\n",
        "まず `load_dataset` 関数を使って、データセットをダウンロードしてキャッシュします。\n",
        "<!-- First, we can use the `load_dataset` function to download and cache the dataset: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPtI4JULJIGi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICIUsuh_JIGi"
      },
      "source": [
        "これは，モデルやトークナイザで見た `from_pretrained` メソッドと同じように動作します  (ただし，キャッシュディレクトリはデフォルトで  _~/.cache/huggingface/dataset_  になっています)。\n",
        "<!-- This works like the `from_pretrained` method we saw for the models and tokenizers (except the cache directory is _~/.cache/huggingface/dataset_ by default). -->\n",
        "\n",
        "`raw_datasets` オブジェクトは，3 つのキーを持つ辞書です。\n",
        "`\"train\"`, `\"test\"`, `\"unsupervised”` の 3 つのキーを持つ辞書です (これはデータセットの 3 つの分割に対応しています)。\n",
        "`\"train”` の分割は訓練に，`\"test”` の分割は検証に使用します。\n",
        "<!-- The `raw_datasets` object is a dictionary with three keys: `\"train\"`, `\"test\"` and `\"unsupervised\"` (which correspond to the three splits of that dataset). \n",
        "We will use the `\"train\"` split for training and the `\"test\"` split for validation. -->\n",
        "\n",
        "データを前処理するためには，トークナイザが必要です。\n",
        "<!-- To preprocess our data, we will need a tokenizer: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PodWI6A-JIGj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6veK0rEJIGk"
      },
      "source": [
        "[前処理](https://huggingface.co/docs/transformers/master/en/preprocessing) で見たように，以下のコマンドでモデル用のテキスト入力を準備することができます (これは例であって，実行できるコマンドではありません)。\n",
        "<!-- As we saw in [preprocessing](https://huggingface.co/docs/transformers/master/en/preprocessing), we can prepare the text inputs for the model with the following command (this is an example, not a command you can execute): -->"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = 'This is a pen.'"
      ],
      "metadata": {
        "id": "kgtzXYTMLxXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMMjOzm3JIGk"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(sentences, padding=\"max_length\", truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy6rEf1xJIGk"
      },
      "source": [
        "これにより，すべてのサンプルは，パディングまたは切り詰めによって，モデルが受け入れられる最大の長さ (ここでは512) になります。\n",
        "\n",
        "しかし，代わりに，`map` メソッドを使って，これらの前処理をデータセットのすべての分割に一度に適用することができます。\n",
        "<!-- This will make all the samples have the maximum length the model can accept (here 512), either by padding or truncating them.\n",
        "\n",
        "However, we can instead apply these preprocessing steps to all the splits of our dataset at once by using the `map` method: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JZE_xzMJIGk"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nQPni31JIGl"
      },
      "source": [
        "マップ法や，データを前処理する他の方法については，🤗 Datasets [document](https://huggingface.co/docs/datasets/) で詳しく説明しています。\n",
        "\n",
        "次に，訓練セットと検証セットの小さなサブセットを生成して，訓練の高速化を図ります。\n",
        "<!-- You can learn more about the map method or the other ways to preprocess the data in the 🤗 Datasets [documentation](https://huggingface.co/docs/datasets/).\n",
        "\n",
        "Next we will generate a small subset of the training and validation set, to enable faster training: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1BlVGVJJIGl"
      },
      "outputs": [],
      "source": [
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "full_train_dataset = tokenized_datasets[\"train\"]\n",
        "full_eval_dataset = tokenized_datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XTxbFk3JIGl"
      },
      "source": [
        "以下のすべての例では，常に `small_train_dataset` と `small_eval_dataset` を使用しています。\n",
        "完全なデータセットで訓練や評価を行うには，これらを _full_ に置き換えるだけです。\n",
        "<!-- In all the examples below, we will always use `small_train_dataset` and `small_eval_dataset`. Just replace them by their _full_ equivalent to train or evaluate on the full dataset. -->\n",
        "\n",
        "<a id='trainer'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Ln_eQ8JIGl"
      },
      "source": [
        "## Trainer API による PyTorch の微調整\n",
        "<!-- ## Fine-tuning in PyTorch with the Trainer API -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": true,
        "id": "JqXM3lL_JIGm",
        "outputId": "1ff3a192-0074-460f-8c32-2387e6830935"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nvBXf7s7vTI?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nvBXf7s7vTI?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKoYDjBWJIGm"
      },
      "source": [
        "PyTorch は訓練ループを提供していないので 🤗 Transformers ライブラリは，🤗 Transformers モデルに最適化された [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) という API を提供しており，幅広い訓練オプションと，ロギング，勾配累積，混合精度などの機能が組み込まれています。\n",
        "\n",
        "まず，モデルを定義しましょう。\n",
        "<!-- Since PyTorch does not provide a training loop, the 🤗 Transformers library provides a [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) API that is optimized for 🤗 Transformers models, with a wide range of training options and with built-in features like logging, gradient accumulation, and mixed precision.\n",
        "\n",
        "First, let's define our model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm_HSteiJIGm"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIYkCVERJIGm"
      },
      "source": [
        "これにより，事前学習された重みの一部が使用されず，一部の重みがランダムに初期化されるという警告が表示されます。\n",
        "これは，BERT モデルの事前学習ヘッドを破棄して，ランダムに初期化された分類ヘッドに置き換えているためです。\n",
        "我々は，事前学習されたモデルの知識を移行しながら，課題上でこのモデルを微調整します (これが，転移学習と呼ばれる理由です)。\n",
        "<!-- This will issue a warning about some of the pretrained weights not being used and some weights being randomly initialized. \n",
        "That's because we are throwing away the pretraining head of the BERT model to replace it with a classification head which is randomly initialized. \n",
        "We will fine-tune this model on our task, transferring the knowledge of the pretrained model to it (which is why doing this is called transfer learning).-->\n",
        "\n",
        "次に [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer)を定義するために， [TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments) をインスタンス化する必要があります。\n",
        "このクラスには，[Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) で調整できるすべてのハイパーパラメータや，サポートしているさまざまな学習オプションを有効にするフラグが含まれています。\n",
        "チェックポイントを保存するためのディレクトリを指定するだけです。\n",
        "<!-- Then, to define our [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer), we will need to instantiate a [TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments). \n",
        "This class contains all the hyperparameters we can tune for the [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) or the flags to activate the different training options it supports. \n",
        "Let's begin by using all the defaults, the only thing we then have to provide is a directory in which the checkpoints will be saved: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CvKhr17JIGm"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"test_trainer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBWQNSqaJIGn"
      },
      "source": [
        "それでは，[Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) を次のように実体化しましょう。\n",
        "<!-- Then we can instantiate a [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) like this: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiiMw_h4JIGn"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIe_osrGJIGn"
      },
      "source": [
        "モデルを微調整するためには，ただ呼び出すだけです。\n",
        "<!-- To fine-tune our model, we just need to call -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlSoweJJJIGn"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sueYX7-wJIGn"
      },
      "source": [
        "を実行すると，プログレスバーで確認できる訓練が開始され，数分で完了します (GPU にアクセスできる限り)。\n",
        "デフォルトでは，訓練中に評価は行われず，[Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) に指標を計算するように指示していないので，モデルの性能がどの程度かについては，実際には何もわかりません。\n",
        "では，その方法を見てみましょう。\n",
        "<!-- which will start a training that you can follow with a progress bar, which should take a couple of minutes to complete (as long as you have access to a GPU). \n",
        "It won't actually tell you anything useful about how well (or badly) your model is performing however as by default, there is no evaluation during training, and we didn't tell the [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) to compute any metrics. Let's have a look on how to do that now!-->\n",
        "\n",
        "[Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) がメトリクスを計算して報告するには，予測値とラベル（[EvalPrediction](https://huggingface.co/docs/transformers/master/en/internal/trainer_utils#transformers.EvalPrediction) という名前のついたタプルにまとめられている) を受け取り，文字列アイテム (メトリクス名) と浮動小数点値 (メトリクス値) を含む辞書を返す `compute_metrics` 関数を与える必要があります。\n",
        "<!-- To have the [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) compute and report metrics, we need to give it a `compute_metrics` function that takes predictions and labels (grouped in a namedtuple called [EvalPrediction](https://huggingface.co/docs/transformers/master/en/internal/trainer_utils#transformers.EvalPrediction)) and return a dictionary with string items (the metric names) and float values (the metric values). -->\n",
        "\n",
        "🤗 Datasets ライブラリでは，NLP  で使われる一般的なメトリクスを `load_metric` 関数で簡単に取得することができます。\n",
        "ここでは，単純に `accuracy` を使います。\n",
        "次に，対数を予測値に変換するだけの `compute_metrics` 関数を定義し (すべての🤗 Transformers モデルは対数を返すことを覚えておいてください)， それをこのメトリクスの  `compute`  メソッドに与えます。\n",
        "<!-- The 🤗 Datasets library provides an easy way to get the common metrics used in NLP with the `load_metric` function. \n",
        "here we simply use accuracy. \n",
        "Then we define the `compute_metrics` function that just convert logits to predictions (remember that all 🤗 Transformers models return the logits) and feed them to `compute` method of this metric. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVQcZY1LJIGn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUOZl_PzJIGn"
      },
      "source": [
        "`compute` 関数は，(ロジットとラベルを含む) タプルを受け取り，文字列キー (メトリック名) と浮動小数点値を含む辞書を返さなければなりません。\n",
        "この関数は，各評価フェーズの終わりに，予測値やラベルの配列全体に対して呼び出されます。\n",
        "<!-- The compute function needs to receive a tuple (with logits and labels) and has to return a dictionary with string keys (the name of the metric) and float values. \n",
        "It will be called at the end of each evaluation phase on the whole arrays of predictions/labels.-->\n",
        "\n",
        "実際に動作するかどうかを確認するために，微調整されたモデルで新しい [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) を作成してみましょう。\n",
        "<!-- To check if this works on practice, let's create a new [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) with our fine-tuned model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRbGXp-gJIGo"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2eTm3imJIGo"
      },
      "source": [
        "で 87.5%の精度が得られました。\n",
        "<!-- which showed an accuracy of 87.5% in our case. -->\n",
        "\n",
        "モデルを微調整し，評価指標を定期的に (例えば，各エポックの終わりに) 報告したい場合，訓練引数をどのように定義すべきかを示します。\n",
        "<!-- If you want to fine-tune your model and regularly report the evaluation metrics (for instance at the end of each epoch), here is how you should define your training arguments: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuf2SZmeJIGo"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"test_trainer\", evaluation_strategy=\"epoch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMKW4aQpJIGo"
      },
      "source": [
        "その他のオプションについては [TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments) のドキュメントを参照してください。\n",
        "<!-- See the documentation of [TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments) for more options. -->\n",
        "\n",
        "\n",
        "<a id='keras'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VIzWScpJIGo"
      },
      "source": [
        "## Keras による微調整\n",
        "<!-- ## Fine-tuning with Keras -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "Jbwz-9e-JIGo",
        "outputId": "64fc7ba5-58aa-445a-bd96-b82add92eb82"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rnTGBy2ax1c?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rnTGBy2ax1c?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g0KbyCsJIGo"
      },
      "source": [
        "モデルは，Keras API を使って TensorFlow でネイティブに訓練することもできます。まず，モデルを定義しましょう。\n",
        "<!-- Models can also be trained natively in TensorFlow using the Keras API. First, let's define our model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYJ39_dYJIGp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiJpTP9JIGp"
      },
      "source": [
        "次に，先ほどのデータセットを標準の `tf.data.Dataset` に変換する必要があります。\n",
        "Shape が固定されているので，以下のように簡単に行うことができます。\n",
        "まず，データセットから _\"text”_ 列を削除し，TensorFlow フォーマットに設定します。\n",
        "<!-- Then we will need to convert our datasets from before in standard `tf.data.Dataset`. \n",
        "Since we have fixed shapes, it can easily be done like this. First we remove the _\"text\"_ column from our datasets and set them in TensorFlow format: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k5Yx8auJIGp"
      },
      "outputs": [],
      "source": [
        "tf_train_dataset = small_train_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
        "tf_eval_dataset = small_eval_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCIFznT-JIGp"
      },
      "source": [
        "そして，すべてを大きなテンソルに変換し，`tf.data.Dataset.from_tensor_slices` メソッドを使用します。\n",
        "<!-- Then we convert everything in big tensors and use the `tf.data.Dataset.from_tensor_slices` method: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeH4SLZFJIGp"
      },
      "outputs": [],
      "source": [
        "train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names}\n",
        "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[\"label\"]))\n",
        "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(8)\n",
        "\n",
        "eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names}\n",
        "eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset[\"label\"]))\n",
        "eval_tf_dataset = eval_tf_dataset.batch(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iVfNrmRJIGp"
      },
      "source": [
        "このようにして作成されたモデルは，他の Keras モデルと同様にコンパイルして学習することができます。\n",
        "<!-- With this done, the model can then be compiled and trained as any Keras model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpGN1awkJIGp"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "\n",
        "model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTjX2uUfJIGp"
      },
      "source": [
        "TensorFlow モデルと PyTorch モデルの間の緊密な相互運用性により，モデルを保存してから PyTorch モデルとして再読み込みすることもできます (またはその逆)。\n",
        "<!-- With the tight interoperability between TensorFlow and PyTorch models, you can even save the model and then reload it as a PyTorch model (or vice-versa): -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruW_HO9RJIGq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model.save_pretrained(\"my_imdb_model\")\n",
        "pytorch_model = AutoModelForSequenceClassification.from_pretrained(\"my_imdb_model\", from_tf=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0lV8tGFJIGq"
      },
      "source": [
        "<a id='pytorch_native'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X9MQqjoJIGq"
      },
      "source": [
        "## 生の PyTorch を使った微調整\n",
        "<!-- ## Fine-tuning in native PyTorch -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "DD_5Ei8HJIGq",
        "outputId": "b7861cfd-4018-4de6-83a2-24f24ee920eb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Dh9CL8fyG80?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Dh9CL8fyG80?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLKgNo11JIGq"
      },
      "source": [
        "この段階でノートブックを再起動してメモリを解放するか，次のコードを実行する必要があるかもしれません。\n",
        "<!-- You might need to restart your notebook at this stage to free some memory, or execute the following code: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3OuovFbJIGq",
        "outputId": "fa1db2de-2f1b-42fc-c4bb-fe1bd278114c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0e7a9cdd293c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mpytorch_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pytorch_model' is not defined"
          ]
        }
      ],
      "source": [
        "del model\n",
        "del pytorch_model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28Mqg8S_JIGq"
      },
      "source": [
        "それでは [trainer section](#trainer) と同じ結果を PyTorch で実現する方法を見てみましょう。\n",
        "まず最初に，バッチを反復処理するためのデータローダを定義する必要があります。\n",
        "その前に `tokenized_datasets` にちょっとした後処理を施す必要があります。\n",
        "<!-- Let's now see how to achieve the same results as in [trainer section](#trainer) in PyTorch. \n",
        "First we need to define the dataloaders, which we will use to iterate over batches. We just need to apply a bit of post-processing to our `tokenized_datasets` before doing that to: -->\n",
        "\n",
        "- モデルが想定していない値に対応するカラムを削除する (ここでは `\"text\"` カラム)。\n",
        "- “label\" カラムの名前を `\"labels\"` に変更する (モデルは引数に `labels` という名前がつくことを想定しているため)。\n",
        "- データセットのフォーマットを設定し，リストではなく PyTorch テンソルを返すようにします。\n",
        "\n",
        "<!-- - remove the columns corresponding to values the model does not expect (here the `\"text\"` column)\n",
        "- rename the column `\"label\"` to `\"labels\"` (because the model expect the argument to be named `labels`)\n",
        "- set the format of the datasets so they return PyTorch Tensors instead of lists. -->\n",
        "\n",
        "\n",
        "我々の _tokenized_datasets_ は，これらのステップごとに 1 つのメソッドを持っています。\n",
        "<!-- Our _tokenized_datasets_ has one method for each of those steps: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR3wlu2wJIGq"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-L_uU8rJIGr"
      },
      "source": [
        "これで，データローダを簡単に定義できるようになりました。\n",
        "<!-- Now that this is done, we can easily define our dataloaders: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjORecmeJIGr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
        "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMGM1H96JIGr"
      },
      "source": [
        "次に，モデルを定義します：\n",
        "<!-- Next, we define our model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_B1km59JIGr",
        "outputId": "94d52fa8-782f-468e-a97a-12c633cfa950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8D12Jq7JIGr"
      },
      "source": [
        "あとは，最適化と学習率スケジューラの 2 つだけです。\n",
        "[Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) で使われているデフォルトの最適化は [AdamW](https://huggingface.co/docs/transformers/master/en/main_classes/optimizer_schedules#transformers.AdamW) です。\n",
        "<!-- We are almost ready to write our training loop, the only two things are missing are an optimizer and a learning rate scheduler. \n",
        "The default optimizer used by the [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) is [AdamW](https://huggingface.co/docs/transformers/master/en/main_classes/optimizer_schedules#transformers.AdamW): -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap42FVCcJIGr"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cZBD-DTJIGr"
      },
      "source": [
        "最後に，デフォルトで使用されている学習率スケジューラは，最大値 (ここでは5e-5) から 0 までの直線的な減衰だけです。\n",
        "<!-- Finally, the learning rate scheduler used by default is just a linear decay from the maximum value (5e-5 here) to 0: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dTcKfNvJIGr"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ8at8DYJIGr"
      },
      "source": [
        "最後に，GPU が利用できる場合は，GPU を使用したいと思います (そうしないと，訓練が数分ではなく，数時間かかる可能性があります)。\n",
        "そのためには，モデルとバッチを配置する「デバイス」を定義します。\n",
        "<!-- One last thing, we will want to use the GPU if we have access to one (otherwise training might take several hours instead of a couple of minutes). \n",
        "To do this, we define a `device` we will put our model and our batches on. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbxKo8-cJIGs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#device"
      ],
      "metadata": {
        "id": "WP4vS2aiSudY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3HXkjKnJIGs"
      },
      "source": [
        "これで訓練の準備が整いました。\n",
        "訓練がいつ終了するかを知るために _tqdm_ ライブラリを使って，訓練のステップ数を示すプログレスバーを追加しました。\n",
        "<!-- We now are ready to train! To get some sense of when it will be finished, we add a progress bar over our number of training steps, using the _tqdm_ library. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPqMg4tNJIGs",
        "outputId": "0f06555d-4095-4c62-c92a-79bf94744b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d881b3ba3b6940e186fdf4db688914a7",
            "040448002bc549f48325ef8b78f343d2",
            "56f7b6dd2b4943908f11058babb89ead",
            "5688fb89c765434098921c99ebe2efd8",
            "e28550d40df74780a7ca6ba6629f18e0",
            "365297c296ae4ac9aab65426f8112302",
            "35dbcc9ccda34eda91921918a3671a53",
            "68c1e7d8f2f84b9a9e748f22bbb3e0e2",
            "03e69c2181184dddb0ca9f701065c87f",
            "6e71d95070704a91bca41d5f83be9c21",
            "cc24003af9414d4fb2025277d19df335"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d881b3ba3b6940e186fdf4db688914a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/375 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIMwZDlKJIGs"
      },
      "source": [
        "もしあなたが (コンピュータビジョンのように) 事前学習したモデルのボディを凍結させることに慣れているのであれば，何の予防措置も取らずにモデル全体を直接微調整しているため，上記は少し奇妙に見えるかもしれないことに注意してください。\n",
        "トランスフォーマーモデルでは，実際にこの方法でうまく機能しています (なので，これは私たち側の見落としではありません)。\n",
        "モデルの「ボディの凍結」が何を意味するのかわからない方は，この段落を読むのを忘れてください。\n",
        "<!-- Note that if you are used to freezing the body of your pretrained model (like in computer vision) the above may seem a bit strange, as we are directly fine-tuning the whole model without taking any precaution. \n",
        "It actually works better this way for Transformers model (so this is not an oversight on our side). \n",
        "If you're not familiar with what \"freezing the body\" of the model means, forget you read this paragraph.-->\n",
        "\n",
        "さて，結果を確認するためには，評価ループを書かなければなりません。\n",
        "[トレーナーセクション](#trainer) のように，データセットライブラリの指標を使います。\n",
        "ここでは，各バッチごとに予測値を蓄積し，ループが終了したときに最終結果を計算します。\n",
        "<!--Now to check the results, we need to write the evaluation loop. Like in the [trainer section](#trainer) we will use a metric from the datasets library. \n",
        "Here we accumulate the predictions at each batch before computing the final result when the loop is finished. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWLFI3fDJIGs"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"accuracy\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr6esigVJIGs"
      },
      "source": [
        "<a id='additional-resources'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UOMav8DJIGs"
      },
      "source": [
        "## 追加資源\n",
        "<!-- ## Additional resources -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt_Ew23lJIGs"
      },
      "source": [
        "より詳細な調整例を見るには，以下を参照してください。\n",
        "<!-- To look at more fine-tuning examples you can refer to:-->\n",
        "\n",
        "- [🤗 Transformers Examples](https://github.com/huggingface/transformers/tree/master/examples) には PyTorch とTensorFlow で一般的な NLP 課題を学習するスクリプトが含まれています。\n",
        "\n",
        "- [🤗 Transformers Notebooks](https://huggingface.co/docs/transformers/master/en/notebooks) には様々なノートブックが含まれており，特に課題ごとのノートブックがあります (_how to finetune a model on xxx_ を探してみてください)。\n",
        "\n",
        "<!-- \n",
        "- [🤗 Transformers Examples](https://github.com/huggingface/transformers/tree/master/examples) which includes scripts to train on all common NLP tasks in PyTorch and TensorFlow.\n",
        "\n",
        "- [🤗 Transformers Notebooks](https://huggingface.co/docs/transformers/master/en/notebooks) which contains various notebooks and in particular one per task (look for the _how to finetune a model on xxx_). -->"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qay2sKxyT7dA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "2022_0104fine_tuning_bert_pretrained_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d881b3ba3b6940e186fdf4db688914a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_040448002bc549f48325ef8b78f343d2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_56f7b6dd2b4943908f11058babb89ead",
              "IPY_MODEL_5688fb89c765434098921c99ebe2efd8",
              "IPY_MODEL_e28550d40df74780a7ca6ba6629f18e0"
            ]
          }
        },
        "040448002bc549f48325ef8b78f343d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56f7b6dd2b4943908f11058babb89ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_365297c296ae4ac9aab65426f8112302",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  2%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_35dbcc9ccda34eda91921918a3671a53"
          }
        },
        "5688fb89c765434098921c99ebe2efd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_68c1e7d8f2f84b9a9e748f22bbb3e0e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 375,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_03e69c2181184dddb0ca9f701065c87f"
          }
        },
        "e28550d40df74780a7ca6ba6629f18e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6e71d95070704a91bca41d5f83be9c21",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9/375 [06:28&lt;4:17:17, 42.18s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc24003af9414d4fb2025277d19df335"
          }
        },
        "365297c296ae4ac9aab65426f8112302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "35dbcc9ccda34eda91921918a3671a53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68c1e7d8f2f84b9a9e748f22bbb3e0e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "03e69c2181184dddb0ca9f701065c87f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6e71d95070704a91bca41d5f83be9c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc24003af9414d4fb2025277d19df335": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}