{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0112quicktour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- date: 2022_0112\n",
        "- source url: https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb\n",
        "- filename: 2022_0112quicktour.ipynb\n"
      ],
      "metadata": {
        "id": "I_9z4nNUHy-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzqpDZC8vVjw"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "isColab = platform.system() == 'Linux'\n",
        "if isColab:\n",
        "    # Transformers installation\n",
        "    !pip install transformers datasets > /dev/null 2>&1\n",
        "    # To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "    #! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if isColab:\n",
        "    # MeCab, fugashi, ipadic のインストール\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1\n",
        "    !pip install mecab-python3 > /dev/null 2>&1\n",
        "    !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "    !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1\n",
        "    \n",
        "    import subprocess\n",
        "    cmd='echo `mecab-config --dicdir`\\\"/mecab-ipadic-neologd\\\"'\n",
        "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                                     shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "    !pip install 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    !python -m unidic download > /dev/null 2>&1\n",
        "    !pip install ipadic > /dev/null 2>&1    "
      ],
      "metadata": {
        "id": "-nQixBxQ0jmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FcDCOywvVjy"
      },
      "source": [
        "# クィックツアー\n",
        "<!-- # Quick tour -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6E3Mt14vVjz"
      },
      "source": [
        "それでは「🤗 Transformers」ライブラリの機能を簡単にご紹介します。\n",
        "このライブラリは，テキストのセンチメントを分析するような自然言語理解 (NLU) 課題や，新しいテキストでプロンプトを完成させたり，別の言語で翻訳したりするような自然言語生成(NLG) 課題のために，事前学習済モデルをダウンロードします。\n",
        "<!-- Let's have a quick look at the 🤗 Transformers library features. \n",
        "The library downloads pretrained models for Natural Language Understanding (NLU) tasks, such as analyzing the sentiment of a text, and Natural Language Generation (NLG), such as completing a prompt with new text or translating in another language.-->\n",
        "\n",
        "まず，パイプライン API を利用して，事前学習済モデルを推論に利用する方法を説明します。\n",
        "次に，ライブラリがどのようにしてモデルにアクセスし，データの前処理を行うのかを見ていきます。\n",
        "<!--First we will see how to easily leverage the pipeline API to quickly use those pretrained models at inference. \n",
        "Then, we will dig a little bit more and see how the library gives you access to those models and helps you preprocess your data. -->\n",
        "\n",
        "<Tip>\n",
        "\n",
        "ドキュメントで紹介しているすべてのコード例では，左上に Pytorch と TensorFlow のスイッチがあります。\n",
        "そうでない場合は，コードは何の変更も必要とせずに両方のバックエンドで動作することが期待されます。\n",
        "<!-- All code examples presented in the documentation have a switch on the top left for Pytorch versus TensorFlow. \n",
        "If not, the code is expected to work for both backends without any change needed. -->\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx2NUpI1vVj0"
      },
      "source": [
        "## パイプラインを使って課題を開始する\n",
        "<!-- ## Getting started on a task with a pipeline -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWc5urSYvVj0"
      },
      "source": [
        "与えられた課題に訓練済モデルを使用する最も簡単な方法は [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) を使用することです。\n",
        "<!-- The easiest way to use a pretrained model on a given task is to use [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline). -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "eEmUvLfyvVj1"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lclqau1TvVj1"
      },
      "source": [
        "🤗 Transformer は，以下のような作業をすぐに行うことができます。\n",
        "<!-- 🤗 Transformers provides the following tasks out of the box:-->\n",
        "\n",
        "- センチメント分析：テキストがポジティブかネガティブか？\n",
        "- テキスト生成 (英語)：プロンプトを与えると，モデルがそれに応じるような文を生成する\n",
        "- 名前実体認識 (NER)：入力文中の各単語に，それが表す実体 (人，場所など) をラベル付けする\n",
        "- 質問応答：いくつかの文脈と質問をモデルに与え，文脈から答えを抽出する\n",
        "- マスキングされたテキストを埋める：マスキングされた単 語 (例：`[MASK]`に置き換えられたもの) を含むテキストが与えられた場合，その空白を埋める\n",
        "- 要約：長いテキストの要約を生成する\n",
        "- 翻訳：テキストを他の言語に翻訳する\n",
        "- 特徴抽出：テキストのテンソル表現を返す\n",
        "\n",
        "\n",
        "<!-- - Sentiment analysis: is a text positive or negative?\n",
        "- Text generation (in English): provide a prompt and the model will generate what follows.\n",
        "- Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)\n",
        "- Question answering: provide the model with some context and a question, extract the answer from the context.\n",
        "- Filling masked text: given a text with masked words (e.g., replaced by `[MASK]`), fill the blanks.\n",
        "- Summarization: generate a summary of a long text.\n",
        "- Translation: translate a text in another language.\n",
        "- Feature extraction: return a tensor representation of the text. -->\n",
        "\n",
        "これが感情分析のためにどのように機能するか見てみましょう (他の課題はすべて [課題の要約](https://huggingface.co/docs/transformers/master/en/task_summary) でカバーされています)。\n",
        "<!-- Let's see how this work for sentiment analysis (the other tasks are all covered in the [task summary](https://huggingface.co/docs/transformers/master/en/task_summary)): -->\n",
        "\n",
        "以下の依存関係をインストールしてください (未インストールの場合)。\n",
        "<!-- Install the following dependencies (if not already installed): -->\n",
        "\n",
        "```bash\n",
        "pip install torch\n",
        "```\n",
        "\n",
        "```bash\n",
        "pip install tensorflow\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eczkJFSTvVj2"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "classifier_ja = pipeline(\"sentiment-analysis\", model=\"daigo/bert-base-japanese-sentiment\",tokenizer=\"daigo/bert-base-japanese-sentiment\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpUUht6VvVj3"
      },
      "source": [
        "このコマンドを初めて入力すると，事前学習されたモデルとそのトークン化器がダウンロードされ，キャッシュされます。\n",
        "トークン化器の役割は，モデルのためにテキストを前処理することで，モデルは予測を行うことになります。\n",
        "パイプラインでは，これらすべてをグループ化し，予測結果を読めるように後処理します。\n",
        "例えば；\n",
        "<!-- When typing this command for the first time, a pretrained model and its tokenizer are downloaded and cached. \n",
        "We will look at both later on, but as an introduction the tokenizer's job is to preprocess the text for the model, which is then responsible for making predictions. \n",
        "The pipeline groups all of that together, and post-process the predictions to make them readable. \n",
        "For instance: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwtW-PATvVj3",
        "outputId": "3735d103-d7a1-4b94-ac56-3edc18436731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]\n",
            "[{'label': 'ネガティブ', 'score': 0.8490612506866455}]\n"
          ]
        }
      ],
      "source": [
        "print(classifier(\"We are very happy to show you the 🤗 Transformers library.\"))\n",
        "\n",
        "print(classifier_ja('そこはかとなく書き綴れば，怪しうこそ物狂おしけれ。'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paH3Wa5XvVj4"
      },
      "source": [
        "心強い限りですね。\n",
        "文章のリストに使用することができ，その文章は前処理された後，モデルに供給され、次のような辞書のリストが返されます。\n",
        "<!-- That's encouraging! \n",
        "You can use it on a list of sentences, which will be preprocessed then fed to the model, returning a list of dictionaries like this one: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3_8N0HSvVj4"
      },
      "outputs": [],
      "source": [
        "results = classifier([\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
        "\n",
        "\n",
        "results_ja = classifier_ja([\"私は腐女子が好きです。\", \"陰キャの特徴でしょうねぇー\", \"ねぇムーミン，こっち向いて\"])\n",
        "for result in results_ja:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn9JD-qqvVj4"
      },
      "source": [
        "大規模なデータセットで使用するには [iterating over a pipeline](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines) を参照してください。\n",
        "<!-- To use with a large dataset, look at [iterating over a pipeline](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines)-->\n",
        "\n",
        "2 つ目の文が陰性(ネガティブ)に分類されていますが (陽性か陰性かである必要があります)，その得点はかなり中立的です。\n",
        "<!-- You can see the second sentence has been classified as negative (it needs to be positive or negative) but its score is fairly neutral. -->\n",
        "\n",
        "デフォルトでは ，このパイプラインのためにダウンロードされたモデルは  `distilbert-base-uncased-finetuned-st-2-english` と呼ばれています。\n",
        "このモデルについての詳しい情報は [モデルページ](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) をご覧ください。\n",
        "[DistilBERT アーキテクチャ](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert) を使用しており，SST-2 というデータセットで感情分析課題のために微調整されています。\n",
        "<!-- By default, the model downloaded for this pipeline is called \"distilbert-base-uncased-finetuned-sst-2-english\". \n",
        "We can look at its [model page](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) to get more information about it. \n",
        "It uses the [DistilBERT architecture](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert) and has been fine-tuned on a dataset called SST-2 for the sentiment analysis task. -->\n",
        "\n",
        "例えば，フランスのデータで訓練された別のモデルを使いたいとします。\n",
        "研究機関が多くのデータで事前に学習したモデルを集めた [モデルハブ](https://huggingface.co/models) や，コミュニティモデル (通常，特定のデータセットで大規模モデルを微調整したもの) を検索することができます。\n",
        "タグ「フランス語」と「テキスト分類」を適用すると `nlptown/bert-base-multilingual-uncased-sentiment` という示唆が得られます。\n",
        "どのように使用できるか見てみましょう。\n",
        "<!-- Let's say we want to use another model; for instance, one that has been trained on French data. \n",
        "We can search through the [model hub](https://huggingface.co/models) that gathers models pretrained on a lot of data by research labs, but also community models (usually fine-tuned versions of those big models on a specific dataset). \n",
        "Applying the tags  \"French\" and \"text-classification\" gives back a suggestion \"nlptown/bert-base-multilingual-uncased-sentiment\". \n",
        "Let's see how we can use it. -->\n",
        "\n",
        "使用するモデルの名前を直接 [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) に渡すことができます。\n",
        "<!-- You can directly pass the name of the model to use to [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline): -->"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kiritubo = \"いずれの御時にか、女御・更衣あまた候い給いける中に、いとやんごとなききわにはあらぬが、すぐれて時めき給うありけり\"\n",
        "print(pipeline(\"sentiment-analysis\",model=\"daigo/bert-base-japanese-sentiment\",tokenizer=\"daigo/bert-base-japanese-sentiment\")(kiritubo))"
      ],
      "metadata": {
        "id": "qTnckjhPzpBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDarey-lvVj5"
      },
      "outputs": [],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "classifier_ja = pipeline(\"sentiment-analysis\", model=\"daigo/bert-base-japanese-sentiment\",tokenizer=\"daigo/bert-base-japanese-sentiment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvS1ointvVj5"
      },
      "source": [
        "この分類器は，英語，フランス語だけでなく，オランダ語，ドイツ語，イタリア語，スペイン語のテキストを扱うことができるようになりました。 \n",
        "(訳注： 日本語も可)\n",
        "この名前の代わりに，事前学習済みのモデルを保存したローカルフォルダを指定することもできます (下記参照)。\n",
        "また，モデルオブジェクトとそれに関連するトークン化器を渡すこともできます。\n",
        "<!-- This classifier can now deal with texts in English, French, but also Dutch, German, Italian and Spanish! \n",
        "You can also replace that name by a local folder where you have saved a pretrained model (see below). \n",
        "You can also pass a model object and its associated tokenizer.-->\n",
        "\n",
        "これには 2 つのクラスが必要になります。\n",
        "1 つ目は [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer) で，選んだモデルに関連するトークン化器をダウンロードして実体化するのに使います。\n",
        "2 つ目は [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (TensorFlowを使用している場合は [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification)) で， これはモデル自体をダウンロードするために使用します。\n",
        "他の課題でライブラリを使用していた場合は，モデルのクラスが変わることに注意してください。\n",
        "チュートリアルの [課題の要約 task summary](https://huggingface.co/docs/transformers/master/en/task_summary) では，どのタスクにどのクラスが使われているかをまとめています。\n",
        "<!-- We will need two classes for this. \n",
        "The first is [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer), which we will use to download the tokenizer associated to the model we picked and instantiate it. \n",
        "The second is [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (or [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification) if you are using TensorFlow), which we will use to download the model itself. \n",
        "Note that if we were using the library on an other task, the class of the model would change. \n",
        "The [task summary](https://huggingface.co/docs/transformers/master/en/task_summary) tutorial summarizes which class is used for which task. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCkBxngEvVj5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvdvZvy-vVj6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNCJnhycvVj6"
      },
      "source": [
        "さて，先ほど見つけたモデルとトークン化器をダウンロードするには `from_pretrained()` メソッドを使うだけです。\n",
        "(ご自由にモデルハブから他のモデルに置き換えてください)。\n",
        "<!-- Now, to download the models and tokenizer we found previously, we just have to use the `from_pretrained()` method (feel free to replace `model_name` by\n",
        "any other model from the model hub): -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tymdubYBvVj6"
      },
      "outputs": [],
      "source": [
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "model_name_ja = \"daigo/bert-base-japanese-sentiment\"\n",
        "model_ja = AutoModelForSequenceClassification.from_pretrained(model_name_ja)\n",
        "tknz_ja = AutoTokenizer.from_pretrained(model_name_ja)\n",
        "classifier_ja = pipeline(\"sentiment-analysis\", model=model_ja, tokenizer=tknz_ja)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSOGa7SqvVj6"
      },
      "outputs": [],
      "source": [
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "\n",
        "# このモデルは PyTorch にしか存在しないので _from_pt_ フラグを使って TensorFlowでそのモデルをインポートします。\n",
        "# This model only exists in PyTorch, so we use the _from_pt_ flag to import that model in TensorFlow.\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "model_name_ja = \"daigo/bert-base-japanese-sentiment\"\n",
        "model_ja = TFAutoModelForSequenceClassification.from_pretrained(model_name_ja, from_pt=True)\n",
        "tknz_ja = AutoTokenizer.from_pretrained(model_name_ja)\n",
        "classifier_ja = pipeline(\"sentiment-analysis\", model=model_ja, tokenizer=tknz_ja)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Ll_z-nvVj6"
      },
      "source": [
        "自分のデータに似たデータで事前学習されたモデルが見つからない場合は，自分のデータで事前学習されたモデルを微調整する必要があります。\n",
        "そのための [サンプルスクリプト](https://huggingface.co/docs/transformers/master/en/examples) を用意しています。\n",
        "調整が終わったら， [このチュートリアル](https://huggingface.co/docs/transformers/master/en/model_sharing) を使って，ハブ上で微調整したモデルをコミュニティと共有することをお忘れなく。\n",
        "<!-- If you don't find a model that has been pretrained on some data similar to yours, you will need to fine-tune a pretrained model on your data. We provide [example scripts](https://huggingface.co/docs/transformers/master/en/examples) to do so. \n",
        "Once you're done, don't forget to share your fine-tuned model on the hub with the community, using [this tutorial](https://huggingface.co/docs/transformers/master/en/model_sharing). -->\n",
        "\n",
        "<a id='pretrained-model'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mlh51GMvVj6"
      },
      "source": [
        "## 内部処理：訓練済モデル\n",
        "<!-- ## Under the hood: pretrained models -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj5NtT57vVj7"
      },
      "source": [
        "これらのパイプラインを使ったときに，内部で何が起こっているのかを見てみましょう。\n",
        "<!-- Let's now see what happens beneath the hood when using those pipelines. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "tZw7IXgYvVj7"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AhChOFRegn4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_vsgsA-vVj7"
      },
      "source": [
        "先ほど見たように，モデルとトークン化器は `from_pretrained` メソッドを使って作成されます。\n",
        "<!-- As we saw, the model and tokenizer are created using the `from_pretrained` method: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGxS_Lz6vVj7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model_name_ja = \"daigo/bert-base-japanese-sentiment\"\n",
        "pt_model_ja = AutoModelForSequenceClassification.from_pretrained(model_name_ja)\n",
        "tknz_ja = AutoTokenizer.from_pretrained(model_name_ja)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl02SkTmvVj7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#model_name_ja = \"daigo/bert-base-japanese-sentiment\"\n",
        "#tf_model_ja = TFAutoModelForSequenceClassification.from_pretrained(model_name_ja)\n",
        "#tknz_ja = AutoTokenizer.from_pretrained(model_name_ja)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IZgamMqvVj7"
      },
      "source": [
        "### トークン化器の使用 \n",
        "<!-- ### Using the tokenizer -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1cQkVyNvVj7"
      },
      "source": [
        "トークン化器は，テキストの前処理を担当すると述べました。\n",
        "まず，与えられたテキストを通常 _tokens_ と呼ばれる単語 (または単語の一部，句読点など) に分割します。\n",
        "この処理には複数の規則があります (詳細は [トークン化器の要約 tokenizer summary](https://huggingface.co/docs/transformers/master/en/tokenizer_summary) をご覧ください)。\n",
        "そのため，モデルの名前を使ってトークン化器を実体化し，モデルが事前学習されたときと同じ規則を使用する必要があります。\n",
        "\n",
        "第 2 ステップは，トークンを数値に変換し，それらからテンソルを構築してモデルに供給できるようにすることです。これを行うために，トークン化器は _vocab_ を持っています。\n",
        "これは `from_pretrained` メソッドで実体化する際にダウンロードする部分で，モデルが事前学習されたときと同じ_vocab_ を使用する必要があるからです。\n",
        "\n",
        "これらのステップを与えられたテキストに適用するには，トークン化器に与えればよいのです。\n",
        "<!-- We mentioned the tokenizer is responsible for the preprocessing of your texts. \n",
        "First, it will split a given text in words (or part of words, punctuation symbols, etc.) usually called _tokens_. \n",
        "There are multiple rules that can govern that process (you can learn more about them in the [tokenizer summary](https://huggingface.co/docs/transformers/master/en/tokenizer_summary)), which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules as when the model was pretrained.\n",
        "\n",
        "The second step is to convert those _tokens_ into numbers, to be able to build a tensor out of them and feed them to the model. To do this, the tokenizer has a _vocab_, which is the part we download when we instantiate it with the `from_pretrained` method, since we need to use the same _vocab_ as when the model was pretrained.\n",
        "\n",
        "To apply these steps on a given text, we can just feed it to our tokenizer: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYa8WIuyvVj8"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")\n",
        "\n",
        "inputs_ja = tokenizer(\"山路を登りながら、こう考えた。智に働けば角かどが立つ。情に棹さおさせば流される。意地を通とおせば窮屈だ。とかくに人の世は住みにくい。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXV4v56uvVj8"
      },
      "source": [
        "上は，辞書文字列を int リストにして返します。\n",
        "前述のように [ids of the tokens](https://huggingface.co/docs/transformers/master/en/glossary#input-ids) が含まれていますが，モデルに有用な追加の引数も含まれています。\n",
        "例えば，ここでは，モデルが系列をより良く理解するために使用する [注意マスク](https://huggingface.co/docs/transformers/master/en/glossary#attention-mask) もあります。\n",
        "<!-- This returns a dictionary string to list of ints. \n",
        "It contains the [ids of the tokens](https://huggingface.co/docs/transformers/master/en/glossary#input-ids), as mentioned before, but also additional arguments that will be useful to the model. Here for instance, we also have an [attention mask](https://huggingface.co/docs/transformers/master/en/glossary#attention-mask) that the model will use to have a better understanding of the sequence: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umH733NVvVj8"
      },
      "outputs": [],
      "source": [
        "print(inputs)\n",
        "print(inputs_ja)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYasHbpqvVj8"
      },
      "source": [
        "文章のリストをトークン化器に直接渡すことができます。\n",
        "文章を一括してモデルに送りたい場合は，すべての文章を同じ長さにして，モデルが受け入れられる最大の長さに切り詰めて，テンソルを返してほしいと思うでしょう。\n",
        "トークン化器にはこれらすべてを指定することができます。\n",
        "<!-- You can pass a list of sentences directly to your tokenizer. \n",
        "If your goal is to send them through your model as a batch, you probably want to pad them all to the same length, truncate them to the maximum length the model can accept and get tensors back. \n",
        "You can specify all of that to the tokenizer: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-39iNY_8vVj8"
      },
      "outputs": [],
      "source": [
        "pt_batch = tokenizer(\n",
        "    [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "pt_batch_ja = tknz_ja(\n",
        "    [\"山路を登りながら、こう考えた。\", \"智に働けば角かどが立つ。\", \"情に棹さおさせば流される。\", \"意地を通とおせば窮屈だ。\", \"とかくに人の世は住みにくい。\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3EBRrJuvVj8"
      },
      "outputs": [],
      "source": [
        "# tf_batch = tokenizer(\n",
        "#     [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n",
        "#     padding=True,\n",
        "#     truncation=True,\n",
        "#     max_length=512,\n",
        "#     return_tensors=\"tf\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbFt-jMDvVj8"
      },
      "source": [
        "パディングは，モデルが予想する側 (ここでは右側) に，モデルが事前に学習したパディング・トークンを用いて自動的に適用されます。\n",
        "注意マスクもパディングを考慮して適応されます。\n",
        "<!-- The padding is automatically applied on the side expected by the model (in this case, on the right), with the padding token the model was pretrained with. \n",
        "The attention mask is also adapted to take the padding into account: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlX7aKRFvVj9"
      },
      "outputs": [],
      "source": [
        "for key, value in pt_batch.items():\n",
        "    print(f\"{key}: {value.numpy().tolist()}\")\n",
        "\n",
        "print('-' * 77)\n",
        "\n",
        "for key, value in pt_batch_ja.items():\n",
        "    print(f\"{key}: {value.numpy().tolist()}\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwLfbI1xvVj9"
      },
      "outputs": [],
      "source": [
        "# for key, value in tf_batch.items():\n",
        "#     print(f\"{key}: {value.numpy().tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h78pVemHvVj9"
      },
      "source": [
        "トークン化器については [こちら](https://huggingface.co/docs/transformers/master/en/preprocessing) をご覧ください。\n",
        "<!-- You can learn more about tokenizers [here](https://huggingface.co/docs/transformers/master/en/preprocessing). -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrDPUzInvVj9"
      },
      "source": [
        "### モデルの使用 \n",
        "<!-- ### Using the model -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx6-G8KJvVj9"
      },
      "source": [
        "入力がトークン化器によって前処理されたら，それを直接モデルに送ることができます。\n",
        "前述したように，モデルが必要とするすべての関連情報が含まれます。\n",
        "TensorFlow モデルを使用している場合は，辞書のキーを直接テンソルに渡すことができますが，PyTorch モデルの場合は，`**` を追加して辞書をアンパックする必要があります。\n",
        "<!-- Once your input has been preprocessed by the tokenizer, you can send it directly to the model. \n",
        "As we mentioned, it will contain all the relevant information the model needs. \n",
        "If you're using a TensorFlow model, you can pass the dictionary keys directly to tensors, for a PyTorch model, you need to unpack the dictionary by adding `**`. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l576uH_vVj9"
      },
      "outputs": [],
      "source": [
        "pt_outputs = pt_model(**pt_batch)\n",
        "pt_outputs_ja = pt_model_ja(**pt_batch_ja)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nASfyazXvVj9"
      },
      "outputs": [],
      "source": [
        "#tf_outputs = tf_model(tf_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyQc24yUvVj-"
      },
      "source": [
        "Transformers では，すべての出力は，モデルの最終的な活性化と他のメタデータを含むオブジェクトです。\n",
        "これらのオブジェクトについては [こちら](https://huggingface.co/docs/transformers/master/en/main_classes/output) で詳しく説明しています。\n",
        "とりあえず，出力を自分で調べてみましょう。\n",
        "<!-- In 🤗 Transformers, all outputs are objects that contain the model's final activations along with other metadata. \n",
        "These objects are described in greater detail [here](https://huggingface.co/docs/transformers/master/en/main_classes/output). \n",
        "For now, let's inspect the output ourselves: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MashngE9vVj-"
      },
      "outputs": [],
      "source": [
        "print(pt_outputs)\n",
        "print(pt_outputs_ja)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60Hj-yARvVj-"
      },
      "outputs": [],
      "source": [
        "#print(tf_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdtL9LSkvVj-"
      },
      "source": [
        "出力オブジェクトが `logits` 属性を持っていることに注目してください。\n",
        "これを使って，モデルの最終的な活性値にアクセスすることができます。\n",
        "<!-- Notice how the output object has a `logits` attribute. You can use this to access the model's final activations. -->\n",
        "\n",
        "<Tip>\n",
        "\n",
        "すべての🤗 Transformers モデル (PyTorch または TensorFlow) は，最終的な活性化関数 (SoftMax のような) の前に，モデルの活性化を返します。\n",
        "<!-- All 🤗 Transformers models (PyTorch or TensorFlow) return the activations of the model *before* the final activation function (like SoftMax) since this final activation function is often fused with the loss. -->\n",
        "\n",
        "</Tip>\n",
        "\n",
        "予測値を得るために SoftMax の活性値を適用してみましょう。\n",
        "<!-- Let's apply the SoftMax activation to get predictions. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEoxxXnTvVj-"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
        "pt_predictions_ja = nn.functional.softmax(pt_outputs_ja.logits, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMBOgyiovVj-"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdUDt7t7vVj-"
      },
      "source": [
        "先ほどの数字が出てきましたね。\n",
        "<!-- We can see we get the numbers from before: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu2z9Q2PvVj_"
      },
      "outputs": [],
      "source": [
        "print(pt_predictions)\n",
        "print(pt_predictions_ja)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7uZkTgNvVj_"
      },
      "outputs": [],
      "source": [
        "#print(tf_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXQ_TQPgvVj_"
      },
      "source": [
        "入力に加えてラベルをモデルに与えた場合，モデルの出力オブジェクトには `loss` 属性も含まれます。\n",
        "<!-- If you provide the model with labels in addition to inputs, the model output object will also contain a `loss` attribute: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAKQADTWvVj_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "pt_outputs = pt_model(**pt_batch, labels=torch.tensor([1, 0]))\n",
        "print(pt_outputs)\n",
        "\n",
        "#pt_outputs_ja = pt_model_ja(**pt_batch_ja, labels=torch.tensor([1, 0]))\n",
        "pt_outputs_ja = pt_model_ja(**pt_batch_ja)\n",
        "print(pt_outputs_ja)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFK6QkjXvVj_"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# tf_outputs = tf_model(tf_batch, labels=tf.constant([1, 0]))\n",
        "# print(tf_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WksHAeHUvVj_"
      },
      "source": [
        "モデルは標準的な [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) または [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) なので，通常の訓練ループで使用することができます。\n",
        "🤗 Transformers では，Keras の `fit()` メソッドを利用するのに対して PyTorch での訓練を支援する [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) クラスも提供しています (分散訓練，混合精度などに対応しています)。\n",
        "詳しくは [training tutorial](https://huggingface.co/docs/transformers/master/en/training) をご覧ください。\n",
        "<!-- Models are standard [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) or [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) so you can use them in your usual training loop. \n",
        "🤗 Transformers also provides a [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) class to help with your training in PyTorch (taking care of things such as distributed training, mixed precision, etc.) whereas you can leverage the `fit()` method in Keras. \n",
        "See the [training tutorial](https://huggingface.co/docs/transformers/master/en/training) for more details. -->\n",
        "\n",
        "<Tip>\n",
        "\n",
        "Pytorch のモデル出力は特別なデータクラスであり，IDE でその属性を自動補完できるようになっています。\n",
        "また，タプルや辞書のように振る舞うこともできます (例えば，整数やスライス，文字列でインデックスを付けることができます)。\n",
        "この場合，設定されていない (`None` 値を持つ) 属性は無視されます。\n",
        "<!-- Pytorch model outputs are special dataclasses so that you can get autocompletion for their attributes in an IDE. \n",
        "They also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes not set (that have `None` values) are ignored. -->\n",
        "\n",
        "</Tip>\n",
        "\n",
        "モデルの微調整が終わったら，次のようにしてトークン化器を使って保存します。\n",
        "<!-- Once your model is fine-tuned, you can save it with its tokenizer in the following way: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoDVlBnXvVj_"
      },
      "outputs": [],
      "source": [
        "pt_save_directory = \"./pt_save_pretrained\"\n",
        "\n",
        "tokenizer.save_pretrained(pt_save_directory)\n",
        "pt_model.save_pretrained(pt_save_directory)\n",
        "\n",
        "pt_save_directory_ja = \"./pt_save_pretrained_ja\"\n",
        "tknz_ja.save_pretrained(pt_save_directory_ja)\n",
        "pt_model_ja.save_pretrained(pt_save_directory_ja)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lt pt_save_pretrained_ja"
      ],
      "metadata": {
        "id": "WafAx4o4DY_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeJcx4IpvVkA"
      },
      "outputs": [],
      "source": [
        "# tf_save_directory = \"./tf_save_pretrained\"\n",
        "# tokenizer.save_pretrained(tf_save_directory)\n",
        "# tf_model.save_pretrained(tf_save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmwuLZauvVkA"
      },
      "source": [
        "そして，モデル名の代わりにディレクトリ名を渡して，`AutoModel.from_pretrained()` メソッドを使ってこのモデルをロードバックすることができます。\n",
        "Transformers の優れた機能の一つに，PyTorch と TensorFlow を簡単に切り替えられることがあります。\n",
        "<!-- You can then load this model back using the `AutoModel.from_pretrained()` method by passing the directory name instead of the model name. \n",
        "One cool feature of 🤗 Transformers is that you can easily switch between PyTorch and TensorFlow: any model saved as before can be loaded back either in PyTorch or TensorFlow.-->\n",
        "\n",
        "保存したモデルを他のフレームワークでロードしたい場合は，まずそれがインストールされていることを確認してください。\n",
        "<!-- If you would like to load your saved model in the other framework, first make sure it is installed: -->\n",
        "\n",
        "```bash\n",
        "pip install torch\n",
        "```\n",
        "\n",
        "```bash\n",
        "pip install tensorflow\n",
        "```\n",
        "\n",
        "そして，対応する Auto クラスを使って，次のように読み込みます。\n",
        "<!-- Then, use the corresponding Auto class to load it like this: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIZmb-BAvVkA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n",
        "# pt_model = AutoModel.from_pretrained(tf_save_directory, from_tf=True)\n",
        "\n",
        "# tknz_ja = AutoTokenizer.from_pretrained(tf_save_directory_ja)\n",
        "# pt_model_ja = AutoModel.from_pretrained(tf_save_directory_ja, from_tf=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nT2JXzWvVkA"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
        "tf_model = TFAutoModel.from_pretrained(pt_save_directory, from_pt=True)\n",
        "\n",
        "tknz_ja = AutoTokenizer.from_pretrained(pt_save_directory_ja)\n",
        "tf_model_ja = TFAutoModel.from_pretrained(pt_save_directory_ja, from_pt=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bqt-kIuvVkA"
      },
      "source": [
        "最後に，必要であれば，すべての隠れた状態とすべての注目度の重みを返すようにモデルに求めることもできます。\n",
        "<!-- Lastly, you can also ask the model to return all hidden states and all attention weights if you need them: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnxjJKC4vVkA"
      },
      "outputs": [],
      "source": [
        "pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)\n",
        "all_hidden_states = pt_outputs.hidden_states\n",
        "all_attentions = pt_outputs.attentions\n",
        "\n",
        "pt_outputs_ja = pt_model_ja(**pt_batch_ja, output_hidden_states=True, output_attentions=True)\n",
        "all_hidden_states_ja = pt_outputs_ja.hidden_states\n",
        "all_attentions_ja = pt_outputs_ja.attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kloeqh7vVkA"
      },
      "outputs": [],
      "source": [
        "# tf_outputs = tf_model(tf_batch, output_hidden_states=True, output_attentions=True)\n",
        "# all_hidden_states = tf_outputs.hidden_states\n",
        "# all_attentions = tf_outputs.attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcf37BJxvVkA"
      },
      "source": [
        "### コードへのアクセス\n",
        "<!-- ### Accessing the code -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieABZz3SvVkA"
      },
      "source": [
        "[AutoModel](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModel) と [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer) クラスは，事前訓練されたモデルであれば自動的に動作するショートカットに過ぎません。\n",
        "舞台裏では，ライブラリはアーキテクチャとクラスの組み合わせごとに 1 つのモデルクラスを持っているので，コードにアクセスして必要に応じて調整するのは簡単です。\n",
        "<!-- The [AutoModel](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModel) and [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer) classes are just shortcuts that will automatically work with any pretrained model. \n",
        "Behind the scenes, the library has one model class per combination of architecture plus class, so the code is easy to access and tweak if you need to.-->\n",
        "\n",
        "先ほどの例では，モデルは `distilbert-base-uncased-finetuned-st-2-english` と呼ばれていましたが，これは [DistilBERT](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert) アーキテクチャを使用していることを意味しています。\n",
        "[AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (TensorFlow を使用している場合は [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification)) が使用されたので，自動的に作成されたモデルは [DistilBertForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification) となります。\n",
        "そのモデルに関連する詳細はドキュメントを見るか，ソースコードを参照してください。\n",
        "このように，自動生成機能を使わずに，モデルとトークン化器を直接実体化することができます。\n",
        "<!--In our previous example, the model was called \"distilbert-base-uncased-finetuned-sst-2-english\", which means it's using the [DistilBERT](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert) architecture. \n",
        "As [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (or [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification) if you are using TensorFlow) was used, the model automatically created is then a [DistilBertForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification). \n",
        "You can look at its documentation for all details relevant to that specific model, or browse the source code. This is how you would directly instantiate model and tokenizer without the auto magic: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u1Z_yqjvVkA"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me_MY48YvVkB"
      },
      "outputs": [],
      "source": [
        "# from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "\n",
        "# model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "# model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0ySQAoGvVkB"
      },
      "source": [
        "### モデルのカスタマイズ\n",
        "<!-- ### Customizing the model -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bViFEy_3vVkB"
      },
      "source": [
        "モデル自体の構築方法を変更したい場合は、カスタム構成クラスを定義することができます。\n",
        "各アーキテクチャにはそれぞれ関連する設定があります。\n",
        "例えば [DistilBertConfig](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig)で は，DistilBERT の隠れ層次元，ドロップアウト率などのパラメータを指定することができます。 \n",
        "隠れ層のサイズの変更などのコアな修正を行った場合，事前訓練されたモデルを使用することができなくなり，ゼロから訓練する必要があります。\n",
        "そして，この設定から直接モデルを実体化します。\n",
        "<!-- If you want to change how the model itself is built, you can define a custom configuration class. \n",
        "Each architecture comes with its own relevant configuration. For example, [DistilBertConfig](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig) allows you to specify\n",
        "parameters such as the hidden dimension, dropout rate, etc for DistilBERT. \n",
        "If you do core modifications, like changing the hidden size, you won't be able to use a pretrained model anymore and will need to train from scratch. \n",
        "You would then instantiate the model directly from this configuration.-->\n",
        "\n",
        "以下では [from_pretrained()](https://huggingface.co/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained) メソッドを使って ，トークン化器用の事前定義された語彙を読み込みます。\n",
        "しかし，トークン化器とは異なり，モデルをゼロから初期化したいと考えています。\n",
        "そのため [DistilBertForSequenceClassification.from_pretrained()](https://huggingface.co/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) メソッドを使う代わりに，設定からモデルをインスタンス化しています。\n",
        "<!-- Below, we load a predefined vocabulary for a tokenizer with the [from_pretrained()](https://huggingface.co/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained) method. However, unlike the tokenizer, we wish to initialize the model from scratch. Therefore, we instantiate the model from a configuration instead of using the [DistilBertForSequenceClassification.from_pretrained()](https://huggingface.co/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) method. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ULCa26UvVkB"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4 * 512)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForSequenceClassification(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iW_lnqKvVkB"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "\n",
        "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4 * 512)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = TFDistilBertForSequenceClassification(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxTlCOOWvVkB"
      },
      "source": [
        "モデルの頭の部分 (例えば，ラベル数) だけを変えるものについては，体 (ボディ) の部分には事前学習済みのモデルを使うことができます。\n",
        "例えば 10 種類のラベルに対応する分類器を，事前に学習された体を使って定義してみましょう。\n",
        "ラベルの数を変更するために，すべてのデフォルト値を持つ新しいコンフィグレーションを作成する代わりに，コンフィグレーションが取る任意の引数を `from_pretrained` メソッドに渡せば，デフォルトのコンフィグレーションを適切に更新してくれます。\n",
        "<!-- For something that only changes the head of the model (for instance, the number of labels), you can still use a pretrained model for the body. \n",
        "For instance, let's define a classifier for 10 different labels using a pretrained body.\n",
        "Instead of creating a new configuration with all the default values just to change the number of labels, we can instead pass any argument a configuration would take to the `from_pretrained` method and it will update the default configuration appropriately: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzMVnaIUvVkB"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCx4ZGHavVkB"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2022_0112quicktour.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}