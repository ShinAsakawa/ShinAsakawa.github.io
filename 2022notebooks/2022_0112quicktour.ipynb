{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0112quicktour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- date: 2022_0112\n",
        "- source url: https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb\n",
        "- filename: 2022_0112quicktour.ipynb\n"
      ],
      "metadata": {
        "id": "I_9z4nNUHy-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzqpDZC8vVjw"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "isColab = platform.system() == 'Linux'\n",
        "if isColab:\n",
        "    # Transformers installation\n",
        "    !pip install transformers datasets > /dev/null 2>&1\n",
        "    # To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "    #! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if isColab:\n",
        "    # MeCab, fugashi, ipadic ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1\n",
        "    !pip install mecab-python3 > /dev/null 2>&1\n",
        "    !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "    !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1\n",
        "    \n",
        "    import subprocess\n",
        "    cmd='echo `mecab-config --dicdir`\\\"/mecab-ipadic-neologd\\\"'\n",
        "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                                     shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "    !pip install 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    !python -m unidic download > /dev/null 2>&1\n",
        "    !pip install ipadic > /dev/null 2>&1    "
      ],
      "metadata": {
        "id": "-nQixBxQ0jmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FcDCOywvVjy"
      },
      "source": [
        "# ã‚¯ã‚£ãƒƒã‚¯ãƒ„ã‚¢ãƒ¼\n",
        "<!-- # Quick tour -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6E3Mt14vVjz"
      },
      "source": [
        "ãã‚Œã§ã¯ã€ŒğŸ¤— Transformersã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ©Ÿèƒ½ã‚’ç°¡å˜ã«ã”ç´¹ä»‹ã—ã¾ã™ã€‚\n",
        "ã“ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ï¼Œãƒ†ã‚­ã‚¹ãƒˆã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚’åˆ†æã™ã‚‹ã‚ˆã†ãªè‡ªç„¶è¨€èªç†è§£ (NLU) èª²é¡Œã‚„ï¼Œæ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®Œæˆã•ã›ãŸã‚Šï¼Œåˆ¥ã®è¨€èªã§ç¿»è¨³ã—ãŸã‚Šã™ã‚‹ã‚ˆã†ãªè‡ªç„¶è¨€èªç”Ÿæˆ(NLG) èª²é¡Œã®ãŸã‚ã«ï¼Œäº‹å‰å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n",
        "<!-- Let's have a quick look at the ğŸ¤— Transformers library features. \n",
        "The library downloads pretrained models for Natural Language Understanding (NLU) tasks, such as analyzing the sentiment of a text, and Natural Language Generation (NLG), such as completing a prompt with new text or translating in another language.-->\n",
        "\n",
        "ã¾ãšï¼Œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ API ã‚’åˆ©ç”¨ã—ã¦ï¼Œäº‹å‰å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«åˆ©ç”¨ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n",
        "æ¬¡ã«ï¼Œãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã©ã®ã‚ˆã†ã«ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ï¼Œãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’è¡Œã†ã®ã‹ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚\n",
        "<!--First we will see how to easily leverage the pipeline API to quickly use those pretrained models at inference. \n",
        "Then, we will dig a little bit more and see how the library gives you access to those models and helps you preprocess your data. -->\n",
        "\n",
        "<Tip>\n",
        "\n",
        "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ç´¹ä»‹ã—ã¦ã„ã‚‹ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã§ã¯ï¼Œå·¦ä¸Šã« Pytorch ã¨ TensorFlow ã®ã‚¹ã‚¤ãƒƒãƒãŒã‚ã‚Šã¾ã™ã€‚\n",
        "ãã†ã§ãªã„å ´åˆã¯ï¼Œã‚³ãƒ¼ãƒ‰ã¯ä½•ã®å¤‰æ›´ã‚‚å¿…è¦ã¨ã›ãšã«ä¸¡æ–¹ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§å‹•ä½œã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚\n",
        "<!-- All code examples presented in the documentation have a switch on the top left for Pytorch versus TensorFlow. \n",
        "If not, the code is expected to work for both backends without any change needed. -->\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx2NUpI1vVj0"
      },
      "source": [
        "## ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ã£ã¦èª²é¡Œã‚’é–‹å§‹ã™ã‚‹\n",
        "<!-- ## Getting started on a task with a pipeline -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWc5urSYvVj0"
      },
      "source": [
        "ä¸ãˆã‚‰ã‚ŒãŸèª²é¡Œã«è¨“ç·´æ¸ˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚\n",
        "<!-- The easiest way to use a pretrained model on a given task is to use [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline). -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "eEmUvLfyvVj1"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lclqau1TvVj1"
      },
      "source": [
        "ğŸ¤— Transformer ã¯ï¼Œä»¥ä¸‹ã®ã‚ˆã†ãªä½œæ¥­ã‚’ã™ãã«è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- ğŸ¤— Transformers provides the following tasks out of the box:-->\n",
        "\n",
        "- ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æï¼šãƒ†ã‚­ã‚¹ãƒˆãŒãƒã‚¸ãƒ†ã‚£ãƒ–ã‹ãƒã‚¬ãƒ†ã‚£ãƒ–ã‹ï¼Ÿ\n",
        "- ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ (è‹±èª)ï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸ãˆã‚‹ã¨ï¼Œãƒ¢ãƒ‡ãƒ«ãŒãã‚Œã«å¿œã˜ã‚‹ã‚ˆã†ãªæ–‡ã‚’ç”Ÿæˆã™ã‚‹\n",
        "- åå‰å®Ÿä½“èªè­˜ (NER)ï¼šå…¥åŠ›æ–‡ä¸­ã®å„å˜èªã«ï¼Œãã‚ŒãŒè¡¨ã™å®Ÿä½“ (äººï¼Œå ´æ‰€ãªã©) ã‚’ãƒ©ãƒ™ãƒ«ä»˜ã‘ã™ã‚‹\n",
        "- è³ªå•å¿œç­”ï¼šã„ãã¤ã‹ã®æ–‡è„ˆã¨è³ªå•ã‚’ãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆï¼Œæ–‡è„ˆã‹ã‚‰ç­”ãˆã‚’æŠ½å‡ºã™ã‚‹\n",
        "- ãƒã‚¹ã‚­ãƒ³ã‚°ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚ã‚‹ï¼šãƒã‚¹ã‚­ãƒ³ã‚°ã•ã‚ŒãŸå˜ èª (ä¾‹ï¼š`[MASK]`ã«ç½®ãæ›ãˆã‚‰ã‚ŒãŸã‚‚ã®) ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆï¼Œãã®ç©ºç™½ã‚’åŸ‹ã‚ã‚‹\n",
        "- è¦ç´„ï¼šé•·ã„ãƒ†ã‚­ã‚¹ãƒˆã®è¦ç´„ã‚’ç”Ÿæˆã™ã‚‹\n",
        "- ç¿»è¨³ï¼šãƒ†ã‚­ã‚¹ãƒˆã‚’ä»–ã®è¨€èªã«ç¿»è¨³ã™ã‚‹\n",
        "- ç‰¹å¾´æŠ½å‡ºï¼šãƒ†ã‚­ã‚¹ãƒˆã®ãƒ†ãƒ³ã‚½ãƒ«è¡¨ç¾ã‚’è¿”ã™\n",
        "\n",
        "\n",
        "<!-- - Sentiment analysis: is a text positive or negative?\n",
        "- Text generation (in English): provide a prompt and the model will generate what follows.\n",
        "- Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)\n",
        "- Question answering: provide the model with some context and a question, extract the answer from the context.\n",
        "- Filling masked text: given a text with masked words (e.g., replaced by `[MASK]`), fill the blanks.\n",
        "- Summarization: generate a summary of a long text.\n",
        "- Translation: translate a text in another language.\n",
        "- Feature extraction: return a tensor representation of the text. -->\n",
        "\n",
        "ã“ã‚ŒãŒæ„Ÿæƒ…åˆ†æã®ãŸã‚ã«ã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã† (ä»–ã®èª²é¡Œã¯ã™ã¹ã¦ [èª²é¡Œã®è¦ç´„](https://huggingface.co/docs/transformers/master/en/task_summary) ã§ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã¾ã™)ã€‚\n",
        "<!-- Let's see how this work for sentiment analysis (the other tasks are all covered in the [task summary](https://huggingface.co/docs/transformers/master/en/task_summary)): -->\n",
        "\n",
        "ä»¥ä¸‹ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ (æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®å ´åˆ)ã€‚\n",
        "<!-- Install the following dependencies (if not already installed): -->\n",
        "\n",
        "```bash\n",
        "pip install torch\n",
        "```\n",
        "\n",
        "```bash\n",
        "pip install tensorflow\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eczkJFSTvVj2"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "classifier_ja = pipeline(\"sentiment-analysis\", model=\"daigo/bert-base-japanese-sentiment\",tokenizer=\"daigo/bert-base-japanese-sentiment\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpUUht6VvVj3"
      },
      "source": [
        "ã“ã®ã‚³ãƒãƒ³ãƒ‰ã‚’åˆã‚ã¦å…¥åŠ›ã™ã‚‹ã¨ï¼Œäº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨ãã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œï¼Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚\n",
        "ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã®å½¹å‰²ã¯ï¼Œãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†ã™ã‚‹ã“ã¨ã§ï¼Œãƒ¢ãƒ‡ãƒ«ã¯äºˆæ¸¬ã‚’è¡Œã†ã“ã¨ã«ãªã‚Šã¾ã™ã€‚\n",
        "ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯ï¼Œã“ã‚Œã‚‰ã™ã¹ã¦ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ï¼Œäºˆæ¸¬çµæœã‚’èª­ã‚ã‚‹ã‚ˆã†ã«å¾Œå‡¦ç†ã—ã¾ã™ã€‚\n",
        "ä¾‹ãˆã°ï¼›\n",
        "<!-- When typing this command for the first time, a pretrained model and its tokenizer are downloaded and cached. \n",
        "We will look at both later on, but as an introduction the tokenizer's job is to preprocess the text for the model, which is then responsible for making predictions. \n",
        "The pipeline groups all of that together, and post-process the predictions to make them readable. \n",
        "For instance: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwtW-PATvVj3",
        "outputId": "3735d103-d7a1-4b94-ac56-3edc18436731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]\n",
            "[{'label': 'ãƒã‚¬ãƒ†ã‚£ãƒ–', 'score': 0.8490612506866455}]\n"
          ]
        }
      ],
      "source": [
        "print(classifier(\"We are very happy to show you the ğŸ¤— Transformers library.\"))\n",
        "\n",
        "print(classifier_ja('ãã“ã¯ã‹ã¨ãªãæ›¸ãç¶´ã‚Œã°ï¼Œæ€ªã—ã†ã“ãç‰©ç‹‚ãŠã—ã‘ã‚Œã€‚'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paH3Wa5XvVj4"
      },
      "source": [
        "å¿ƒå¼·ã„é™ã‚Šã§ã™ã­ã€‚\n",
        "æ–‡ç« ã®ãƒªã‚¹ãƒˆã«ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãï¼Œãã®æ–‡ç« ã¯å‰å‡¦ç†ã•ã‚ŒãŸå¾Œï¼Œãƒ¢ãƒ‡ãƒ«ã«ä¾›çµ¦ã•ã‚Œã€æ¬¡ã®ã‚ˆã†ãªè¾æ›¸ã®ãƒªã‚¹ãƒˆãŒè¿”ã•ã‚Œã¾ã™ã€‚\n",
        "<!-- That's encouraging! \n",
        "You can use it on a list of sentences, which will be preprocessed then fed to the model, returning a list of dictionaries like this one: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3_8N0HSvVj4"
      },
      "outputs": [],
      "source": [
        "results = classifier([\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
        "\n",
        "\n",
        "results_ja = classifier_ja([\"ç§ã¯è…å¥³å­ãŒå¥½ãã§ã™ã€‚\", \"é™°ã‚­ãƒ£ã®ç‰¹å¾´ã§ã—ã‚‡ã†ã­ã‡ãƒ¼\", \"ã­ã‡ãƒ ãƒ¼ãƒŸãƒ³ï¼Œã“ã£ã¡å‘ã„ã¦\"])\n",
        "for result in results_ja:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn9JD-qqvVj4"
      },
      "source": [
        "å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ä½¿ç”¨ã™ã‚‹ã«ã¯ [iterating over a pipeline](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
        "<!-- To use with a large dataset, look at [iterating over a pipeline](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines)-->\n",
        "\n",
        "2 ã¤ç›®ã®æ–‡ãŒé™°æ€§(ãƒã‚¬ãƒ†ã‚£ãƒ–)ã«åˆ†é¡ã•ã‚Œã¦ã„ã¾ã™ãŒ (é™½æ€§ã‹é™°æ€§ã‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)ï¼Œãã®å¾—ç‚¹ã¯ã‹ãªã‚Šä¸­ç«‹çš„ã§ã™ã€‚\n",
        "<!-- You can see the second sentence has been classified as negative (it needs to be positive or negative) but its score is fairly neutral. -->\n",
        "\n",
        "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ ï¼Œã“ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãŸã‚ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯  `distilbert-base-uncased-finetuned-st-2-english` ã¨å‘¼ã°ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "ã“ã®ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã®è©³ã—ã„æƒ…å ±ã¯ [ãƒ¢ãƒ‡ãƒ«ãƒšãƒ¼ã‚¸](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
        "[DistilBERT ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert) ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šï¼ŒSST-2 ã¨ã„ã†ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ„Ÿæƒ…åˆ†æèª²é¡Œã®ãŸã‚ã«å¾®èª¿æ•´ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "<!-- By default, the model downloaded for this pipeline is called \"distilbert-base-uncased-finetuned-sst-2-english\". \n",
        "We can look at its [model page](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) to get more information about it. \n",
        "It uses the [DistilBERT architecture](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert) and has been fine-tuned on a dataset called SST-2 for the sentiment analysis task. -->\n",
        "\n",
        "ä¾‹ãˆã°ï¼Œãƒ•ãƒ©ãƒ³ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚ŒãŸåˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ãŸã„ã¨ã—ã¾ã™ã€‚\n",
        "ç ”ç©¶æ©Ÿé–¢ãŒå¤šãã®ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰ã«å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’é›†ã‚ãŸ [ãƒ¢ãƒ‡ãƒ«ãƒãƒ–](https://huggingface.co/models) ã‚„ï¼Œã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ¢ãƒ‡ãƒ« (é€šå¸¸ï¼Œç‰¹å®šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã‚‚ã®) ã‚’æ¤œç´¢ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ã‚¿ã‚°ã€Œãƒ•ãƒ©ãƒ³ã‚¹èªã€ã¨ã€Œãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã€ã‚’é©ç”¨ã™ã‚‹ã¨ `nlptown/bert-base-multilingual-uncased-sentiment` ã¨ã„ã†ç¤ºå”†ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚\n",
        "ã©ã®ã‚ˆã†ã«ä½¿ç”¨ã§ãã‚‹ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Let's say we want to use another model; for instance, one that has been trained on French data. \n",
        "We can search through the [model hub](https://huggingface.co/models) that gathers models pretrained on a lot of data by research labs, but also community models (usually fine-tuned versions of those big models on a specific dataset). \n",
        "Applying the tags  \"French\" and \"text-classification\" gives back a suggestion \"nlptown/bert-base-multilingual-uncased-sentiment\". \n",
        "Let's see how we can use it. -->\n",
        "\n",
        "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®åå‰ã‚’ç›´æ¥ [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) ã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- You can directly pass the name of the model to use to [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline): -->"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kiritubo = \"ã„ãšã‚Œã®å¾¡æ™‚ã«ã‹ã€å¥³å¾¡ãƒ»æ›´è¡£ã‚ã¾ãŸå€™ã„çµ¦ã„ã‘ã‚‹ä¸­ã«ã€ã„ã¨ã‚„ã‚“ã”ã¨ãªããã‚ã«ã¯ã‚ã‚‰ã¬ãŒã€ã™ãã‚Œã¦æ™‚ã‚ãçµ¦ã†ã‚ã‚Šã‘ã‚Š\"\n",
        "print(pipeline(\"sentiment-analysis\",model=\"daigo/bert-base-japanese-sentiment\",tokenizer=\"daigo/bert-base-japanese-sentiment\")(kiritubo))"
      ],
      "metadata": {
        "id": "qTnckjhPzpBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDarey-lvVj5"
      },
      "outputs": [],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "classifier_ja = pipeline(\"sentiment-analysis\", model=\"daigo/bert-base-japanese-sentiment\",tokenizer=\"daigo/bert-base-japanese-sentiment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvS1ointvVj5"
      },
      "source": [
        "ã“ã®åˆ†é¡å™¨ã¯ï¼Œè‹±èªï¼Œãƒ•ãƒ©ãƒ³ã‚¹èªã ã‘ã§ãªãï¼Œã‚ªãƒ©ãƒ³ãƒ€èªï¼Œãƒ‰ã‚¤ãƒ„èªï¼Œã‚¤ã‚¿ãƒªã‚¢èªï¼Œã‚¹ãƒšã‚¤ãƒ³èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ‰±ã†ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚ \n",
        "(è¨³æ³¨ï¼š æ—¥æœ¬èªã‚‚å¯)\n",
        "ã“ã®åå‰ã®ä»£ã‚ã‚Šã«ï¼Œäº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ (ä¸‹è¨˜å‚ç…§)ã€‚\n",
        "ã¾ãŸï¼Œãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ãã‚Œã«é–¢é€£ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’æ¸¡ã™ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n",
        "<!-- This classifier can now deal with texts in English, French, but also Dutch, German, Italian and Spanish! \n",
        "You can also replace that name by a local folder where you have saved a pretrained model (see below). \n",
        "You can also pass a model object and its associated tokenizer.-->\n",
        "\n",
        "ã“ã‚Œã«ã¯ 2 ã¤ã®ã‚¯ãƒ©ã‚¹ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚\n",
        "1 ã¤ç›®ã¯ [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer) ã§ï¼Œé¸ã‚“ã ãƒ¢ãƒ‡ãƒ«ã«é–¢é€£ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å®Ÿä½“åŒ–ã™ã‚‹ã®ã«ä½¿ã„ã¾ã™ã€‚\n",
        "2 ã¤ç›®ã¯ [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (TensorFlowã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification)) ã§ï¼Œ ã“ã‚Œã¯ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "ä»–ã®èª²é¡Œã§ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã„ãŸå ´åˆã¯ï¼Œãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ©ã‚¹ãŒå¤‰ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n",
        "ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã® [èª²é¡Œã®è¦ç´„ task summary](https://huggingface.co/docs/transformers/master/en/task_summary) ã§ã¯ï¼Œã©ã®ã‚¿ã‚¹ã‚¯ã«ã©ã®ã‚¯ãƒ©ã‚¹ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚\n",
        "<!-- We will need two classes for this. \n",
        "The first is [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer), which we will use to download the tokenizer associated to the model we picked and instantiate it. \n",
        "The second is [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (or [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification) if you are using TensorFlow), which we will use to download the model itself. \n",
        "Note that if we were using the library on an other task, the class of the model would change. \n",
        "The [task summary](https://huggingface.co/docs/transformers/master/en/task_summary) tutorial summarizes which class is used for which task. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCkBxngEvVj5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvdvZvy-vVj6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNCJnhycvVj6"
      },
      "source": [
        "ã•ã¦ï¼Œå…ˆã»ã©è¦‹ã¤ã‘ãŸãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ `from_pretrained()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã†ã ã‘ã§ã™ã€‚\n",
        "(ã”è‡ªç”±ã«ãƒ¢ãƒ‡ãƒ«ãƒãƒ–ã‹ã‚‰ä»–ã®ãƒ¢ãƒ‡ãƒ«ã«ç½®ãæ›ãˆã¦ãã ã•ã„)ã€‚\n",
        "<!-- Now, to download the models and tokenizer we found previously, we just have to use the `from_pretrained()` method (feel free to replace `model_name` by\n",
        "any other model from the model hub): -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tymdubYBvVj6"
      },
      "outputs": [],
      "source": [
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "model_name_ja = \"daigo/bert-base-japanese-sentiment\"\n",
        "model_ja = AutoModelForSequenceClassification.from_pretrained(model_name_ja)\n",
        "tknz_ja = AutoTokenizer.from_pretrained(model_name_ja)\n",
        "classifier_ja = pipeline(\"sentiment-analysis\", model=model_ja, tokenizer=tknz_ja)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSOGa7SqvVj6"
      },
      "outputs": [],
      "source": [
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "\n",
        "# ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ PyTorch ã«ã—ã‹å­˜åœ¨ã—ãªã„ã®ã§ _from_pt_ ãƒ•ãƒ©ã‚°ã‚’ä½¿ã£ã¦ TensorFlowã§ãã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\n",
        "# This model only exists in PyTorch, so we use the _from_pt_ flag to import that model in TensorFlow.\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "model_name_ja = \"daigo/bert-base-japanese-sentiment\"\n",
        "model_ja = TFAutoModelForSequenceClassification.from_pretrained(model_name_ja, from_pt=True)\n",
        "tknz_ja = AutoTokenizer.from_pretrained(model_name_ja)\n",
        "classifier_ja = pipeline(\"sentiment-analysis\", model=model_ja, tokenizer=tknz_ja)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Ll_z-nvVj6"
      },
      "source": [
        "è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã«ä¼¼ãŸãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ï¼Œè‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "ãã®ãŸã‚ã® [ã‚µãƒ³ãƒ—ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://huggingface.co/docs/transformers/master/en/examples) ã‚’ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚\n",
        "èª¿æ•´ãŒçµ‚ã‚ã£ãŸã‚‰ï¼Œ [ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://huggingface.co/docs/transformers/master/en/model_sharing) ã‚’ä½¿ã£ã¦ï¼Œãƒãƒ–ä¸Šã§å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã™ã‚‹ã“ã¨ã‚’ãŠå¿˜ã‚Œãªãã€‚\n",
        "<!-- If you don't find a model that has been pretrained on some data similar to yours, you will need to fine-tune a pretrained model on your data. We provide [example scripts](https://huggingface.co/docs/transformers/master/en/examples) to do so. \n",
        "Once you're done, don't forget to share your fine-tuned model on the hub with the community, using [this tutorial](https://huggingface.co/docs/transformers/master/en/model_sharing). -->\n",
        "\n",
        "<a id='pretrained-model'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mlh51GMvVj6"
      },
      "source": [
        "## å†…éƒ¨å‡¦ç†ï¼šè¨“ç·´æ¸ˆãƒ¢ãƒ‡ãƒ«\n",
        "<!-- ## Under the hood: pretrained models -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj5NtT57vVj7"
      },
      "source": [
        "ã“ã‚Œã‚‰ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ã£ãŸã¨ãã«ï¼Œå†…éƒ¨ã§ä½•ãŒèµ·ã“ã£ã¦ã„ã‚‹ã®ã‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Let's now see what happens beneath the hood when using those pipelines. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "tZw7IXgYvVj7"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AhChOFRegn4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_vsgsA-vVj7"
      },
      "source": [
        "å…ˆã»ã©è¦‹ãŸã‚ˆã†ã«ï¼Œãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã¯ `from_pretrained` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã£ã¦ä½œæˆã•ã‚Œã¾ã™ã€‚\n",
        "<!-- As we saw, the model and tokenizer are created using the `from_pretrained` method: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGxS_Lz6vVj7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model_name_ja = \"daigo/bert-base-japanese-sentiment\"\n",
        "pt_model_ja = AutoModelForSequenceClassification.from_pretrained(model_name_ja)\n",
        "tknz_ja = AutoTokenizer.from_pretrained(model_name_ja)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl02SkTmvVj7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#model_name_ja = \"daigo/bert-base-japanese-sentiment\"\n",
        "#tf_model_ja = TFAutoModelForSequenceClassification.from_pretrained(model_name_ja)\n",
        "#tknz_ja = AutoTokenizer.from_pretrained(model_name_ja)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IZgamMqvVj7"
      },
      "source": [
        "### ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã®ä½¿ç”¨ \n",
        "<!-- ### Using the tokenizer -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1cQkVyNvVj7"
      },
      "source": [
        "ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã¯ï¼Œãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã‚’æ‹…å½“ã™ã‚‹ã¨è¿°ã¹ã¾ã—ãŸã€‚\n",
        "ã¾ãšï¼Œä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’é€šå¸¸ _tokens_ ã¨å‘¼ã°ã‚Œã‚‹å˜èª (ã¾ãŸã¯å˜èªã®ä¸€éƒ¨ï¼Œå¥èª­ç‚¹ãªã©) ã«åˆ†å‰²ã—ã¾ã™ã€‚\n",
        "ã“ã®å‡¦ç†ã«ã¯è¤‡æ•°ã®è¦å‰‡ãŒã‚ã‚Šã¾ã™ (è©³ç´°ã¯ [ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã®è¦ç´„ tokenizer summary](https://huggingface.co/docs/transformers/master/en/tokenizer_summary) ã‚’ã”è¦§ãã ã•ã„)ã€‚\n",
        "ãã®ãŸã‚ï¼Œãƒ¢ãƒ‡ãƒ«ã®åå‰ã‚’ä½¿ã£ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’å®Ÿä½“åŒ–ã—ï¼Œãƒ¢ãƒ‡ãƒ«ãŒäº‹å‰å­¦ç¿’ã•ã‚ŒãŸã¨ãã¨åŒã˜è¦å‰‡ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "ç¬¬ 2 ã‚¹ãƒ†ãƒƒãƒ—ã¯ï¼Œãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ•°å€¤ã«å¤‰æ›ã—ï¼Œãã‚Œã‚‰ã‹ã‚‰ãƒ†ãƒ³ã‚½ãƒ«ã‚’æ§‹ç¯‰ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«ä¾›çµ¦ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã‚’è¡Œã†ãŸã‚ã«ï¼Œãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã¯ _vocab_ ã‚’æŒã£ã¦ã„ã¾ã™ã€‚\n",
        "ã“ã‚Œã¯ `from_pretrained` ãƒ¡ã‚½ãƒƒãƒ‰ã§å®Ÿä½“åŒ–ã™ã‚‹éš›ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éƒ¨åˆ†ã§ï¼Œãƒ¢ãƒ‡ãƒ«ãŒäº‹å‰å­¦ç¿’ã•ã‚ŒãŸã¨ãã¨åŒã˜_vocab_ ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‰ã§ã™ã€‚\n",
        "\n",
        "ã“ã‚Œã‚‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«é©ç”¨ã™ã‚‹ã«ã¯ï¼Œãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã«ä¸ãˆã‚Œã°ã‚ˆã„ã®ã§ã™ã€‚\n",
        "<!-- We mentioned the tokenizer is responsible for the preprocessing of your texts. \n",
        "First, it will split a given text in words (or part of words, punctuation symbols, etc.) usually called _tokens_. \n",
        "There are multiple rules that can govern that process (you can learn more about them in the [tokenizer summary](https://huggingface.co/docs/transformers/master/en/tokenizer_summary)), which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules as when the model was pretrained.\n",
        "\n",
        "The second step is to convert those _tokens_ into numbers, to be able to build a tensor out of them and feed them to the model. To do this, the tokenizer has a _vocab_, which is the part we download when we instantiate it with the `from_pretrained` method, since we need to use the same _vocab_ as when the model was pretrained.\n",
        "\n",
        "To apply these steps on a given text, we can just feed it to our tokenizer: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYa8WIuyvVj8"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n",
        "\n",
        "inputs_ja = tokenizer(\"å±±è·¯ã‚’ç™»ã‚ŠãªãŒã‚‰ã€ã“ã†è€ƒãˆãŸã€‚æ™ºã«åƒã‘ã°è§’ã‹ã©ãŒç«‹ã¤ã€‚æƒ…ã«æ£¹ã•ãŠã•ã›ã°æµã•ã‚Œã‚‹ã€‚æ„åœ°ã‚’é€šã¨ãŠã›ã°çª®å±ˆã ã€‚ã¨ã‹ãã«äººã®ä¸–ã¯ä½ã¿ã«ãã„ã€‚\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXV4v56uvVj8"
      },
      "source": [
        "ä¸Šã¯ï¼Œè¾æ›¸æ–‡å­—åˆ—ã‚’ int ãƒªã‚¹ãƒˆã«ã—ã¦è¿”ã—ã¾ã™ã€‚\n",
        "å‰è¿°ã®ã‚ˆã†ã« [ids of the tokens](https://huggingface.co/docs/transformers/master/en/glossary#input-ids) ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ãŒï¼Œãƒ¢ãƒ‡ãƒ«ã«æœ‰ç”¨ãªè¿½åŠ ã®å¼•æ•°ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "ä¾‹ãˆã°ï¼Œã“ã“ã§ã¯ï¼Œãƒ¢ãƒ‡ãƒ«ãŒç³»åˆ—ã‚’ã‚ˆã‚Šè‰¯ãç†è§£ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ [æ³¨æ„ãƒã‚¹ã‚¯](https://huggingface.co/docs/transformers/master/en/glossary#attention-mask) ã‚‚ã‚ã‚Šã¾ã™ã€‚\n",
        "<!-- This returns a dictionary string to list of ints. \n",
        "It contains the [ids of the tokens](https://huggingface.co/docs/transformers/master/en/glossary#input-ids), as mentioned before, but also additional arguments that will be useful to the model. Here for instance, we also have an [attention mask](https://huggingface.co/docs/transformers/master/en/glossary#attention-mask) that the model will use to have a better understanding of the sequence: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umH733NVvVj8"
      },
      "outputs": [],
      "source": [
        "print(inputs)\n",
        "print(inputs_ja)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYasHbpqvVj8"
      },
      "source": [
        "æ–‡ç« ã®ãƒªã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã«ç›´æ¥æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "æ–‡ç« ã‚’ä¸€æ‹¬ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«é€ã‚ŠãŸã„å ´åˆã¯ï¼Œã™ã¹ã¦ã®æ–‡ç« ã‚’åŒã˜é•·ã•ã«ã—ã¦ï¼Œãƒ¢ãƒ‡ãƒ«ãŒå—ã‘å…¥ã‚Œã‚‰ã‚Œã‚‹æœ€å¤§ã®é•·ã•ã«åˆ‡ã‚Šè©°ã‚ã¦ï¼Œãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã—ã¦ã»ã—ã„ã¨æ€ã†ã§ã—ã‚‡ã†ã€‚\n",
        "ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã«ã¯ã“ã‚Œã‚‰ã™ã¹ã¦ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- You can pass a list of sentences directly to your tokenizer. \n",
        "If your goal is to send them through your model as a batch, you probably want to pad them all to the same length, truncate them to the maximum length the model can accept and get tensors back. \n",
        "You can specify all of that to the tokenizer: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-39iNY_8vVj8"
      },
      "outputs": [],
      "source": [
        "pt_batch = tokenizer(\n",
        "    [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "pt_batch_ja = tknz_ja(\n",
        "    [\"å±±è·¯ã‚’ç™»ã‚ŠãªãŒã‚‰ã€ã“ã†è€ƒãˆãŸã€‚\", \"æ™ºã«åƒã‘ã°è§’ã‹ã©ãŒç«‹ã¤ã€‚\", \"æƒ…ã«æ£¹ã•ãŠã•ã›ã°æµã•ã‚Œã‚‹ã€‚\", \"æ„åœ°ã‚’é€šã¨ãŠã›ã°çª®å±ˆã ã€‚\", \"ã¨ã‹ãã«äººã®ä¸–ã¯ä½ã¿ã«ãã„ã€‚\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3EBRrJuvVj8"
      },
      "outputs": [],
      "source": [
        "# tf_batch = tokenizer(\n",
        "#     [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
        "#     padding=True,\n",
        "#     truncation=True,\n",
        "#     max_length=512,\n",
        "#     return_tensors=\"tf\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbFt-jMDvVj8"
      },
      "source": [
        "ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¯ï¼Œãƒ¢ãƒ‡ãƒ«ãŒäºˆæƒ³ã™ã‚‹å´ (ã“ã“ã§ã¯å³å´) ã«ï¼Œãƒ¢ãƒ‡ãƒ«ãŒäº‹å‰ã«å­¦ç¿’ã—ãŸãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒ»ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”¨ã„ã¦è‡ªå‹•çš„ã«é©ç”¨ã•ã‚Œã¾ã™ã€‚\n",
        "æ³¨æ„ãƒã‚¹ã‚¯ã‚‚ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è€ƒæ…®ã—ã¦é©å¿œã•ã‚Œã¾ã™ã€‚\n",
        "<!-- The padding is automatically applied on the side expected by the model (in this case, on the right), with the padding token the model was pretrained with. \n",
        "The attention mask is also adapted to take the padding into account: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlX7aKRFvVj9"
      },
      "outputs": [],
      "source": [
        "for key, value in pt_batch.items():\n",
        "    print(f\"{key}: {value.numpy().tolist()}\")\n",
        "\n",
        "print('-' * 77)\n",
        "\n",
        "for key, value in pt_batch_ja.items():\n",
        "    print(f\"{key}: {value.numpy().tolist()}\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwLfbI1xvVj9"
      },
      "outputs": [],
      "source": [
        "# for key, value in tf_batch.items():\n",
        "#     print(f\"{key}: {value.numpy().tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h78pVemHvVj9"
      },
      "source": [
        "ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã«ã¤ã„ã¦ã¯ [ã“ã¡ã‚‰](https://huggingface.co/docs/transformers/master/en/preprocessing) ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
        "<!-- You can learn more about tokenizers [here](https://huggingface.co/docs/transformers/master/en/preprocessing). -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrDPUzInvVj9"
      },
      "source": [
        "### ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ \n",
        "<!-- ### Using the model -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx6-G8KJvVj9"
      },
      "source": [
        "å…¥åŠ›ãŒãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã«ã‚ˆã£ã¦å‰å‡¦ç†ã•ã‚ŒãŸã‚‰ï¼Œãã‚Œã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã«é€ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "å‰è¿°ã—ãŸã‚ˆã†ã«ï¼Œãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã¨ã™ã‚‹ã™ã¹ã¦ã®é–¢é€£æƒ…å ±ãŒå«ã¾ã‚Œã¾ã™ã€‚\n",
        "TensorFlow ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ï¼Œè¾æ›¸ã®ã‚­ãƒ¼ã‚’ç›´æ¥ãƒ†ãƒ³ã‚½ãƒ«ã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ãŒï¼ŒPyTorch ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ï¼Œ`**` ã‚’è¿½åŠ ã—ã¦è¾æ›¸ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "<!-- Once your input has been preprocessed by the tokenizer, you can send it directly to the model. \n",
        "As we mentioned, it will contain all the relevant information the model needs. \n",
        "If you're using a TensorFlow model, you can pass the dictionary keys directly to tensors, for a PyTorch model, you need to unpack the dictionary by adding `**`. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l576uH_vVj9"
      },
      "outputs": [],
      "source": [
        "pt_outputs = pt_model(**pt_batch)\n",
        "pt_outputs_ja = pt_model_ja(**pt_batch_ja)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nASfyazXvVj9"
      },
      "outputs": [],
      "source": [
        "#tf_outputs = tf_model(tf_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyQc24yUvVj-"
      },
      "source": [
        "Transformers ã§ã¯ï¼Œã™ã¹ã¦ã®å‡ºåŠ›ã¯ï¼Œãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚çš„ãªæ´»æ€§åŒ–ã¨ä»–ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚\n",
        "ã“ã‚Œã‚‰ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã¤ã„ã¦ã¯ [ã“ã¡ã‚‰](https://huggingface.co/docs/transformers/master/en/main_classes/output) ã§è©³ã—ãèª¬æ˜ã—ã¦ã„ã¾ã™ã€‚\n",
        "ã¨ã‚Šã‚ãˆãšï¼Œå‡ºåŠ›ã‚’è‡ªåˆ†ã§èª¿ã¹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- In ğŸ¤— Transformers, all outputs are objects that contain the model's final activations along with other metadata. \n",
        "These objects are described in greater detail [here](https://huggingface.co/docs/transformers/master/en/main_classes/output). \n",
        "For now, let's inspect the output ourselves: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MashngE9vVj-"
      },
      "outputs": [],
      "source": [
        "print(pt_outputs)\n",
        "print(pt_outputs_ja)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60Hj-yARvVj-"
      },
      "outputs": [],
      "source": [
        "#print(tf_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdtL9LSkvVj-"
      },
      "source": [
        "å‡ºåŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒ `logits` å±æ€§ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã«æ³¨ç›®ã—ã¦ãã ã•ã„ã€‚\n",
        "ã“ã‚Œã‚’ä½¿ã£ã¦ï¼Œãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚çš„ãªæ´»æ€§å€¤ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- Notice how the output object has a `logits` attribute. You can use this to access the model's final activations. -->\n",
        "\n",
        "<Tip>\n",
        "\n",
        "ã™ã¹ã¦ã®ğŸ¤— Transformers ãƒ¢ãƒ‡ãƒ« (PyTorch ã¾ãŸã¯ TensorFlow) ã¯ï¼Œæœ€çµ‚çš„ãªæ´»æ€§åŒ–é–¢æ•° (SoftMax ã®ã‚ˆã†ãª) ã®å‰ã«ï¼Œãƒ¢ãƒ‡ãƒ«ã®æ´»æ€§åŒ–ã‚’è¿”ã—ã¾ã™ã€‚\n",
        "<!-- All ğŸ¤— Transformers models (PyTorch or TensorFlow) return the activations of the model *before* the final activation function (like SoftMax) since this final activation function is often fused with the loss. -->\n",
        "\n",
        "</Tip>\n",
        "\n",
        "äºˆæ¸¬å€¤ã‚’å¾—ã‚‹ãŸã‚ã« SoftMax ã®æ´»æ€§å€¤ã‚’é©ç”¨ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "<!-- Let's apply the SoftMax activation to get predictions. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEoxxXnTvVj-"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
        "pt_predictions_ja = nn.functional.softmax(pt_outputs_ja.logits, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMBOgyiovVj-"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdUDt7t7vVj-"
      },
      "source": [
        "å…ˆã»ã©ã®æ•°å­—ãŒå‡ºã¦ãã¾ã—ãŸã­ã€‚\n",
        "<!-- We can see we get the numbers from before: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu2z9Q2PvVj_"
      },
      "outputs": [],
      "source": [
        "print(pt_predictions)\n",
        "print(pt_predictions_ja)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7uZkTgNvVj_"
      },
      "outputs": [],
      "source": [
        "#print(tf_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXQ_TQPgvVj_"
      },
      "source": [
        "å…¥åŠ›ã«åŠ ãˆã¦ãƒ©ãƒ™ãƒ«ã‚’ãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆãŸå ´åˆï¼Œãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã¯ `loss` å±æ€§ã‚‚å«ã¾ã‚Œã¾ã™ã€‚\n",
        "<!-- If you provide the model with labels in addition to inputs, the model output object will also contain a `loss` attribute: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAKQADTWvVj_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "pt_outputs = pt_model(**pt_batch, labels=torch.tensor([1, 0]))\n",
        "print(pt_outputs)\n",
        "\n",
        "#pt_outputs_ja = pt_model_ja(**pt_batch_ja, labels=torch.tensor([1, 0]))\n",
        "pt_outputs_ja = pt_model_ja(**pt_batch_ja)\n",
        "print(pt_outputs_ja)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFK6QkjXvVj_"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# tf_outputs = tf_model(tf_batch, labels=tf.constant([1, 0]))\n",
        "# print(tf_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WksHAeHUvVj_"
      },
      "source": [
        "ãƒ¢ãƒ‡ãƒ«ã¯æ¨™æº–çš„ãª [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) ã¾ãŸã¯ [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) ãªã®ã§ï¼Œé€šå¸¸ã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ğŸ¤— Transformers ã§ã¯ï¼ŒKeras ã® `fit()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆ©ç”¨ã™ã‚‹ã®ã«å¯¾ã—ã¦ PyTorch ã§ã®è¨“ç·´ã‚’æ”¯æ´ã™ã‚‹ [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) ã‚¯ãƒ©ã‚¹ã‚‚æä¾›ã—ã¦ã„ã¾ã™ (åˆ†æ•£è¨“ç·´ï¼Œæ··åˆç²¾åº¦ãªã©ã«å¯¾å¿œã—ã¦ã„ã¾ã™)ã€‚\n",
        "è©³ã—ãã¯ [training tutorial](https://huggingface.co/docs/transformers/master/en/training) ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
        "<!-- Models are standard [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) or [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) so you can use them in your usual training loop. \n",
        "ğŸ¤— Transformers also provides a [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) class to help with your training in PyTorch (taking care of things such as distributed training, mixed precision, etc.) whereas you can leverage the `fit()` method in Keras. \n",
        "See the [training tutorial](https://huggingface.co/docs/transformers/master/en/training) for more details. -->\n",
        "\n",
        "<Tip>\n",
        "\n",
        "Pytorch ã®ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã¯ç‰¹åˆ¥ãªãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã§ã‚ã‚Šï¼ŒIDE ã§ãã®å±æ€§ã‚’è‡ªå‹•è£œå®Œã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚\n",
        "ã¾ãŸï¼Œã‚¿ãƒ—ãƒ«ã‚„è¾æ›¸ã®ã‚ˆã†ã«æŒ¯ã‚‹èˆã†ã“ã¨ã‚‚ã§ãã¾ã™ (ä¾‹ãˆã°ï¼Œæ•´æ•°ã‚„ã‚¹ãƒ©ã‚¤ã‚¹ï¼Œæ–‡å­—åˆ—ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä»˜ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™)ã€‚\n",
        "ã“ã®å ´åˆï¼Œè¨­å®šã•ã‚Œã¦ã„ãªã„ (`None` å€¤ã‚’æŒã¤) å±æ€§ã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚\n",
        "<!-- Pytorch model outputs are special dataclasses so that you can get autocompletion for their attributes in an IDE. \n",
        "They also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes not set (that have `None` values) are ignored. -->\n",
        "\n",
        "</Tip>\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ãŒçµ‚ã‚ã£ãŸã‚‰ï¼Œæ¬¡ã®ã‚ˆã†ã«ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ä½¿ã£ã¦ä¿å­˜ã—ã¾ã™ã€‚\n",
        "<!-- Once your model is fine-tuned, you can save it with its tokenizer in the following way: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoDVlBnXvVj_"
      },
      "outputs": [],
      "source": [
        "pt_save_directory = \"./pt_save_pretrained\"\n",
        "\n",
        "tokenizer.save_pretrained(pt_save_directory)\n",
        "pt_model.save_pretrained(pt_save_directory)\n",
        "\n",
        "pt_save_directory_ja = \"./pt_save_pretrained_ja\"\n",
        "tknz_ja.save_pretrained(pt_save_directory_ja)\n",
        "pt_model_ja.save_pretrained(pt_save_directory_ja)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lt pt_save_pretrained_ja"
      ],
      "metadata": {
        "id": "WafAx4o4DY_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeJcx4IpvVkA"
      },
      "outputs": [],
      "source": [
        "# tf_save_directory = \"./tf_save_pretrained\"\n",
        "# tokenizer.save_pretrained(tf_save_directory)\n",
        "# tf_model.save_pretrained(tf_save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmwuLZauvVkA"
      },
      "source": [
        "ãã—ã¦ï¼Œãƒ¢ãƒ‡ãƒ«åã®ä»£ã‚ã‚Šã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’æ¸¡ã—ã¦ï¼Œ`AutoModel.from_pretrained()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã£ã¦ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "Transformers ã®å„ªã‚ŒãŸæ©Ÿèƒ½ã®ä¸€ã¤ã«ï¼ŒPyTorch ã¨ TensorFlow ã‚’ç°¡å˜ã«åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "<!-- You can then load this model back using the `AutoModel.from_pretrained()` method by passing the directory name instead of the model name. \n",
        "One cool feature of ğŸ¤— Transformers is that you can easily switch between PyTorch and TensorFlow: any model saved as before can be loaded back either in PyTorch or TensorFlow.-->\n",
        "\n",
        "ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä»–ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ãƒ­ãƒ¼ãƒ‰ã—ãŸã„å ´åˆã¯ï¼Œã¾ãšãã‚ŒãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
        "<!-- If you would like to load your saved model in the other framework, first make sure it is installed: -->\n",
        "\n",
        "```bash\n",
        "pip install torch\n",
        "```\n",
        "\n",
        "```bash\n",
        "pip install tensorflow\n",
        "```\n",
        "\n",
        "ãã—ã¦ï¼Œå¯¾å¿œã™ã‚‹ Auto ã‚¯ãƒ©ã‚¹ã‚’ä½¿ã£ã¦ï¼Œæ¬¡ã®ã‚ˆã†ã«èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n",
        "<!-- Then, use the corresponding Auto class to load it like this: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIZmb-BAvVkA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n",
        "# pt_model = AutoModel.from_pretrained(tf_save_directory, from_tf=True)\n",
        "\n",
        "# tknz_ja = AutoTokenizer.from_pretrained(tf_save_directory_ja)\n",
        "# pt_model_ja = AutoModel.from_pretrained(tf_save_directory_ja, from_tf=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nT2JXzWvVkA"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
        "tf_model = TFAutoModel.from_pretrained(pt_save_directory, from_pt=True)\n",
        "\n",
        "tknz_ja = AutoTokenizer.from_pretrained(pt_save_directory_ja)\n",
        "tf_model_ja = TFAutoModel.from_pretrained(pt_save_directory_ja, from_pt=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bqt-kIuvVkA"
      },
      "source": [
        "æœ€å¾Œã«ï¼Œå¿…è¦ã§ã‚ã‚Œã°ï¼Œã™ã¹ã¦ã®éš ã‚ŒãŸçŠ¶æ…‹ã¨ã™ã¹ã¦ã®æ³¨ç›®åº¦ã®é‡ã¿ã‚’è¿”ã™ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã«æ±‚ã‚ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n",
        "<!-- Lastly, you can also ask the model to return all hidden states and all attention weights if you need them: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnxjJKC4vVkA"
      },
      "outputs": [],
      "source": [
        "pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)\n",
        "all_hidden_states = pt_outputs.hidden_states\n",
        "all_attentions = pt_outputs.attentions\n",
        "\n",
        "pt_outputs_ja = pt_model_ja(**pt_batch_ja, output_hidden_states=True, output_attentions=True)\n",
        "all_hidden_states_ja = pt_outputs_ja.hidden_states\n",
        "all_attentions_ja = pt_outputs_ja.attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kloeqh7vVkA"
      },
      "outputs": [],
      "source": [
        "# tf_outputs = tf_model(tf_batch, output_hidden_states=True, output_attentions=True)\n",
        "# all_hidden_states = tf_outputs.hidden_states\n",
        "# all_attentions = tf_outputs.attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcf37BJxvVkA"
      },
      "source": [
        "### ã‚³ãƒ¼ãƒ‰ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹\n",
        "<!-- ### Accessing the code -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieABZz3SvVkA"
      },
      "source": [
        "[AutoModel](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModel) ã¨ [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer) ã‚¯ãƒ©ã‚¹ã¯ï¼Œäº‹å‰è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Œã°è‡ªå‹•çš„ã«å‹•ä½œã™ã‚‹ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã«éãã¾ã›ã‚“ã€‚\n",
        "èˆå°è£ã§ã¯ï¼Œãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã‚¯ãƒ©ã‚¹ã®çµ„ã¿åˆã‚ã›ã”ã¨ã« 1 ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã‚’æŒã£ã¦ã„ã‚‹ã®ã§ï¼Œã‚³ãƒ¼ãƒ‰ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã™ã‚‹ã®ã¯ç°¡å˜ã§ã™ã€‚\n",
        "<!-- The [AutoModel](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModel) and [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer) classes are just shortcuts that will automatically work with any pretrained model. \n",
        "Behind the scenes, the library has one model class per combination of architecture plus class, so the code is easy to access and tweak if you need to.-->\n",
        "\n",
        "å…ˆã»ã©ã®ä¾‹ã§ã¯ï¼Œãƒ¢ãƒ‡ãƒ«ã¯ `distilbert-base-uncased-finetuned-st-2-english` ã¨å‘¼ã°ã‚Œã¦ã„ã¾ã—ãŸãŒï¼Œã“ã‚Œã¯ [DistilBERT](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert) ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¦ã„ã¾ã™ã€‚\n",
        "[AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (TensorFlow ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification)) ãŒä½¿ç”¨ã•ã‚ŒãŸã®ã§ï¼Œè‡ªå‹•çš„ã«ä½œæˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ [DistilBertForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification) ã¨ãªã‚Šã¾ã™ã€‚\n",
        "ãã®ãƒ¢ãƒ‡ãƒ«ã«é–¢é€£ã™ã‚‹è©³ç´°ã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¦‹ã‚‹ã‹ï¼Œã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
        "ã“ã®ã‚ˆã†ã«ï¼Œè‡ªå‹•ç”Ÿæˆæ©Ÿèƒ½ã‚’ä½¿ã‚ãšã«ï¼Œãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ç›´æ¥å®Ÿä½“åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!--In our previous example, the model was called \"distilbert-base-uncased-finetuned-sst-2-english\", which means it's using the [DistilBERT](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert) architecture. \n",
        "As [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (or [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification) if you are using TensorFlow) was used, the model automatically created is then a [DistilBertForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification). \n",
        "You can look at its documentation for all details relevant to that specific model, or browse the source code. This is how you would directly instantiate model and tokenizer without the auto magic: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u1Z_yqjvVkA"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me_MY48YvVkB"
      },
      "outputs": [],
      "source": [
        "# from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "\n",
        "# model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "# model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0ySQAoGvVkB"
      },
      "source": [
        "### ãƒ¢ãƒ‡ãƒ«ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º\n",
        "<!-- ### Customizing the model -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bViFEy_3vVkB"
      },
      "source": [
        "ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã®æ§‹ç¯‰æ–¹æ³•ã‚’å¤‰æ›´ã—ãŸã„å ´åˆã¯ã€ã‚«ã‚¹ã‚¿ãƒ æ§‹æˆã‚¯ãƒ©ã‚¹ã‚’å®šç¾©ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "å„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã¯ãã‚Œãã‚Œé–¢é€£ã™ã‚‹è¨­å®šãŒã‚ã‚Šã¾ã™ã€‚\n",
        "ä¾‹ãˆã° [DistilBertConfig](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig)ã§ ã¯ï¼ŒDistilBERT ã®éš ã‚Œå±¤æ¬¡å…ƒï¼Œãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ãªã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ \n",
        "éš ã‚Œå±¤ã®ã‚µã‚¤ã‚ºã®å¤‰æ›´ãªã©ã®ã‚³ã‚¢ãªä¿®æ­£ã‚’è¡Œã£ãŸå ´åˆï¼Œäº‹å‰è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ããªããªã‚Šï¼Œã‚¼ãƒ­ã‹ã‚‰è¨“ç·´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "ãã—ã¦ï¼Œã“ã®è¨­å®šã‹ã‚‰ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿä½“åŒ–ã—ã¾ã™ã€‚\n",
        "<!-- If you want to change how the model itself is built, you can define a custom configuration class. \n",
        "Each architecture comes with its own relevant configuration. For example, [DistilBertConfig](https://huggingface.co/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig) allows you to specify\n",
        "parameters such as the hidden dimension, dropout rate, etc for DistilBERT. \n",
        "If you do core modifications, like changing the hidden size, you won't be able to use a pretrained model anymore and will need to train from scratch. \n",
        "You would then instantiate the model directly from this configuration.-->\n",
        "\n",
        "ä»¥ä¸‹ã§ã¯ [from_pretrained()](https://huggingface.co/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained) ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã£ã¦ ï¼Œãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ç”¨ã®äº‹å‰å®šç¾©ã•ã‚ŒãŸèªå½™ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n",
        "ã—ã‹ã—ï¼Œãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã¨ã¯ç•°ãªã‚Šï¼Œãƒ¢ãƒ‡ãƒ«ã‚’ã‚¼ãƒ­ã‹ã‚‰åˆæœŸåŒ–ã—ãŸã„ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚\n",
        "ãã®ãŸã‚ [DistilBertForSequenceClassification.from_pretrained()](https://huggingface.co/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã†ä»£ã‚ã‚Šã«ï¼Œè¨­å®šã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¦ã„ã¾ã™ã€‚\n",
        "<!-- Below, we load a predefined vocabulary for a tokenizer with the [from_pretrained()](https://huggingface.co/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained) method. However, unlike the tokenizer, we wish to initialize the model from scratch. Therefore, we instantiate the model from a configuration instead of using the [DistilBertForSequenceClassification.from_pretrained()](https://huggingface.co/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) method. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ULCa26UvVkB"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4 * 512)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForSequenceClassification(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iW_lnqKvVkB"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "\n",
        "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4 * 512)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = TFDistilBertForSequenceClassification(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxTlCOOWvVkB"
      },
      "source": [
        "ãƒ¢ãƒ‡ãƒ«ã®é ­ã®éƒ¨åˆ† (ä¾‹ãˆã°ï¼Œãƒ©ãƒ™ãƒ«æ•°) ã ã‘ã‚’å¤‰ãˆã‚‹ã‚‚ã®ã«ã¤ã„ã¦ã¯ï¼Œä½“ (ãƒœãƒ‡ã‚£) ã®éƒ¨åˆ†ã«ã¯äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ä¾‹ãˆã° 10 ç¨®é¡ã®ãƒ©ãƒ™ãƒ«ã«å¯¾å¿œã™ã‚‹åˆ†é¡å™¨ã‚’ï¼Œäº‹å‰ã«å­¦ç¿’ã•ã‚ŒãŸä½“ã‚’ä½¿ã£ã¦å®šç¾©ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "ãƒ©ãƒ™ãƒ«ã®æ•°ã‚’å¤‰æ›´ã™ã‚‹ãŸã‚ã«ï¼Œã™ã¹ã¦ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’æŒã¤æ–°ã—ã„ã‚³ãƒ³ãƒ•ã‚£ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ä»£ã‚ã‚Šã«ï¼Œã‚³ãƒ³ãƒ•ã‚£ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå–ã‚‹ä»»æ„ã®å¼•æ•°ã‚’ `from_pretrained` ãƒ¡ã‚½ãƒƒãƒ‰ã«æ¸¡ã›ã°ï¼Œãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚³ãƒ³ãƒ•ã‚£ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é©åˆ‡ã«æ›´æ–°ã—ã¦ãã‚Œã¾ã™ã€‚\n",
        "<!-- For something that only changes the head of the model (for instance, the number of labels), you can still use a pretrained model for the body. \n",
        "For instance, let's define a classifier for 10 different labels using a pretrained body.\n",
        "Instead of creating a new configuration with all the default values just to change the number of labels, we can instead pass any argument a configuration would take to the `from_pretrained` method and it will update the default configuration appropriately: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzMVnaIUvVkB"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCx4ZGHavVkB"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2022_0112quicktour.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}