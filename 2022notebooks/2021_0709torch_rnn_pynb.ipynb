{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_0709torch_rnn.pynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOeLp9rvFOBzLn9TuQdrVn3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2021_0709torch_rnn_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09H8nee13sJy"
      },
      "source": [
        "# RNN デモ\n",
        "\n",
        "- date: 2021_0709\n",
        "- filename: 2021_0709torch_rnn.pynb\n",
        "- author: 浅川伸一\n",
        "- 概要: オノマトペの音韻表現を RNN で"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG_rAFsQxTbX"
      },
      "source": [
        "# 形態素分析ライブラリーMeCab と 辞書(mecab-ipadic-NEologd)のインストール \n",
        "# reference: https://qiita.com/jun40vn/items/78e33e29dce3d50c2df1\n",
        "!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab #> /dev/null\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git #> /dev/null \n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n #> /dev/null 2>&1\n",
        "!pip install mecab-python3 # > /dev/null\n",
        "\n",
        "# シンボリックリンクによるエラー回避\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc\n",
        "\n",
        "!pip install --upgrade xlrd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjjU-XapxmuC"
      },
      "source": [
        "#必要なファイルの取得\n",
        "!wget https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/2021code/ja_util.py -O ja_util.py\n",
        "!wget https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/2021code/torch_rnn.py -O torch_rnn.py\n",
        "!wget https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/2021code/2021-0325日本語オノマトペ辞典4500より.xls -O 2021-0325日本語オノマトペ辞典4500より.xls\n",
        "\n",
        "#note https://raw.github.com/<username>/<repo>/<branch>/some_directory/file.rb\n",
        "\n",
        "!pip install jaconv\n",
        "\n",
        "import ja_util\n",
        "import torch_rnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9jOsUqpxqZM"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "import time\n",
        "import jaconv\n",
        "\n",
        "import torch\n",
        "import torch.nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # ご自身の PC からファイルをアップロードして下さい\""
      ],
      "metadata": {
        "id": "hmP9EAhLwTl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "739fsDjq19E8"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "#注意: '日本語オノマトペ辞典4500より.xls' は著作権の問題があり，公にできません。\n",
        "#そのため Google Colab での解法，ローカルファイルよりアップロードする\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()  # ここで `日本語オノマトペ辞典4500より.xls` を指定してアップロードする\n",
        "ccap_base = '.'\n",
        "onomatopea_excel = '2021-0325日本語オノマトペ辞典4500より.xls'\n",
        "onmtp2761 = pd.read_excel(os.path.join(ccap_base, onomatopea_excel), sheet_name='2761語')\n",
        "onomatopea = list(set(sorted(onmtp2761['オノマトペ'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBjWMQv02w3x"
      },
      "source": [
        "#データの準備\n",
        "mora_dict = {}\n",
        "roman_dict = {}\n",
        "for word in onomatopea:\n",
        "    mora = ja_util.mora_wakati().parse(word)\n",
        "    roman = ja_util.mora_wakati().parse2romaji(word)\n",
        "    roman_phon = \"\".join(ch for ch in roman)\n",
        "    #print(word, roman, mora)\n",
        "    #sys.exit()\n",
        "    if len(mora) > 0:\n",
        "        mora_dict[word] = {'mora':mora, 'roman':roman, 'roman_phon':roman_phon}\n",
        "        roman_dict[roman_phon] = {'word': word, 'mora': mora, 'roman': roman}\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.wrd2idx = {}  #単語を単語インデックスへ変換する辞書\n",
        "        self.idx2wrd = []  #単語インデックスから単語へ変換するための配列\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.wrd2idx:\n",
        "            self.idx2wrd.append(word)\n",
        "            self.wrd2idx[word] = len(self.idx2wrd) - 1\n",
        "        return self.wrd2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2wrd)\n",
        "\n",
        "\n",
        "class onomatopea_Corpus(object):\n",
        "    def __init__(self, train_dataset, valid_dataset, test_dataset):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.dictionary.add_word('<eow>')\n",
        "\n",
        "        self.train = self.tokenize(train_dataset)\n",
        "        self.valid = self.tokenize(valid_dataset)\n",
        "        self.test = self.tokenize(test_dataset)\n",
        "\n",
        "    def tokenize(self, dataset):\n",
        "        \"\"\"Tokenizes a dataset.\"\"\"\n",
        "\n",
        "        for word in dataset:\n",
        "            #chrs = word.split() + ['<eow>']\n",
        "            for ch in word:\n",
        "                self.dictionary.add_word(ch)\n",
        "\n",
        "        # Tokenize the content\n",
        "        idss, ids = [], []\n",
        "        for word in dataset:\n",
        "            #chrs = word.split() + ['<eow>']\n",
        "            ids = []\n",
        "            #for ch in chrs:\n",
        "            for ch in word:\n",
        "                ids.append(self.dictionary.wrd2idx[ch])\n",
        "            ids.append(self.dictionary.wrd2idx['<eow>'])\n",
        "            idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "        return ids\n",
        "\n",
        "onmtp_phon = [mora_dict[x]['roman_phon'] for x in mora_dict]\n",
        "    \n",
        "#データのシャッフル\n",
        "onmtp_phon = list(np.random.permutation(onmtp_phon))\n",
        "# # 10 分割して train, test, valid 8:1:1 で分割する\n",
        "# split = len(onmtp_phon) // 10\n",
        "# onmtp_phon_test  = onmtp_phon[:split]\n",
        "# onmtp_phon_valid = onmtp_phon[split:2*split]\n",
        "# onmtp_phon_train = onmtp_phon[2*split:]\n",
        "    \n",
        "# 10 分割して train, test, valid 8:1:1 で分割する\n",
        "split = len(onmtp_phon) // 10\n",
        "ono_test  = onmtp_phon[:split]\n",
        "ono_valid = onmtp_phon[split:2*split]\n",
        "ono_train = onmtp_phon[2*split:]\n",
        "\n",
        "onomatopea_corpus = onomatopea_Corpus(ono_train, ono_train, ono_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XzQDKRq20c1"
      },
      "source": [
        "#上記データの準備の確認作業\n",
        "# n 個のデータを表示して確認\n",
        "n = 10 \n",
        "print([(roman_dict[word]['word'],roman_dict[word]['roman']) for word in ono_train[:n]])\n",
        "    \n",
        "for idx in onomatopea_corpus.valid[:n]:\n",
        "    print(f'{idx}:{onomatopea_corpus.dictionary.idx2wrd[idx]}', end=\" \")\n",
        "\n",
        "print(onomatopea[:n])\n",
        "print(ono_train[:n])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuuK3xYM27VT"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# sorce: study/2020pytorch_examples.git/word_language_model/train.py\n",
        "import torch_rnn\n",
        "import math\n",
        "import random\n",
        "\n",
        "class onomatopea_rnn_model(object):\n",
        "    \"\"\"\n",
        "    以下の実習で指定可能なオプションについて\n",
        "\n",
        "    * 比較のための model_name として指定できる選択肢:\n",
        "        - LSTM: 既定値\n",
        "        - GRU\n",
        "        - RNN_TANH\n",
        "        - RNN_RELU\n",
        "        - Transformer\n",
        "\n",
        "    * lr: float\n",
        "        学習係数: 既定値 20\n",
        "    \n",
        "    * emsize: int\n",
        "        埋め込み次元数 既定値 32\n",
        "    \n",
        "    * nlayers: int\n",
        "        層数 既定値:2\n",
        "    \n",
        "    * nhid: int\n",
        "        中間層のニューロン数: 既定値 32\n",
        "\n",
        "    * nhead: int\n",
        "        多頭注意のヘッド数: 既定値 2\n",
        "        Transformer で使用\n",
        "    \n",
        "    * clip: float\n",
        "        勾配爆発抑制のため: 既定値 1.0\n",
        "    \n",
        "    * dropout: float\n",
        "        ドロップアウト率 既定値 0\n",
        "\n",
        "    * save_filename: string\n",
        "        学習結果を保存するファイル名 既定値 'onomatopea_model.pt'\n",
        "    \n",
        "    * tied: boolean\n",
        "        入出力のマッピングに同じ結合係数を用いるか否か 既定値 Flase\n",
        "    \n",
        "    * seed: int\n",
        "        乱数の種 結果の再現性保証のため，既定値 111\n",
        "    \n",
        "    * log_interval: int\n",
        "        途中経過の表示間隔\n",
        "    \n",
        "    * dry_run: boolean\n",
        "        デバッグ用\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, corpus, model_name='LSTM', \n",
        "                 emsize=32, nhid=32, nlayers=2, lr=20, clip=1,\n",
        "                 batch_size=32, epochs=10 ** 3, bptt=10, dropout=0,\n",
        "                 tied=False, seed=111, log_interval=None,\n",
        "                 save_filename='onomatopea_model.pt',\n",
        "                 nhead=2, cuda=False, dry_run=False):\n",
        "        torch.manual_seed(seed)\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        self.corpus = corpus\n",
        "        self.ntokens = len(corpus.dictionary)\n",
        "        self.device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "        self.batch_size = 32\n",
        "        self.train_data = self.batchify(self.corpus.train, self.batch_size)\n",
        "        self.val_data = self.batchify(self.corpus.valid, self.batch_size)\n",
        "        self.test_data = self.batchify(self.corpus.test, self.batch_size)\n",
        "\n",
        "        self.n_tokens = len(self.corpus.dictionary)\n",
        "        print(f'n_tokens:{self.n_tokens}')\n",
        "\n",
        "        self.emsize = emsize # 32 * 2\n",
        "        self.nhid = nhid # 32 * 2\n",
        "        self.nlayers = nlayers # 2\n",
        "        #lr = 20\n",
        "        self.lr = lr # 1e-1\n",
        "        #clip = 0.25\n",
        "        self.clip = clip   # 5\n",
        "        self.epochs = epochs # 10 ** 3\n",
        "        #epochs = 2\n",
        "        self.batch_size = batch_size # 32\n",
        "        self.bptt = bptt # 10\n",
        "        self.dropout = dropout # 0.2\n",
        "        self.tied = tied #True\n",
        "        self.seed = seed #111\n",
        "        self.cuda = cuda # False\n",
        "        if not log_interval:\n",
        "            self.log_interval = self.epochs >> 3\n",
        "        else:\n",
        "            self.log_interval = log_interval\n",
        "        self.save_filename = save_filename # '2021_0623onomatopea_LSTM_model.pt'\n",
        "        #onnx_export = onnx'' # \n",
        "        self.nhead = nhead # 2\n",
        "        self.dry_run = dry_run # False\n",
        "        self.model_name = model_name #'LSTM'\n",
        "        #model_name = 'RNN_TANH'\n",
        "        print(f'model_name:{model_name}')\n",
        "        #type of model_name: RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "\n",
        "        if self.model_name == 'Transformer':\n",
        "            self.model = torch_rnn.TransformerModel(self.n_tokens, self.emsize, \n",
        "                                                    self.nhead, self.nhid, self.nlayers, \n",
        "                                                    self.dropout).to(self.device)\n",
        "        else:\n",
        "            self.model = torch_rnn.RNNModel(self.model_name, self.n_tokens, self.emsize, \n",
        "                                            self.nhid, self.nlayers, self.dropout, \n",
        "                                            self.tied).to(self.device)\n",
        "\n",
        "        self.criterion = torch.nn.NLLLoss()\n",
        "\n",
        "\n",
        "    def batchify(self, data, bsz):\n",
        "        # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "        nbatch = data.size(0) // bsz\n",
        "        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "        data = data.narrow(0, 0, nbatch * bsz)\n",
        "        # Evenly divide the data across the bsz batches.\n",
        "        data = data.view(bsz, -1).t().contiguous()\n",
        "        return data.to(self.device)\n",
        "\n",
        "\n",
        "    def repackage_hidden(self, h):\n",
        "        \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "        if isinstance(h, torch.Tensor):\n",
        "            return h.detach()\n",
        "        else:\n",
        "            return tuple(self.repackage_hidden(v) for v in h)\n",
        "\n",
        "        \n",
        "    def get_batch(self, source, i):\n",
        "        \"\"\"\n",
        "        get_batch subdivides the source data into chunks of length bptt.\n",
        "        If source is equal to the example output of the batchify function, with\n",
        "        a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "        ┌ a g m s ┐ ┌ b h n t ┐\n",
        "        └ b h n t ┘ └ c i o u ┘\n",
        "        Note that despite the name of the function, the subdivison of data is not\n",
        "        done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "        by the batchify function. The chunks are along dimension 0, corresponding\n",
        "        to the seq_len dimension in the LSTM.\n",
        "        \"\"\"\n",
        "        seq_len = min(self.bptt, len(source) - 1 - i)\n",
        "        data = source[i:i+seq_len]\n",
        "        target = source[i+1:i+1+seq_len].view(-1)\n",
        "        return data, target\n",
        "\n",
        "\n",
        "    def evaluate(self, dataset):\n",
        "        # Turn on evaluation mode which disables dropout.\n",
        "        self.model.eval()\n",
        "        self.total_loss = 0.\n",
        "        if self.model_name != 'Transformer':\n",
        "            self.hidden = self.model.init_hidden(self.batch_size)\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, dataset.size(0) - 1, self.bptt):\n",
        "                data, targets = self.get_batch(dataset, i)\n",
        "                if self.model_name == 'Transformer':\n",
        "                    self.output = self.model(data)\n",
        "                    self.output = self.output.view(-1, self.n_tokens)\n",
        "                else:\n",
        "                    self.output, self.hidden = self.model(data, self.hidden)\n",
        "                    self.hidden = self.repackage_hidden(self.hidden)\n",
        "                self.total_loss += len(data) * self.criterion(self.output, targets).item()\n",
        "        return self.total_loss / (len(dataset) - 1)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        # At any point you can hit Ctrl + C to break out of training early.\n",
        "        # Loop over epochs.\n",
        "        self.best_val_loss = None\n",
        "        self.interval = self.epochs >> 2\n",
        "        epoch_start_time = time.time()\n",
        "        for epoch in range(1, self.epochs+1):\n",
        "            self._train(epoch)\n",
        "            val_loss = self.evaluate(self.val_data)\n",
        "            if epoch % self.interval == 0:\n",
        "                time_elasped = time.time() - epoch_start_time\n",
        "                print(f'エポック: {epoch:>3d} ',\n",
        "                      f'(経過時間:{time_elasped:5.2f}s) ',\n",
        "                      f'検証データ損失:{val_loss:5.2f} ',\n",
        "                      f' 検証データ錯綜度(パープレキシティ):{math.exp(val_loss):8.2f}')\n",
        "                epoch_start_time = time.time()\n",
        "            # Save the model if the validation loss is the best we've seen so far.\n",
        "            if not self.best_val_loss or val_loss < self.best_val_loss:\n",
        "                with open(self.save_filename, 'wb') as f:\n",
        "                    torch.save(self.model, f)\n",
        "                    self.best_val_loss = val_loss\n",
        "            else:\n",
        "                # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "                self.lr /= 4.0\n",
        "\n",
        "                \n",
        "    def test_model(self):\n",
        "        # Load the best saved model.\n",
        "        with open(self.save_filename, 'rb') as f:\n",
        "            self.model = torch.load(f)\n",
        "            # after load the rnn params are not a continuous chunk of memory\n",
        "            # this makes them a continuous chunk, and will speed up forward pass\n",
        "            # Currently, only rnn model supports flatten_parameters function.\n",
        "            if self.model_name in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n",
        "                self.model.rnn.flatten_parameters()\n",
        "\n",
        "        # Run on test data.\n",
        "        self.test_loss = self.evaluate(self.test_data)\n",
        "\n",
        "        print(f'テストデータ損失 {test_loss:5.2f} ',\n",
        "              f' | テストデータ錯綜度(パープレキシティ) {math.exp(test_loss):8.2f}')\n",
        "        \n",
        "\n",
        "    def _train(self, epoch):\n",
        "        # Turn on training mode which enables dropout.\n",
        "        self.model.train()\n",
        "        self.total_loss = 0.\n",
        "        self.start_time = time.time()\n",
        "        if self.model_name != 'Transformer':\n",
        "            self.hidden = self.model.init_hidden(self.batch_size)\n",
        "        for batch, i in enumerate(range(0, self.train_data.size(0) - 1, self.bptt)):\n",
        "            data, targets = self.get_batch(self.train_data, i)\n",
        "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "            self.model.zero_grad()\n",
        "            if self.model_name == 'Transformer':\n",
        "                self.output = self.model(data)\n",
        "                self.output = self.output.view(-1, self.ntokens)\n",
        "            else:\n",
        "                self.hidden = self.repackage_hidden(self.hidden)\n",
        "                self.output, self.hidden = self.model(data, self.hidden)\n",
        "            loss = self.criterion(self.output, targets)\n",
        "            loss.backward()\n",
        "\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
        "            for p in self.model.parameters():\n",
        "                p.data.add_(p.grad, alpha=-self.lr)\n",
        "\n",
        "            self.total_loss += loss.item()\n",
        "\n",
        "            if self.dry_run:\n",
        "                break\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "294zwNco3N_f"
      },
      "source": [
        "#訓練モデルのパラメータについては，上記の関数をみてください\n",
        "model1 = onomatopea_rnn_model(onomatopea_corpus, model_name='LSTM', lr=1, epochs=100, emsize=32, nhid=32, nlayers=2)\n",
        "model1.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV0Eqsku3SCd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}