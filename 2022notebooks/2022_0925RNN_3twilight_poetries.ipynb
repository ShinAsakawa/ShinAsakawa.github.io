{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0925RNN_3twilight_poetries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5035b29-7ec7-4bb9-a0ce-2097f04ce930",
      "metadata": {
        "id": "e5035b29-7ec7-4bb9-a0ce-2097f04ce930"
      },
      "source": [
        "---\n",
        "filename: 2022_0925RNN_3twilight_poetries.ipynb\n",
        "\n",
        "---\n",
        "\n",
        "# 三夕の歌 (寂蓮，西行，定家) を学習するリカレントニューラルネットワーク\n",
        "\n",
        "## 目次\n",
        "\n",
        "1. 語彙辞書の作成\n",
        "2. 埋草処理\n",
        "3. ワンホット符号化\n",
        "4. 言語モデルの定義\n",
        "5. 言語モデルの訓練の実施\n",
        "6. 学習結果の評価\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac447b87-c06d-4891-9b59-1e8fa86476bb",
      "metadata": {
        "id": "ac447b87-c06d-4891-9b59-1e8fa86476bb"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "try:\n",
        "    import bit\n",
        "except ImportError:\n",
        "    !pip install ipynbname --upgrade\n",
        "    !git clone https://github.com/ShinAsakawa/bit.git\n",
        "    import bit  \n",
        "isColab = bit.isColab\n",
        "HOME = bit.HOME"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52e51290-9396-4921-b0f5-a68e14f44e75",
      "metadata": {
        "id": "52e51290-9396-4921-b0f5-a68e14f44e75"
      },
      "source": [
        "# 1 語彙辞書の作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3597d33c-d85c-4e92-a307-868cc988cf16",
      "metadata": {
        "id": "3597d33c-d85c-4e92-a307-868cc988cf16"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "twilight_poetries = ['さびしさは 其の色としも なかりけり まき立つ山の 秋の夕暮',  #寂蓮\n",
        "                     '心なき 身にもあはれは しられけり 鴫立つ沢の 秋の夕暮',     #西行\n",
        "                     'み渡せば 花ももみぢも なかりけり 浦の苫屋の 秋の夕暮'      #定家\n",
        "                    ]\n",
        "\n",
        "# twilight_poetries では変数名として長いから，タイプミスしやすい。そこで名前を変更\n",
        "data = twilight_poetries\n",
        "\n",
        "# 3 つ短歌を結合し，結合した文章からユニークな文字を抽出\n",
        "tokens = sorted(set(''.join(data)))\n",
        "\n",
        "# トークン ID 番号から文字を返す辞書\n",
        "idx2tkn = dict(enumerate(tokens))\n",
        "\n",
        "# 文字からトークン ID を返す辞書\n",
        "tkn2idx = {ch:idx for idx, ch in idx2tkn.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121086ac-2597-4fa4-833a-a824bd2d3401",
      "metadata": {
        "id": "121086ac-2597-4fa4-833a-a824bd2d3401"
      },
      "outputs": [],
      "source": [
        "print(idx2tkn)\n",
        "print(tkn2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d49612-6aaa-483c-be48-a3f0f4b3d1bd",
      "metadata": {
        "id": "70d49612-6aaa-483c-be48-a3f0f4b3d1bd"
      },
      "source": [
        "次に，すべての文がサンプルの長さになるように，入力文のパディングを行います。\n",
        "RNN は通常，様々なサイズの入力を取り込むことができますが，通常は，学習処理を高速化するために，学習データを一括して送り込みたいと考えるでしょう。\n",
        "バッチを使用してデータを学習するためには，入力データ内の各系列が同じサイズ (同一系列長) であることを確認する必要があります。\n",
        "<!-- Next, we'll be padding our input sentences to ensure that all the sentences are of the sample length. \n",
        "While RNNs are typically able to take in variably sized inputs, we will usually want to feed training data in batches to speed up the training process. -->\n",
        "\n",
        "そのため，多くの場合，短すぎる配列は 埋め草 ID 例えば  **0** で埋め，長すぎる配列は切り捨てることで，パディングを行うことができます。\n",
        "今回の場合は，最も長い配列の長さを求め，その長さに合わせて残りの文章を空白でパディングすることにします。\n",
        "<!-- In order to used batches to train on our data, we'll need to ensure that each sequence within the input data are of equal size. -->\n",
        "\n",
        "したがって，ほとんどの場合，パディングは短すぎる配列を **0** の値で埋め，長すぎる配列を切り詰めることで行うことができます。\n",
        "今回は，最も長い配列の長さを求め，その長さに合わせて残りの文章を空白でパディングすることにします。\n",
        "<!-- Therefore, in most cases, padding can be done by filling up sequences that are too short with **0** values and trimming sequences that are too long. \n",
        "In our case, we'll be finding the length of the longest sequence and padding the rest of the sentences with blank spaces to match that length. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da174f90-88f8-4abf-84c3-1f0d9b0fefb4",
      "metadata": {
        "id": "da174f90-88f8-4abf-84c3-1f0d9b0fefb4"
      },
      "outputs": [],
      "source": [
        "maxlen = len(max(data, key=len))\n",
        "print(f\"最長文字列: {maxlen} 文字\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa755cb-6df4-4976-95b7-cbb27478b2cb",
      "metadata": {
        "id": "afa755cb-6df4-4976-95b7-cbb27478b2cb"
      },
      "source": [
        "# 2 埋め草処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e632c9a-69ff-4092-aca0-6deda78c5173",
      "metadata": {
        "id": "8e632c9a-69ff-4092-aca0-6deda78c5173"
      },
      "outputs": [],
      "source": [
        "# パディング\n",
        "# 文のリストを繰り返して，文長さが最長文長に一致するまで空白文字 ‘ ' を追加する単純な繰り返し\n",
        "for i in range(len(data)):\n",
        "    while len(data[i]) < maxlen:\n",
        "        data[i] += ' '\n",
        "\n",
        "for datum in data:\n",
        "    print(f'-{datum}-')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a11126cd-8987-4c94-b924-1ccd16e24e1e",
      "metadata": {
        "id": "a11126cd-8987-4c94-b924-1ccd16e24e1e"
      },
      "source": [
        "各時刻で次字を予測するため，各歌を以下ように分解します\n",
        "\n",
        "- 入力データ: 最後の入力文字はモデルに入れる必要がないため除外する\n",
        "- 標的/正解ラベル: 入力データより 1 時刻前の時間。これが入力データに対応する各時刻でのモデルの「正解」となる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "738c0950-f023-4a5f-898e-d3a5b0273535",
      "metadata": {
        "id": "738c0950-f023-4a5f-898e-d3a5b0273535"
      },
      "outputs": [],
      "source": [
        "# 入力配列とターゲット配列を格納するリストの作成\n",
        "inputs_seq = []\n",
        "target_seq = []\n",
        "\n",
        "for i in range(len(data)):\n",
        "    # 入力系列の最後の文字を削除\n",
        "    inputs_seq.append(data[i][:-1])\n",
        "    \n",
        "    # 標的配列の最初の文字を削除\n",
        "    target_seq.append(data[i][1:])\n",
        "    print(f\"入力系列: {inputs_seq[i]}\")\n",
        "    print(f\"目標系列: {target_seq[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380a3504-b278-415a-8512-8ca550e1c6fd",
      "metadata": {
        "id": "380a3504-b278-415a-8512-8ca550e1c6fd"
      },
      "source": [
        "# 3 ワンホット符号化\n",
        "\n",
        "ここで，入力配列と標的配列を，上で作成した辞書を使って写像することで，文字ではなく，整数の配列に変換することができます。\n",
        "これによって，入力系列をワンホットベクトルへ符号化できるようになります。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d2947a2-32af-490e-a3bc-d43f4ff9c26f",
      "metadata": {
        "id": "7d2947a2-32af-490e-a3bc-d43f4ff9c26f"
      },
      "outputs": [],
      "source": [
        "inputs_ids, target_ids = [], []  # トークン ID を入れておくリストを用意\n",
        "for i in range(len(data)):\n",
        "    inputs_ids.append([tkn2idx[ch] for ch in inputs_seq[i]])\n",
        "    target_ids.append([tkn2idx[ch] for ch in target_seq[i]])\n",
        "    \n",
        "print(inputs_ids)\n",
        "print(target_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6839e020-4bec-4376-95db-05e3e08130f0",
      "metadata": {
        "id": "6839e020-4bec-4376-95db-05e3e08130f0"
      },
      "source": [
        "入力配列をワンホットベクトルに符号化する前に，3 つの重要な変数を定義しておきます。\n",
        "\n",
        "- **dict_size**: テキストに含まれるユニークな文字の数。\n",
        "各文字がそのベクトル内の割り当てられたインデックスを持つように，ワンホットベクトルのサイズを決定します。\n",
        "- **seq_len**: モデルに入力する配列の長さ。\n",
        "全ての文章の長さを最長の文章と同じになるように標準化したので，この値は最後の文字の入力を削除したため，最大の長さ - 1 となる\n",
        "- **batch_size**: バッチとして定義され，モデルに投入される文の数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0409f0ff-2c32-4141-bfeb-c94bff0946df",
      "metadata": {
        "id": "0409f0ff-2c32-4141-bfeb-c94bff0946df"
      },
      "outputs": [],
      "source": [
        "dic_size = len(tkn2idx)\n",
        "seq_len = maxlen - 1\n",
        "batch_size = len(data)\n",
        "\n",
        "def one_hot_encode(seq:list,\n",
        "                   dic_size:int, \n",
        "                   seq_len:int, \n",
        "                   batch_size:int):\n",
        "    # すべての要素が 0 である，3 元テンソルを定義\n",
        "    ret = np.zeros((batch_size, seq_len, dic_size), dtype=np.float32)\n",
        "    \n",
        "    # 上で定義した 3 元テンソルに対して該当するトークン ID の要素を 1 にする\n",
        "    for batch in range(batch_size):\n",
        "        for t in range(seq_len):\n",
        "            ret[batch, t, seq[batch][t]] = 1\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f523e2d2-f762-4d27-8f40-08f351f3546a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f523e2d2-f762-4d27-8f40-08f351f3546a",
        "outputId": "be15e368-a2b5-4403-c212-0f9fc63fde49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力データ形状: (3, 29, 39) --> (バッチサイズ, 系列長, ワンホット埋め込みベクトル次元)\n"
          ]
        }
      ],
      "source": [
        "inputs = one_hot_encode(seq=inputs_ids, \n",
        "                        dic_size=dic_size, \n",
        "                        seq_len=seq_len, \n",
        "                        batch_size=batch_size)\n",
        "print(f\"入力データ形状: {inputs.shape} --> (バッチサイズ, 系列長, ワンホット埋め込みベクトル次元)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "035b4dc4-5869-4934-ba60-ecd105024916",
      "metadata": {
        "id": "035b4dc4-5869-4934-ba60-ecd105024916"
      },
      "source": [
        "データの前処理はすべて終わったので，次はデータを numpy の配列から PyTorch 独自のデータ構造である **Torch Tensors** に変換します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "98af5827-f87b-4617-8b1d-16ea48ca71b9",
      "metadata": {
        "id": "98af5827-f87b-4617-8b1d-16ea48ca71b9"
      },
      "outputs": [],
      "source": [
        "inputs_seq = torch.from_numpy(inputs)\n",
        "target_seq = torch.Tensor(target_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c731e0f7-41f6-475c-a0e1-f1092e9dfca0",
      "metadata": {
        "id": "c731e0f7-41f6-475c-a0e1-f1092e9dfca0"
      },
      "source": [
        "# 4 言語モデルの定義\n",
        "Torch ライブラリを使ってモデルを定義していきます。\n",
        "ここで，完全連結層，畳み込み層，バニラ RNN 層，LSTM 層，その他いろいろな層を追加したり削除したりすることができます! \n",
        "\n",
        "モデルの構築を始める前に，PyTorch のビルドイン機能を使って，実行しているデバイス (CP Uか GPU か) を確認してみましょう。\n",
        "この実装では，学習が本当に簡潔なので，GPU は必要ありません。\n",
        "しかし，大規模なデータセットや数百万の学習可能なパラメータを持つモデルに進んでいくと，GPU を使うことは学習を高速で進めることができるようになります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe8b69f-1cc9-4eaf-876e-990bad2e100f",
      "metadata": {
        "id": "1fe8b69f-1cc9-4eaf-876e-990bad2e100f"
      },
      "outputs": [],
      "source": [
        "# もし GPU が利用可能であれば，デバイスを GPU に設定します。\n",
        "# このデバイス変数は後ほどコード内で使用します。\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f'device:{device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c65d6b-c412-4bac-a8fb-dae899b6c8b3",
      "metadata": {
        "id": "25c65d6b-c412-4bac-a8fb-dae899b6c8b3"
      },
      "source": [
        "独自のニューラルネットワークモデルの構築を開始するには、すべてのニューラルネットワークモジュールのための PyTorch の基本クラス (`nn.module`) を継承するクラスを定義することができます。\n",
        "その後，コンストラクタで変数とモデルの層を定義します。\n",
        "このモデルでは，RNN の 1 層と全連結層を使用します。\n",
        "全連結層は，RNN の出力を希望する出力形状に変換する役割を担います。\n",
        "\n",
        "また，クラスメソッドとして `forward()` の下にフォワードパス関数を定義する必要があります。\n",
        "`forward()`  関数は順番に実行されるので，入力とゼロ初期化された隠れ状態をまず RNN 層に渡し，その後に RNN の出力を全連結層に渡すことになります。\n",
        "コンストラクタで定義した層を使っていることに注意してください。\n",
        "\n",
        "\n",
        "最後に定義するのは，先ほど隠れ状態を初期化するために呼び出したメソッド，`init_hidden()` です。\n",
        "これは基本的に，隠れ層の形をしたゼロのテンソルを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "30b1d668-be1d-4b7e-afd4-1ee4c9639245",
      "metadata": {
        "id": "30b1d668-be1d-4b7e-afd4-1ee4c9639245"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN_Model(nn.Module):\n",
        "    \"\"\"このメソッドは，PyTorch 公式サイトにあるサンプルコードをわかりやすく書き換えたもの。\n",
        "    メソッドを実体化 (インスタンス化) する際には，4 つの整数引数と 1 つの文字列引数を指定します。\n",
        "    整数とは次の 4 つ: input_size, output_size, hidden_size, num_layers, \n",
        "    文字列引数とは `rnn_type`  であり，`rnn_type` には `LSTM`, `GRU`, `RNN_TANH`, `RNN_RELU` の\n",
        "    いずれかが指定できる。\n",
        "    \n",
        "    このメソッドを呼び出す際には，入力テンソル (`torch.tensor`) を与える。\n",
        "    このとき，入力テンソルの形状 (サイズ) は [`バッチ`, `系列`, `データ`] である必要がある。\n",
        "    PyTorch の実装では，`batch_first` オプションにより，入力テンソルタの第 1 次元が，系列かバッチかを選択可能である。\n",
        "    駄菓子菓子，ここでは `batch_first` を決め打ちしている\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 input_size:int, \n",
        "                 output_size:int, \n",
        "                 hidden_size:int, \n",
        "                 num_layers:int=1,\n",
        "                 rnn_type:str='RNN_TANH',\n",
        "                 dropout:float=0.,\n",
        "                 device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "                ):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn = getattr(torch.nn, rnn_type)(\n",
        "                input_size=input_size, \n",
        "                hidden_size=hidden_size, \n",
        "                num_layers=num_layers, \n",
        "                batch_first=True,\n",
        "                dropout=dropout).to(device)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity = {'RNN_TANH': 'tanh', \n",
        "                                'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError:\n",
        "                raise ValueError( \"\"\"rnn_type で指定可能なモデルは ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU'] です\"\"\")\n",
        "            self.rnn = torch.nn.RNN(\n",
        "                input_size=input_size, \n",
        "                hidden_size=hidden_size, \n",
        "                num_layers=num_layers, \n",
        "                nonlinearity=nonlinearity, \n",
        "                batch_first=True,\n",
        "                dropout=dropout).to(device)\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # 全結合層\n",
        "        self.fc = nn.Linear(\n",
        "            in_features=hidden_size, \n",
        "            out_features=output_size)\n",
        "    \n",
        "    def forward(self, \n",
        "                x:torch.Tensor=None):\n",
        "        \n",
        "        # batch_first を仮定しているので 0 次元目がミニバッチ長を表す\n",
        "        batch_size = x.size(0)  \n",
        "\n",
        "        #以下で定義するメソッドを使用して、最初の入力に対して隠れ層状態を初期化\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # 入力と隠れ層状態をモデルに渡して出力を得る\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        \n",
        "        # 全結合層に収まるように出力を整形\n",
        "        out = out.contiguous().view(-1, self.hidden_size)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, \n",
        "                    batch_size:int=1):\n",
        "        \"\"\"順向パスで使用する時刻 0 の隠れ層を生成\n",
        "        上で指定したデバイスに隠れ層の状態を保持したテンソルを返す\"\"\"\n",
        "        \n",
        "        if self.rnn_type == 'LSTM':\n",
        "            cell = torch.zeros(self.num_layers, \n",
        "                               batch_size, \n",
        "                               self.hidden_size).to(device)\n",
        "            hidden = torch.zeros(self.num_layers, \n",
        "                                 batch_size, \n",
        "                                 self.hidden_size).to(device)\n",
        "            return cell, hidden\n",
        "\n",
        "        else:\n",
        "            hidden = torch.zeros(self.num_layers, \n",
        "                                 batch_size, \n",
        "                                 self.hidden_size).to(device)\n",
        "            return hidden\n",
        "    \n",
        "# test_model = RNN_Model(input_size=3, output_size=2, hidden_size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59e26549-859f-4ea7-a280-6d2088e997e4",
      "metadata": {
        "id": "59e26549-859f-4ea7-a280-6d2088e997e4"
      },
      "source": [
        "上記のモデルを定義した後，関連するパラメータでモデルを実体化し，同様にハイパーパラメータを定義する必要があります。\n",
        "ハイパーパラメータは以下のように定義します。\n",
        "<!-- After defining the model above, we'll have to instantiate the model with the relevant parameters and define our hyperparamters as well. \n",
        "The hyperparameters we're defining below are:-->\n",
        "\n",
        "- **`n_epochs`**:  エポック数。モデルが訓練データセット全体を通過する回数\n",
        "- **`lr`**: 学習率。誤差逆伝播による学習が行われるたびに，モデルが結合係数を更新する率。\n",
        "    - 学習率が小さいと，モデルはより小さな大きさで重みの値を変更することを意味する\n",
        "    - 学習率が大きいと，各時刻で重みがより大きく更新されることを意味する。\n",
        "\n",
        "<!-- - *n_epochs*: Number of Epochs -- This refers to the number of times our model will go through the entire training dataset\n",
        "- *lr*: Learning Rate -- This affects the rate at which our model updates the weights in the cells each time backpropogation is done\n",
        "    - A smaller learning rate means that the model changes the values of the weight with a smaller magnitude\n",
        "    - A larger learning rate means that the weights are updated to a larger extent for each time step -->\n",
        "    \n",
        "他のニューラルネットワークと同様，オプティマイザと損失関数を定義する必要があります。\n",
        "最終的な出力は基本的に分類課題なので，`CrossEntropyLoss` を使用することにします。\n",
        "<!-- Similar to other neural networks, we have to define the optimizer and loss function as well. We’ll be using CrossEntropyLoss as the final output is basically a classification task. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c4b7342-859f-4d73-a9f5-317f7838b275",
      "metadata": {
        "id": "7c4b7342-859f-4d73-a9f5-317f7838b275"
      },
      "outputs": [],
      "source": [
        "# 言語モデルを実体化\n",
        "model = RNN_Model(\n",
        "    rnn_type = 'LSTM',  # or choose among `RNN_TANH`, `RNN_RELU`, `GRU`, and `LSTM`\n",
        "    input_size=dic_size, \n",
        "    output_size=dic_size, \n",
        "    hidden_size=8,     # 任意の整数に変更可能なハイパーパラメータ\n",
        "    num_layers=1)\n",
        "\n",
        "# モデルを予め定めておいたデバイスに設定 (`cpu` または `cuda`)\n",
        "model = model.to(device)\n",
        "\n",
        "# ハイパーパラメータの定義\n",
        "n_epochs=500  # エポック数\n",
        "lr=0.01       # 学習率\n",
        "\n",
        "# 損失関数と最適化手法の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c521c5ee-dcad-47fa-bc0f-6fbb549c8aa5",
      "metadata": {
        "id": "c521c5ee-dcad-47fa-bc0f-6fbb549c8aa5"
      },
      "source": [
        "# 5 訓練 (学習) の実施"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c8d7d83-f814-4fe4-9678-b8bfe7ed658e",
      "metadata": {
        "id": "0c8d7d83-f814-4fe4-9678-b8bfe7ed658e"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    \n",
        "    optimizer.zero_grad()  # 前エポックから残存する勾配を 0 で初期化\n",
        "    \n",
        "    inputs_seq = inputs_seq.to(device)\n",
        "    output, hidden = model(inputs_seq)\n",
        "    output = output.to(device)\n",
        "    \n",
        "    target_seq = target_seq.to(device)\n",
        "    loss = criterion(output, target_seq.view(-1).long())\n",
        "    loss.backward()  # 誤差逆伝播を行って勾配を計算\n",
        "    optimizer.step() # 勾配の計算に従って結合係数を更新\n",
        "    \n",
        "    if epoch % (n_epochs >> 2) == 0:\n",
        "        print(f'エポック: {epoch:5d}/{n_epochs:5d} :',\n",
        "              f'損失値: {loss.item():.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cbddbd4-035f-452b-a5d0-532a8f111c9e",
      "metadata": {
        "id": "4cbddbd4-035f-452b-a5d0-532a8f111c9e"
      },
      "source": [
        "# 6 結果の評価\n",
        "モデルを検査して，どのような出力が得られるか見てみましょう。\n",
        "そのために，モデルの出力 ID を文字に戻すためのヘルパー関数を定義します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3bbd8249-a986-49ac-bb4a-4e47a4a9ef54",
      "metadata": {
        "id": "3bbd8249-a986-49ac-bb4a-4e47a4a9ef54"
      },
      "outputs": [],
      "source": [
        "def predict(model=nn.Module, \n",
        "            inputs:str=\"\"):\n",
        "    \n",
        "    # 入力とモデルが合致するようにワンホット符号化\n",
        "    _inputs_ids = np.array([[tkn2idx[x] for x in inputs]])\n",
        "    inputs_ids = one_hot_encode(_inputs_ids, dic_size, _inputs_ids.shape[1], 1)\n",
        "    \n",
        "    # ワンホットベクトルを torch.Tensor に変換\n",
        "    inputs_ids = torch.from_numpy(inputs_ids)   \n",
        "    \n",
        "    # デバイスに転送\n",
        "    inputs_ids = inputs_ids.to(device)         \n",
        "    \n",
        "    # モデルの実行\n",
        "    out, hidden = model(inputs_ids)\n",
        "    \n",
        "    # ソフトマックス関数に通して確率に変換\n",
        "    probs = nn.functional.softmax(out[-1], dim=0).data\n",
        "\n",
        "    # 出力から最も高い確率の得点を持つクラスを取り出す\n",
        "    output_ids = torch.max(probs, dim=0)[1].item()\n",
        "\n",
        "    return idx2tkn[output_ids], hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7cb1db81-64ac-47df-bf6b-c8f2e5c73d69",
      "metadata": {
        "id": "7cb1db81-64ac-47df-bf6b-c8f2e5c73d69"
      },
      "outputs": [],
      "source": [
        "def sample(model:nn.Module, \n",
        "           out_len:int=0, \n",
        "           start:str=''):\n",
        "    model.eval()\n",
        "    \n",
        "    # 最初に開始時刻 (t=0) の文字を実行するために設定\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    \n",
        "    # 現時刻までに出力された文字列を渡して次の文字を取得し，付け加える\n",
        "    for _ in range(size):\n",
        "        out, h = predict(model=model, inputs=chars)\n",
        "        chars.append(out)\n",
        "        #print(\"\".join(chars))\n",
        "\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd82344d-4994-488f-8846-7f3efb0c7892",
      "metadata": {
        "id": "cd82344d-4994-488f-8846-7f3efb0c7892"
      },
      "outputs": [],
      "source": [
        "print(sample(model, 30, 'み'))\n",
        "print(sample(model, 30, '心'))\n",
        "print(sample(model, 30, 'さ'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c10af9e-997a-46a7-ae52-d71eab17541a",
      "metadata": {
        "id": "9c10af9e-997a-46a7-ae52-d71eab17541a"
      },
      "source": [
        "## 演習\n",
        "\n",
        "1. 言語モデルを定義する際に, `rnn_type=` の引数で，以下の 4 種類を試せ。\n",
        "`['RNN_TANH', 'RNN_RELU', 'GRU', 'LSTM']` 結果に違いが生じるか?\n",
        "2. 言語モデルの中間層のニューロン数 `hidden_size=` を変化させて結果を観察せよ\n",
        "3. 言語モデルの総数 `num_layers=` を変化させて結果を観察せよ\n",
        "4. 学習率を変化させて見よ。結果を観察してみよ\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}