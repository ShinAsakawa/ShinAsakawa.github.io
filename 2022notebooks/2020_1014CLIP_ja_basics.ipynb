{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2020_1014CLIP_ja_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UexpkI250ndG"
      },
      "source": [
        "# 日本語版CLIPモデルの利用例\n",
        "\n",
        "- 画像とテキストの類似性計算\n",
        "- 画像やテキストの埋め込み計算\n",
        "- テキストや画像による類似画像検索"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8fDGSrLSm3V"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPIpQyqm0tbW"
      },
      "source": [
        "## 依存ライブラリの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "isColab =  'google.colab' in str(get_ipython())\n",
        "\n",
        "if isColab:\n",
        "    !pip install unidic-lite\n",
        "    !pip install fugashi\n",
        "    !pip install ipadic\n",
        "    !pip install transformers # ==4.14.0\n",
        "\n",
        "import fugashi, ipadic, transformers\n",
        "\n",
        "try:\n",
        "    import jaapnize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib"
      ],
      "metadata": {
        "id": "kUogSx1i5ZuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgybggnr0be4"
      },
      "source": [
        "## サンプル画像のダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_3yi2M1Wpgd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sonoisa/clip-japanese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "754s5Z3b0iaI"
      },
      "source": [
        "## CLIP用クラス定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "81pJWB_hz9Hd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "class ClipTextModel(nn.Module):\n",
        "    def __init__(self, model_name_or_path, device=None):\n",
        "        super(ClipTextModel, self).__init__()\n",
        "\n",
        "        if os.path.exists(model_name_or_path):\n",
        "            # load from file system\n",
        "            output_linear_state_dict = torch.load(os.path.join(model_name_or_path, \"output_linear.bin\"), map_location=device)\n",
        "        else:\n",
        "            # download from the Hugging Face model hub\n",
        "            filename = hf_hub_download(repo_id=model_name_or_path, filename=\"output_linear.bin\")\n",
        "            output_linear_state_dict = torch.load(filename)\n",
        "\n",
        "        self.model = AutoModel.from_pretrained(model_name_or_path)\n",
        "        config = self.model.config\n",
        "\n",
        "        self.max_cls_depth = 6\n",
        "\n",
        "        sentence_vector_size = output_linear_state_dict[\"bias\"].shape[0]\n",
        "        self.sentence_vector_size = sentence_vector_size\n",
        "        self.output_linear = nn.Linear(self.max_cls_depth * config.hidden_size, sentence_vector_size)\n",
        "        self.output_linear.load_state_dict(output_linear_state_dict)\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\n",
        "                                                       is_fast=True, do_lower_case=True)\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = torch.device(device)\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "    ):\n",
        "        output_states = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=None,\n",
        "            head_mask=None,\n",
        "            inputs_embeds=None,\n",
        "            output_attentions=None,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        token_embeddings = output_states[0]\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        hidden_states = output_states[\"hidden_states\"]\n",
        "\n",
        "        output_vectors = []\n",
        "\n",
        "        # cls tokens\n",
        "        for i in range(1, self.max_cls_depth + 1):\n",
        "            cls_token = hidden_states[-1 * i][:, 0]\n",
        "            output_vectors.append(cls_token)\n",
        "\n",
        "        output_vector = torch.cat(output_vectors, dim=1)\n",
        "        logits = self.output_linear(output_vector)\n",
        "\n",
        "        output = (logits,) + output_states[2:]\n",
        "        return output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_text(self, texts, batch_size=8, max_length=64):\n",
        "        self.eval()\n",
        "        all_embeddings = []\n",
        "        iterator = range(0, len(texts), batch_size)\n",
        "        for batch_idx in iterator:\n",
        "            batch = texts[batch_idx:batch_idx + batch_size]\n",
        "\n",
        "            encoded_input = self.tokenizer.batch_encode_plus(\n",
        "                batch, max_length=max_length, padding=\"longest\",\n",
        "                truncation=True, return_tensors=\"pt\").to(self.device)\n",
        "            model_output = self(**encoded_input)\n",
        "            text_embeddings = model_output[0].cpu()\n",
        "\n",
        "            all_embeddings.extend(text_embeddings)\n",
        "\n",
        "        # return torch.stack(all_embeddings).numpy()\n",
        "        return torch.stack(all_embeddings)\n",
        "\n",
        "    def save(self, output_dir):\n",
        "        self.model.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "        torch.save(self.output_linear.state_dict(), os.path.join(output_dir, \"output_linear.bin\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Me-bU3cj0Cz_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import transformers\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "class ClipVisionModel(nn.Module):\n",
        "    def __init__(self, model_name_or_path, device=None):\n",
        "        super(ClipVisionModel, self).__init__()\n",
        "\n",
        "        if os.path.exists(model_name_or_path):\n",
        "            # load from file system\n",
        "            visual_projection_state_dict = torch.load(os.path.join(model_name_or_path, \"visual_projection.bin\"))\n",
        "        else:\n",
        "            # download from the Hugging Face model hub\n",
        "            filename = hf_hub_download(repo_id=model_name_or_path, filename=\"visual_projection.bin\")\n",
        "            visual_projection_state_dict = torch.load(filename)\n",
        "\n",
        "        self.model = transformers.CLIPVisionModel.from_pretrained(model_name_or_path)\n",
        "        config = self.model.config\n",
        "\n",
        "        self.feature_extractor = transformers.CLIPFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "\n",
        "        vision_embed_dim = config.hidden_size\n",
        "        projection_dim = 512\n",
        "\n",
        "        self.visual_projection = nn.Linear(vision_embed_dim, projection_dim, bias=False)\n",
        "        self.visual_projection.load_state_dict(visual_projection_state_dict)\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = torch.device(device)\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        output_states = self.model(\n",
        "            pixel_values=pixel_values,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        image_embeds = self.visual_projection(output_states[1])\n",
        "\n",
        "        return image_embeds\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_image(self, images, batch_size=8):\n",
        "        self.eval()\n",
        "        all_embeddings = []\n",
        "        iterator = range(0, len(images), batch_size)\n",
        "        for batch_idx in iterator:\n",
        "            batch = images[batch_idx:batch_idx + batch_size]\n",
        "\n",
        "            encoded_input = self.feature_extractor(batch, return_tensors=\"pt\").to(self.device)\n",
        "            model_output = self(**encoded_input)\n",
        "            image_embeddings = model_output.cpu()\n",
        "\n",
        "            all_embeddings.extend(image_embeddings)\n",
        "\n",
        "        # return torch.stack(all_embeddings).numpy()\n",
        "        return torch.stack(all_embeddings)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_alpha_channel(image):\n",
        "        image.convert(\"RGBA\")\n",
        "        alpha = image.convert('RGBA').split()[-1]\n",
        "        background = Image.new(\"RGBA\", image.size, (255, 255, 255))\n",
        "        background.paste(image, mask=alpha)\n",
        "        image = background.convert(\"RGB\")\n",
        "        return image\n",
        "\n",
        "    def save(self, output_dir):\n",
        "        self.model.save_pretrained(output_dir)\n",
        "        self.feature_extractor.save_pretrained(output_dir)\n",
        "        torch.save(self.visual_projection.state_dict(), os.path.join(output_dir, \"visual_projection.bin\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OXa1Gqvvy4BK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "\n",
        "class ClipModel(nn.Module):\n",
        "    def __init__(self, model_name_or_path, device=None):\n",
        "        super(ClipModel, self).__init__()\n",
        "\n",
        "        if os.path.exists(model_name_or_path):\n",
        "            # load from file system\n",
        "            repo_dir = model_name_or_path\n",
        "        else:\n",
        "            # download from the Hugging Face model hub\n",
        "            repo_dir = snapshot_download(model_name_or_path)\n",
        "\n",
        "        self.text_model = ClipTextModel(repo_dir, device=device)\n",
        "        self.vision_model = ClipVisionModel(os.path.join(repo_dir, \"vision_model\"), device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logit_scale = nn.Parameter(torch.ones([]) * 2.6592)\n",
        "            logit_scale.set_(torch.load(os.path.join(repo_dir, \"logit_scale.bin\"), map_location=device).clone().cpu())\n",
        "            self.logit_scale = logit_scale\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = torch.device(device)\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, pixel_values, input_ids, attention_mask, token_type_ids):\n",
        "        image_features = self.vision_model(pixel_values=pixel_values)\n",
        "        text_features = self.text_model(input_ids=input_ids,\n",
        "                                        attention_mask=attention_mask,\n",
        "                                        token_type_ids=token_type_ids)[0]\n",
        "\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "        logits_per_text = logits_per_image.t()\n",
        "\n",
        "        return logits_per_image, logits_per_text\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode(self, images, texts, batch_size=8, max_length=64):\n",
        "        model.eval()\n",
        "        image_features = self.vision_model.encode_image(images, batch_size=batch_size)\n",
        "        text_features = self.text_model.encode_text(texts, batch_size=batch_size, max_length=max_length)\n",
        "\n",
        "        image_features = image_features.to(self.device)\n",
        "        text_features = text_features.to(self.device)\n",
        "\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "        logits_per_text = logits_per_image.t()\n",
        "\n",
        "        logits_per_image = logits_per_image.cpu()\n",
        "        logits_per_text = logits_per_text.cpu()\n",
        "\n",
        "        return logits_per_image, logits_per_text\n",
        "\n",
        "    def save(self, output_dir):\n",
        "        torch.save(self.logit_scale, os.path.join(output_dir, \"logit_scale.bin\"))\n",
        "        self.text_model.save(output_dir)\n",
        "        self.vision_model.save(os.path.join(output_dir, \"vision_model\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zt44QMsBsms"
      },
      "source": [
        "## テキストの正規化処理定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gAuNeNpECPZ0"
      },
      "outputs": [],
      "source": [
        "# https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja から引用・一部改変\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def unicode_normalize(cls, s):\n",
        "    pt = re.compile('([{}]+)'.format(cls))\n",
        "\n",
        "    def norm(c):\n",
        "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
        "\n",
        "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
        "    s = re.sub('－', '-', s)\n",
        "    return s\n",
        "\n",
        "def remove_extra_spaces(s):\n",
        "    s = re.sub('[ 　]+', ' ', s)\n",
        "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
        "                      '\\u3040-\\u309F',  # HIRAGANA\n",
        "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
        "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
        "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
        "                      ))\n",
        "    basic_latin = '\\u0000-\\u007F'\n",
        "\n",
        "    def remove_space_between(cls1, cls2, s):\n",
        "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
        "        while p.search(s):\n",
        "            s = p.sub(r'\\1\\2', s)\n",
        "        return s\n",
        "\n",
        "    s = remove_space_between(blocks, blocks, s)\n",
        "    s = remove_space_between(blocks, basic_latin, s)\n",
        "    s = remove_space_between(basic_latin, blocks, s)\n",
        "    return s\n",
        "\n",
        "def normalize_neologd(s):\n",
        "    s = s.strip()\n",
        "    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
        "\n",
        "    def maketrans(f, t):\n",
        "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
        "\n",
        "    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
        "    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
        "    s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
        "    s = s.translate(\n",
        "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
        "              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
        "\n",
        "    s = remove_extra_spaces(s)\n",
        "    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
        "    s = re.sub('[’]', '\\'', s)\n",
        "    s = re.sub('[”]', '\"', s)\n",
        "    s = s.lower()\n",
        "    return s\n",
        "\n",
        "def normalize_text(text):\n",
        "    return normalize_neologd(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDxNKu4NXBOC"
      },
      "source": [
        "## 学習済み日本語CLIPモデルのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn63CjCA0GQx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = ClipModel(\"sonoisa/clip-vit-b-32-japanese-v1\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEv-WK78h4Zh"
      },
      "source": [
        "# 1. 画像とテキストの類似性計算\n",
        "\n",
        "16枚の画像について、文章との類似度を求めます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scUEvF9bE3ca"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# 類似度を求める対象の画像（16枚）\n",
        "images = [Image.open(f\"/content/clip-japanese/sample_images/{i}.jpeg\") for i in range(1, 17)]\n",
        "\n",
        "# タイリング表示\n",
        "plt.figure(dpi=140, figsize=(10,10))\n",
        "\n",
        "for i in range(len(images)):\n",
        "    sp = plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(images[i])\n",
        "    text = sp.text(-16, 0, f\"{i + 1}\", ha=\"right\", va=\"top\", color=\"black\", fontsize=12)\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx-upBJhKkxk"
      },
      "source": [
        "類似度を計算します。\n",
        "\n",
        "- logits_per_imageのsoftmaxをとると、1つの画像に関する各文章の類似度（合計1.0）になります。\n",
        "- logits_per_textのsoftmaxをとると、1つの文章に関する各画像の類似度（合計1.0）になります。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sE2jJU19kKzS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# 画像との類似度を求める文章\n",
        "texts = [\"猫\",\n",
        "         \"ロゼッタストーン\",\n",
        "         \"恐竜と子供\", \"恐竜\", \"子供\",  # 複数の物体が写っているとき、全体を見て類似性を判定するか？\n",
        "         \"考えるスティーブ・ジョブズの人形\",  # 考える人との混同が起きないか？\n",
        "         \"レゴでできたマリオやルイージなど\",  # レゴやマリオといった固有名詞を認識できるか？\n",
        "         \"レゴでできた時計\",  # 時計に見えるかギリギリのものを認識できるか？\n",
        "         \"魔女ランダと聖獣バロン\", \"特殊合体するとシヴァ神\",  # あまりメジャーではなさそうな存在を認識できるか？\n",
        "         \"彫刻「考える人」\",  # 考えるスティーブ・ジョブズとの混同が起きないか？\n",
        "         \"水槽の中のアンモナイト\",  # コクテンフグと見分けがつくか？\n",
        "         \"鶏とヒヨコのおもちゃ\",  # 抽象的な造形表現を認識できるか？\n",
        "         \"水槽の中の犬\", \"水槽の中のコクテンフグ\",  # 犬と錯覚するか？\n",
        "         \"お菓子が1個\", \"お菓子が2個\", \"お菓子が3個\", \"お菓子が4個\", \"お菓子が5個\",  # 数勘定できるか？\n",
        "         \"彫刻「午後の日」\", \"芸術作品\",  # 日本の芸術作品を認識できるか？\n",
        "         \"眠るコアラ\", \"木登りするコアラ\",  # 行動を識別できるか？\n",
        "         \"Apple\", \"Pineapple\",  # アルファベットを認識できるか？（英語版CLIPではできるため、できなくなっていないかの確認）\n",
        "         \"りんご\", \"パイナップル\",  # ひらがなを認識できるか？\n",
        "        ]\n",
        "\n",
        "texts = [normalize_text(text) for text in texts]  # この正規化は必須です。行わないと精度が落ちることがあります。\n",
        "\n",
        "logits_per_image, logits_per_text = model.encode(images, texts)\n",
        "\n",
        "similarity_per_image = torch.softmax(logits_per_image, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wiX7jIyIN5t"
      },
      "source": [
        "画像と文章との類似度を可視化します。  \n",
        "画像（縦軸）ごとに、その画像と各文章の類似度（横軸）を示します。  \n",
        "赤丸は正解です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoq58PYO90OB"
      },
      "outputs": [],
      "source": [
        "# ref. https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import japanize_matplotlib\n",
        "import numpy as np\n",
        "\n",
        "def heatmap(x_labels, y_labels, expected_answers, values):\n",
        "    fig, ax = plt.subplots(dpi=140, figsize=(8, 8))\n",
        "    im = ax.imshow(values, vmin=0, vmax=1, cmap=\"viridis\")\n",
        "\n",
        "    ax.set_xticks(np.arange(len(x_labels)))\n",
        "    ax.set_yticks(np.arange(len(y_labels)))\n",
        "    ax.set_xticklabels(x_labels)\n",
        "    ax.set_yticklabels(y_labels)\n",
        "    ax.set_xlabel(\"テキスト\")\n",
        "    ax.set_ylabel(\"画像\")\n",
        "\n",
        "    plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\", fontsize=10,\n",
        "            rotation_mode=\"anchor\")\n",
        "\n",
        "    for i in range(len(y_labels)):\n",
        "        for j in range(len(x_labels)):\n",
        "            ax.text(j, i, \"%.2f\" % values[i, j],\n",
        "                    ha=\"center\", va=\"center\", color=\"w\", fontsize=6)\n",
        "            if expected_answers[i] == j:\n",
        "                c = patches.Circle(xy=(j, i), radius=0.5, ec='r', fill=False)\n",
        "                ax.add_patch(c)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "x_labels = texts\n",
        "y_labels = [\"1. 猫\", \"2. ロゼッタストーン\", \"3. 恐竜と子供\", \"4. ジョブズの人形\",\n",
        "            \"5. レゴでできたマリオなど\", \"6. レゴでできた時計\", \"7. 魔女ランダと聖獣バロン\",\n",
        "            \"8. 彫刻「考える人」\", \"9. アンモナイト\", \"10. 鶏とヒヨコのおもちゃ\", \"11. コクテンフグ\",\n",
        "            \"12. 和菓子\", \"13. 彫刻「午後の日」\", \"14. コアラ\", \"15. Apple\",\n",
        "            \"16. りんご\"]\n",
        "expected_answers = [0, 1, 2, 5, 6, 7, 8, 10, 11, 12, 14, 18, 20, 22, 24, 26]\n",
        "heatmap(x_labels, y_labels, expected_answers, similarity_per_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6y8TVb0EoGP"
      },
      "source": [
        "# 2. 画像とテキストの埋め込み計算\n",
        "\n",
        "画像とテキストの埋め込みベクトルを得る方法について説明します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4a41Pl6fCFg"
      },
      "source": [
        "画像の埋め込みベクトルを計算します。  \n",
        "model.vision_modelのencode_image(images)を呼び出すだけです。  \n",
        "この例では 3 x 512 次元のテンソルが得られます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lSFBxDsn-4dT"
      },
      "outputs": [],
      "source": [
        "sample_images_3 = [Image.open(f\"/content/clip-japanese/sample_images/{i}.jpeg\") for i in range(1, 4)]\n",
        "image_features = model.vision_model.encode_image(sample_images_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuYp9b5DfW3G"
      },
      "source": [
        "同様に、テキストの埋め込みベクトルを計算します。  \n",
        "model.text_modelのencode_text(texts)を呼び出すだけです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ubhimR9Xefim"
      },
      "outputs": [],
      "source": [
        "sample_texts_3 = [\"猫\", \"ロゼッタストーン\", \"恐竜と子供\"]\n",
        "sample_texts_3 = [normalize_text(text) for text in sample_texts_3]  # この正規化は必須です。行わないと精度が落ちることがあります。\n",
        "text_features = model.text_model.encode_text(sample_texts_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppWtOwUSgIcw"
      },
      "source": [
        "この画像とテキストの埋め込みベクトルのコサイン類似度を求めてみます。  \n",
        "行と列両方について対角要素が最も大きな値になることが期待されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsvTauPigwAv"
      },
      "outputs": [],
      "source": [
        "# 単位ベクトル化\n",
        "image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "similarity_matrix = image_features @ text_features.t()\n",
        "print(similarity_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K19ohExXiFFs"
      },
      "source": [
        "# 3. 類似画像検索\n",
        "\n",
        "テキストや画像を用いた画像検索をしてみます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9NoMnZxyiJy7"
      },
      "outputs": [],
      "source": [
        "# 検索対象となる画像の埋め込みベクトルを計算しておく。\n",
        "target_images = [Image.open(f\"/content/clip-japanese/sample_images/{i}.jpeg\") for i in range(1, 17)]\n",
        "target_vectors = model.vision_model.encode_image(target_images).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "nngxAZ92jLXR"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "\n",
        "def search_image(query_vector, target_vectors, target_images, closest_n=3):\n",
        "    distances = cdist(\n",
        "        query_vector, target_vectors, metric=\"cosine\"\n",
        "    )[0]\n",
        "\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    # タイリング表示\n",
        "    plt.figure(dpi=140, figsize=(6,6))\n",
        "\n",
        "    for i, (idx, distance) in enumerate(results[0:closest_n]):\n",
        "        image = target_images[idx]\n",
        "\n",
        "        sp = plt.subplot(math.ceil(closest_n / 4), 4, i + 1)\n",
        "        plt.imshow(image)\n",
        "        text = sp.text(-32, 0, f\"{i + 1}: {distance:0.5f}\", ha=\"left\", va=\"bottom\", color=\"black\", fontsize=12)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "def search_image_by_text(text, target_vectors, target_images, closest_n=3):\n",
        "    text = normalize_text(text)\n",
        "    text_features = model.text_model.encode_text([text]).numpy()\n",
        "    search_image(text_features, target_vectors, target_images, closest_n)\n",
        "\n",
        "def search_image_by_image(image, target_vectors, target_images, closest_n=3):\n",
        "    image_features = model.vision_model.encode_image([image]).numpy()\n",
        "    search_image(image_features, target_vectors, target_images, closest_n)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQK2lb_Mt0Pz"
      },
      "source": [
        "テキストで画像を検索してみます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiGzVVHBj6Ba"
      },
      "outputs": [],
      "source": [
        "text = \"猫\"\n",
        "search_image_by_text(text, target_vectors, target_images, closest_n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fub3ijw9t4aE"
      },
      "source": [
        "与えられた画像に似た画像を探します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "085y5sgHlfvE"
      },
      "outputs": [],
      "source": [
        "image = Image.open(f\"/content/clip-japanese/sample_images/1.jpeg\")\n",
        "search_image_by_image(image, target_vectors, target_images, closest_n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF5mfoiomWM5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}