{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0316Huggingface_tutorial_custom_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJwP_RFZpP-T"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7l-d06UpP-V"
      },
      "source": [
        "# ä¸€èˆ¬çš„ãªä¸‹æµä½œæ¥­ã®ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•\n",
        "<!-- # How to fine-tune a model for common downstream tasks -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7QGU7X4pP-V"
      },
      "source": [
        "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ï¼Œä¸€èˆ¬çš„ãªä¸‹æµèª²é¡Œã®ãŸã‚ã« ğŸ¤— Transformers ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n",
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç´ æ—©ããƒ­ãƒ¼ãƒ‰ã—ã¦å‰å‡¦ç†ã‚’è¡Œã„ï¼ŒPyTorch ã¨ TensorFlow ã§å­¦ç¿’ã™ã‚‹ãŸã‚ã®æº–å‚™ã‚’ã™ã‚‹ãŸã‚ã« ğŸ¤— Datasets ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "<!-- This guide will show you how to fine-tune ğŸ¤— Transformers models for common downstream tasks. \n",
        "You will use the ğŸ¤— Datasets library to quickly load and preprocess the datasets, getting them ready for training with PyTorch and TensorFlow.-->\n",
        "\n",
        "å§‹ã‚ã‚‹å‰ã« ğŸ¤— Datasets ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
        "è©³ã—ã„ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã«ã¤ã„ã¦ã¯ ğŸ¤— Datasets [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒšãƒ¼ã‚¸](https://huggingface.co/docs/datasets/installation.html) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n",
        "ã“ã®ã‚¬ã‚¤ãƒ‰ã®ã™ã¹ã¦ã®ä¾‹ã§ã¯ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†ã« ğŸ¤— Datasets ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "<!-- Before you begin, make sure you have the ğŸ¤— Datasets library installed. For more detailed installation instructions, refer to the ğŸ¤— Datasets [installation page](https://huggingface.co/docs/datasets/installation.html). \n",
        "All of the examples in this guide will use ğŸ¤— Datasets to load and preprocess a dataset. -->\n",
        "\n",
        "```bash\n",
        "pip install datasets\n",
        "```\n",
        "\n",
        "ã“ã“ã§ã¯ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™:\n",
        "<!-- Learn how to fine-tune a model for: -->\n",
        "\n",
        "- [seq_imdb](#seq_imdb)\n",
        "- [tok_ner](#tok_ner)\n",
        "- [qa_squad](#qa_squad)\n",
        "\n",
        "<a id='seq_imdb'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWyD-4jtpP-W"
      },
      "source": [
        "## IMDbãƒ¬ãƒ“ãƒ¥ãƒ¼ã«ã‚ˆã‚‹ç³»åˆ—åˆ†é¡\n",
        "<!-- ## Sequence classification with IMDb reviews -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjtjXUHspP-W"
      },
      "source": [
        "ç³»åˆ—åˆ†é¡ã¨ã¯ï¼Œæ–‡ã®ç³»åˆ—ã‚’ï¼Œä¸ãˆã‚‰ã‚ŒãŸæ•°ã®ã‚¯ãƒ©ã‚¹ã«å¾“ã£ã¦åˆ†é¡ã™ã‚‹èª²é¡Œã®ã“ã¨ã§ã™ã‚‹ã€‚\n",
        "ã“ã®ä¾‹ã§ã¯ï¼Œãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒè‚¯å®šçš„ã‹å¦å®šçš„ã‹ã‚’æ±ºå®šã™ã‚‹ãŸã‚ã«ï¼Œ[IMDb dataset](https://huggingface.co/datasets/imdb) ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚\n",
        "<!-- Sequence classification refers to the task of classifying sequences of text according to a given number of classes. \n",
        "In this example, learn how to fine-tune a model on the [IMDb dataset](https://huggingface.co/datasets/imdb) to determine whether a review is positive or negative. -->\n",
        "\n",
        "<Tip>\n",
        "\n",
        "æ–‡åˆ†é¡ã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ï¼Œã‚ˆã‚Šè©³ç´°ãªä¾‹ã«ã¤ã„ã¦ã¯ï¼Œå¯¾å¿œã™ã‚‹ [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb) ã¾ãŸã¯ [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification-tf.ipynb) ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
        "<!-- For a more in-depth example of how to fine-tune a model for text classification, take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb) or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification-tf.ipynb). -->\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOdpupLypP-W"
      },
      "source": [
        "### IMDb ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰\n",
        "<!-- ### Load IMDb dataset -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufWyOD0KpP-X"
      },
      "source": [
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª (ğŸ¤—Datasets) ã‚’ä½¿ç”¨ã™ã‚‹ã¨ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç°¡å˜ã«èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- The ğŸ¤— Datasets library makes it simple to load a dataset: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HQksqntpP-X"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "imdb = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7ctQYj7pP-X"
      },
      "source": [
        "ã“ã‚Œã¯ `DatasetDict` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ï¼Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¦ä¾‹ã‚’è¦‹ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
        "<!-- This loads a `DatasetDict` object which you can index into to view an example: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq-EVztDpP-Y"
      },
      "outputs": [],
      "source": [
        "imdb[\"train\"][0]\n",
        "{\n",
        "    \"label\": 1,\n",
        "    \"text\": \"Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \\\"Teachers\\\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \\\"Teachers\\\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKl0KUynpP-Y"
      },
      "source": [
        "### å‰å‡¦ç†\n",
        "<!-- ### Preprocess -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUMRFTVvpP-Y"
      },
      "source": [
        "æ¬¡ã®æ®µéšã¯ï¼Œæ–‡ã‚’ãƒ¢ãƒ‡ãƒ«ã§èª­ã¿å–ã‚Šå¯èƒ½ãªå½¢å¼ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ã“ã¨ã§ã™ã€‚\n",
        "é©åˆ‡ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸå˜èªã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«ï¼Œãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã—ãŸã®ã¨åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚\n",
        "DistilBERT ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’ [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer) ã¨å…±ã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n",
        "ã“ã‚Œã¯ï¼Œæœ€çµ‚çš„ã«ï¼Œäº‹å‰ã«å­¦ç¿’ã—ãŸ [DistilBERT](https://huggingface.co/distilbert-base-uncased) ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦åˆ†é¡å™¨ã‚’è¨“ç·´ã™ã‚‹ãŸã‚ã§ã™ã€‚\n",
        "<!-- The next step is to tokenize the text into a readable format by the model. \n",
        "It is important to load the same tokenizer a model was trained with to ensure appropriately tokenized words. \n",
        "Load the DistilBERT tokenizer with the [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer) because we will eventually train a classifier using a pretrained [DistilBERT](https://huggingface.co/distilbert-base-uncased) model: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-rHe38kpP-Y"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SejV6Vw6pP-Y"
      },
      "source": [
        "ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ã‚’å®Ÿä½“åŒ–ã—ãŸã®ã§ï¼Œæ–‡ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚\n",
        "ã¾ãŸï¼Œæ–‡å†…ã®é•·ã„ç³»åˆ—ã‚’åˆ‡ã‚Šæ¨ã¦ã¦ï¼Œãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§å…¥åŠ›é•·ä»¥ä¸‹ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "<!-- Now that you have instantiated a tokenizer, create a function that will tokenize the text. \n",
        "You should also truncate longer sequences in the text to be no longer than the model's maximum input length: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-UyFelOpP-Z"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0kGRGiCpP-Z"
      },
      "source": [
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«å‰å‡¦ç†é–¢æ•°ã‚’é©ç”¨ã™ã‚‹ã«ã¯ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(ğŸ¤— Datasets ) `map` é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "ã¾ãŸ  `batched=True` ã‚’è¨­å®šã™ã‚‹ã¨ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¤‡æ•°ã®è¦ç´ ã«å¯¾ã—ã¦ä¸€åº¦ã«å‰å‡¦ç†é–¢æ•°ã‚’é©ç”¨ã—ã¦ï¼Œã‚ˆã‚Šé«˜é€Ÿã«å‰å‡¦ç†ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- Use ğŸ¤— Datasets `map` function to apply the preprocessing function to the entire dataset. \n",
        "You can also set `batched=True` to apply the preprocessing function to multiple elements of the dataset at once for faster preprocessing: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73rVX2zKpP-Z"
      },
      "outputs": [],
      "source": [
        "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XABFxPGepP-Z"
      },
      "source": [
        "æœ€å¾Œã«ï¼Œæ–‡ã®é•·ã•ãŒä¸€å®šã«ãªã‚‹ã‚ˆã†ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¾ã™ã€‚\n",
        "ãƒˆãƒ¼ã‚¯ãƒ³åŒ–é–¢æ•°ã§ `padding=True` ã‚’è¨­å®šã™ã‚Œã°ï¼Œæ–‡ã‚’è©°ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ã§ã™ãŒï¼Œãƒãƒƒãƒå†…ã®æœ€ã‚‚é•·ã„è¦ç´ ã®é•·ã•ã«åˆã‚ã›ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’è©°ã‚ã‚‹æ–¹ãŒåŠ¹ç‡çš„ã§ã™ã€‚\n",
        "ã“ã‚Œã¯ **ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°** ã¨å‘¼ã°ã‚Œã¾ã™ã€‚\n",
        "ã“ã‚Œã¯ `DataCollatorWithPadding` é–¢æ•°ã§è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "<!-- Lastly, pad your text so they are a uniform length. \n",
        "While it is possible to pad your text in the `tokenizer` function by setting `padding=True`, it is more efficient to only pad the text to the length of the longest element in its batch. \n",
        "This is known as **dynamic padding**. You can do this with the `DataCollatorWithPadding` function: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RzAHU-fpP-Z"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D18YsoIapP-Z"
      },
      "source": [
        "### Trainer API ã‚’ç”¨ã„ãŸå¾®èª¿æ•´\n",
        "<!-- ### Fine-tune with the Trainer API -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URMCty4npP-Z"
      },
      "source": [
        "ã“ã“ã§ï¼Œãƒ¢ãƒ‡ãƒ«ã« [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification) ã‚¯ãƒ©ã‚¹ã¨äºˆæƒ³ã•ã‚Œã‚‹ãƒ©ãƒ™ãƒ«ã®æ•°ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™: \n",
        "<!-- Now load your model with the [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification) class along with the number of expected labels: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuZii27gpP-Z"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FipICK9vpP-a"
      },
      "source": [
        "ã“ã®æ™‚ç‚¹ã§ï¼Œæ®‹ã‚‹ã¯ 3 ã¤ã®æ®µéšã®ã¿ã§ã™: \n",
        "<!-- At this point, only three steps remain:-->\n",
        "\n",
        "1. [TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments) ã§è¨“ç·´ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã™ã‚‹ã€‚\n",
        "2. [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) ã«ãƒ¢ãƒ‡ãƒ«ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼Œãƒˆãƒ¼ã‚¯ãƒ³åŒ–å™¨ï¼Œãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã¨ä¸€ç·’ã«å­¦ç¿’å¼•æ•°ã‚’æ¸¡ã—ã¾ã™ã€‚\n",
        "3. `Trainer.train()` ã‚’å‘¼ã³å‡ºã—ã¦ï¼Œãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã‚’è¡Œã†ã€‚\n",
        "\n",
        "<!-- 1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments).\n",
        "2. Pass the training arguments to a [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, and data collator.\n",
        "3. Call `Trainer.train()` to fine-tune your model. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deN7NBGKpP-a",
        "outputId": "dd9c93f9-17fa-4921-ca04-4f7586255a2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 25000\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 7815\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='7815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   9/7815 04:42 < 87:35:26, 0.02 it/s, Epoch 0.01/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb[\"train\"],\n",
        "    eval_dataset=tokenized_imdb[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNo-T43YpP-a"
      },
      "source": [
        "### TensorFlow ã‚’ç”¨ã„ãŸå¾®èª¿æ•´ (è¨³å‡ºãªã—)\n",
        "<!-- ### Fine-tune with TensorFlow -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4PUR-YlpP-a"
      },
      "source": [
        "Fine-tuning with TensorFlow is just as easy, with only a few differences.\n",
        "\n",
        "Start by batching the processed examples together with dynamic padding using the [DataCollatorWithPadding](https://huggingface.co/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding) function.\n",
        "Make sure you set `return_tensors=\"tf\"` to return `tf.Tensor` outputs instead of PyTorch tensors!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrMU68NppP-a"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evHDKbAypP-a"
      },
      "source": [
        "Next, convert your datasets to the `tf.data.Dataset` format with `to_tf_dataset`. Specify inputs and labels in the\n",
        "`columns` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bru5o7QkpP-a"
      },
      "outputs": [],
      "source": [
        "tf_train_set = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_validation_set = tokenized_imdb[\"test\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wokPMt-pP-a"
      },
      "source": [
        "Set up an optimizer function, learning rate schedule, and some training hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U56W1YUpP-a"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 16\n",
        "num_train_epochs = 5\n",
        "batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n",
        "total_train_steps = int(batches_per_epoch * num_train_epochs)\n",
        "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdTRQpV-pP-a"
      },
      "source": [
        "Load your model with the [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification) class along with the number of expected labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHktSs6EpP-a"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RYqeCgOpP-a"
      },
      "source": [
        "Compile the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7kAO8SSpP-a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9m7DmjOpP-a"
      },
      "source": [
        "Finally, fine-tune the model by calling `model.fit`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ldWYKcVpP-a"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    tf_train_set,\n",
        "    validation_data=tf_validation_set,\n",
        "    epochs=num_train_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPWh1HGepP-b"
      },
      "source": [
        "<a id='tok_ner'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WTQdGiNpP-b"
      },
      "source": [
        "## WNUTã‚¨ãƒãƒ¼ã‚¸ãƒ³ã‚°ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«ã‚ˆã‚‹ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡\n",
        "<!-- ## Token classification with WNUT emerging entities -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGYEetPopP-b"
      },
      "source": [
        "ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã¨ã¯ï¼Œæ–‡ä¸­ã®å€‹ã€…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åˆ†é¡ã™ã‚‹ä½œæ¥­ã®ã“ã¨ã§ã‚ã‚‹ã€‚\n",
        "ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã®æœ€ã‚‚ä¸€èˆ¬çš„ãªèª²é¡Œã® 1 ã¤ãŒåå‰ä»˜ãå›ºæœ‰è¡¨ç¾èªè­˜ (NER) ã§ã™ã€‚\n",
        "NER ã¯ï¼Œäººï¼Œå ´æ‰€ï¼Œçµ„ç¹”ãªã©ï¼Œæ–‡ä¸­ã®å„å®Ÿä½“ã«å¯¾å¿œã™ã‚‹ãƒ©ãƒ™ãƒ«ã‚’è¦‹ã¤ã‘ã‚ˆã†ã¨ã™ã‚‹ã‚‚ã®ã§ã™ã€‚\n",
        "ã“ã®ä¾‹ã§ã¯ [WNUT 17](https://huggingface.co/datasets/wnut_17) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¦ï¼Œæ–°ã—ã„å®Ÿä½“ã‚’æ¤œå‡ºã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚\n",
        "<!-- Token classification refers to the task of classifying individual tokens in a sentence. \n",
        "One of the most common token classification tasks is Named Entity Recognition (NER). \n",
        "NER attempts to find a label for each entity in a sentence, such as a person, location, or organization. \n",
        "In this example, learn how to fine-tune a model on the [WNUT 17](https://huggingface.co/datasets/wnut_17) dataset to detect new entities. -->\n",
        "\n",
        "<Tip>\n",
        "\n",
        "ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ï¼Œã‚ˆã‚Šè©³ç´°ãªä¾‹ã«ã¤ã„ã¦ã¯ï¼Œå¯¾å¿œã™ã‚‹ [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb) ã¾ãŸã¯ [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification-tf.ipynb) ã‚’è¦‹ã¦ã¿ã¦ãã ã•ã„ã€‚\n",
        "<!-- For a more in-depth example of how to fine-tune a model for token classification, take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb) or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification-tf.ipynb). -->\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDhUz5dcpP-b"
      },
      "source": [
        "### WNUT 17 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰\n",
        "<!-- ### Load WNUT 17 dataset -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLpQ550ppP-b"
      },
      "source": [
        "WNUT 17 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (ğŸ¤— Datasets) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n",
        "<!-- Load the WNUT 17 dataset from the ğŸ¤— Datasets library: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADurpy2IpP-b"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "wnut = load_dataset(\"wnut_17\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18aE05PwpP-b"
      },
      "source": [
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã–ã£ã¨è¦‹ã‚‹ã¨ï¼Œæ–‡ä¸­ã®å„å˜èªã«é–¢é€£ã™ã‚‹ãƒ©ãƒ™ãƒ«ãŒè¡¨ç¤ºã•ã‚Œã¦ã¾ã™:\n",
        "<!-- A quick look at the dataset shows the labels associated with each word in the sentence: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2xS4UkOpP-b"
      },
      "outputs": [],
      "source": [
        "wnut[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ2M2mzQpP-b"
      },
      "source": [
        "View the specific NER tags by:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJxahXBXpP-b",
        "outputId": "561b7911-4799-44c4-9b10-d52c05720efa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\n",
              "    \"O\",\n",
              "    \"B-corporation\",\n",
              "    \"I-corporation\",\n",
              "    \"B-creative-work\",\n",
              "    \"I-creative-work\",\n",
              "    \"B-group\",\n",
              "    \"I-group\",\n",
              "    \"B-location\",\n",
              "    \"I-location\",\n",
              "    \"B-person\",\n",
              "    \"I-person\",\n",
              "    \"B-product\",\n",
              "    \"I-product\",\n",
              "]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n",
        "label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee1xsOfQpP-b"
      },
      "source": [
        "A letter prefixes each NER tag which can mean:\n",
        "\n",
        "- `B-` indicates the beginning of an entity.\n",
        "- `I-` indicates a token is contained inside the same entity (e.g., the `State` token is a part of an entity like\n",
        "  `Empire State Building`).\n",
        "- `0` indicates the token doesn't correspond to any entity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQzkgbzbpP-b"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXHtm1J2pP-b"
      },
      "source": [
        "Now you need to tokenize the text. Load the DistilBERT tokenizer with an [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m-0MuBdpP-b"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DDAidEtpP-b"
      },
      "source": [
        "Since the input has already been split into words, set `is_split_into_words=True` to tokenize the words into\n",
        "subwords:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOTFBQ-CpP-b",
        "outputId": "0f4102b9-a31f-4bd7-9b8a-e49fdf9daf59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', '@', 'paul', '##walk', 'it', \"'\", 's', 'the', 'view', 'from', 'where', 'i', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52-blna4pP-c"
      },
      "source": [
        "The addition of the special tokens `[CLS]` and `[SEP]` and subword tokenization creates a mismatch between the\n",
        "input and labels. Realign the labels and tokens by:\n",
        "\n",
        "1. Mapping all tokens to their corresponding word with the `word_ids` method.\n",
        "2. Assigning the label `-100` to the special tokens `[CLS]` and ``[SEP]``` so the PyTorch loss function ignores\n",
        "   them.\n",
        "3. Only labeling the first token of a given word. Assign `-100` to the other subtokens from the same word.\n",
        "\n",
        "Here is how you can create a function that will realign the labels and tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eLQh04MpP-c"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xy-7mmBpP-c"
      },
      "source": [
        "Now tokenize and align the labels over the entire dataset with ğŸ¤— Datasets `map` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD3DFKgYpP-c"
      },
      "outputs": [],
      "source": [
        "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlIOVsMXpP-c"
      },
      "source": [
        "Finally, pad your text and labels, so they are a uniform length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZjy5jMCpP-c"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tueh_rdnpP-c"
      },
      "source": [
        "### Fine-tune with the Trainer API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB5sgswcpP-c"
      },
      "source": [
        "Load your model with the [AutoModelForTokenClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForTokenClassification) class along with the number of expected labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NxzDkWepP-c"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(label_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOLGhz2VpP-c"
      },
      "source": [
        "Gather your training arguments in [TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj-sovUbpP-c"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SdwjYmDpP-c"
      },
      "source": [
        "Collect your model, training arguments, dataset, data collator, and tokenizer in [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLfNa0sHpP-c"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_wnut[\"train\"],\n",
        "    eval_dataset=tokenized_wnut[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCdGk0_IpP-c"
      },
      "source": [
        "Fine-tune your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R52cM-1pP-c"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfLE_VeIpP-c"
      },
      "source": [
        "### Fine-tune with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFjt6yYKpP-d"
      },
      "source": [
        "Batch your examples together and pad your text and labels, so they are a uniform length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7s3-Rm6pP-d"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvOBhNqBpP-d"
      },
      "source": [
        "Convert your datasets to the `tf.data.Dataset` format with `to_tf_dataset`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srKEQb2UpP-d"
      },
      "outputs": [],
      "source": [
        "tf_train_set = tokenized_wnut[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_validation_set = tokenized_wnut[\"validation\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exp_u5xypP-d"
      },
      "source": [
        "Load the model with the [TFAutoModelForTokenClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForTokenClassification) class along with the number of expected labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHL3Tj50pP-d"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForTokenClassification\n",
        "\n",
        "model = TFAutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(label_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TAoyaI_pP-d"
      },
      "source": [
        "Set up an optimizer function, learning rate schedule, and some training hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ufv04DppP-d"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "batch_size = 16\n",
        "num_train_epochs = 3\n",
        "num_train_steps = (len(tokenized_datasets[\"train\"]) // batch_size) * num_train_epochs\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        "    num_warmup_steps=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqID4Mo8pP-d"
      },
      "source": [
        "Compile the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLMfuPQDpP-d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HErGMBYFpP-d"
      },
      "source": [
        "Call `model.fit` to fine-tune your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBHfCm_wpP-d"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    tf_train_set,\n",
        "    validation_data=tf_validation_set,\n",
        "    epochs=num_train_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QOOwgtGpP-d"
      },
      "source": [
        "<a id='qa_squad'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVZj4_SepP-d"
      },
      "source": [
        "## Question Answering with SQuAD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F098QApwpP-d"
      },
      "source": [
        "There are many types of question answering (QA) tasks. Extractive QA focuses on identifying the answer from the text\n",
        "given a question. In this example, learn how to fine-tune a model on the [SQuAD](https://huggingface.co/datasets/squad) dataset.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "For a more in-depth example of how to fine-tune a model for question answering, take a look at the corresponding\n",
        "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n",
        "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering-tf.ipynb).\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmJkgs15pP-d"
      },
      "source": [
        "### Load SQuAD dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNnxxHLDpP-d"
      },
      "source": [
        "Load the SQuAD dataset from the ğŸ¤— Datasets library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i35Z8yaOpP-d"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "squad = load_dataset(\"squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnS4Zcc7pP-d"
      },
      "source": [
        "Take a look at an example from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO5FN0FnpP-d",
        "outputId": "21a93788-52cd-4c60-b4ba-0ab6af44af5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n",
              " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'id': '5733be284776f41900661182',\n",
              " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
              " 'title': 'University_of_Notre_Dame'\n",
              "}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9AvTKF9pP-d"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRZDPXIepP-d"
      },
      "source": [
        "Load the DistilBERT tokenizer with an [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts2Cwwe-pP-d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA5z_UhSpP-d"
      },
      "source": [
        "There are a few things to be aware of when preprocessing text for question answering:\n",
        "\n",
        "1. Some examples in a dataset may have a very long `context` that exceeds the maximum input length of the model. You\n",
        "   can deal with this by truncating the `context` and set `truncation=\"only_second\"`.\n",
        "2. Next, you need to map the start and end positions of the answer to the original context. Set\n",
        "   `return_offset_mapping=True` to handle this.\n",
        "3. With the mapping in hand, you can find the start and end tokens of the answer. Use the `sequence_ids` method to\n",
        "   find which part of the offset corresponds to the question, and which part of the offset corresponds to the context.\n",
        "\n",
        "Assemble everything in a preprocessing function as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccn40ygMpP-e"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "031pJ7X6pP-e"
      },
      "source": [
        "Apply the preprocessing function over the entire dataset with ğŸ¤— Datasets `map` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iUyY-i-pP-e"
      },
      "outputs": [],
      "source": [
        "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N4YBZZzpP-e"
      },
      "source": [
        "Batch the processed examples together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlb6tV6ipP-e"
      },
      "outputs": [],
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1CNGyD2pP-e"
      },
      "source": [
        "### Fine-tune with the Trainer API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOOt91PkpP-e"
      },
      "source": [
        "Load your model with the [AutoModelForQuestionAnswering](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForQuestionAnswering) class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMizNEddpP-e"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guV0qtczpP-e"
      },
      "source": [
        "Gather your training arguments in [TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wclhocg_pP-e"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNhtaxsIpP-e"
      },
      "source": [
        "Collect your model, training arguments, dataset, data collator, and tokenizer in [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnyY4ZpVpP-e"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_squad[\"train\"],\n",
        "    eval_dataset=tokenized_squad[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjkaUrbkpP-e"
      },
      "source": [
        "Fine-tune your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL7yNMJ8pP-e"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0qW4qEwpP-e"
      },
      "source": [
        "### Fine-tune with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiD00u-npP-e"
      },
      "source": [
        "Batch the processed examples together with a TensorFlow default data collator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUZpikeFpP-e"
      },
      "outputs": [],
      "source": [
        "from transformers.data.data_collator import tf_default_collator\n",
        "\n",
        "data_collator = tf_default_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owOPdbLmpP-e"
      },
      "source": [
        "Convert your datasets to the `tf.data.Dataset` format with the `to_tf_dataset` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXFvmzNMpP-e"
      },
      "outputs": [],
      "source": [
        "tf_train_set = tokenized_squad[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"start_positions\", \"end_positions\"],\n",
        "    dummy_labels=True,\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_validation_set = tokenized_squad[\"validation\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"start_positions\", \"end_positions\"],\n",
        "    dummy_labels=True,\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6WVlKBnpP-e"
      },
      "source": [
        "Set up an optimizer function, learning rate schedule, and some training hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogt6ouCbpP-e"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "batch_size = 16\n",
        "num_epochs = 2\n",
        "total_train_steps = (len(tokenized_squad[\"train\"]) // batch_size) * num_epochs\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=total_train_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJz6bKYqpP-f"
      },
      "source": [
        "Load your model with the [TFAutoModelForQuestionAnswering](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForQuestionAnswering) class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJCqX0vNpP-f"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForQuestionAnswering\n",
        "\n",
        "model = TFAutoModelForQuestionAnswering(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDFPqphRpP-f"
      },
      "source": [
        "Compile the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bu-g72upP-f"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2tTIwPapP-f"
      },
      "source": [
        "Call `model.fit` to fine-tune the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cTjodInpP-f"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    tf_train_set,\n",
        "    validation_data=tf_validation_set,\n",
        "    epochs=num_train_epochs,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2022_0316Huggingface_tutorial_custom_datasets.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}