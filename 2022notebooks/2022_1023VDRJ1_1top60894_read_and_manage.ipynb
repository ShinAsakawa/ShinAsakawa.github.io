{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPzMxVWlbpEVE3PC5aOWiqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_1023VDRJ1_1top60894_read_and_manage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 日本語を読むための”ＴＭ語彙リスト”（総合版）Ver.4.0 の読み込み\n",
        "\n",
        "* [松下言語学習ラボ](http://www17408ui.sakura.ne.jp/tatsum/database.html) 日本語を読むための語彙データベース Ver. 1.11 より\n",
        "[重要度順語彙データベース (Top 60894) 重要度順位 00001-60894 (42MB)](http://www17408ui.sakura.ne.jp/tatsum/database/VDRJ_Ver1_1_Research_Top60894.xlsx) を利用する"
      ],
      "metadata": {
        "id": "nkaD3Gsyrzhp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHjV6SjGrpkT"
      },
      "outputs": [],
      "source": [
        "# Mac の retina ディスプレイの場合，高解像度の画面を使用する\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "try:\n",
        "    import bit\n",
        "except ImportError:\n",
        "    !pip install ipynbname\n",
        "    !git clone https://github.com/ShinAsakawa/bit.git\n",
        "import bit\n",
        "isColab = bit.isColab\n",
        "HOME = bit.HOME\n",
        "#if isColab:\n",
        "#    !pip install transformers fugashi ipadic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ローカルと colab との相違を吸収するために\n",
        "# 本ファイルを Google Colaboratory 上で実行する場合に，必要となるライブラリをインストール\n",
        "if isColab:\n",
        "    #!git clone https://github.com/ShinAsakawa/ccap.git\n",
        "\n",
        "    # MeCab, fugashi, ipadic のインストール\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1\n",
        "    !pip install mecab-python3 > /dev/null 2>&1\n",
        "    !pip install --upgrade ipadic\n",
        "    !python -m unidic download\n",
        "\n",
        "    !pip install --upgrade openpyxl    # エクセルファイルを読むこむ際にバージョンの相違で動作しない場合があるので念の為\n",
        "    !pip install --upgrade pandas      # 同上\n",
        "    \n",
        "    !pip install japanize_matplotlib\n",
        "    !pip install jaconv"
      ],
      "metadata": {
        "id": "rwLcM3iprtcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データの作成"
      ],
      "metadata": {
        "id": "QJ5oXyWorySX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "import jaconv\n",
        "import datetime\n",
        "\n",
        "def make_VDRJ_dict(cover_rate:float=0.99,\n",
        "                   verbose:bool=False,\n",
        "                  ):\n",
        "    \"\"\"\n",
        "    日本語を読むための語彙データベース（VDRJ） Ver. 1.1　（＝日本語を読むための”ＴＭ語彙リスト”（総合版）　Ver.4.0）\n",
        "    を読み込んで，「単語」と対応する「読み」のリストを返す\n",
        "    \n",
        "    引数:\n",
        "        cover_rate:float\n",
        "            松下コーパスの `(Fw)累積テキストカバー率（想定既知語彙分を含む)` に基づいて語彙を選択するためのカバー率\n",
        "            デフォルトでは 0.99 だが変更可能\n",
        "    戻り値:\n",
        "        X:dict 以下のキーを持つ辞書\n",
        "             'lexeme': 語彙素\n",
        "              'orth': 書記素\n",
        "              'yomi': よみ\n",
        "              'mora': よみのモーラ\n",
        "              'cover_r': 累積カバー率\n",
        "            \n",
        "    \"\"\"\n",
        "    \n",
        "    vdrj_url='http://www17408ui.sakura.ne.jp/tatsum/database/VDRJ_Ver1_1_Research_Top60894.xlsx'\n",
        "    excel_fname = vdrj_url.split('/')[-1]  # 直上行の url からエクセルファイル名を切り出す\n",
        "\n",
        "    if not os.path.exists(excel_fname): # もしエクセルファイルが存在しなかったら ダウンロード\n",
        "        print(f'エクセルファイルのダウンロード {datetime.datetime.now()}...') if verbose else None\n",
        "        r = requests.get(vdrj_url)\n",
        "        with open(excel_fname, 'wb') as f:\n",
        "            total_length = int(r.headers.get('content-length'))\n",
        "            print('Downloading {0} - {1} bytes'.format(excel_fname, (total_length)))\n",
        "            f.write(r.content)\n",
        "        print(f'done {datetime.datetime.now()}') if verbose else None\n",
        "    \n",
        "    # 実際のエクセルファイルの読み込み\n",
        "    sheet_name='重要度順語彙リスト60894語'  # シート名を指定\n",
        "    print(f'エクセルファイルの読み込み {datetime.datetime.now()}...') if verbose else None\n",
        "    df = pd.read_excel(excel_fname, sheet_name=sheet_name)\n",
        "    print(f'done. {datetime.datetime.now()}') if verbose else None\n",
        "\n",
        "    print(f'データ作成 {datetime.datetime.now()}...') if verbose else None\n",
        "    # 累積カバー率が `cover_rate` 以下の語を選択\n",
        "    df = df[df['(Fw)累積テキストカバー率（想定既知語彙分を含む）\\nFw Cumulative Text Coverage including Assumed Known Words']<cover_rate]\n",
        "    \n",
        "    # 品詞 POS が '名詞-普通名詞-一般' のみを抽出\n",
        "    df = df[df['品詞\\nPart of Speech']=='名詞-普通名詞-一般']\n",
        "    \n",
        "    # NaN を含む行を削除\n",
        "    #df = df[df['標準的読み方（カタカナ）\\nStandard Reading (Katakana)'] != pd.np.nan]\n",
        "    #df = df[df['標準的（新聞）表記\\nStandard (Newspaper) Orthography'] != pd.np.nan]\n",
        "    df = df[['見出し語彙素\\nLexeme',\n",
        "             '標準的（新聞）表記\\nStandard (Newspaper) Orthography',\n",
        "             '標準的読み方（カタカナ）\\nStandard Reading (Katakana)', \n",
        "             '品詞\\nPart of Speech',\n",
        "             '(Fw)累積テキストカバー率（想定既知語彙分を含む）\\nFw Cumulative Text Coverage including Assumed Known Words',\n",
        "             'ID',\n",
        "            ]].dropna()\n",
        "    \n",
        "    # 必要となる情報のみをリスト化: ['語彙素', '書記素', 'よみ', '累積カバー率', 'ID']\n",
        "    Lexeme = df['見出し語彙素\\nLexeme'].to_list()\n",
        "    CoverR = df['(Fw)累積テキストカバー率（想定既知語彙分を含む）\\nFw Cumulative Text Coverage including Assumed Known Words'].to_list()\n",
        "    ID     = df['ID'].to_list()\n",
        "    Ortho  = df['標準的（新聞）表記\\nStandard (Newspaper) Orthography'].to_list()\n",
        "    Yomi   = df['標準的読み方（カタカナ）\\nStandard Reading (Katakana)'].to_list()\n",
        "\n",
        "    # 結果を Python の辞書にまとめる\n",
        "    X, orth_cnt = {}, {}\n",
        "    fish3words = []\n",
        "    for l, o, y, r, idx in zip(Lexeme, Ortho, Yomi, CoverR, ID):\n",
        "        \n",
        "        # `よみ` に '/' が含まれる項目 `ALT` を `オルト/エイエルティー` の場合最初のエントリだけを採用する\n",
        "        if isinstance(y, str):\n",
        "            y = jaconv.normalize(y)\n",
        "            if '/' in y:\n",
        "                y = y.split('/')[0]\n",
        "                print(f'{y}, idx:{idx}')\n",
        "                \n",
        "        # `語彙素` に '鱻' が含まれる項目は '鱻' 以降を切り捨てる\n",
        "        if isinstance(l, str):\n",
        "            if '鱻' in l:\n",
        "                fish3words.append({'idx':idx, 'lexeme':l})\n",
        "                l = l.split('鱻')[0]\n",
        "                \n",
        "        if isinstance(l, str):\n",
        "            lexeme = jaconv.normalize(l)  # 書記素を UTF-8 NKCD に正規化\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if isinstance(o, str):\n",
        "            orth = jaconv.normalize(o)\n",
        "            \n",
        "            # 書記素が '*' になっている場合があるので，語彙素情報で置き換える\n",
        "            orth = lexeme if orth == '*' else orth\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if isinstance(y, str):\n",
        "            yomi   = jaconv.hira2kata(y)\n",
        "            hira = jaconv.kata2hira(yomi)\n",
        "            mora = jaconv.hiragana2julius(hira).split(' ')\n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "        # lexeme = jaconv.normalize(l)  # 書記素を UTF-8 NKCD に正規化\n",
        "        # orth   = jaconv.normalize(o)  # 形態素を UTF-8 NKCD に正規化\n",
        "        # yomi   = jaconv.hira2kata(y)  #           \n",
        "        # hira = jaconv.kata2hira(y)\n",
        "        # mora = jaconv.hiragana2julius(hira).split(' ')\n",
        "        \n",
        "        X[idx] = {'lexeme': lexeme,\n",
        "                  'orth': orth,\n",
        "                  'yomi': yomi,\n",
        "                  'mora': mora,\n",
        "                  'cover_r': r,\n",
        "                 }\n",
        "        # if X[idx]['orth'] == '*':\n",
        "        #         # 書記素情報が '*' になっている場合があるので，語彙素情報で置き換える\n",
        "        #         X[idx]['orth'] = X[idx]['lexeme']\n",
        "\n",
        "    print(fish3words)\n",
        "    print(f'done {datetime.datetime.now()}...') if verbose else None\n",
        "    return X, orth_cnt\n",
        "\n",
        "# 下行で，例えば cover_rate=0.95 とすれば，累積カバー率 95 % のデータを得る\n",
        "X, orth_cnt = make_VDRJ_dict(cover_rate=0.99, verbose=True)"
      ],
      "metadata": {
        "id": "FXhj2Vitr96V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 読み込んだ松下語彙データの確認. 基本統計量の計算"
      ],
      "metadata": {
        "id": "otVY0PqtsWqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "\n",
        "# モーラ，よみ，語彙素，書記素，のそれぞれについて頻度グラフを描画\n",
        "for key in ['mora', 'yomi', 'lexeme', 'orth']:\n",
        "    cont = {}\n",
        "    for k,v in X.items():\n",
        "        for ch in X[k][key]:\n",
        "            if not ch in cont:\n",
        "                cont[ch] = 1\n",
        "            else:\n",
        "                cont[ch] += 1\n",
        "\n",
        "    cont_sorted = sorted(cont.items(), key=operator.itemgetter(1),reverse=True)\n",
        "    \n",
        "    plt.figure(figsize=(14,4))\n",
        "    plt.bar(range(len(cont_sorted)), [x[1] for x in cont_sorted])\n",
        "    plt.xticks(ticks=range(len(cont_sorted)), labels=[c[0] for c in cont_sorted])\n",
        "    plt.title(f'{key} 頻度')\n",
        "    plt.show()\n",
        "    #print(cont_sorted)\n",
        "\n",
        "for key in ['mora', 'yomi', 'lexeme', 'orth']:\n",
        "    cont = {}\n",
        "    for k,v in X.items():\n",
        "        _len = len(X[k][key])\n",
        "        if not _len in cont:\n",
        "            cont[_len] = 1\n",
        "        else:\n",
        "            cont[_len] += 1\n",
        "    cont_sorted = sorted(cont.items(), key=operator.itemgetter(0))\n",
        "    #cont_sorted = sorted(cont.items(), key=operator.itemgetter(1),reverse=True)\n",
        "    \n",
        "    plt.figure(figsize=(14,4))\n",
        "    plt.bar(range(len(cont_sorted)), [x[1] for x in cont_sorted])\n",
        "    plt.xticks(ticks=range(len(cont_sorted)), labels=[c[0] for c in cont_sorted])\n",
        "    plt.title(f'語長 {key} 頻度')\n",
        "    plt.show()\n",
        "    print(cont_sorted)\n",
        "    \n",
        "    max_length = cont_sorted[-1][0]\n",
        "    print(f'max_length:{max_length}')\n",
        "    for k,v in X.items():\n",
        "        _len = len(X[k][key])\n",
        "        if _len == max_length:\n",
        "            print(f'{k} 最長項目:{X[k]}') "
      ],
      "metadata": {
        "id": "fDmWQ2n_sXbk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}