{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022_0129DETR_fine_turing.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOv5bhjBlEw6XwDkg9bP6cA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0129DETR_fine_turing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- original: [Transformerを使った初めての物体検出「DETR」第2回 アーキテクチャの詳細解説とFine-Tuning](https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/part2.html)\n",
        "- author: オージス総研 技術部 データエンジニアリングセンター 堀 裕太\n",
        "- date: 2021年11月25日\n",
        "\n"
      ],
      "metadata": {
        "id": "MDJDXkYXw6ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. DETR のアーキテクチャの詳細説明\n",
        "\n",
        "第1回目の記事でも図示しましたが、[DETR の論文](https://arxiv.org/pdf/2005.12872.pdf) に記載されているアーキテクチャの図を、改めて下図に示します。\n",
        "また、DETR の Transformer 部分のアーキテクチャ図も示します。\n",
        "前回よりも細かい解説と各次元の要素数 (以下、shape)を明記しながら解説していきます。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-009.png\" width=\"59%\"><br/>\n",
        "\n",
        "出典 : [End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872.pdf)\n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-010.png\" width=\"39%\"><br/>\n",
        "\n",
        "出典 : [End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872.pdf)\n",
        "</center>\n",
        "\n",
        "\n",
        "## 3-1.①backbone\n",
        "\n",
        "前章で説明したTransformerと比較すると、「2-1.Embedding」 に相当します。\n",
        "ここでは、主に2つの処理を行います。\n",
        "\n",
        "1. インプット画像に対して CNN で畳み込みを行い、$d$ 次元の特徴マップに変換\n",
        "2. ``Transformer`` のインプットにするために ``reshape``\n",
        "\n",
        "CNN では、ImageNet で事前学習した [ResNet-50](https://arxiv.org/pdf/1512.03385.pdf) を TorchVision からインポートし、最終層を削除して $d$ 次元に変換する 2 次元畳み込み層を追加したネットワークを使います。\n",
        "また、[第1回目の記事]() でも掲載した処理イメージの図を見た方が分かりやすいかと思いますので、再掲します。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-011.png\" width=\"49%><br/>\n",
        "</center>\n",
        "\n",
        "バッチサイズを ``B``、入力画像の幅･高さ･チャネル数をそれぞれ ``W, H, C``, CNN 適用後の特徴マップの幅･高さ･チャネル数をそれぞれ ``W'、H'、d`` で表しています (論文では W’=W/32, H’=H/32, d=256)。\n",
        "インプット画像の ``shape`` は ``(B, H, W, C)`` だったものが、最終的に ``(B, H'×W', d)`` になります。\n",
        "\n",
        "\n",
        "## 3-2.②positional encoding\n",
        "\n",
        "* **3-1.①backbone**  で生成した特徴マップに対して、位置情報を付与します。\n",
        "上図の DETR の ``Transformer`` 部分のアーキテクチャを見ていただければ分かる通り、DETR には Positional encoding として ``Spatial positional encoding`` と ``Object queries`` の 2 種類が使われています。\n",
        "また、インプット時に 1 回だけ ``Positional Encoding`` を足し合わせるのではなく、``Attention`` 実行前に毎回 ``Positional Encoding`` を足し合わせています。\n",
        "\n",
        "* **(1) Spatial positional encoding**\n",
        "空間位置エンコーディングと呼ばれます。\n",
        "``Attention is All You Need`` の論文と同じく、正弦波の固定値を使います。\n",
        "詳細は ``2-2.Positional Encoding``  をご確認ください。\n",
        "``Encoder`` の ``Attention`` 前、``Decoder`` の ``SourceTarget-Attention`` の前に ``Encoder`` のアウトプットに対して足し合わせます。\n",
        "\n",
        "* **(2) Object queries**\n",
        "オブジェクトクエリと呼ばれます。\n",
        "``Object queries`` の本来の役割は ``Decoder`` のインプットですが、``Decoder`` 側の ``Positional Encoding`` としても使われています (Object queries の詳細は 「3-4.④decoder」 で説明します)。\n",
        "また、``Object queries`` は正弦波の固定値ではなく、初期値がランダムな学習パラメータになります。\n",
        "\n",
        "なぜ ``Positional Encoding`` が 2 種類あって、``Spatial positional encoding`` には正弦波の固定値、``Object queries`` には初期値がランダムな学習パラメータを使っているのかというと、様々な組み合わせを実験した結果、この組み合わせが一番性能が高かったためです。\n",
        "実験結果が DETR の論文に記載されているので、下図に示します。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-012.png\" width=\"49%\"><br/>\n",
        "\n",
        "出典 : [End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872.pdf)\n",
        "</center>\n",
        "\n",
        "上記表では、左側が ``Positional Encoding`` の組み合わせ、右側が性能を表しています。\n",
        "``spatial pos. enc.`` の ``encoder`` が ``Encoder`` の ``Attention`` 前、``decoder`` が ``Decoder`` の``SourceTarget-Attention`` の前の ``Encoder`` のアウトプットに足し合わせる箇所を指します。\n",
        "また、``output pos. enc.`` の ``decoder`` は ``Decoder`` 側の ``Positional Encoding`` のことです。\n",
        "それぞれ色々なパターンを試しています。\n",
        "``none``  は ``Positional Encoding`` 自体を使いません。\n",
        "``sine at input``  は ``Attention`` 前ではなくインプット時に 1 回だけ正弦波の固定値を足し合わせます。\n",
        "`` sine at attn.`` は ``Attention`` ごとに正弦波の固定値を付与。\n",
        "``learned at input``  と ``learned at attn.`` は学習パラメータをそれぞれ使った場合を意味します。\n",
        "最終的に、表の一番下の行の性能が一番良かったため、このような組み合わせで ``Positional Encoding`` を使うことになったわけです。\n",
        "また、``Positional Encoding`` の ``shape`` は特徴マップと同じく ``(B, H'×W', d)`` になります。"
      ],
      "metadata": {
        "id": "uSxsPbm6xdEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5-1.Open Image Datasetのフォーマット変換\n",
        "\n",
        "Fine-Tuning元の学習済みモデルには、第1回目の記事 でも紹介した、Model Zooに公開されている「DETR-DC5」を使います。\n",
        "このモデルは COCO dataset を使って学習しているため、Fine-Tuning をするために COCO dataset 以外のデータセットを用意します。\n",
        "COCO dataset と重複していないクラスラベルを持つデータセットを使いたいので、今回は Open Image Dataset 1 を使うことにします。\n",
        "しかし Open Image Dataset はそのままでは DETR では使えません。\n",
        "元々の DETR は COCO dataset しか対応していないため、COCO dataset format に変換する必要がありますので、フォーマット変換方法も合わせて紹介します。\n",
        "\n",
        "まずは Open Image Dataset をダウンロードする方法です。\n",
        "2021 年時点の Open Image Dataset の最新バージョンは v6 ですが、今回は v4 をダウンロードできるツールである OIDv4 ToolKit 7 を利用します。\n",
        "本来は 500GB 超えのデータセットをダウンロードする必要があるので Colab 上だと展開できないですが、ツールを使うことで必要なクラスラベルを必要な枚数だけダウンロードすることができるので、Colab 上でも問題なくダウンロードすることができます。\n",
        "\n",
        "では、早速 OIDv4 ToolKit の Git リポジトリをクローンして必要なパッケージをインストールします。\n",
        "requirements.txt だけではバージョンの互換性エラーが出るため、urllib3 と folium はバージョンを指定して個別にインストールしています。\n"
      ],
      "metadata": {
        "id": "RqV8mnzOorTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-3.③encoder\n",
        "\n",
        "DETR の ``Transformer`` アーキテクチャ図の左側部分に相当します。\n",
        "``2-9.Transformer``  で解説した内容とほぼ同じ仕組みですが、DETR では ``Self-Attention`` ごとに ``Positional Encoding`` を足し合わせる点が異なります。\n",
        "\n",
        "補足になりますが、DETR の論文では ``Encoder`` の ``Self-Attention`` を可視化していたので、下図に示します。\n",
        "それぞれ赤点から見て、各地点の ``Attention Score`` の高い箇所を黄色く表示したものになります。\n",
        "下図を見たら分かるように、``Encoder`` の時点である程度オブジェクトを分離できていることが分かります。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-013.png\" width=\"66%\"><br/>\n",
        "\n",
        "出典 : [End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872.pdf)\n",
        "</center>\n",
        "\n",
        "## 3-4.④decoder\n",
        "\n",
        "DETR の Transformer アーキテクチャ図の右側部分に相当します。\n",
        "``Encoder`` と同様、基本は ``2-9.Transformer``  で解説した内容とほぼ同じです。\n",
        "違いはインプットに ``Object queries`` を使うことと、自己回帰構造ではなくなったので並列的に処理できるようになったことです(並列デコーダについては後述)。\n",
        "\n",
        "``Object queries`` とは何かというと、主な役割は ``Decoder`` へのインプットで、その実態は画像内から物体検出とラベル分類をするために学習されるパラメータのことです。\n",
        "また、前述の通り ``Decoder`` の ``Positional Encoding`` として使う、という 2 種類の役割を持っています。\n",
        "初期値はランダムなベクトル値であり、$d$ 次元のベクトルが任意の $N$ 個 (論文では $N=100$) で、それら全てが学習パラメータになります。\n",
        "下図は物体検出結果から、対応する ``Object queries`` のうち 20 個を可視化したものです (``Object queries`` 自体のベクトルを可視化したものではありません。\n",
        "``Decoder`` のアウトプットの ``Object queries`` を FFN に通して、バウンディングボックスを検出した結果を可視化しています)。\n",
        "バウンディングボックスの中心位置に点を図示しており、赤色の点は大きな横長のバウンディングボックス、青色の点は大きな縦長のバウンディングボックス、緑色は小さなバウンディングボックスを表します。\n",
        "``Object queries`` の 1 つ 1 つが別々のエリアやボックスサイズに特化して学習しているので、$N$ 個の物体を検出できるということになります。\n",
        "そのため、100 個以上の物体を検出したい場合は $N>100$ にする必要があります。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-014.png\" width=\"66%\"><br/>\n",
        "\n",
        "出典 : [End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872.pdf)\n",
        "</center>\n",
        "\n",
        "``Encoder`` の補足と同様に、``Decoder`` の ``Attention`` を可視化したものを下図に示します。\n",
        "可視化した箇所は象の鼻など色がついて光っている箇所で、バウンディングボックスではないことにご注意ください。\n",
        "また、``Object queries`` ごとに別の色で可視化しています。\n",
        "``Encoder`` で大まかなオブジェクト分離は完了しているため、``Decoder`` ではオブジェクト境界 (足や耳など) を集中して注目していることが分かります。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-015.png\" width=\"66%\"><br/>\n",
        "\n",
        "出典 : [End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872.pdf)\n",
        "</center>\n",
        "\n",
        "また、``Decoder`` のアウトプットの ``shape`` はインプットの ``shape`` と同じになるため、``(B, N, d)`` になります。\n",
        "\n",
        "## 3-5.⑤prediction heads\n",
        "\n",
        "``Transformer`` の ``Decoder`` で出力された予測済みの ``Object queries`` を、FFN に通してバウンディングボックスの座標とクラスラベルに変換します。\n",
        "FFN は、バウンディングボックス側は ``Linear(ReLU) - Linear`` の 2 層ネットワーク、クラスラベル側は ``Linear`` 層によって計算されます。\n",
        "ここで注意なのが、結果が必ず $N$ 個出力される点です。\n",
        "$N$ は通常、画像内の実際のオブジェクト数よりも多く設定するので、大半はオブジェクトが検出されないことになります。\n",
        "そのため、オブジェクトに紐付かないことを示すクラスラベル ∅ (no object) を設定します。\n",
        "また、学習時に予測結果と正解を紐づける必要がありますが、紐付け方法は ``4-2.2部マッチングロス (Bipartite Matching Loss)`` で後述します。\n"
      ],
      "metadata": {
        "id": "rscE_rUp0e2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.DETRで使われた新技術\n",
        "\n",
        "2 章と 3 章では、元々の ``Transformer`` と DETR について詳しく解説しました。\n",
        "本章では、DETR から新たに使われている技術について紹介します。\n",
        "\n",
        "## 4-1.並列デコーダ (Parallel Decoding)\n",
        "\n",
        "元々の ``Transformer`` の ``Decoder`` は ``2-9.Transformer``  で解説した通り、自己回帰で単語を生成したり予測を行います。\n",
        "そのため、出力データ長に比例して推論コストが非常に高くなってしまいます。\n",
        "しかし DETR では、``Object queries`` を使うことによって、回帰構造ではなく並列処理ができる ``Decoder`` にすることができました。\n",
        "\n",
        "## 4-2.2部マッチングロス (Bipartite Matching Loss)\n",
        "\n",
        "DETR に画像を入力すると、``Object queries`` の数 $N(=100)$ 個のバウンディングボックスとクラスラベルが出力されます。\n",
        "DETR で学習する際には、この出力結果と正解ラベルとを比較してロスを計算する必要があります。 \n",
        "例えば下の図のように $N=4$ の時を考えた場合、それぞれ対応するバウンディングボックスに対してロス計算をしていきます。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-016.png\" width=\"66%\"><br/>\n",
        "</center>\n",
        "\n",
        "上記画像のように学習がある程度進んでいれば対応付けは容易ですが、学習初期などは出力結果は滅茶苦茶になっていると考えられます。\n",
        "また、$N=100$ など個数が多くなってくると、出力結果と正解ラベルとの紐付けが非常に困難です。\n",
        "この問題を ``2部マッチング問題`` と言います。\n",
        "DETR では、``2部マッチング問題`` を効率的に解くために ``ハンガリアンアルゴリズム``を使います。\n",
        "ハンガリアンアルゴリズムは最も効率の良いマッチングパターンを見つける手法であり、計算コストも $O(n!)$ から $O(n^{3})$ になります。\n",
        "\n",
        "以下に、実際にハンガリアンアルゴリズムを使ったロス計算の数式を紹介します。\n",
        "最初はハンガリアンアルゴリズムを使って、正解と推論結果の組み合わせでロス総和が最小になるようなマッチングパターン $\\hat{\\sigma}$ を求める数式です\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma} =\\arg\\min_{\\sigma\\in S_{N}}\\sum_{i}^{N} L_{\\text{match}}\\left(y_{i}, \\hat{y}_{\\sigma(i)}\\right) \\tag{1}\n",
        "$$\n",
        "\n",
        "$y_{i}$ は正解、$\\hat{y}_{i}$ は推論結果を表します。\n",
        "また $y_{i}=\\{c_{i}, b_{i}\\}$ となり、それぞれ $c_{i}$ は正解のクラスラベル、$b_{i}$ は正解のバウンディングボックス位置を表します ($\\hat{c}_{i}$ と $\\hat{b}_{i}$ も同じく、推論結果のクラスラベルとバウンディングボックス位置です)。\n",
        "$N$ は ``Object queries`` の数です。\n",
        "また、$S_{N}$ は $N$ 次の対称群を表します。\n",
        "対称群とは集合の用語で、DETR では正解と推論結果の組み合わせが存在する群を示しています。\n",
        "つまり $\\sigma(i)$ は、正解の $i$ 番目に対応する推論結果のインデックス番号を返します。\n",
        "数式の(1) 部分は、$S_{N}$ 上で (2) 部分が最小となるマッチングパターン $\\sigma$ の集合という意味になります。\n",
        "数式の(2) 部分は、ロス $L_{\\text{match}}$ の総和を意味します。\n",
        "\n",
        "損失 $L_{\\text{match}}$ の数式は以下の通りです。\n",
        "左項はクラスラベルの損失、右項はバウンディングボックス位置の損失に相当します。\n",
        "ここで正解 $y_{i}$ と推論結果 $\\hat{y}_{\\sigma(i)}$ をマッチングした時に生じるロスを計算しています。\n",
        "\n",
        "$$\n",
        "L_{\\text{match}}(y_{i},\\hat{y}_{\\sigma(i)})=\\mathbb{1}_{c_{i}\\ne \\Phi}(c_{i})+\\mathbb{1}_{c_{i}\\ne\\Phi}L_{\\text{box}}(b_{i},\\hat{b}_{\\sigma(i)})\\tag{2}\n",
        "$$\n",
        "\n",
        "太文字の $\\mathbb{1}$ は集合を表していて、数式の (3) 部分は、該当クラスなし (no object) の場合は 0 の集合、それ以外は 1 の集合にするという意味です。\n",
        "また、数式の (4) 部分では正解クラスラベルである確率のことを表します。\n",
        "右項も同様に、該当クラスなし (no object) の場合は 0 の集合にして、それ以外の場合は $L_{\\text{box}}$ を計算します。\n",
        "\n",
        "$L_{\\text{box}}$ の数式は以下になります。\n",
        "バウンディングボックス位置に対する損失を表していて、左項では物体予測位置に対する損失である ``Generalized IoU`` (以下 ``GIoU``)損失、右項は ``L1 損失`` を計算しています。\n",
        "バウンディングボックス位置に対する損失の計算には、通常は $L^{P}$ 損失が使われますが、大きな物体は損失が大きくなり、小さな物体は損失が小さくなってしまいます。\n",
        "そのため、スケール依存性を小さくするために、``GIoU 損失`` と ``L1 損失`` を足し合わせています。\n",
        "\n",
        "$$\n",
        "L_{\\text{box}}\\left(b_{i},\\hat{b}_{\\sigma(i)}\\right)=\n",
        "\\lambda_{\\text{iou}} L_{\\text{iou}}\\left(b_{i},\\hat{b}_{\\sigma(i)}\\right)\n",
        "+\\lambda_{L1}\\left\\|b_{i}-\\hat{b}_{\\sigma(i)}\\right\\|_{1}\\tag{3}\n",
        "$$\n",
        "\n",
        "$\\lambda_{\\text{iou}}$ と $\\lambda_{L1}$ はハイパーパラメータです。\n",
        "数式の (6) 部分では L1ノルム (各成分の絶対値の和) を計算しています。\n",
        "(5)部分は GIoU 損失関数で、数式は以下の通りです。\n",
        "なお、 GIoU の数式をひとまとめにして記述すると分かり難いため、GIoU(A,B) として別式で記載しています。\n",
        "\n",
        "\n",
        "$$\n",
        "L_{iou}\\left(b_{i},\\hat{b}_{\\sigma(i)}\\right)=1\\text{GIoU}\\left(b_{i},\\hat{b}_{\\sigma(i)}\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{GIoU}(A,B)=\\frac{\\left|A\\cap B\\right|}{\\left|A\\cup B\\right|}-\n",
        "\\frac{\\left|C\\neg (A\\cup B)\\right|}{\\left|C\\right|}\n",
        "$$\n",
        "\n",
        "$cap$ は積集合、$\\cup$ は和集合、バックスラッシュは差集合を表しています。\n",
        "また、$A$ は $b_{i}$， $B$ は $b_{\\sigma(i)}$，$C$ は $A$ と $B$ の領域を囲う最小の矩形領域を示しています。\n",
        "\n",
        "ここまでの数式で、損失の総和が最小になるようなマッチングパターン $\\hat{\\sgima$$ を求めることができます。\n",
        "このマッチングパターン $\\hat{\\sigma}$ を使い、ハンガリアン損失を以下の数式で求めます。\n",
        "\n",
        "$$\n",
        "L_{\\text{hungarian}}(y,\\hat{y})=\\sum_{i=1}^{N}\n",
        "\\left[ -\\log \\hat{P}_{\\hat{\\sigma}(i)}  (c_{i}) +\\mathbb{1}_{c_{i}\\ne\\Phi} L_{\\text{box}}(b_{i},\\hat{b}_{i}) \\right]\n",
        "$$\n",
        "\n",
        "数式の(7)部分では、正解クラスラベルの対数確率を求めています。右項はLmatch と同じくバウンディングボックス位置のロスを計算します。つまり LHungarian では、最適なマッチングパターン σ^ からクラスラベルロスとバウンディングボックス位置のロスの和を求め、全オブジェクトの総和を求めています。\n",
        "\n"
      ],
      "metadata": {
        "id": "UePnvURa2YYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. DETR の 微調整 Fine-Tuning\n",
        "\n",
        "ここからは、本題の DETR の ``Fine-Tuning`` 方法を紹介します。\n",
        "DETR には ``main.py`` が用意されており、python コマンドで引数を渡すことで学習や推論、``Fine-Tuning`` が可能になっています。\n",
        "しかし、[COCO dataset](https://arxiv.org/pdf/1512.03385.pdf) 以外を使うことが想定されておらず、``--dataset_file`` 引数で指定できるパラメータが ‘coco’ または 'coco_panoptic’ しかありません。\n",
        "自分でパラメータを追加してもいいのですが、既に有志の方が ``COCO dataset`` 以外を使えるように修正したソースコードがありましたので、今回はそちらを使った ``Fine-Tuning`` 方法をご紹介したいと思います。\n",
        "修正版のソースコードは [woctezuma の GitHub](https://github.com/woctezuma/detr/tree/finetune) になります。\n",
        "また、オリジナルの DETR からの修正箇所は [こちら](https://github.com/woctezuma/detr/compare/master...finetune?diff=split) です。\n",
        "``--dataset_file`` 引数に 'custom’ を追加されていることが分かるかと思います。\n",
        "\n",
        "\n",
        "[第1回目の記事](https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/part1.html) と同じく、``Google Colaboratory`` (以下、``Colab``) 上にソースコードを上から順番にコピーすれば動作するように記載していきます。\n",
        "しかし，推論と違い学習の場合は GPU モードにする必要があります。\n",
        "以下の画像のように ``Colab`` のメニューの 「ランタイム」→「ランタイムのタイプを変更」 から 「GPU」 に変更してください。\n",
        "ただし、``Colab`` の ``GPU`` は使いすぎるとアクセス制限がかかることがあります。\n",
        "今回ご紹介するソースコードを Tesla K80 が割り当てられた Colab で筆者が試したところ、7 epochs 程度でアクセス制限に引っかかってしまいました。\n",
        "もっと ``Fine-Tuning`` を回したい場合は、AWS や GCP を使う等別途環境を用意する必要があります。\n",
        "\n",
        "\n",
        "まずは ``Open Image Dataset`` をダウンロードする方法です。\n",
        "2021 年時点の ``Open Image Dataset`` の最新バージョンは v6 ですが、今回は v4 をダウンロードできるツールである [OIDv4 ToolKit](https://github.com/EscVM/OIDv4_ToolKit) を利用します。\n",
        "本来は 500GB 超えのデータセットをダウンロードする必要があるので ``Colab`` 上だと展開できないですが、ツールを使うことで必要なクラスラベルを必要な枚数だけダウンロードすることができるので、``Colab`` 上でも問題なくダウンロードすることができます。\n",
        "\n",
        "では、早速 ``OIDv4 ToolKit`` の Git リポジトリをクローンして必要なパッケージをインストールします。\n",
        "``requirements.txt`` だけではバージョンの互換性エラーが出るため、``urllib3`` と ``folium`` はバージョンを指定して個別にインストールしています。"
      ],
      "metadata": {
        "id": "gxfabScnFyc4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNxgQ8wtoTca"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/EscVM/OIDv4_ToolKit.git\n",
        "!pip install urllib3==1.25.11 folium==0.2.1\n",
        "!pip install -r OIDv4_ToolKit/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "次は、``OIDv4 ToolKit`` を使って画像をダウンロードします。\n",
        "例として、今回は丸いものを物体検出できるように ``Fine-Tuning`` するために、「Apple」「Ball」「Balloon」「Clock」「Orange」の 5 つのクラスラベルを使うことにします。\n",
        "いくつかは ``COCO dataset`` にも存在するクラスラベルですが、「Balloon」などは ``OpenImage Dataset`` にしか存在しないクラスラベルになります。\n",
        "ですので、``COCO dataset`` に存在しない「Balloon」を検出できる、かつ ``COCO dataset`` に存在して今回の 5 つのクラスラベルに存在しないクラス( Umbrella など)が検出できなくなっていれば、``Fine-Tuning`` は成功していると言えます。\n",
        "\n",
        "また、ダウンロードに時間がかかるため、今回は各クラスのダウンロード枚数を 100 枚に設定して行います。\n",
        "(その分精度は落ちてしまうので、ちゃんと精度を出したい場合は全データをダウンロードした方が良いです。)\n"
      ],
      "metadata": {
        "id": "ESRRNheSpDu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ファイル数上限を100枚にするので--limitを指定\n",
        "!python OIDv4_ToolKit/main.py downloader -y --classes Apple Orange Ball Balloon Clock --type_csv train --limit 100\n",
        "!python OIDv4_ToolKit/main.py downloader -y --classes Apple Orange Ball Balloon Clock --type_csv validation --limit 100"
      ],
      "metadata": {
        "id": "9EBApa0KonEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記プログラムを実行することで、「/content/OID/Dataset」に画像ファイルがダウンロードされます。\n",
        "また、OIDv4 ToolKit のオプション引数の詳細は公式の [GitHub](https://github.com/EscVM/OIDv4_ToolKit) に記載されていますが、今回使ったものについて簡単に解説します。\n",
        "\n",
        "* -y : 不足しているcsvファイルが必要な時、自動的に「yes」を選択する。\n",
        "* --classes : ダウンロードする対象のクラスラベル(複数指定可能)。\n",
        "指定できるクラスラベルは、公式サイト の「Category」欄のもの。\n",
        "* --type_csv : train、validation、test のいずれかを指定。\n",
        "* --limit :  各クラスラベルのダウンロード上限枚数を指定。\n",
        "\n",
        "Open Image Dataset をダウンロードできたので、次はアノテーションデータを Open Image format から COCO format に変換を行います。\n",
        "各フォーマットの詳細は省略しますが、気になる方は kenichiro-yamoto 氏の [はじめての Google Open Images Dataset V6](https://qiita.com/kenichiro-yamato/items/e0c0d6f6138b1c64acd0#oidv6-train-annotations-vrdcsv-%E3%82%92%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89%E3%81%A7%E3%81%8D%E3%82%8B) や、harmegiddo 氏の [COCO Format の作り方](https://qiita.com/harmegiddo/items/da131ae5bcddbbbde41f) で詳しく解説されていますのでご確認ください。\n",
        "また、GitHub に [openimages2coco](https://github.com/bethgelab/openimages2coco) という変換ツールが公開されていますが、OIDv4 ToolKit を使った場合はうまく動作しませんでしたので、今回は自分で変換プログラムを作成しました。\n",
        "変換プログラムを作成する際には、soumenpramanik 氏の [Convert-Pascal-VOC-to-COCO](https://github.com/soumenpramanik/Convert-Pascal-VOC-to-COCO/blob/master/convertVOC2COCO.py) を参考にさせていただきました。"
      ],
      "metadata": {
        "id": "k5ZUdg2JpZmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, glob\n",
        "\n",
        "def OID2JSON(OIDFiles, saveName, subset):\n",
        "  \"\"\"\n",
        "  アノテーションをOpenImage format(txt)からCOCO format(json)に変換\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  OIDFiles : string\n",
        "    OpenImageDatasetのフォルダパス\n",
        "  saveName : string\n",
        "    保存ファイル名(json)\n",
        "  subset : string\n",
        "    変換したいtype_csv。train、validation、testのいずれかを指定。\n",
        "  \"\"\"\n",
        "  attrDict = dict()\n",
        "  # categories要素の設定\n",
        "  attrDict['categories'] = []\n",
        "  categories = sorted(os.listdir(os.path.join(OIDFiles, 'Dataset', subset)))\n",
        "  for i in range(len(categories)):\n",
        "    attrDict['categories'].append({'supercategory': 'none', 'id': i, 'name': categories[i]})\n",
        "\n",
        "  images = list()\n",
        "  annotations = list()\n",
        "  filenames = list()\n",
        "  image_id = 1\n",
        "  anno_id = 1\n",
        "  for category in attrDict['categories']:\n",
        "    for jpg_file in glob.glob(os.path.join(OIDFiles, 'Dataset', subset, category['name'], '*.jpg')):\n",
        "      filename = os.path.splitext(os.path.basename(jpg_file))[0]\n",
        "      # カテゴリ全体で同じファイル名が存在する場合、imageとannoをリネーム\n",
        "      if filename in filenames:\n",
        "        rename_filename = filename + '_' + str(image_id)\n",
        "        os.rename(jpg_file, os.path.join(OIDFiles, 'Dataset', subset, category['name'], rename_filename + '.jpg'))\n",
        "        os.rename(os.path.join(OIDFiles, 'Dataset', subset, category['name'], 'Label', filename + '.txt'),\n",
        "                  os.path.join(OIDFiles, 'Dataset', subset, category['name'], 'Label', rename_filename + '.txt'))\n",
        "        filename = rename_filename\n",
        "      filenames.append(filename)\n",
        "      # images要素の設定\n",
        "      # ※DETRではheightとwidthを使わないので、'none'を設定\n",
        "      image = {'file_name': filename + '.jpg', 'height': 'none', 'width': 'none', 'id': image_id}\n",
        "      images.append(image)\n",
        "      # annotations要素の設定\n",
        "      anno_path = os.path.join(OIDFiles, 'Dataset', subset, category['name'], 'Label', filename + '.txt')\n",
        "      with open(anno_path) as f:\n",
        "        for line in f:\n",
        "          splitline = line.split(' ')\n",
        "          # カテゴリがcategories要素に存在しないバウンディングボックスは使わない\n",
        "          if splitline[0] in [d.get('name') for d in attrDict['categories']]:\n",
        "            # OpenImageの座標は(xmin, ymin, xmax, ymax)、COCOの座標は(x, y, width, height)\n",
        "            x1 = int(float(splitline[1]))\n",
        "            y1 = int(float(splitline[2]))\n",
        "            x2 = int(float(splitline[3])) - x1\n",
        "            y2 = int(float(splitline[4])) - y1\n",
        "            # areaはピクセル数(float)\n",
        "            area = float(x2 * y2)\n",
        "            # segmentationは(x1, y1, x2, y2, ...)と順番に定義\n",
        "            segmentation = [[x1, y1, x1, (y1+y2), (x1+x2), (y1+y2), (x1+x2), y1]]\n",
        "            annotation = {'iscrowd': 0, 'image_id': image_id, 'bbox': [x1, y1, x2, y2], 'area': area,\n",
        "                          'category_id': category['id'], 'ignore': 0, 'id': anno_id, 'segmentation': segmentation}\n",
        "            anno_id += 1\n",
        "            annotations.append(annotation)\n",
        "      image_id = image_id + 1\n",
        "\n",
        "  attrDict['images'] = images\n",
        "  attrDict['annotations'] = annotations\n",
        "  attrDict['type'] = 'instances'\n",
        "  jsonString = json.dumps(attrDict)\n",
        "  with open(saveName, 'w') as f:\n",
        "    f.write(jsonString)"
      ],
      "metadata": {
        "id": "ey4ql7RspSVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記関数を実行することで Open Image Dataset のアノテーションを、Open Image format(.txt) からCOCO format(.json) に変換することができます。\n",
        "Open Image format では画像ごとにアノテーションデータが txt ファイルに用意されていますが、COCO format では train, validation, test ごとに 1 つの json ファイルに記載する必要があります。\n",
        "また、クラスラベルをまたぐと同名のファイルが存在するのでリネームしたり、座標の記載方法が違うので変換する等の処理を行っています。\n",
        "詳細はプログラムにコメントを記載していますので、そちらをご覧ください。\n",
        "\n",
        "それでは上記関数を使って、実際に COCO format の json ファイルを生成します。"
      ],
      "metadata": {
        "id": "8JulX1_WqWrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OID2JSON('/content/OID', 'custom_train.json', 'train')\n",
        "OID2JSON('/content/OID', 'custom_val.json', 'validation')"
      ],
      "metadata": {
        "id": "1K9X0gI-qWUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "これで COCO format のアノテーションデータの準備は完了です。\n",
        "\n",
        "## 5-2.データセットのディレクトリ構成変更\n",
        "\n",
        "前章はフォーマット変換を行いました。\n",
        "次は、画像ファイルや前章で作成したアノテーションデータを指定のディレクトリに配置します。\n",
        "元々が COCO dataset を使うように想定されているため、ディレクトリ構成も COCO dataset に合わせる必要があるためです。\n",
        "ディレクトリ構成は DETR の GitHub にも記載がありますが、以下のような構成になります。\n",
        "\n",
        "```\n",
        "# path/to/coco/\n",
        "# ├ annotations/  # JSON annotations\n",
        "# │  ├ custom_train.json\n",
        "# │  └ custom_val.json\n",
        "# ├ train2017/    # training images\n",
        "# └ val2017/      # validation images\n",
        "```\n",
        "\n",
        "それでは、「/content/data」フォルダ内にファイルを移動してみましょう。"
      ],
      "metadata": {
        "id": "oZnCZbVkqnPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source_train_paths = glob.glob(os.path.join('/content/OID/Dataset', 'train', '**/'))\n",
        "source_val_paths = glob.glob(os.path.join('/content/OID/Dataset', 'validation', '**/'))\n",
        "train_path = '/content/data/custom/train2017/'\n",
        "val_path = '/content/data/custom/val2017/'\n",
        "convert_anno_path = '/content/data/custom/annotations/'\n",
        "# ディレクトリ作成\n",
        "os.makedirs(train_path, exist_ok=True)\n",
        "os.makedirs(val_path, exist_ok=True)\n",
        "os.makedirs(convert_anno_path, exist_ok=True)\n",
        "# train移動\n",
        "for source_train_path in source_train_paths:\n",
        "  for img_path in glob.glob(os.path.join(source_train_path, '*.jpg')):\n",
        "    shutil.move(img_path, train_path)\n",
        "# val移動\n",
        "for source_val_path in source_val_paths:\n",
        "  for img_path in glob.glob(os.path.join(source_val_path, '*.jpg')):\n",
        "    shutil.move(img_path, val_path)\n",
        "# anno移動\n",
        "shutil.move('/content/custom_train.json', convert_anno_path)\n",
        "shutil.move('/content/custom_val.json', convert_anno_path)"
      ],
      "metadata": {
        "id": "IGuzfRjTqtXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記プログラムを実行すれば、ディレクトリ構成変更作業は完了です。\n",
        "`/content/data/train2017/` 以下にクラスラベル関係なく画像ファイルが置かれていて、 `/content/data/annotations/` 以下に前章で作成したアノテーションファイルが置かれていれば OK です。"
      ],
      "metadata": {
        "id": "Kyl0ktlHrAhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/data/custom\n",
        "!ls /content/detr"
      ],
      "metadata": {
        "id": "eSk9qWUkr5tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-3.Fine-Tuning\n",
        "\n",
        "前章でデータセットの準備は完了したので、本章では Fine-Tuning について解説します。\n",
        "まずは必要なパッケージやライブラリを import します。\n",
        "ここで注意なのが、PyTorch のバージョンが 1.9.0 以上だと Fine-Tuning 時にエラーとなってしまいました。\n",
        "ですので、1.8.0 にダウングレードを行っています。\n",
        "\n"
      ],
      "metadata": {
        "id": "NF7ypYZ8rIpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch==1.8.0 torchvision==0.9.0 torchtext==0.9.0\n",
        "\n",
        "import torch, torchvision\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "print(torch.__version__)           # 1.8.0\n",
        "print(torchvision.__version__)     # 0.9.0\n",
        "print(torch.cuda.is_available())   # True\n",
        "torch.set_grad_enabled(False);"
      ],
      "metadata": {
        "id": "hJd8wtzkrFam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に、COCO dataset 以外に対応した、woctezuma 氏の DETR をチェックアウトします。\n",
        "ブランチも切り替える必要があるので、finetune ブランチに切り替えています。"
      ],
      "metadata": {
        "id": "0RjHgFbbrXe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!rm -rf detr\n",
        "!git clone https://github.com/woctezuma/detr.git\n",
        "# ブランチの切替\n",
        "%cd detr/\n",
        "!git checkout finetune"
      ],
      "metadata": {
        "id": "oO3G_zj8raMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content に detr というフォルダができていれば成功です。\n",
        "次は、学習済みモデルをダウンロードして、それをベースに Fine-Tuning を行います。\n",
        "学習済みモデルは、Model Zoo に公開されている「DETR-DC5」を使います。\n",
        "また、このモデルは COCO dataset で使う想定であるため、COCO dataset のクラス数である 92 クラスに分類するようになっています。\n",
        "今回は 6 クラス (Open Image Dataset の 5 クラス＋no-object) に分類したいため、該当箇所の重みを削除して一から学習する必要があります。\n",
        "その方法は DETR の issues に載っていましたので、そちらを参考にしながらコーディングを進めます。\n",
        "下記ソースコードを実行すると、学習済みモデルのパラメータから不要なパラメータを削除したモデルを 「detr-r50_no-class-head.pth」 というファイル名で保存します。"
      ],
      "metadata": {
        "id": "AaLedfuWrkeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習済みモデルの取得\n",
        "checkpoint = torch.hub.load_state_dict_from_url(\n",
        "    url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n",
        "    map_location='cpu',\n",
        "    check_hash=True\n",
        ")\n",
        "\n",
        "# 分類ヘッドの削除\n",
        "del checkpoint['model']['class_embed.weight']\n",
        "del checkpoint['model']['class_embed.bias']\n",
        "\n",
        "# 保存\n",
        "torch.save(checkpoint, 'detr-r50_no-class-head.pth')"
      ],
      "metadata": {
        "id": "h3hS8bkurwSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次はいよいよ Fine-Tuning です。\n",
        "クラスラベルの数を指定する必要があるので、``main.py`` を実行する前に定義しています。\n",
        "ここで注意ですが no-object は除いたクラスラベル数を定義する必要があります。\n",
        "no-object 自体は、DETR のプログラム内で自動的に「num_classes」と同じ ID で採番されるためです。\n",
        "(つまり num_classes=5 の場合、'Apple’=0, 'Ball’=1, 'Balloon’=2, 'Clock’=3, 'Orange’=4, 'no-object’=5 のように ID が採番されます)\n",
        "\n",
        "Fine-Tuning を行うと処理に時間がかかりますので、とりあえず下記に示すソースコードでは、10 epochs だけ学習するようにしています。\n",
        "精度が高いモデルを作りたい場合は、 OIDv4 ToolKit の ``--limit`` オプションを撤廃して画像枚数を増やしたり、``--epochs`` オプションの値を大きくして学習回数を増やしてみてください。\n",
        "その分処理時間がかかってしまいますので、そこはご注意ください。\n",
        "\n",
        "補足として、筆者が試した場合の処理時間を記載します。また、Colab の GPU は Tesla K80 が割り当てられている場合になります。\n",
        "\n",
        "* ``--limit=100`` : 1 epoch に 5 分程度。\n",
        "* ``--limit`` なし : 1 epoch に 1 時間弱。"
      ],
      "metadata": {
        "id": "HjEYLPYTsQCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 5\n",
        "\n",
        "%cd /content/detr/\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "# 学習\n",
        "!python main.py \\\n",
        "    --dataset_file \"custom\" \\\n",
        "    --coco_path \"/content/data/custom/\" \\\n",
        "    --output_dir \"outputs\" \\\n",
        "    --resume \"detr-r50_no-class-head.pth\" \\\n",
        "    --num_classes $num_classes \\\n",
        "    --epochs 10"
      ],
      "metadata": {
        "id": "riS7wOW8su8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "エラーが発生せず学習が進んでいれば、 Fine-Tuning は成功です。\n",
        "しかし、Colab で実行すると [GPU の使用制限](https://note.com/npaka/n/n1aa6f8c973d0) に引っかかってしまう等の問題があり、筆者が ``--limitなし`` で Colab で試したところ、7 epochs 程度しか学習できませんでした。\n",
        "実際に Fine-Tuning を何十 epochs も回す場合は、``checkpoint`` と ``resume`` を実装して途中保存しながら学習するか、AWS や GCP 等の環境を使った方が良さそうです。"
      ],
      "metadata": {
        "id": "WPTjGVyEtIha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-4. Fine-Tuning 結果の確認\n",
        "\n",
        "本章では、Fine-Tuning を行ったモデルを使って正しく学習できているか、正しく物体検出できているかを確認します。\n",
        "モデルについては、前章で説明したとおり Colab では厳しかったため、GCP に環境を用意して ``--limitなし`` かつ 50 epochs 程度学習したモデルを使って結果を確認します。\n",
        "モデルは ``/content/detr/outputs`` に配置されているものとし、解説していきます。\n",
        "\n",
        "まずはモデルの ``loss`` と ``mAP`` を確認してみます。\n",
        "``mAP`` は物体検知モデルに使われる評価指標であり、``mAP`` が高いほどモデルの精度が高く常に自信があるモデルといえます。\n",
        "本記事では ``mAP`` について解説はしませんが、Jonathan Hui 氏の [mAP(mean Average Precision) for Object Detection](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173) という記事が非常に分かりやすかったので、興味のある方はこちらをご確認いただければと思います。\n",
        "``loss`` と ``mAP`` の確認方法ですが、DETR では既に確認用プログラムを作ってくださっているので、そちらを使って確認していきます。"
      ],
      "metadata": {
        "id": "8bK2PjF2talE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from util.plot_utils import plot_logs\n",
        "from pathlib import Path\n",
        "\n",
        "%cd /content/detr/\n",
        "log_directory = [Path('/content/detr/outputs')]\n",
        "\n",
        "# 実線 ... トレーニング結果(train_loss)\n",
        "# 破線 ... 検証結果(val_loss)\n",
        "fields_of_interest = (\n",
        "    'loss',\n",
        "    'mAP',\n",
        ")\n",
        "plot_logs(log_directory, fields_of_interest)"
      ],
      "metadata": {
        "id": "C4_G_f2HtVLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記プログラムを実行すると、下画像のようなグラフが出力されます。\n",
        "``loss`` を見てみると、``train_loss`` は学習が進むにつれて下がっているので正しく学習できていそうですし、``val_loss`` も上がっていないので過学習にもなっていないことが分かります。\n",
        "``mAP`` を見ても、学習が進むにつれて右肩上がりになっています。\n",
        "正しく学習できていますが、まだ収束していないので、もう少し学習を回した方が良かったかもしれません。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-023.png\" width=\"49%\">\n",
        "</center>\n",
        "\n",
        "次は、実際に画像を入力して物体検出をしてみます。\n",
        "最初にモデルをロードします。\n",
        "``Fine-Tuning`` したモデルと ``Fine-Tuning`` していないモデルを比較したいので、2 種類のモデルをロードしておきます。\n",
        "``Fine-Tuning`` していないモデルに関しては、[第 1 回目の記事](https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/part1.html) で使用したモデルを使います。"
      ],
      "metadata": {
        "id": "uUv4-McKuSYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model = torch.hub.load('facebookresearch/detr',\n",
        "                       'detr_resnet50',\n",
        "                       pretrained=False,\n",
        "                       num_classes=num_classes)\n",
        "checkpoint = torch.load('/content/detr/outputs/checkpoint.pth',\n",
        "                        map_location='cpu')\n",
        "finetuned_model.load_state_dict(checkpoint['model'], strict=False)\n",
        "finetuned_model.eval()\n",
        "\n",
        "original_model = torch.hub.load('facebookresearch/detr', 'detr_resnet50_dc5', pretrained=True)\n",
        "original_model.eval()"
      ],
      "metadata": {
        "id": "w4ZMxT_ruYIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルの準備が完了したので、次は実際に画像を入力して物体検出を行うための前処理や画像表示処理を定義します。\n",
        "なお前処理等は、[第1回目の記事](https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/part1.html) で紹介した内容と同じなので、詳細な解説は前回の記事を見てください。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e91wlW0puzJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 可視化用クラスラベル\n",
        "oid_labels = [\n",
        "  'Apple',\n",
        "  'Ball',\n",
        "  'Balloon',\n",
        "  'Clock',\n",
        "  'Orange',\n",
        "]\n",
        "coco_labels = [\n",
        "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
        "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
        "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
        "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]\n",
        "# 可視化用COLOR\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "# 標準的なPyTorchのmean-std入力画像の正規化\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    \"\"\"\n",
        "    (center_x, center_y, width, height)から(xmin, ymin, xmax, ymax)に座標変換\n",
        "    \"\"\"\n",
        "    # unbind(1)でTensor次元を削除\n",
        "    # (center_x, center_y, width, height)*N → (center_x*N, center_y*N, width*N, height*N)\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    # (center_x, center_y, width, height)*N の形に戻す\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    \"\"\"\n",
        "    バウンディングボックスのリスケール\n",
        "    \"\"\"\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    # バウンディングボックスの[0～1]から元画像の大きさにリスケール\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "def filter_bboxes_from_outputs(outputs, threshold=0.7):\n",
        "  # 閾値以上の信頼度を持つ予測値のみを保持\n",
        "  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "  keep = probas.max(-1).values > threshold\n",
        "  probas_to_keep = probas[keep]\n",
        "  # [0, 1]のボックスを画像のスケールに変換\n",
        "  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
        "  return probas_to_keep, bboxes_scaled\n",
        "\n",
        "# 結果の表示\n",
        "def plot_finetuned_results(pil_img, prob=None, boxes=None, labels=None):\n",
        "  plt.figure(figsize=(16, 10))\n",
        "  plt.imshow(pil_img)\n",
        "  ax = plt.gca()\n",
        "  colors = COLORS * 100\n",
        "  if prob is not None and boxes is not None:\n",
        "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
        "      ax.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin,\n",
        "                                 fill=False, color=c, linewidth=3))\n",
        "      cl = p.argmax()\n",
        "      print(labels, p)\n",
        "      text = f'{labels[cl]}: {p[cl]:0.2f}'\n",
        "      ax.text(xmin, ymin, text, fontsize=15,\n",
        "              bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "# 物体検出\n",
        "def run_worflow(my_image, my_model, labels, threshold=0.7):\n",
        "  # mean-std入力画像の正規化(バッチサイズ : 1)\n",
        "  img = transform(my_image).unsqueeze(0)\n",
        "  # モデルに反映\n",
        "  outputs = my_model(img)\n",
        "\n",
        "  probas_to_keep, bboxes_scaled = filter_bboxes_from_outputs(outputs, threshold=threshold)\n",
        "  plot_finetuned_results(my_image, probas_to_keep, bboxes_scaled, labels)"
      ],
      "metadata": {
        "id": "Sxxj6ZHCu47h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "これで物体検出の準備ができました。\n",
        "それでは、実際に物体検出をしてみましょう。\n",
        "試しに、``Open Image Dataset`` から ``Balloon`` の画像をダウンロードして物体検出してみます。\n",
        "``Fine-Tuning`` したモデルでは検出できて、``Fine-Tuning`` していないモデルでは検出できないはずです。\n",
        "また、閾値は 0.9 を指定しています。"
      ],
      "metadata": {
        "id": "wP1mFJdxvHey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "\n",
        "url = 'https://farm7.staticflickr.com/52/106887535_a29c34113b_o.jpg'\n",
        "response = requests.get(url)\n",
        "im = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Fine-Tuningモデルで物体検出(閾値0.9)\n",
        "run_worflow(im, finetuned_model, oid_labels, 0.9)\n",
        "# Fine-Tuningしていないモデルで物体検出(閾値0.9)\n",
        "run_worflow(im, original_model, coco_labels, 0.9)"
      ],
      "metadata": {
        "id": "RCo9zpRBvOIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "実行すると、下画像のような結果が表示されます。\n",
        "左側が ``Fine-Tuning`` したモデル、右側が ``Fine-Tuning`` していないモデルです。\n",
        "想定通り、``Fine-Tuning`` した場合は ``Balloon`` が検出できていますが、``Fine-Tuning`` していない場合は検出できていません。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-024.png\" width=\"33%\">\n",
        "</center>\n",
        "\n",
        "``Open Image Dataset`` や ``COCO dataset`` の画像を使い色々なパターンを試してみたので、以下に結果を列挙します。\n",
        "上記画像と同じく、左側が ``Fine-Tuning`` したモデル、右側が ``Fine-Tuning`` していないモデルとなります。\n",
        "``Fine-Tuning`` で学習しているりんごやオレンジ、アナログ時計に関しては ``Fine-Tuning`` 前と同程度の検出精度ですが、学習していない傘や人や犬、デジタル時計などは検出しないようになっていて、正しく ``Fine-Tuning`` できていることが分かるかと思います。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-025.png\" width=\"49%\">\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-026.png\" width=\"49%\">\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-027.png\" width='49%'>\n",
        "<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202111-028.png\" width=\"49%\">\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "NaMgO73BvR4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.おわりに\n",
        "\n",
        "今回の記事では、前半では DETR の詳細と自然言語処理との Transformer の違いを解説し、後半は DETR での ``Fine-Tuning`` 方法と結果を確認しました。\n",
        "DETR の Fine-Tuning 方法を紹介している記事が無さそうでしたので、誰かのお役に立てれば幸いです。\n",
        "また、DETR だからこそできる物体検出があるわけではなく、既存の FasterR-CNN と同程度の性能ですので、案件への応用例などは既存手法と変わりません。\n",
        "しかし、既存手法と全く違うアーキテクチャでありながら、既存手法と同程度の性能が出せることが非常に素晴らしいので、今後 DETR をベースにした手法が出てくることでしょう。\n",
        "\n",
        "今回で DETR の記事は終了です。\n",
        "次回の記事では、DETR と自然言語処理で使われる RoBERTa を組み合わせた、[MDETR](https://arxiv.org/pdf/2104.12763.pdf) を紹介予定です。\n",
        "画像とテキストを混ぜたマルチモーダル推論モデルで、両方とも ``Transformer`` を使うため既存手法よりも自由度が高い推論ができるものになっています。\n",
        "\n"
      ],
      "metadata": {
        "id": "qoV6zaU0wSzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9QxHt19nvXo7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}