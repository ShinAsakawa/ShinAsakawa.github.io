{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0904exercise_mecab_etc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91684ad4-6536-47a4-a9d5-445c180576f2",
      "metadata": {
        "id": "91684ad4-6536-47a4-a9d5-445c180576f2"
      },
      "source": [
        "* filename: `2022_0904exercise_around_mecab.ipynb`\n",
        "* author: 浅川伸一\n",
        "\n",
        "* MeCab と松下データを使って任意の文章を，分かち書きして，各品詞の松下データの頻度を検索して，その頻度最大値を返す。\n",
        "2022_0821 の zoom ミーティングからの宿題に対する回答コード\n",
        "\n",
        "* サンプルデータとして，京都大学黒橋研究室作成の SNLI データセットを用いることとした"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2c30d1b3-4059-4ef6-af62-fdee50f22805",
      "metadata": {
        "id": "2c30d1b3-4059-4ef6-af62-fdee50f22805",
        "outputId": "788e34f1-cbb6-4cd3-cd95-c3c0b6703a4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "日付: \u001b[1m\u001b[32m2022-09-18\u001b[0m\n",
            "HOSTNAME: \u001b[1m\u001b[32m3f41b67d0103\u001b[0m\n",
            "ユーザ名: \u001b[1m\u001b[32mroot\u001b[0m\n",
            "HOME: \u001b[1m\u001b[32m/root\u001b[0m\n",
            "ファイル名: \u001b[1m\u001b[32m/fileId=https%3A%2F%2Fgithub.com%2FShinAsakawa%2FShinAsakawa.github.io%2Fblob%2Fmaster%2F2022notebooks%2F2022_0904exercise_mecab_etc.ipynb\u001b[0m\n",
            "torch.__version__: \u001b[1m\u001b[32m1.12.1+cu113\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Mac の retina ディスプレイの場合，高解像度の画面を使用する\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# 下準備，実行環境諸元の表示と設定\n",
        "try:\n",
        "    import bit\n",
        "except ImportError:\n",
        "    !pip install ipynbname\n",
        "    !git clone https://github.com/ShinAsakawa/bit.git\n",
        "import bit\n",
        "isColab = bit.isColab\n",
        "HOME = bit.HOME\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a81e0cb7-1146-4ba3-86d8-a64437e8a853",
      "metadata": {
        "id": "a81e0cb7-1146-4ba3-86d8-a64437e8a853"
      },
      "source": [
        "## colab 上で必要なライブラリのインストールなど"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if isColab:\n",
        "#     !apt install aptitude\n",
        "#     !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "#     !pip install mecab-python3==0.7"
      ],
      "metadata": {
        "id": "bDTlRx_H1elC"
      },
      "id": "bDTlRx_H1elC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f8889916-4435-4c96-9bf4-11fca74f12cf",
      "metadata": {
        "id": "f8889916-4435-4c96-9bf4-11fca74f12cf",
        "outputId": "15b332ad-87b0-4fcc-8e3f-5f8064188ee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ccap' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jaconv in /usr/local/lib/python3.7/dist-packages (0.3)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ローカルと colab との相違を吸収するために\n",
        "# 本ファイルを Google Colaboratory 上で実行する場合に，必要となるライブラリをインストール\n",
        "if isColab:\n",
        "    !git clone https://github.com/ShinAsakawa/ccap.git\n",
        "    !pip install japanize_matplotlib > /dev/null 2>&1\n",
        "    !pip install jaconv > /dev/null 2>&1\n",
        "\n",
        "    # MeCab, fugashi, ipadic のインストール\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1\n",
        "    !pip install mecab-python3==0.7 > /dev/null 2>&1\n",
        "    !pip install --upgrade ipadic > /dev/null 2>&1\n",
        "\n",
        "    #!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "    #!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1\n",
        "    \n",
        "    #import subprocess\n",
        "    #cmd='echo `mecab-config --dicdir`\\\"/mecab-ipadic-neologd\\\"'\n",
        "    #path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "    #                                 shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "    #!pip install 'konoha[mecab]'\n",
        "    #!pip install 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    #!python -m unidic download > /dev/null 2>&1\n",
        "    !pip install jaconv\n",
        "    #!pip install transformers fugashi ipadic    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "268e8f8d-9435-4e37-9fd0-b8cdf18ebabb",
      "metadata": {
        "id": "268e8f8d-9435-4e37-9fd0-b8cdf18ebabb"
      },
      "source": [
        "## インストール補足"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "91d9ecb1-2d76-4d66-8343-a4d01543e836",
      "metadata": {
        "id": "91d9ecb1-2d76-4d66-8343-a4d01543e836",
        "outputId": "20851799-842c-48e6-f4e9-006aa0bfc471",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (3.0.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install --upgrade openpyxl    # エクセルファイルを読むこむ際にバージョンの相違で動作しない場合があるので念の為\n",
        "    !pip install --upgrade pandas      # 同上\n",
        "    #!python -m unidic download\n",
        "    #!pip install --upgrade fugashi[unidic-lite]\n",
        "    #!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e67242d7-8a65-48b0-b887-0fb03891ff71",
      "metadata": {
        "id": "e67242d7-8a65-48b0-b887-0fb03891ff71"
      },
      "source": [
        "# 日本語を読むための”ＴＭ語彙リスト”（総合版）　Ver.4.0 の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2725525f-a796-4f26-a43b-07fd8983ba23",
      "metadata": {
        "id": "2725525f-a796-4f26-a43b-07fd8983ba23"
      },
      "outputs": [],
      "source": [
        "#東大の松下研究室の辞書を読み込む\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "import jaconv\n",
        "\n",
        "# 日本語を読むための語彙データベース（VDRJ） Ver. 1.1　（＝日本語を読むための”ＴＭ語彙リスト”（総合版）　Ver.4.0）\n",
        "vdrj_url='http://www17408ui.sakura.ne.jp/tatsum/database/VDRJ_Ver1_1_Research_Top60894.xlsx'\n",
        "\n",
        "# 直上行の url からエクセルファイル名を切り出す\n",
        "excel_fname = vdrj_url.split('/')[-1]\n",
        "\n",
        "# もしエクセルファイルが存在しなかったら ダウンロードする\n",
        "if not os.path.exists(excel_fname):       \n",
        "    r = requests.get(vdrj_url)\n",
        "    with open(excel_fname, 'wb') as f:\n",
        "        total_length = int(r.headers.get('content-length'))\n",
        "        print('Downloading {0} - {1} bytes'.format(excel_fname, (total_length)))\n",
        "        f.write(r.content)\n",
        "        \n",
        "# 松下データの読み込み，シート名を指定\n",
        "sheet_name='重要度順語彙リスト60894語'\n",
        "\n",
        "# 実際のエクセルファイルの読み込み\n",
        "vdrj_df = pd.read_excel(excel_fname, sheet_name=sheet_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5addb04-8c5d-45fb-9c83-880f38e4c38e",
      "metadata": {
        "id": "f5addb04-8c5d-45fb-9c83-880f38e4c38e"
      },
      "source": [
        "## 読み込んだ語彙リストの変換"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5c936bc6-cfbc-4fe0-af56-4f81a3130f35",
      "metadata": {
        "id": "5c936bc6-cfbc-4fe0-af56-4f81a3130f35",
        "outputId": "d2deb93c-ca95-49d2-f906-87e655814d2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60894\n",
            "60894\n",
            "60894\n",
            "60894\n",
            "59925 単語: ping 重複\n",
            "60600 単語: 突然 重複\n",
            "単語: 突然, 重複回数: 2\n",
            "単語: ping, 重複回数: 2\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "# `日本語を読むための”ＴＭ語彙リスト”（総合版）　Ver.4.0` の列名データ\n",
        "cols = ['留学生用\\n語彙レベル\\nWord Level for International Students',\n",
        "        '留学生用語彙ランク\\nWord Ranking for International Students',\n",
        "        '一般語彙レベル\\nWord Level for General Learners',\n",
        "        '一般語彙ランク\\nWord Ranking for General Learners',\n",
        "        '書きことば語彙レベル\\nWord Level for Written Japanese',\n",
        "        '書きことば重要度ランク（想定既知語彙を除く）\\nU Ranking for Written Japanese excluding Assumed Known Words',\n",
        "       '旧日本語能力試験出題基準レベル\\nOld JLPT Level',\n",
        "        '語彙階層ラベル Word Tier Label\\n\\n学術共通語彙：4Dまたは3Dを含む語\\n（旧日能試4級語彙および留学生用語彙レベル21K+の語彙を除く）\\n\\n限定学術領域語彙：2Dまたは1Dを含む語（旧日能試4級語彙および留学生用語彙レベル21K+の語彙を除く）',\n",
        "        '見出し語彙素\\nLexeme', \n",
        "        '標準的（新聞）表記\\nStandard (Newspaper) Orthography',\n",
        "        '標準的読み方（カタカナ）\\nStandard Reading (Katakana)', \n",
        "        '品詞\\nPart of Speech',\n",
        "        '使用度数\\nFrequency',\n",
        "        '修正済み使用度数（総延べ語数32656221語中）\\nCorrected Frequency (Out of Total Token 32656221)',\n",
        "        '修正度数\\nFrequency for Correction ',\n",
        "        '10分野100万語あたり使用頻度(Fw)\\nStandardized Freq/\\nmillion in 10 Written Domains (Fw)',\n",
        "        '(Fw)累積テキストカバー率（想定既知語彙分を含む）\\nFw Cumulative Text Coverage including Assumed Known Words',\n",
        "        '\\n書字形（例）\\nOrthographic Form Example',\n",
        "        '発音形（例）\\nPhonological Form Example', '語彙素読み\\nReading of Lexeme',\n",
        "        '活用型\\nConjugation Type', '活用形\\nConjugated Form Example',\n",
        "        '語形\\nWord Form', 'ID',\n",
        "        'ホームポジション並べ替え用ID\\nID for Sorting by the Original Order']\n",
        "\n",
        "x = vdrj_df[cols]\n",
        "\n",
        "# 列名が長くてダルいので簡易な列名に変換しておく\n",
        "vdrj_df_ = x.rename(columns = {\n",
        "    '留学生用\\n語彙レベル\\nWord Level for International Students': 'wlevel_int',\n",
        "    '留学生用語彙ランク\\nWord Ranking for International Students': 'wrank_int',\n",
        "    '一般語彙レベル\\nWord Level for General Learners'           : 'wlevel_gen',\n",
        "    '一般語彙ランク\\nWord Ranking for General Learners'         : 'wrand_gen',\n",
        "    '書きことば語彙レベル\\nWord Level for Written Japanese'      : 'wrank_gen',\n",
        "    '書きことば重要度ランク（想定既知語彙を除く）\\nU Ranking for Written Japanese excluding Assumed Known Words': 'Urank',\n",
        "    '旧日本語能力試験出題基準レベル\\nOld JLPT Level': 'jlpt_level',\n",
        "    '語彙階層ラベル Word Tier Label\\n\\n学術共通語彙：4Dまたは3Dを含む語\\n（旧日能試4級語彙および留学生用語彙レベル21K+の語彙を除く）\\n\\n限定学術領域語彙：2Dまたは1Dを含む語（旧日能試4級語彙および留学生用語彙レベル21K+の語彙を除く）': 'word_tier_label',\n",
        "    '見出し語彙素\\nLexeme': 'lexeme', \n",
        "    '標準的（新聞）表記\\nStandard (Newspaper) Orthography': 'newspaper',\n",
        "    '標準的読み方（カタカナ）\\nStandard Reading (Katakana)': 'katakana',\n",
        "    '品詞\\nPart of Speech'                              : 'pos',\n",
        "    '使用度数\\nFrequency'                                : 'freq',\n",
        "    '修正済み使用度数（総延べ語数32656221語中）\\nCorrected Frequency (Out of Total Token 32656221)': 'freq_correct',\n",
        "    '修正度数\\nFrequency for Correction '                : 'freq_correct2',\n",
        "    '10分野100万語あたり使用頻度(Fw)\\nStandardized Freq/\\nmillion in 10 Written Domains (Fw)'    : 'freq_std',\n",
        "    '(Fw)累積テキストカバー率（想定既知語彙分を含む）\\nFw Cumulative Text Coverage including Assumed Known Words': 'cum_freq',\n",
        "    '\\n書字形（例）\\nOrthographic Form Example'           : 'orth_example',\n",
        "    '発音形（例）\\nPhonological Form Example'             : 'phon_example', \n",
        "    '語彙素読み\\nReading of Lexeme'                       : 'lexeme_reading',\n",
        "    '活用型\\nConjugation Type' : 'conj_type', \n",
        "    '活用形\\nConjugated Form Example': 'conj_example',\n",
        "    '語形\\nWord Form': 'word_form', \n",
        "    'ID': 'ID',\n",
        "    'ホームポジション並べ替え用ID\\nID for Sorting by the Original Order': 'ID_sort'})\n",
        "\n",
        "\n",
        "def jaconv_normalize(word:str, mode='NFKC'):\n",
        "    \"\"\"松下データの 'Lexeme' と '語形' には文字列ではないデータや空欄がある。\n",
        "    そのため，例外処理をすることにした\n",
        "    \"\"\"\n",
        "    if not isinstance(word, str):\n",
        "        return str(word)\n",
        "    else:\n",
        "        return unicodedata.normalize(mode, word)\n",
        "        #return jaconv.normalize(word)\n",
        "\n",
        "# 上で定義した関数 `jaconv_normalize()` を使って松下データ vdrj の文字を NFKC に正規化\n",
        "# おそらく松下データは windows で作成された NFD であると思われるので，念の為\n",
        "vdrj_lexeme    = [jaconv_normalize(w) for w in vdrj_df_['lexeme'].to_list()]\n",
        "vdrj_word_form = [jaconv_normalize(w) for w in vdrj_df_['word_form'].to_list()]\n",
        "vdrj_freq      = [jaconv_normalize(w) for w in vdrj_df_['freq']]\n",
        "vdrj_ids = [int(w) for w in vdrj_df_['ID'].to_list()]\n",
        "\n",
        "#確認用 すべて同じ数 60894 になっているはず\n",
        "for x in [vdrj_lexeme, vdrj_word_form, vdrj_freq, vdrj_ids]:\n",
        "    print(len(x))\n",
        "\n",
        "# データのチェック\n",
        "# 松下データには重複があるらしい\n",
        "word_count = {}\n",
        "for i, w in enumerate(vdrj_lexeme):\n",
        "    if not w in word_count:\n",
        "        word_count[w] = 1\n",
        "    else:\n",
        "        word_count[w] += 1\n",
        "        print(f'{i:5d} 単語: {w} 重複')\n",
        "for k, v in word_count.items():\n",
        "    if word_count[k] != 1:\n",
        "        print(f'単語: {k}, 重複回数: {v}')\n",
        "        \n",
        "\n",
        "vdrj = {}          # ここで用いる松下データの本体\n",
        "vdrj_wrd2idx = {}  # 任意の単語を松下 ID に変換するための辞書\n",
        "for i, (lexeme, word_form, freq, idx) in enumerate(zip(vdrj_lexeme, vdrj_word_form, vdrj_freq, vdrj_ids)):\n",
        "    vdrj[idx] = {'lexeme': lexeme,\n",
        "                 'word_form': word_form,\n",
        "                 'freq': int(freq),\n",
        "                }\n",
        "    vdrj_wrd2idx[lexeme] = idx\n",
        "#list(vdrj.keys())[3:10]\n",
        "#vdrj_wrd2idx['突然']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2906690b-8376-4546-bfd9-49a49d30cae1",
      "metadata": {
        "id": "2906690b-8376-4546-bfd9-49a49d30cae1"
      },
      "source": [
        "# 日本語版 NLI データセットの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05e70308-4b43-4aba-bdd1-a7b04853c050",
      "metadata": {
        "id": "05e70308-4b43-4aba-bdd1-a7b04853c050"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "jsnli_url = 'https://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=https://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JSNLI/jsnli_1.1.zip&name=JSNLI.zip'\n",
        "zip_fname = jsnli_url.split('/')[-1].split('&')[0]\n",
        "#zip_fname = jsnli_zip_fname\n",
        "if not os.path.exists(zip_fname):  # もしエクセルファイルが存在しなかったら ダウンロードする\n",
        "    r = requests.get(jsnli_url)\n",
        "    with open(zip_fname, 'wb') as f:\n",
        "        total_length = int(r.headers.get('content-length'))\n",
        "        print('Downloading {0} - {1} bytes'.format(zip_fname, (total_length)))\n",
        "        f.write(r.content)\n",
        "\n",
        "def extract_zip(input_zip):\n",
        "    input_zip=ZipFile(input_zip)\n",
        "    return {name: input_zip.read(name) for name in input_zip.namelist()}        \n",
        "\n",
        "a = extract_zip(zip_fname)\n",
        "jsnli_train_wo = a['jsnli_1.1/train_wo_filtering.tsv'].decode('utf-8').split('\\n')\n",
        "jsnli_train_w  = a['jsnli_1.1/train_w_filtering.tsv'].decode('utf-8').split('\\n')\n",
        "jsnli_dev      = a['jsnli_1.1/dev.tsv'].decode('utf-8').split('\\n')\n",
        "\n",
        "def jsnli_strip_sentenses(data:list):\n",
        "    ret = []\n",
        "    for line in data:\n",
        "        x = []\n",
        "        for p in line.split('\\t'):\n",
        "            x.append(p.replace(' ',''))\n",
        "        ret.append(x)\n",
        "    return ret\n",
        "\n",
        "JSNLI = {}\n",
        "for k,v in {'train_wo':jsnli_train_wo, 'train_w':jsnli_train_w, 'dev':jsnli_dev}.items():\n",
        "    JSNLI[k] = jsnli_strip_sentenses(v)\n",
        "\n",
        "# JSNLI データセットのサイズを表示    \n",
        "for k, v in JSNLI.items():\n",
        "    print(f'データセット名:{k}, サイズ:{len(v)}')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p  = tagger.parse('本日は悪天なり。').strip().split('\\n')\n",
        "for _p in p[:-1]:\n",
        "    print(_p, _p.split(','), _p.split(',')[-3])"
      ],
      "metadata": {
        "id": "ABFPkN5A2oWr"
      },
      "id": "ABFPkN5A2oWr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d89c79dd-c717-43b6-b2c1-331cac57ddf4",
      "metadata": {
        "id": "d89c79dd-c717-43b6-b2c1-331cac57ddf4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from termcolor import colored\n",
        "import MeCab\n",
        "\n",
        "tagger = MeCab.Tagger()\n",
        "\n",
        "def mecab_get_origin(sent:str):\n",
        "    \"\"\"MeCab を使って，文を分解し，原形と品詞からなるリストを返す\"\"\"\n",
        "    p = tagger.parse(sent).strip().split('\\n')\n",
        "    ret = []\n",
        "    for _p in p[:-1]:\n",
        "        # 分解された各要素の 4 番目を取り出す\n",
        "        x = _p.split(',')\n",
        "        orig = x[-3]\n",
        "        orig = orig.split('-')[0] if '-' in orig else orig\n",
        "        pos = x[0]\n",
        "        ret.append((orig, pos))\n",
        "    return ret\n",
        "    \n",
        "def get_vdrj_wrd2idx(wrd:str,\n",
        "                 wrd2idx:dict=vdrj_wrd2idx,\n",
        "                ):\n",
        "    if wrd in wrd2idx:\n",
        "        return wrd2idx[wrd]\n",
        "    else:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c369b1d-39d6-48de-9a8b-5601e15b8ab2",
      "metadata": {
        "id": "7c369b1d-39d6-48de-9a8b-5601e15b8ab2"
      },
      "outputs": [],
      "source": [
        "sent = '明日天気なーれ。'\n",
        "\n",
        "# 上で定義した sent を使って，MeCab を用いて，単語の原形を取り出す\n",
        "print(f'文の品詞分解と原形: {mecab_get_origin(sent)}')\n",
        "\n",
        "# 取り出した単語の原形のリストから松下データの　ID を取り出す\n",
        "print(f'各単語の ID リスト: {[get_vdrj_wrd2idx(x[0]) for x in mecab_get_origin(sent)]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800c3e61-1378-41d6-962e-ed8c305bd337",
      "metadata": {
        "id": "800c3e61-1378-41d6-962e-ed8c305bd337"
      },
      "outputs": [],
      "source": [
        "for d in JSNLI['train_wo'][:10]:\n",
        "    _, s1, s2 = d\n",
        "    for s in [s1, s2]:\n",
        "        mecab_parsed = mecab_get_origin(s)\n",
        "        ids = [get_vdrj_wrd2idx(x[0]) for x in mecab_parsed]\n",
        "        print(colored(f'最大松下ID:{np.max(ids)}', 'blue', attrs=['bold']),\n",
        "              f'原文:{s}', \n",
        "              f'ID リスト:{ids}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf0cabf-2b75-44a8-97e3-adcf57a6249e",
      "metadata": {
        "id": "8bf0cabf-2b75-44a8-97e3-adcf57a6249e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}