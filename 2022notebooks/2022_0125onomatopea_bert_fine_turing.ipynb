{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2022_0125onomatopea_bert_fine_turing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41f11cc3-a7a0-408b-be54-c99e95f9ca41",
      "metadata": {
        "id": "41f11cc3-a7a0-408b-be54-c99e95f9ca41"
      },
      "source": [
        "- filename: 2021_1222onomatopea_bert_fine_tuing.ipynb\n",
        "- memo: 2022年01月22日現在，\n",
        "- author: 浅川伸一 asakawa@ieee.org\n",
        "- lincense: MIT\n",
        "\n",
        "* transformers は M1 Mac では動作しない。具体的には，最適化関数 Adamw を呼び出すと halt する。\n",
        "Intel Mac such as pasiphae では動作する。おそらく，最適化関数の下請けとして呼び出している C で書かれた関数が intel cpu や arm に特化したコードを使っている，あるいは，intel cpu や arm cpu でコンパイル済のライブラリを呼び出しているためではないかと思う。\n",
        "M1 Mac は arm cpu なのだが，やや特殊なようで，libffi.dylib などのダイナミックライブラリなどでは，ターゲット cpu を arm64e としないといけないように見える。\n",
        "詳細は未確認である。2022_0124\n",
        "\n",
        "# このコードの概要，ねらい\n",
        "\n",
        "- huggingface が提供する `transformers` から BERT を呼び出す。\n",
        "`transformers` に登録されているモデルのうち，東北大学乾研提供の 日本語化 BERT モデルを微調整 fine-tuning する。\n",
        "モデル名としては `cl-tohoku/bert-base-japanese` である。\n",
        "\n",
        "- この日本語化された BERT モデル (BERT-MLM) を `BertForMaskedLM` (マスク化言語モデルに特化した BERT) として呼び出し，オノマトペ予測課題としみなして訓練を行うことである。\n",
        "\n",
        "## 本コードの具体的な手順\n",
        "\n",
        "1. 必要なライブラリを輸入 import \n",
        "2. 小野編 「オノマトペ辞典4500」の読み込み\n",
        "3. 訓練済 日本語 BERT モデルの読み込み\n",
        "4. 日本語 BERT モデルで提供されているトークナイザに，小野編「オノマトペ辞典」を登録\n",
        "5. 訓練テキストデータ (original.csv) の読み込み\n",
        "6. 小野版オノマトペ辞典の，各オノマトペ記述文に出てくるオノマトペを [MASK] で置換する\n",
        "7. PyTorch の流儀に従って Dataset, DataLoader を定義する\n",
        "8. 最適化関数を定義する\n",
        "9. 訓練関数を定義して訓練を行う\n",
        "10. 結果の損失関数の減衰曲線を描画する\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1.  必要なライブラリを輸入 import "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6640bdc1-716e-4c76-a12c-829914444287",
      "metadata": {
        "id": "6640bdc1-716e-4c76-a12c-829914444287",
        "outputId": "9fc8373a-df9e-44a4-ef8b-1cb6f6a579f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "from termcolor import colored\n",
        "\n",
        "# 本ファイルを Google Colaboratory 上で実行する場合に，必要となるライブラリをインストールする\n",
        "import platform\n",
        "isColab = platform.system() == 'Linux'\n",
        "if isColab:\n",
        "    !pip install transformers > /dev/null 2>&1 \n",
        "\n",
        "    # MeCab, fugashi, ipadic のインストール\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1\n",
        "    !pip install mecab-python3 > /dev/null 2>&1\n",
        "    !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "    !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1\n",
        "    \n",
        "    import subprocess\n",
        "    cmd='echo `mecab-config --dicdir`\\\"/mecab-ipadic-neologd\\\"'\n",
        "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                                     shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "    !pip install 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    !python -m unidic download > /dev/null 2>&1\n",
        "    !pip install ipadic > /dev/null 2>&1\n",
        "    !pip install jaconv > /dev/null 2>&1\n",
        "    !pip install japanize_matplotlib > /dev/null 2>&1    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch の seed の設定関連 再現性確保のため\n",
        "# https://qiita.com/takubb/items/7d45ae701390912c7629\n",
        "# https://qiita.com/si1242/items/d2f9195c08826d87d6ad\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# リソースの選択（CPU/GPU）\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 乱数シード固定（再現性の担保）\n",
        "def fix_seed(seed):\n",
        "    # random\n",
        "    random.seed(seed)\n",
        "    # numpy\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # pytorch\\n\",\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed = 42\n",
        "fix_seed(seed)\n",
        "# データローダーのサブプロセスの乱数のseedが固定\n",
        "def worker_init_fn(worker_id):\n",
        "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
        "    print(worker_init_fn(1))\n",
        "    \n",
        "# # データローダーの作成\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "#                                            batch_size=16,  # バッチサイズ\n",
        "#                                            shuffle=True,  # データシャッフル\n",
        "#                                            num_workers=2,  # 高速化\n",
        "#                                            pin_memory=True,  # 高速化\n",
        "#                                            worker_init_fn=worker_init_fn\n",
        "#                                            )"
      ],
      "metadata": {
        "id": "MhOZfAkoVFwk"
      },
      "id": "MhOZfAkoVFwk",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "182d190a-d984-45e8-bf76-7c18248131d7",
      "metadata": {
        "id": "182d190a-d984-45e8-bf76-7c18248131d7"
      },
      "source": [
        "## 2. 小野編 「オノマトペ辞典4500」の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a514e28d-3aaa-4c58-b519-6d7940e663ac",
      "metadata": {
        "id": "a514e28d-3aaa-4c58-b519-6d7940e663ac"
      },
      "outputs": [],
      "source": [
        "# 2021/Jan 近藤先生からいただいたオノマトペ辞典のデータの読み込み\n",
        "\n",
        "#'日本語オノマトペ辞典4500より.xls' は著作権の問題があり，公にできません。\n",
        "# そのため Google Colab での解法，ローカルファイルよりアップロードしてください\n",
        "if isColab:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # ここで `日本語オノマトペ辞典4500より.xls` を指定してアップロードする\n",
        "    data_dir = '.'\n",
        "else:\n",
        "    data_dir = '/Users/asakawa/study/2021ccap/notebooks'\n",
        "\n",
        "import pandas as pd\n",
        "import jaconv\n",
        "\n",
        "onomatopea_excel = '日本語オノマトペ辞典4500より.xls'\n",
        "onmtp2761 = pd.read_excel(os.path.join(data_dir, onomatopea_excel), sheet_name='2761語')\n",
        "\n",
        "#すべてカタカナ表記にしてデータとして利用する場合\n",
        "#`日本語オノマトペ辞典4500` はすべてひらがな表記だが，一般にオノマトペはカタカナ表記されることが多いはず\n",
        "#onomatopea = list(sorted(set([jaconv.hira2kata(o) for o in onmtp2761['オノマトペ']])))\n",
        "\n",
        "# Mac と Windows の表記の相違を吸収\n",
        "onomatopea = list(sorted(set([jaconv.normalize(o) for o in onmtp2761['オノマトペ']])))\n",
        "print(f'データファイル名: {os.path.join(data_dir, onomatopea_excel)}\\n',\n",
        "      f'オノマトペ単語総数: len(onomatopea):{len(onomatopea)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48955785-8fae-4c3f-a5c5-9af0d55e700a",
      "metadata": {
        "id": "48955785-8fae-4c3f-a5c5-9af0d55e700a"
      },
      "source": [
        "## 3. 訓練済 日本語 BERT モデルの読み込みと，小野編「オノマトペ辞典」のトークナイザへの登録"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f44ca863-d272-4e74-9183-20936642c9f3",
      "metadata": {
        "tags": [],
        "id": "f44ca863-d272-4e74-9183-20936642c9f3"
      },
      "outputs": [],
      "source": [
        "# transformers, huggingface 版の BERT 実装の読み込み\n",
        "import torch\n",
        "from transformers import BertConfig\n",
        "from transformers import BertForPreTraining\n",
        "from transformers import BertJapaneseTokenizer\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "model_ja_name = 'cl-tohoku/bert-base-japanese'  # 東北大学乾研による 日本語 BERT 実装\n",
        "model = BertForMaskedLM.from_pretrained(model_ja_name) # マスク化言語モデルを指定\n",
        "config = BertConfig.from_pretrained(model_ja_name)\n",
        "\n",
        "# GPU が利用可能であれば利用する\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "tknz1 = BertJapaneseTokenizer.from_pretrained(model_ja_name)\n",
        "# BPE (or sentencepiece) による下位単語分割あり"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b44fb71e-0fe6-474f-ab3a-f6e435816083",
      "metadata": {
        "id": "b44fb71e-0fe6-474f-ab3a-f6e435816083"
      },
      "source": [
        "## 4. 日本語 BERT モデルで提供されているトークナイザに，小野編「オノマトペ辞典」を登録\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33fb87ee-2dff-47d7-a652-c615fe0e038f",
      "metadata": {
        "tags": [],
        "id": "33fb87ee-2dff-47d7-a652-c615fe0e038f"
      },
      "outputs": [],
      "source": [
        "# トークナイザ の修正，実際には onomatopea 単語リストを引数に指定して `add_tokens()` を呼び出すだけ\n",
        "# ただし，語彙数 tknz.vocab は変更されない。追加された語彙，本コードの場合はオノマトペは，\n",
        "# `tknz1.added_tokens_encoder` と `tknz1.added_tokens_decoder` に反映されているためである\n",
        "num_added = tknz1.add_tokens(onomatopea)\n",
        "print(f'追加されたトークン数:{num_added}/オノマトペ数:{len(onomatopea)}') \n",
        "model.resize_token_embeddings(len(tknz1))\n",
        "\n",
        "print(f' len(tknz1):{len(tknz1)}\\n', \n",
        "      f'len(tknz1.vocab):{len(tknz1.vocab)}\\n',  # 一見すると，この数字からオノマトペが追加されていないように見える。\n",
        "      f'tknz1.vocab_size:{tknz1.vocab_size}')    # 駄菓子菓子，下で見るように，正しく動作しているように見受けられる\n",
        "\n",
        "print('# 確認用')\n",
        "for w in onomatopea[-5:]:\n",
        "    idx = tknz1.convert_tokens_to_ids(w)\n",
        "    w_ = tknz1.convert_ids_to_tokens(idx)\n",
        "    print(f'単語:{w}(id:{idx}) -> token:{w_}')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edfae422-37a3-439d-8bdc-e4814be6fc36",
      "metadata": {
        "id": "edfae422-37a3-439d-8bdc-e4814be6fc36"
      },
      "source": [
        "## 5. 訓練テキストデータ (original.csv) の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "685260e5-527b-4a09-8129-43a25cad554e",
      "metadata": {
        "id": "685260e5-527b-4a09-8129-43a25cad554e"
      },
      "outputs": [],
      "source": [
        "# 近藤先生 (2021年12月22日） から送っていただいた，オノマトペ文章データ 'original.csv' を読み込む\n",
        "import jaconv\n",
        "\n",
        "if isColab:\n",
        "    uploaded = files.upload()  # original.csv をアップロード\n",
        "    data_dir = '.'\n",
        "else:\n",
        "    data_dir = '/Users/asakawa/study/2021kondo_project'\n",
        "\n",
        "original = []\n",
        "n = 0\n",
        "with open(os.path.join(data_dir,'original.csv'), 'r', encoding='utf8') as f:\n",
        "    s = f.read()\n",
        "    for s_ in s.split('\\n'):\n",
        "        if n == 0:\n",
        "            n += 1\n",
        "            continue\n",
        "        idx, sent = s_.split(',')\n",
        "        \n",
        "        # Mac と Windows との unicode 符号化の差分を吸収する\n",
        "        # jaconv.normalize は内部で unicodedata.normalize('NFKC') を呼び出しているので\n",
        "        # 差異 between Mac and Windows を吸収できる\n",
        "        sent = ''.join(jaconv.normalize(x) for x in sent)\n",
        "        original.append(sent)\n",
        "        #original[int(idx)] = sent\n",
        "\n",
        "print(f'{len(original)} has been read')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd09930d-d6ac-49d8-af31-54d88f0aa6e2",
      "metadata": {
        "id": "bd09930d-d6ac-49d8-af31-54d88f0aa6e2"
      },
      "source": [
        "## 6. 小野版オノマトペ辞典の，各オノマトペ記述文に出てくるオノマトペを [MASK] で置換する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128263c1-29e0-4ad4-8687-26150d3d7cec",
      "metadata": {
        "id": "128263c1-29e0-4ad4-8687-26150d3d7cec"
      },
      "outputs": [],
      "source": [
        "max_token_len = np.array([len(tknz1(s).input_ids) for s in original]).max()\n",
        "max_token_len += 2  # 保険のため 2 くらい加えておく\n",
        "print(f'max_token_len:{max_token_len}')\n",
        "\n",
        "# トークナイザにかけて出力を得る。`max_length` のデフォルトは 512 だが，今回は長文である必要がないと考えられる。\n",
        "# ここでは `max_token_len = 23` にしている。512 でも動作するが，学習に要する時間が増える\n",
        "text = tuple(original)  # 全文をタプルに変換\n",
        "inputs = tknz1(text, \n",
        "               return_tensors='pt', \n",
        "               max_length=max_token_len, \n",
        "               truncation=True, \n",
        "               padding='max_length')\n",
        "\n",
        "#`labels` キーを追加する。実際には inputs_ids なのでラベルではなくトークンID の系列\n",
        "inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "\n",
        "#トークン ID を走査して，オノマトペ単語であれば，[MASK] トークンに置き換える。\n",
        "l_ = []\n",
        "for l in inputs['labels']:\n",
        "    l_.append([tknz1.mask_token_id if w in onomatopea else tknz1.convert_tokens_to_ids(w) for w in tknz1.convert_ids_to_tokens(l)])\n",
        "\n",
        "inputs['input_ids'] = torch.LongTensor(l_)\n",
        "#print(inputs['input_ids'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a282a5e-07d3-4749-9e38-b5ad8fd0f30b",
      "metadata": {
        "id": "5a282a5e-07d3-4749-9e38-b5ad8fd0f30b"
      },
      "source": [
        "## 7. PyTorch の流儀に従って Dataset, DataLoader を定義する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d68e713d-9e71-43ea-a540-b47d01067f85",
      "metadata": {
        "tags": [],
        "id": "d68e713d-9e71-43ea-a540-b47d01067f85"
      },
      "outputs": [],
      "source": [
        "#データセットのためのクラスを定義\n",
        "class onmtpDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encoder):\n",
        "        self.encoder = encoder\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return {key:torch.tensor(val[idx]) for key, val in self.encoder.items()}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encoder.input_ids)\n",
        "    \n",
        "dataset = onmtpDataset(inputs)\n",
        "\n",
        "#データローダを準備\n",
        "loader = torch.utils.data.DataLoader(dataset, \n",
        "                                     batch_size=8, \n",
        "                                     shuffle=True,\n",
        "                                     worker_init_fn=worker_init_fn\n",
        "                                     )\n",
        "\n",
        "# GPU/CPU 使用を設定し，モデルの訓練モードを起動 #Setup GPU/CPU usage and activate the training mode of our model.\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# そしてモデルを選択したデバイスに移動 # and move our model over to the selected device\n",
        "model.to(device)\n",
        "# 訓練モードに設定 #activate training mode\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e35d01d-54fc-4872-8f81-7293819cb298",
      "metadata": {
        "id": "2e35d01d-54fc-4872-8f81-7293819cb298"
      },
      "source": [
        "## 8. 最適化関数を定義する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "247fbd4e-1469-453a-8b62-c728945f0f0e",
      "metadata": {
        "id": "247fbd4e-1469-453a-8b62-c728945f0f0e"
      },
      "outputs": [],
      "source": [
        "#最適化関数を初期化 (AdamW は重み付き崩壊で，過学習の可能性を減らす) \n",
        "#Initialize our optimizer (Adam with weighted decay - reduces chance of overfitting).\n",
        "from transformers import AdamW\n",
        "#最適化関数を初期化 # initialize optimizer\n",
        "optim = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0ee9a31-40d2-40f9-918d-9ac485c965d3",
      "metadata": {
        "id": "c0ee9a31-40d2-40f9-918d-9ac485c965d3"
      },
      "source": [
        "## 9. 訓練関数を定義して訓練を行う"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec40b7be-141b-48ff-91fd-3d0d4e555be9",
      "metadata": {
        "id": "ec40b7be-141b-48ff-91fd-3d0d4e555be9"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm  # 進捗状況の可視化のため\n",
        "\n",
        "epochs = 50  # 学習回数を指定\n",
        "\n",
        "losses = []\n",
        "for epoch in range(epochs):\n",
        "    # setup loop with TQDM and dataloader\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch in loop:\n",
        "\n",
        "        optim.zero_grad()  # 学習に用いる BERT パラメータの勾配を 0 で初期化\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)  # ミニバッチサイズだけデータを取得\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # BERT を呼び出して結果を得る\n",
        "        outputs = model(input_ids, \n",
        "                        attention_mask=attention_mask,\n",
        "                        labels=labels)\n",
        "\n",
        "        loss = outputs.loss  # 損失値を取得\n",
        "        loss.backward()      # 取得した損失値に基づいて BERT のパラメータを逆伝播\n",
        "        optim.step()         # BERT パラメータの更新 すなわち学習\n",
        "        \n",
        "        loop.set_description(f'Epoch {epoch}') # 進行状況の表示\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "        losses.append(loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a443034c-2739-4bab-87b0-8e69626fabe7",
      "metadata": {
        "id": "a443034c-2739-4bab-87b0-8e69626fabe7"
      },
      "source": [
        "## 10. 結果の損失関数の減衰曲線を描画する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "949c8343-2940-413d-99ab-0baf710b24e5",
      "metadata": {
        "id": "949c8343-2940-413d-99ab-0baf710b24e5"
      },
      "outputs": [],
      "source": [
        "len(losses)\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "\n",
        "plt.plot(losses[:])\n",
        "plt.xlabel('訓練時間')\n",
        "plt.ylabel('損失値')\n",
        "plt.title('オノマトペ微調整における学習の推移 (損失値) の減少')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592b9da1-b8fc-4b1e-9cec-d99de088f3d4",
      "metadata": {
        "id": "592b9da1-b8fc-4b1e-9cec-d99de088f3d4"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def eval_an_output(N, original=original, tknz1=tknz1, inputs=inputs, print_flag=True):\n",
        "    \"\"\"\n",
        "    引数として 数字を 1 つ入力すると (N)，`original.csv` の N 行目のデータを読み込んで，\n",
        "    その文のオノマトペを [MASK] に置き換えて，マスク化言語モデルで [MASK] を予測する。\n",
        "    結果を表示する場合には 引数 `print_flag=True` として呼び出す\n",
        "    \"\"\"\n",
        "    if N >= len(original) or (not isinstance(N, int)):\n",
        "        return\n",
        "\n",
        "    _out = model(inputs.input_ids[N].unsqueeze(0).to(device), attention_mask=inputs.attention_mask[N].unsqueeze(0).to(device), labels=inputs.labels[N].to(device))\n",
        "    _x = _out.logits.detach()\n",
        "    __x = _x.squeeze(0).detach().clone()\n",
        "    _pred_idx  = torch.argmax(__x, dim=1, keepdim=True)\n",
        "    _pred_s    = \"/\".join(tknz1.convert_ids_to_tokens(_pred_idx)).replace('/[PAD]','')\n",
        "    \n",
        "    _orig      = original[N] # 原文\n",
        "    _inp_idx   = tknz1.convert_ids_to_tokens(inputs.input_ids[N]) # 入力トークンID\n",
        "    _inp_s     = \"/\".join(_inp_idx).replace('/[PAD]','')          # 入力文\n",
        "    _teach_idx = tknz1.convert_ids_to_tokens(inputs.labels[N])    # 教師信号トークンID\n",
        "    _teach_s   = \"/\".join(_teach_idx).replace('/[PAD]','')        # 教師信号文\n",
        "    \n",
        "    _mask_pos = np.where(inputs.input_ids[N].detach().numpy() == tknz1.mask_token_id)\n",
        "    _teach_tokens = inputs.labels[N][_mask_pos].detach().squeeze().numpy()\n",
        "    _pred_tokens  = _pred_idx[_mask_pos].detach().squeeze().cpu().numpy()\n",
        "\n",
        "    _n_hit = np.array([_teach_tokens == _pred_tokens]).sum()       # 正解したか否か\n",
        "    if print_flag:\n",
        "        color = 'grey' if _n_hit > 0 else 'red'\n",
        "        print(f'{N:5,d}   原文:{_orig}')\n",
        "        print(f'\\t入力:{_inp_s}')\n",
        "        print(f'\\t正解:{_teach_s}')\n",
        "        print(colored(f'\\t出力:{_pred_s}',color))\n",
        "        print(f'\\tmask 位置:{_mask_pos}')\n",
        "        print(f'\\t正解トークン:{_teach_tokens}', f'予測トークン:{_pred_tokens}', \n",
        "              f'{np.array([_teach_tokens == _pred_tokens]).sum() > 0}')\n",
        "        print(f'\\t_out.loss:{_out.loss:.3f}')\n",
        "    \n",
        "    return _out.loss, _n_hit\n",
        "\n",
        "total_hit = 0\n",
        "for i in range(len(original)):\n",
        "    _, hit = eval_an_output(i, print_flag=False) # print_flag = True にすると推論結果を表示します. 逆に False にすれば正解率だけ計算します\n",
        "    total_hit += hit\n",
        "\n",
        "print(f'正解数:{total_hit}/{len(original)}= {total_hit/len(original) * 100:.3f} %')    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c664678-e8c9-488e-a14b-b22fc57052d9",
      "metadata": {
        "id": "3c664678-e8c9-488e-a14b-b22fc57052d9"
      },
      "outputs": [],
      "source": [
        "out_fname = '2022_0125onomatopea_mlm_finetuned.pt'\n",
        "torch.save(model.state_dict(), out_fname)\n",
        "#model = BertForMaskedLM.from_pretrained(model_ja_name)\n",
        "model.load_state_dict(torch.load(out_fname))\n",
        "files.download(out_fname)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/2022_0125onomatopea_mlm_finetuned.pt drive/MyDrive"
      ],
      "metadata": {
        "id": "qXjTN077uzFh"
      },
      "id": "qXjTN077uzFh",
      "execution_count": 53,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "2022_0125onomatopea_bert_fine_turing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}