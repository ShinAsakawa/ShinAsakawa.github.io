{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2022notebooks/2021_1222onomatopea_bert_fine_turing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41f11cc3-a7a0-408b-be54-c99e95f9ca41"
      },
      "source": [
        "- filename: 2021_1222onomatopea_bert_fine_tuing.ipynb\n",
        "- memo: 2021年12月25日現在，\n",
        "\n",
        "transformers は M1 Mac では動作しない。Intel Mac such as pasiphae では動作する。"
      ],
      "id": "41f11cc3-a7a0-408b-be54-c99e95f9ca41"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6640bdc1-716e-4c76-a12c-829914444287"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "from termcolor import colored\n",
        "\n",
        "# 本ファイルを Google Colaboratory 上で実行する場合に，必要となるライブラリをインストールする\n",
        "import platform\n",
        "isColab = platform.system() == 'Linux'\n",
        "if isColab:\n",
        "    !pip install transformers > /dev/null 2>&1 \n",
        "\n",
        "    # MeCab, fugashi, ipadic のインストール\n",
        "    !apt install aptitude swig > /dev/null 2>&1\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null 2>&1\n",
        "    !pip install mecab-python3 > /dev/null 2>&1\n",
        "    !git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "    !echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a > /dev/null 2>&1\n",
        "    \n",
        "    import subprocess\n",
        "    cmd='echo `mecab-config --dicdir`\\\"/mecab-ipadic-neologd\\\"'\n",
        "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
        "                                     shell=True).communicate()[0]).decode('utf-8')\n",
        "\n",
        "    !pip install 'fugashi[unidic]' > /dev/null 2>&1\n",
        "    !python -m unidic download > /dev/null 2>&1\n",
        "    !pip install ipadic > /dev/null 2>&1"
      ],
      "id": "6640bdc1-716e-4c76-a12c-829914444287"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a514e28d-3aaa-4c58-b519-6d7940e663ac",
        "outputId": "d0d174d0-9fbd-4fa0-b064-287bac1780d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2469 has been read\n"
          ]
        }
      ],
      "source": [
        "# 近藤先生 (2021年12月22日） から送っていただいた，オノマトペ文章データを読み込む\n",
        "import jaconv\n",
        "data_dir = '/Users/asakawa/study/2021kondo_project'\n",
        "original = {}\n",
        "n = 0\n",
        "with open(os.path.join(data_dir,'original.csv'), 'r', encoding='utf8') as f:\n",
        "    s = f.read()\n",
        "    for s_ in s.split('\\n'):\n",
        "        if n == 0:\n",
        "            n += 1\n",
        "            continue\n",
        "        idx, sent = s_.split(',')\n",
        "        \n",
        "        # Mac と Windows との unicode 符号化の差分を吸収する\n",
        "        # jaconv.normalize は内部で unicodedata.normalize('NFKC') を呼び出しているので\n",
        "        # 差異 between Mac and Windows を吸収できる\n",
        "        sent = ''.join(jaconv.normalize(x) for x in sent)\n",
        "        original[int(idx)] = sent\n",
        "\n",
        "print(f'{len(original)} has been read')"
      ],
      "id": "a514e28d-3aaa-4c58-b519-6d7940e663ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "685260e5-527b-4a09-8129-43a25cad554e",
        "outputId": "4fc7c035-b11d-40d1-f845-22f6dbb39e61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "データファイル名: /Users/asakawa/study/2021ccap/notebooks/2021-0325日本語オノマトペ辞典4500より.xls\n",
            " オノマトペ単語総数: len(onomatopea):1741\n"
          ]
        }
      ],
      "source": [
        "# 2021/Jan 近藤先生からいただいたオノマトペ辞典のデータの読み込み\n",
        "import pandas as pd\n",
        "import jaconv\n",
        "\n",
        "#ひとつ下の '日本語オノマトペ辞典4500より.xls' は著作権の問題があり，公にできません。\n",
        "# そのため Google Colab での解法，ローカルファイルよりアップロードする\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()  # ここで `日本語オノマトペ辞典4500より.xls` を指定してアップロードする\n",
        "data_dir = '/Users/asakawa/study/2021ccap/notebooks'\n",
        "#onomatopea_excel = '日本語オノマトペ辞典4500より.xlsx'  # オリジナルファイル名，次行は勝手に rename したファイル名\n",
        "onomatopea_excel = '2021-0325日本語オノマトペ辞典4500より.xls'\n",
        "onmtp2761 = pd.read_excel(os.path.join(data_dir, onomatopea_excel), sheet_name='2761語')\n",
        "\n",
        "#すべてカタカナ表記にしてデータとして利用する場合\n",
        "#`日本語オノマトペ辞典4500` はすべてひらがな表記だが，一般にオノマトペはカタカナ表記されることが多いはず\n",
        "#onomatopea = list(sorted(set([jaconv.hira2kata(o) for o in onmtp2761['オノマトペ']])))\n",
        "\n",
        "# Mac と Windows の表記の相違を吸収\n",
        "onomatopea = list(sorted(set([jaconv.normalize(o) for o in onmtp2761['オノマトペ']])))\n",
        "\n",
        "print(f'データファイル名: {os.path.join(data_dir, onomatopea_excel)}\\n',\n",
        "      f'オノマトペ単語総数: len(onomatopea):{len(onomatopea)}')"
      ],
      "id": "685260e5-527b-4a09-8129-43a25cad554e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4976eb75-17a4-49a8-894f-d2597808de18"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import torch\n",
        "from transformers import BertConfig\n",
        "from transformers import BertModel\n",
        "from transformers import BertForPreTraining\n",
        "from transformers import BertJapaneseTokenizer\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "model_ja_name = 'cl-tohoku/bert-base-japanese' \n",
        "model = BertModel.from_pretrained(model_ja_name)\n",
        "config = BertConfig.from_pretrained(model_ja_name)\n",
        "\n",
        "# トークナイザ の修正\n",
        "tknz1 = BertJapaneseTokenizer.from_pretrained(model_ja_name) # BPE (or sentencepiece) による下位単語分割あり\n",
        "#siz = len(tknz1)\n",
        "#siz = len(onomatopea)\n",
        "\n",
        "#tknz1.ids_to_tokens.update({i+len(onomatopea):o for i,o in enumerate(onomatopea)})\n",
        "#tknz1.vocab.update({i+len(onomatopea):o for i,o in enumerate(onomatopea)})\n",
        "tknz1.vocab.update(tknz1.added_tokens_decoder)\n",
        "tknz1.ids_to_tokens.update(tknz1.added_tokens_decoder)\n",
        "tknz1.add_tokens(onomatopea)\n",
        "model.resize_token_embeddings(len(tknz1))\n",
        "\n",
        "print(len(tknz1), len(tknz1.vocab), tknz1.vocab_size)\n",
        "print(tknz1.convert_tokens_to_ids(onomatopea[-10:]))\n",
        "print(tknz1.convert_ids_to_tokens(tknz1.convert_tokens_to_ids(onomatopea[-10:])))"
      ],
      "id": "4976eb75-17a4-49a8-894f-d2597808de18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97942f79-ac22-4a10-9559-28888785f390"
      },
      "outputs": [],
      "source": [
        "w = 'わやわや'\n",
        "print(w in tknz1.vocab)\n",
        "print(tknz1.tokenize(w))\n",
        "id_ = tknz1(w)['input_ids'][1]\n",
        "print(tknz1.convert_ids_to_tokens(id_))"
      ],
      "id": "97942f79-ac22-4a10-9559-28888785f390"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c71c10bd-08e2-4169-bf66-3f52622c7133"
      },
      "outputs": [],
      "source": [
        "print(len(tknz1), len(tknz1.vocab), tknz1.vocab_size)\n",
        "print(tknz1.convert_tokens_to_ids(onomatopea[-10:]))\n",
        "print(tknz1.convert_ids_to_tokens(tknz1.convert_tokens_to_ids(onomatopea[-10:])))\n",
        "\n",
        "#tknz1.add_tokens(onomatopea)\n",
        "\n",
        "print(len(tknz1), len(tknz1.vocab), tknz1.vocab_size)\n",
        "print(tknz1.convert_tokens_to_ids(onomatopea[-10:]))\n",
        "print(tknz1.convert_ids_to_tokens(tknz1.convert_tokens_to_ids(onomatopea[-10:])))\n"
      ],
      "id": "c71c10bd-08e2-4169-bf66-3f52622c7133"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44121703-ceac-4b99-9977-b2b256a4008c"
      },
      "outputs": [],
      "source": [
        "tknz2 = BertJapaneseTokenizer.from_pretrained(model_ja_name)"
      ],
      "id": "44121703-ceac-4b99-9977-b2b256a4008c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dfb308f-15cc-4d9b-8d5f-930cc2a624dd"
      },
      "outputs": [],
      "source": [
        "print(tknz1.tokenize('雨がしとしとと降る'))\n",
        "print(tknz2.tokenize('雨がしとしとと降る'))\n",
        "#help(tknz1.add_tokens)\n",
        "#tknz1.add_tokens(onomatopea)"
      ],
      "id": "3dfb308f-15cc-4d9b-8d5f-930cc2a624dd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f55efc23-ceef-4b34-8f65-f6fcd5d8c053"
      },
      "outputs": [],
      "source": [
        "s = 'しとしと'\n",
        "print(s in onomatopea)\n",
        "print(tknz1.tokenize(s))\n",
        "print(tknz1(s))\n",
        "print(tknz1.convert_ids_to_tokens(tknz1(s)['input_ids'][1]))\n",
        "\n",
        "#tknz1.vocab.update(tknz1.added_tokens_decoder)\n",
        "#tknz1.ids_to_tokens.update(tknz1.added_tokens_decoder)\n",
        "print(tknz1.convert_ids_to_tokens(33710))\n",
        "print(tknz1.convert_ids_to_tokens(32000))\n",
        "\n",
        "#list(tknz1.vocab)[-3:]\n",
        "#tknz1.ids_to_tokens[32000]\n",
        "tknz1.convert_ids_to_tokens(32000)\n",
        "\n",
        "print(original[3])"
      ],
      "id": "f55efc23-ceef-4b34-8f65-f6fcd5d8c053"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04315406-2538-440a-aefc-edb187fd562d",
        "outputId": "33ff2dab-3dc3-410d-b765-5b5032b9825c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m美しいものなどにうっとり(と)晄惚とする\u001b[0m\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'tknz1' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/6g/j7x38zn134n7vlw9jj56rdww0000gn/T/ipykernel_14295/1409841883.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 送っていただいた元の文\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'分かち書き'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtknz1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# その分かち書き\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ID 化'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtknz1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 分かち書き結果の単語 ID 化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tknz1' is not defined"
          ]
        }
      ],
      "source": [
        "import re\n",
        "# ランダムサンプリングしてデータを印字して確認\n",
        "for _ in range(5):\n",
        "    N = np.random.randint(low=0, high=len(original))\n",
        "    sent0 = original[N]\n",
        "    sent1 = re.sub(r'\\(.*\\)','',original[N]) # original に含まれる `(と)` のような表現を削除する\n",
        "    \n",
        "    print(colored(sent0, attrs=['bold']))  # 送っていただいた元の文\n",
        "    print(colored('分かち書き','blue'), tknz1.tokenize(sent0)) # その分かち書き\n",
        "    print(colored('ID 化', 'blue'), tknz1.encode(sent0))   # 分かち書き結果の単語 ID 化\n",
        "\n",
        "    if sent0 != sent1:\n",
        "        print(colored(sent1, attrs=['bold']))   # (と) を取り去った文\n",
        "        print(colored('分かち書き','blue'), tknz1.tokenize(sent1)) # その分かち書き\n",
        "        print(colored('ID 化', 'blue'), tknz1.encode(sent1))   # 分かち書き結果の単語 ID 化\n",
        "\n",
        "# MeCab で単語分割が行われて、MeCab が単語として認識しても、その単語が語鎮リスト vocab.txt に登録されていない場合は\n",
        "# subword である WordPiece が起動され、その単語が適当に分割されます。そのように分割された単語には '##' が単語の前に付与されます。\n",
        "# また、未知語の場合もWordPieceが起動され、同様に分割されます。\n",
        "print(colored(f'\\ntknz.all_special_ids:{tknz1.all_special_ids}',attrs=['bold']))  #  [1, 3, 0, 2, 4]\n",
        "print(colored(f'tknz.all_special_tokens:{tknz1.all_special_tokens}', attrs=['bold']))  #  ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ],
      "id": "04315406-2538-440a-aefc-edb187fd562d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "128263c1-29e0-4ad4-8687-26150d3d7cec"
      },
      "outputs": [],
      "source": [
        "#len(tknz1.vocab)\n",
        "\n",
        "#print(dir(tknz1))  # トークナイザの内部を調べてみましょう\n",
        "\n",
        "# # トークナイザの属性値を表示して理解を深める\n",
        "# for k in ['sep_token', 'sep_token_id', 'slow_tokenizer_class', \n",
        "#           'unk_token', 'unk_token_id', 'verbose', #'vocab', \n",
        "#           'vocab_files_names', 'vocab_size', 'word_tokenizer', 'word_tokenizer_type',\n",
        "#           'special_tokens_map', 'special_tokens_map_extended', \n",
        "#           'subword_toke0nizer', 'subword_tokenizer_type',\n",
        "#           'all_special_ids', 'all_special_tokens', 'unk_token', 'unk_token_id', \n",
        "#           'special_tokens_map', 'special_tokens_map_extended', \n",
        "#           'subword_tokenizer', 'subword_tokenizer_type', \n",
        "#           'tokenize',\n",
        "#           'max_model_input_sizes', 'mecab_kwargs', 'model_input_names', 'model_max_length']:\n",
        "#     print(colored(f'{k}','blue', attrs=['bold']), colored(f'{getattr(tknz1,k)}','green', attrs=['bold']))"
      ],
      "id": "128263c1-29e0-4ad4-8687-26150d3d7cec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d68e713d-9e71-43ea-a540-b47d01067f85"
      },
      "outputs": [],
      "source": [
        "tknz1.save_vocabulary('vocab_saved.txt')  # 後に利用可能なように，語彙辞書をテキストファイルとして書き出す\n",
        "\n",
        "# 結果の確認\n",
        "!head vocab_saved.txt\n",
        "!tail vocab_saved.txt\n",
        "#help(tknz.save_vocabulary)"
      ],
      "id": "d68e713d-9e71-43ea-a540-b47d01067f85"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad066140-8919-4a59-90a7-714022e32ec5"
      },
      "outputs": [],
      "source": [
        "# for w in ['ひゅー', 'ぴゅー', 'すっかり']:\n",
        "#     print(w, w in onomatopea)\n",
        "    \n",
        "# print(len(original))    "
      ],
      "id": "ad066140-8919-4a59-90a7-714022e32ec5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "247fbd4e-1469-453a-8b62-c728945f0f0e"
      },
      "outputs": [],
      "source": [
        "#for idx in original: \n",
        "for idx in range(8): \n",
        "    sent = original[idx]\n",
        "    print(f'sent:{sent}', end=\"---\")\n",
        "    print('/'.join(tknz1.tokenize(sent)))\n",
        "    input_ids = tknz1(sent)['input_ids']\n",
        "    #print(tknz1.ids_to_tokens[input_ids])\n",
        "    for idx in input_ids:\n",
        "        print(f'{tknz1.convert_ids_to_tokens(idx)}', end=\"/\")\n",
        "    print()"
      ],
      "id": "247fbd4e-1469-453a-8b62-c728945f0f0e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec40b7be-141b-48ff-91fd-3d0d4e555be9"
      },
      "outputs": [],
      "source": [
        "# オノマトペにマッチする部分を検索し，[MASK] で置き換える\n",
        "masked_onmtp = {}\n",
        "\n",
        "#for N in np.random.randint(len(original), size=100):\n",
        "for N in range(0,4):\n",
        "    sent = original[N] # orginal.csv の内容を 1 行取り出す\n",
        "    print(f'{N:5,d}', end=\" \")\n",
        "    #print(tknz1.tokenize(sent))\n",
        "    #print(tknz1(sent)['input_ids'])\n",
        "    \n",
        "    for w in tknz1.tokenize(sent):\n",
        "        color = 'red' if w in onomatopea else 'grey'\n",
        "        print(colored(w,color), end='/')\n",
        "    print('\\t-->original', colored(sent, 'cyan'))\n",
        "                \n",
        "    for i, o in enumerate(onomatopea):\n",
        "        if sent.find(o) != -1:\n",
        "            target_id = i\n",
        "            o_ = o  # 最後にマッチしたオノマトペだけ取り出す\n",
        "            \n",
        "    if o_ not in masked_onmtp:\n",
        "        masked_onmtp[o_] = [sent.replace(o_,'[MASK]')]\n",
        "    else:\n",
        "        masked_onmtp[o_].append(sent.replace(o_,'[MASK]'))\n",
        "\n",
        "#   3 すっ/かり/日/が/とっぷり/(/と/)/暮れ/##る/    from すっかり日がとっぷり(と)暮れる\n",
        "# 633 泥/##酔/し/て/へ/##べ/##れ/##け/(/と/)/視点/や/口/##調/が/乱れ/##る/    from 泥酔してへべれけ(と)視点や口調が乱れる\n",
        "# 690 気/どっ/たり/おほん/(/と/)/い/ばっ/たり/する/    from 気どったりおほん(と)いばったりする    #if N > 2500: break\n",
        "# 924 こちん/と/[UNK]/に/さわ/##る/    from こちんと癪にさわる\n",
        "#1498 しっかり/と/べたん/と/くっ/つく/    from しっかりとべたんとくっつく\n",
        "#1829 もの/が/ぎ/##っと/一/瞬/##き/し/##む/    from ものがぎっと一瞬きしむ\n",
        "#1830 かた/##い/もの/が/ぎ/##し/り/ぎ/##し/り/(/と/)/何/度/も/こ/##す/##れる/    from かたいものがぎしりぎしり(と)何度もこすれる\n",
        "#1917 金属/を/じゃ/ん/と/たた/##く/    from 金属をじゃんとたたく\n",
        "#2056 もの/が/他/の/もの/に/ぼ/こ/つ/と/打ち/当たる/    from ものが他のものにぼこつと打ち当たる\n",
        "#2402 すば/##や/くす/##い/と/動い/たり/変化/する/    from すばやくすいと動いたり変化する"
      ],
      "id": "ec40b7be-141b-48ff-91fd-3d0d4e555be9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8445ab1a-381f-4a04-bed6-ca840a1eaf39"
      },
      "outputs": [],
      "source": [
        "# オノマトペにマッチする部分を検索し，[MASK] で置き換える\n",
        "\n",
        "with open('2022_0119ono_test.txt','w') as fo:\n",
        "    for N in range(len(original)):\n",
        "        sent = original[N] # orginal.csv の内容を 1 行取り出す\n",
        "        fo.write(f'{N:5,d} ')\n",
        "        for w in tknz1.tokenize(sent):\n",
        "            fo.write(w+'/')\n",
        "        fo.write(f'\\t-->original {sent}\\n')"
      ],
      "id": "8445ab1a-381f-4a04-bed6-ca840a1eaf39"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15a295f8-5d10-4301-a80f-c66285b8aab8"
      },
      "outputs": [],
      "source": [
        "!tail 2022_0119ono_test.txt"
      ],
      "id": "15a295f8-5d10-4301-a80f-c66285b8aab8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e44bdab-5356-4bbb-b79b-b7dc68dc7be8"
      },
      "outputs": [],
      "source": [
        "# 632 酔っ/て/べろんべろん/(/と/)/体/が/まとも/に/動か/せ/ず/    from 酔ってべろんべろん(と)体がまともに動かせず\n",
        "# 690 気/どっ/たり/おほん/(/と/)/い/ばっ/たり/する/    from 気どったりおほん(と)いばったりする\n",
        "\n",
        "for w in ['かん高い', 'かん高く', '忙しく', 'じゃんが','へべれけ','ひゅー', 'ぴゅー', 'すっかり']:\n",
        "    print(w, w in onomatopea)\n",
        "    \n",
        "print(len(original))    "
      ],
      "id": "1e44bdab-5356-4bbb-b79b-b7dc68dc7be8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ff0b8f7-4453-410c-ad0e-3325ec5f10cb"
      },
      "outputs": [],
      "source": [
        "# from transformers import BertForMaskedLM\n",
        "# masked_lm = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese')\n",
        "\n",
        "# tknz.add_tokens(onomatopea)\n",
        "# #print(len(tknz.vocab), len(onomatopea))\n",
        "# n_added = tknz.add_tokens(onomatopea)    # 東北大学 BERT に登録されていないオノマトペを加える。\n",
        "# n_added_ = tknz_.add_tokens(onomatopea)  # 東北大学 BERT に登録されていないオノマトペを加える。\n",
        "# tknz.ids_to_tokens.update()\n",
        "# tknz_.ids_to_tokens.update()\n",
        "# masked_lm.resize_token_embeddings(len(tknz))\n",
        "\n",
        "# #print(tknz.vocab_size)  # vocab_size は変わらないのか\n",
        "# #help(tknz.get_vocab)\n",
        "# #print(tknz.get_added_vocab())\n",
        "# print(len(tknz), len(tknz.vocab), tknz.vocab_size)\n",
        "\n",
        "# print(tknz.convert_tokens_to_ids(onomatopea[-10:]))\n",
        "# print(tknz.convert_ids_to_tokens(tknz.convert_tokens_to_ids(onomatopea[-13:])))\n",
        "\n",
        "# print(tknz_.convert_tokens_to_ids(onomatopea[-10:]))\n",
        "# print(tknz_.convert_ids_to_tokens(tknz.convert_tokens_to_ids(onomatopea[-13:])))"
      ],
      "id": "5ff0b8f7-4453-410c-ad0e-3325ec5f10cb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca6a6e93-52b7-473c-93ca-bb85bb26b8b9"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForMaskedLM\n",
        "masked_lm1 = BertForMaskedLM.from_pretrained(model_ja_name)\n",
        "masked_lm2 = BertForMaskedLM.from_pretrained(model_ja_name)\n",
        "\n",
        "tknz1.add_tokens(onomatopea)\n",
        "n_added = tknz1.add_tokens(onomatopea)    # 東北大学 BERT に登録されていないオノマトペを加える。\n",
        "tknz1.ids_to_tokens.update()\n",
        "masked_lm1.resize_token_embeddings(len(tknz1))\n",
        "\n",
        "tknz2.add_tokens(onomatopea)\n",
        "n_added_ = tknz2.add_tokens(onomatopea)  # 東北大学 BERT に登録されていないオノマトペを加える。\n",
        "tknz2.ids_to_tokens.update()\n",
        "masked_lm2.resize_token_embeddings(len(tknz2))\n",
        "\n",
        "n_limit = 2\n",
        "for i, s in enumerate(masked_onmtp):\n",
        "    print(colored(f'{i} {s}',attrs=['bold']), masked_onmtp[s])\n",
        "    for s_ in masked_onmtp[s]:\n",
        "        print(tknz1.tokenize(s_), end=\"\")\n",
        "        print(colored(tknz1(s_)['input_ids'], 'green'), end=\"\") # with sentencepiece\n",
        "        x = torch.LongTensor(tknz1(s_)['input_ids']).unsqueeze(0)\n",
        "        a = masked_lm1(x)\n",
        "        print(a[0].shape)\n",
        "\n",
        "        print(tknz2.tokenize(s_), end=\"\")\n",
        "        print(colored(tknz2(s_)['input_ids'], 'cyan'), end=\"\") # without sentencepiece\n",
        "        x = torch.LongTensor(tknz2(s_)['input_ids']).unsqueeze(0)\n",
        "        a = masked_lm2(x)\n",
        "        print(a[0].shape)\n",
        "\n",
        "        idxs = tknz2(s_)['input_ids']\n",
        "        maskpos = idxs.index(tknz2.mask_token_id)\n",
        "        top_n = 5\n",
        "        b = torch.topk(a.to_tuple()[0][0][maskpos], k=top_n)\n",
        "        b_idx = b[1].detach().numpy()\n",
        "        b_wrd = list(tknz2.convert_ids_to_tokens(b_idx))\n",
        "        for k in range(top_n):\n",
        "            for j, idx in enumerate(idxs):\n",
        "                if j != maskpos:\n",
        "                    print(colored(tknz2.convert_ids_to_tokens(idx),'blue'), end=\" \")\n",
        "                else:\n",
        "                    print(colored(b_wrd[k],'red', attrs=['bold']),end=\" \")\n",
        "                    \n",
        "            print()\n",
        "    if i > n_limit:\n",
        "        break"
      ],
      "id": "ca6a6e93-52b7-473c-93ca-bb85bb26b8b9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79fc417c-c4b8-432a-88a4-902f0c7ace5b"
      },
      "outputs": [],
      "source": [
        "def print_message(a, s_, tokenizer=tknz1, top_n=5):\n",
        "    idxs = tokenizer(s_)['input_ids']\n",
        "    maskpos = idxs.index(tokenizer.mask_token_id)\n",
        "    b = torch.topk(a.to_tuple()[0][0][maskpos], k=top_n)\n",
        "    b_idx = b[1].detach().numpy()\n",
        "    b_wrd = list(tokenizer.convert_ids_to_tokens(b_idx))\n",
        "    for k in range(top_n):\n",
        "        print(k, end=\":\")\n",
        "        for j, idx in enumerate(idxs):\n",
        "            if j != maskpos:\n",
        "                print(colored(tokenizer.convert_ids_to_tokens(idx),'grey'), end=\" \")\n",
        "            else:\n",
        "                print(colored(b_wrd[k],'red', attrs=['bold']),end=\" \")\n",
        "        print()\n",
        "    \n",
        "#for i, s in enumerate(masked_onmtp):\n",
        "for i in range(5):\n",
        "    N = np.random.choice(len(masked_onmtp))\n",
        "    N = 49\n",
        "    N = 4\n",
        "    ono = list(masked_onmtp.keys())[N]\n",
        "    s = masked_onmtp[ono]\n",
        "    print(colored(f'{i} N:{N} オノマトペ:{ono} {s}', 'blue', attrs=['bold']))\n",
        "    for s_ in s:\n",
        "        print(tknz1.tokenize(s_), end=\"\")\n",
        "        print(colored(tknz1(s_)['input_ids'], 'green'), end=\"\") # with sentencepiece\n",
        "        x = torch.LongTensor(tknz1(s_)['input_ids']).unsqueeze(0)\n",
        "        a = masked_lm1(x)\n",
        "        print(a[0].shape)\n",
        "        \n",
        "        print('=' * 7, 'with wordpice')\n",
        "        print_message(a, s_, tokenizer=tknz1)\n",
        "        print('+' * 7, 'withoout wordpeice')\n",
        "        print_message(a, s_, tokenizer=tknz2)\n"
      ],
      "id": "79fc417c-c4b8-432a-88a4-902f0c7ace5b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b376fa0c-dcf1-454f-9ee4-e4b8081c8ad8"
      },
      "outputs": [],
      "source": [
        "print(tknz1.tokenize('日が射したり光が[MASK]とともる'))\n",
        "print(tknz2.tokenize('日が射したり光が[MASK]とともる'))"
      ],
      "id": "b376fa0c-dcf1-454f-9ee4-e4b8081c8ad8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee966ef5-f17d-4a23-8c87-9c4c06ee0bfc"
      },
      "outputs": [],
      "source": [
        "tknz1.ids_to_tokens[1]"
      ],
      "id": "ee966ef5-f17d-4a23-8c87-9c4c06ee0bfc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "857919f9-13c0-4b61-a449-6fd43a58f002"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "857919f9-13c0-4b61-a449-6fd43a58f002"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d15d2d53-9d4a-42de-b9a0-96de447aac1f"
      },
      "source": [
        "## 5.7 I BertForMaskedLM の利用 (新納2001, p.128)\n",
        "\n",
        "BERT モデルはMasked Language Model によっても学習が行われているので、BERT のモデル自体に MASK された単語を推定する機構が含まれています。\n",
        "MASK された単語を推定するには、モデルからその機構の部分を取り出さなければなりません。\n",
        "これを行うのが ``BertForMaskedLM`` です。\n",
        "例文「私は犬が好き。」の「犬」の部分を MASK した以下の単語列に対して、MASK の単語を推定してみます。\n",
        "\n",
        "> 例文： 私は [MASK] が好き。\n",
        "\n",
        "まずこの単語列をid 列に変換します(※5)\n",
        "\n",
        "```python\n",
        "ids = tknz.encode(\"私は[MASK]が好き。＂）\n",
        "ids\n",
        "[ 2, 1325, 9, 4, 14, 3596, 8, 3]\n",
        "```\n",
        "\n",
        "また、以下により [MASK] の位置を確認しておきます。\n",
        "\n",
        "```python\n",
        "mskpos = ids.index(tknz.mask_token_id)\n",
        "mskpos\n",
        "# 3\n",
        "```\n",
        "\n",
        "BERT モデルからの Masked Language Model の取り出しは以下のように行います。\n",
        "\n",
        "```python\n",
        "from transformers import BertForMaskedLM\n",
        "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese')\n",
        "```\n",
        "\n",
        "\n",
        "次にモデルに単語のid 列を与えると、各単語の位置に現れる単語の分布が得られます。\n",
        "```python\n",
        "x = torch.LongTensor(ids).unsqueeze(O)\n",
        "a = model(x)\n",
        "```\n",
        "\n",
        "上記に示したモデルからの出力 `a` は要素が 1つのタプルです。\n",
        "`a[O]` の形状は以下のとおりです。\n",
        "\n",
        "> ［ バッチサイズ， 単語列の長さ， 登録単語の数］\n",
        "\n",
        "この例の場合、1 データだけなのでバッチサイズは 1、単語列の長さは 8 です。\n",
        "そして登録単語の数はそのモデルの持つ登録単語数です。\n",
        "この登録単語数は `tknz.vocab_size` から参照できます。\n",
        "このモデルの場合は `32000` になっています。\n",
        "\n",
        "```python\n",
        "a[0].shape  # torch.Size([1, 8, 32000])\n",
        "tknz.vocab_size # 32000\n",
        "```\n",
        "\n",
        "<font size=\"+1\" color=\"green\"> 新納本(2021) 近藤先生から自炊版をもらった pdf では，Bert の出力が 3 連 tuple と記載されている (p.124) が実際は違う\n",
        "実際には 出力を .to_tuple() で tuple に変換しないといけない。</font>\n",
        "\n",
        "\n",
        "この例の場合、MASK の位置は `mskpos` だったので、たとえば k=100 番目の登録単語が MASK の位置に現れる程度（確率）は以下のコードで得られます。\n",
        "```python\n",
        "k = 100\n",
        "a[O][O][mskpos][k]\n",
        "# tensor(-5.5299, grad_fn=<SelectBackward>)\n",
        "```\n",
        "変数 `k` を 0 から 31999 まで動かして最も大きな値を持つ $\\hat{k}$ を求めれば、$\\hat{k}$ 番目の登録単語が MASK の位置に最も高い確率で現れる単語と推定できます。\n",
        "このようにベタに調べるよりも、torch には `topk` という便利なメソッドがあります。\n",
        "これはベクトルの要素の中からその値の高いものを上から順に K 個取り出すものです。\n",
        "以下のように使います。\n",
        "\n",
        "```ptyhon\n",
        "b = torch.topk(a[0][0[mskpos],k=5)\n",
        "b[0]  # 上位 5 つの値\n",
        "tensor([8.5864, 8.0724, 7.6974, 7.6480, 7.5863] ,...)\n",
        "b[1］ ＃ 上位 5 つの index\n",
        "tensor([1301, 1201, 705, 6968, 450])\n",
        "```"
      ],
      "id": "d15d2d53-9d4a-42de-b9a0-96de447aac1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af151b7e-08c5-4c9b-815f-95b639f29ef0"
      },
      "outputs": [],
      "source": [
        "idxs = tknz.encode('私は[MASK]が好き。')\n",
        "print(f'idxs :{idxs}')\n",
        "maskpos = idxs.index(tknz.mask_token_id)\n",
        "print(f'[MASK] の位置:{maskpos}')\n",
        "\n",
        "from transformers import BertForMaskedLM\n",
        "masked_lm = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese')\n",
        "\n",
        "x = torch.LongTensor(idxs).unsqueeze(0)\n",
        "a = masked_lm(x)\n",
        "\n",
        "print(a.to_tuple()[0].shape)  # torch.Size([1, 8, 32000])\n",
        "print(tknz.vocab_size) # 32000\n",
        "\n",
        "b = torch.topk(a.to_tuple()[0][0][maskpos],k=5)\n",
        "print(b[0])     # 上位 5 つの値\n",
        "# tensor([8.5864, 8.0724, 7.6974, 7.6480, 7.5863] ,...)\n",
        "\n",
        "print(b[1])     # 上位 5 つの index\n",
        "# tensor([1301, 1201, 705, 6968, 450])\n",
        "\n",
        "for idx in b[1].detach().numpy():\n",
        "    print('idx:', colored(f'{idx}', 'red' ,attrs=['bold']), 'トークン:', colored(f'{tknz.ids_to_tokens[idx]}','red', attrs=['bold']))\n",
        "\n",
        "for idx in b[1].detach().numpy():\n",
        "    print('idx:', colored(f'{idx}', 'red' ,attrs=['bold']), 'トークン:', colored(f'{tknz.ids_to_tokens[idx]}','red', attrs=['bold']))"
      ],
      "id": "af151b7e-08c5-4c9b-815f-95b639f29ef0"
    }
  ],
  "metadata": {
    "colab": {
      "name": "2021_1222onomatopea_bert_fine_turing.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}