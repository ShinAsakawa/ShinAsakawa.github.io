{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "2020SightVisit_RL_SARSA_Qlearning_demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020SightVisit_RL_SARSA_Qlearning_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG1EiY4_pnZH"
      },
      "source": [
        "# SARSA と Q 学習の比較\n",
        "\n",
        "- author: 浅川伸一\n",
        "- date: 2020-1009\n",
        "\n",
        "- title: Reinforcement learning: Temporal-Difference, SARSA, Q-Learning & Expected SARSA in python\n",
        "- author: Vaibhav Kumar\n",
        "- Date: May 9, 2019\n",
        "- Original: https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eN4LPhzpnZI"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--6zJiJ0pnZL"
      },
      "source": [
        "\"\"\"\n",
        "SARSA on policy learning python implementation.\n",
        "This is a python implementation of the SARSA algorithm in the Sutton and Barto's book on\n",
        "RL. It's called SARSA because - (state, action, reward, state, action). The only difference\n",
        "between SARSA and Qlearning is that SARSA takes the next action based on the current policy\n",
        "while qlearning takes the action with maximum utility of next state.\n",
        "Using the simplest gym environment for brevity: https://gym.openai.com/envs/FrozenLake-v0/\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def init_q(s, a, type=\"ones\"):\n",
        "    \"\"\" Q テーブルの初期化\n",
        "    @param s the number of states\n",
        "    @param a the number of actions\n",
        "    @param type random, ones or zeros for the initialization\n",
        "    \"\"\"\n",
        "    if type == \"ones\":\n",
        "        return np.ones((s, a))\n",
        "    elif type == \"random\":\n",
        "        return np.random.random((s, a))\n",
        "    elif type == \"zeros\":\n",
        "        return np.zeros((s, a))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKzhnnr5pnZO"
      },
      "source": [
        "def epsilon_greedy(Q, epsilon, n_actions, s, train=False):\n",
        "    \"\"\" イプシロン貪欲な行動選択の定義\n",
        "    @param Q Q values state x action -> value\n",
        "    @param epsilon for exploration\n",
        "    @param s number of states\n",
        "    @param train if true then no random actions selected\n",
        "    \"\"\"\n",
        "    if train or np.random.rand() < epsilon:\n",
        "        action = np.argmax(Q[s, :])\n",
        "    else:\n",
        "        action = np.random.randint(0, n_actions)\n",
        "    return action"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at77AG1bpnZQ"
      },
      "source": [
        "def sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests, render=False, test=False):\n",
        "    \"\"\"SARSA の定義\n",
        "    @param alpha learning rate\n",
        "    @param gamma decay factor\n",
        "    @param epsilon for exploration\n",
        "    @param max_steps for max step in each episode\n",
        "    @param n_tests number of test episodes\n",
        "    \"\"\"\n",
        "    #env = gym.make('Taxi-v2')\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        #print(f\"Episode: {episode}\")\n",
        "        total_reward = 0\n",
        "        s = env.reset()\n",
        "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "        t = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            s_, reward, done, info = env.step(a)\n",
        "            total_reward += reward\n",
        "            a_ = epsilon_greedy(Q, epsilon, n_actions, s_)\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_] ) - Q[s, a] )\n",
        "            s, a = s_, a_\n",
        "            if done:\n",
        "                if render:\n",
        "                    print(f\"This episode took {t} timesteps and reward {total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Y7AmtdpnZS"
      },
      "source": [
        "def qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, render=False, test=False):\n",
        "    \"\"\" Q 学習の定義\n",
        "    @param alpha learning rate\n",
        "    @param gamma decay factor\n",
        "    @param epsilon for exploration\n",
        "    @param max_steps for max step in each episode\n",
        "    @param n_tests number of test episodes\n",
        "    \"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        #print(f\"Episode: {episode}\")\n",
        "        s = env.reset()\n",
        "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "        t = 0\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            s_, reward, done, info = env.step(a)\n",
        "            total_reward += reward\n",
        "            a_ = np.argmax(Q[s_, :])\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_]) - Q[s, a] )\n",
        "            s, a = s_, a_\n",
        "            if done:\n",
        "                if render:\n",
        "                    print(f\"This episode took {t} timesteps and reward: {total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO6LUPfVpnZV"
      },
      "source": [
        "def test_agent(Q, env, n_tests, n_actions, delay=0.1):\n",
        "    for test in range(n_tests):\n",
        "        print(f\"Test #{test}\")\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        epsilon = 0\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            time.sleep(delay)\n",
        "            env.render()\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
        "            print(f\"Chose action {a} for state {s}\")\n",
        "            s, reward, done, info = env.step(a)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                print(f\"Episode reward: {total_reward}\")\n",
        "                time.sleep(1)\n",
        "                break"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6K5CeSdpnZX"
      },
      "source": [
        "alpha = 0.4\n",
        "gamma = 0.999\n",
        "epsilon = 0.9\n",
        "episodes = 3000\n",
        "episodes = 1000\n",
        "max_steps = 2500\n",
        "max_steps = 1000\n",
        "n_tests = 20"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOeh-t2DpnZZ"
      },
      "source": [
        "PGs = {'sarsa': {'pg': sarsa}, 'Qlearning':{'pg': qlearning}}\n",
        "for k, pg in PGs.items():\n",
        "    PGs[k]['timestep_reward'] = pg['pg'](alpha, gamma, epsilon, episodes, max_steps, n_tests, render=False, test=False)\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOloP8PQpnZd"
      },
      "source": [
        "#print(timestep_reward[-30:])\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#len(PGs['Qlearning']['timestep_reward'])\n",
        "plt.plot(range(600), PGs['Qlearning']['timestep_reward'][:600]); plt.show()\n",
        "plt.plot(range(600), PGs['sarsa']['timestep_reward'][:600]); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhEmQxpA0nm7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}