{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "2020-0809mlm_task.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020_0809mlm_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZVr_PdSUSNk",
        "colab_type": "text"
      },
      "source": [
        "- https://github.com/pytorch/text/blob/master/examples/BERT/mlm_task.py\n",
        "- source: mlm_task.py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6uXSAjFUlsx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "8f17fa75-0904-4158-f6b4-f89f85d6763c"
      },
      "source": [
        "!pip install torchtext --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/f9/224b3893ab11d83d47fde357a7dcc75f00ba219f34f3d15e06fe4cb62e05/torchtext-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5MB 11.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 52.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.6.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.91 torchtext-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfE5usHdUSNl",
        "colab_type": "text"
      },
      "source": [
        "# torchtext による BERT\n",
        "\n",
        "この例では PyTorch と torchtext のみで BERT モデルを訓練する方法を示しています。\n",
        "加えて，その後 Q and A 課題用の 事前訓練済 BERT をファインチューニングの方法を示します。\n",
        "\n",
        "## 事前訓練済 BERT の生成\n",
        "\n",
        "マスク化言語モデル課題と次文予測課題 で BERT モデルを訓練します。 ローカル GPU または CPU 上で以下を実行します。\n",
        "\n",
        "```bash\n",
        "python mlm_task.py\n",
        "python ns_task.py\n",
        "```\n",
        "\n",
        "mlm_task の錯乱度 (ppl) 最終結果は訓練データセットで 18.97899. \n",
        "ns_task の損失関数は訓練データセットで 0.05446 です。\n",
        "\n",
        "### Q and A 課題のための訓練済 BERT モデルのファインチューニング\n",
        "\n",
        "SQuAD (スタフォード大学による Q and A ) データセットを用いて，訓練済 BERT モデルによる Q and A 課題:\n",
        "\n",
        "```bash\n",
        "python qa_task.py --bert-model ns_bert.pt --epochs 30\n",
        "```\n",
        "\n",
        "訓練済 BERT と vocab は以下から利用可能です:\n",
        "\n",
        "* [bert_vocab.pt](https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/bert_vocab.pt)\n",
        "* [mlm_bert.pt](https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/mlm_bert.pt)\n",
        "* [ns_bert.pt](https://pytorch.s3.amazonaws.com/models/text/torchtext_bert_example/ns_bert.pt)\n",
        "\n",
        "訓練/検証/テスト の例は以下のとおりです:\n",
        "\n",
        "```bash\n",
        "    | epoch   1 |   200/ 1055 batches | lr 5.00000 | ms/batch 746.33 | loss  3.70 | ppl    40.45\n",
        "    | epoch   1 |   400/ 1055 batches | lr 5.00000 | ms/batch 746.78 | loss  3.06 | ppl    21.25\n",
        "    | epoch   1 |   600/ 1055 batches | lr 5.00000 | ms/batch 746.83 | loss  2.84 | ppl    17.15\n",
        "    | epoch   1 |   800/ 1055 batches | lr 5.00000 | ms/batch 746.55 | loss  2.69 | ppl    14.73\n",
        "    | epoch   1 |  1000/ 1055 batches | lr 5.00000 | ms/batch 745.48 | loss  2.55 | ppl    12.85\n",
        "    -----------------------------------------------------------------------------------------\n",
        "    | end of epoch   1 | time: 821.25s | valid loss  2.33 | exact   40.052% | f1   52.595%\n",
        "    -----------------------------------------------------------------------------------------\n",
        "...\n",
        "    -----------------------------------------------------------------------------------------\n",
        "    | epoch  10 |   200/ 1055 batches | lr 0.00500 | ms/batch 749.25 | loss  1.25 | ppl     3.50\n",
        "    | epoch  10 |   400/ 1055 batches | lr 0.00500 | ms/batch 745.81 | loss  1.24 | ppl     3.47\n",
        "    | epoch  10 |   600/ 1055 batches | lr 0.00500 | ms/batch 744.89 | loss  1.26 | ppl     3.51\n",
        "    | epoch  10 |   800/ 1055 batches | lr 0.00500 | ms/batch 746.02 | loss  1.23 | ppl     3.42\n",
        "    | epoch  10 |  1000/ 1055 batches | lr 0.00500 | ms/batch 746.61 | loss  1.25 | ppl     3.50\n",
        "    -----------------------------------------------------------------------------------------\n",
        "    | end of epoch  10 | time: 821.85s | valid loss  2.05 | exact   51.648% | f1   63.811%\n",
        "    -----------------------------------------------------------------------------------------\n",
        "    =========================================================================================\n",
        "    | End of training | test loss  2.05 | exact   51.337% | f1   63.645%\n",
        "    =========================================================================================\n",
        "\n",
        "```\n",
        "\n",
        "## サンプルファイルの説明\n",
        "\n",
        "### model.py\n",
        "\n",
        "<!--\n",
        "This file defines the Transformer and MultiheadAttention models used for BERT. \n",
        "The embedding layer include PositionalEncoding and TokenTypeEncoding layers. MLMTask, NextSentenceTask, and QuestionAnswerTask are the models for the three tasks mentioned above.\n",
        "-->\n",
        "\n",
        "このファイルは BERT で使用される `Transformer` と `MultiheadAttention` モデルを定義している。\n",
        "埋め込み層には `PositionalEncoding` と `TokenTypeEncoding` 層が含まれる。\n",
        "MLMTask (マスク化言語モデル), NextSentenceTask(次文予測課題), QuestionAnswerTask（Q and A 課題） は 上述の3つのタスクのモデルである。\n",
        "\n",
        "### data.py\n",
        "\n",
        "<!--\n",
        "This file provides a few datasets required to train the BERT model and question-answer task. \n",
        "Please note that BookCorpus dataset is not available publicly.\n",
        "-->\n",
        "\n",
        "このファイルは BERT モデルと Q and A課題を訓練するために必要なデータセットをいくつか提供します。\n",
        "BookCorpus のデータセットは公開されていないことに注意してください。\n",
        "\n",
        "\n",
        "### mlm_task.py, ns_task.py, qa_task.py\n",
        "\n",
        "<!--\n",
        "Those three files define the train/valid/test process for the tasks.\n",
        "-->\n",
        "これらの 3 つのファイルは 課題の訓練/検証/テストのプロセスを定義します。\n",
        "\n",
        "\n",
        "### metrics.py\n",
        "\n",
        "<!--This file provides two metrics (F1 and exact score) for question-answer task-->\n",
        "\n",
        "このファイルは Q and A 課題 の 2 つの尺度 (F1 と精度) を提供します。\n",
        "\n",
        "\n",
        "### utils.py\n",
        "\n",
        "<!--This file provides a few utils used by the three tasks.-->\n",
        "このファイルは 3 つ課題で 使用されるいくつかのユーティリティを提供します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSRspiuAUSNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import argparse\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from model import MLMTask\n",
        "#from utils import run_demo, run_ddp, wrap_up\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS0mzURzUSNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Dropout, LayerNorm, TransformerEncoder\n",
        "from torchtext.nn import MultiheadAttentionContainer, InProjContainer, ScaledDotProduct\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        S, N = x.size()\n",
        "        pos = torch.arange(S,\n",
        "                           dtype=torch.long,\n",
        "                           device=x.device).unsqueeze(0).expand((N, S)).t()\n",
        "        return self.pos_embedding(pos)\n",
        "\n",
        "\n",
        "class TokenTypeEncoding(nn.Module):\n",
        "    def __init__(self, type_token_num, d_model):\n",
        "        super(TokenTypeEncoding, self).__init__()\n",
        "        self.token_type_embeddings = nn.Embedding(type_token_num, d_model)\n",
        "\n",
        "    def forward(self, seq_input, token_type_input):\n",
        "        S, N = seq_input.size()\n",
        "        if token_type_input is None:\n",
        "            token_type_input = torch.zeros((S, N),\n",
        "                                           dtype=torch.long,\n",
        "                                           device=seq_input.device)\n",
        "        return self.token_type_embeddings(token_type_input)\n",
        "\n",
        "\n",
        "class BertEmbedding(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, dropout=0.5):\n",
        "        super(BertEmbedding, self).__init__()\n",
        "        self.ninp = ninp\n",
        "        self.ntoken = ntoken\n",
        "        self.pos_embed = PositionalEncoding(ninp)\n",
        "        self.embed = nn.Embedding(ntoken, ninp)\n",
        "        self.tok_type_embed = TokenTypeEncoding(2, ninp)  # Two sentence type\n",
        "        self.norm = LayerNorm(ninp)\n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, token_type_input):\n",
        "        src = self.embed(src) + self.pos_embed(src) \\\n",
        "            + self.tok_type_embed(src, token_type_input)\n",
        "        return self.dropout(self.norm(src))\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048,\n",
        "                 dropout=0.1, activation=\"gelu\"):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        in_proj_container = InProjContainer(Linear(d_model, d_model),\n",
        "                                            Linear(d_model, d_model),\n",
        "                                            Linear(d_model, d_model))\n",
        "        self.mha = MultiheadAttentionContainer(nhead, in_proj_container,\n",
        "                                               ScaledDotProduct(), Linear(d_model, d_model))\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        if activation == \"relu\":\n",
        "            self.activation = F.relu\n",
        "        elif activation == \"gelu\":\n",
        "            self.activation = F.gelu\n",
        "        else:\n",
        "            raise RuntimeError(\"only relu/gelu are supported, not {}\".format(activation))\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.mha.in_proj_container.query_proj.init_weights()\n",
        "        self.mha.in_proj_container.key_proj.init_weights()\n",
        "        self.mha.in_proj_container.value_proj.init_weights()\n",
        "        self.mha.out_proj.init_weights()\n",
        "        self.linear1.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        self.linear2.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        self.norm1.bias.data.zero_()\n",
        "        self.norm1.weight.data.fill_(1.0)\n",
        "        self.norm2.bias.data.zero_()\n",
        "        self.norm2.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        attn_output, attn_output_weights = self.mha(src, src, src, attn_mask=src_mask)\n",
        "        src = src + self.dropout1(attn_output)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "\n",
        "class BertModel(nn.Module):\n",
        "    \"\"\"Contain a transformer encoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(BertModel, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.bert_embed = BertEmbedding(ntoken, ninp)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.ninp = ninp\n",
        "\n",
        "    def forward(self, src, token_type_input):\n",
        "        src = self.bert_embed(src, token_type_input)\n",
        "        output = self.transformer_encoder(src)\n",
        "        return output\n",
        "\n",
        "    \n",
        "class MLMTask(nn.Module):\n",
        "    \"\"\"Contain a transformer encoder plus MLM head.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(MLMTask, self).__init__()\n",
        "        self.bert_model = BertModel(ntoken, ninp, nhead, nhid, nlayers, dropout=0.5)\n",
        "        self.mlm_span = Linear(ninp, ninp)\n",
        "        self.activation = F.gelu\n",
        "        self.norm_layer = LayerNorm(ninp, eps=1e-12)\n",
        "        self.mlm_head = Linear(ninp, ntoken)\n",
        "\n",
        "    def forward(self, src, token_type_input=None):\n",
        "        src = src.transpose(0, 1)  # Wrap up by nn.DataParallel\n",
        "        output = self.bert_model(src, token_type_input)\n",
        "        output = self.mlm_span(output)\n",
        "        output = self.activation(output)\n",
        "        output = self.norm_layer(output)\n",
        "        output = self.mlm_head(output)\n",
        "        return output\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y8kN1tYUSNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from util.py\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import os\n",
        "import torch.multiprocessing as mp\n",
        "import math\n",
        "\n",
        "#run_demo, run_ddp, wrap_up\n",
        "\n",
        "def run_demo(demo_fn, main_fn, args):\n",
        "    mp.spawn(demo_fn,\n",
        "             args=(main_fn, args,),\n",
        "             nprocs=args.world_size,\n",
        "             join=True)\n",
        "\n",
        "    \n",
        "def run_ddp(rank, main_fn, args):\n",
        "    setup(rank, args.world_size, args.seed)\n",
        "    main_fn(args, rank)\n",
        "    cleanup()\n",
        "\n",
        "def print_loss_log(file_name, train_loss, val_loss, test_loss, args=None):\n",
        "    with open(file_name, 'w') as f:\n",
        "        if args:\n",
        "            for item in args.__dict__:\n",
        "                f.write(item + ':    ' + str(args.__dict__[item]) + '\\n')\n",
        "        for idx in range(len(train_loss)):\n",
        "            f.write('epoch {:3d} | train loss {:8.5f}'.format(idx + 1,\n",
        "                                                              train_loss[idx]) + '\\n')\n",
        "        for idx in range(len(val_loss)):\n",
        "            f.write('epoch {:3d} | val loss {:8.5f}'.format(idx + 1,\n",
        "                                                            val_loss[idx]) + '\\n')\n",
        "        f.write('test loss {:8.5f}'.format(test_loss) + '\\n')\n",
        "\n",
        "\n",
        "def wrap_up(train_loss_log, val_loss_log, test_loss, args, model, ns_loss_log, model_filename):\n",
        "    print('=' * 89)\n",
        "    print('| End of training | test loss {:8.5f} | test ppl {:8.5f}'.format(test_loss, math.exp(test_loss)))\n",
        "    print('=' * 89)\n",
        "    print_loss_log(ns_loss_log, train_loss_log, val_loss_log, test_loss)\n",
        "    with open(args.save, 'wb') as f:\n",
        "        torch.save(model.bert_model.state_dict(), f)\n",
        "    with open(model_filename, 'wb') as f:\n",
        "        torch.save(model.state_dict(), f)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVwzQYHgUSNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_batch(batch_data, args, mask_id, cls_id):\n",
        "    batch_data = torch.tensor(batch_data).long().view(args.batch_size, -1).t().contiguous()\n",
        "    # Generate masks with args.mask_frac\n",
        "    data_len = batch_data.size(0)\n",
        "    ones_num = int(data_len * args.mask_frac)\n",
        "    zeros_num = data_len - ones_num\n",
        "    lm_mask = torch.cat([torch.zeros(zeros_num), torch.ones(ones_num)])\n",
        "    lm_mask = lm_mask[torch.randperm(data_len)]\n",
        "    batch_data = torch.cat((torch.tensor([[cls_id] * batch_data.size(1)]).long(), batch_data))\n",
        "    lm_mask = torch.cat((torch.tensor([0.0]), lm_mask))\n",
        "\n",
        "    targets = torch.stack([batch_data[i] for i in range(lm_mask.size(0)) if lm_mask[i]]).view(-1)\n",
        "    batch_data = batch_data.masked_fill(lm_mask.bool().unsqueeze(1), mask_id)\n",
        "    return batch_data, lm_mask, targets\n",
        "\n",
        "\n",
        "def process_raw_data(raw_data, args):\n",
        "    _num = raw_data.size(0) // (args.batch_size * args.bptt)\n",
        "    raw_data = raw_data[:(_num * args.batch_size * args.bptt)]\n",
        "    return raw_data\n",
        "\n",
        "\n",
        "def evaluate(data_source, model, vocab, ntokens, criterion, args, device):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    mask_id = vocab.stoi['<MASK>']\n",
        "    cls_id = vocab.stoi['<cls>']\n",
        "    dataloader = DataLoader(data_source, batch_size=args.batch_size * args.bptt,\n",
        "                            shuffle=False, collate_fn=lambda b: collate_batch(b, args, mask_id, cls_id))\n",
        "    with torch.no_grad():\n",
        "        for batch, (data, lm_mask, targets) in enumerate(dataloader):\n",
        "            if args.parallel == 'DDP':\n",
        "                data = data.to(device[0])\n",
        "                targets = targets.to(device[0])\n",
        "            else:\n",
        "                data = data.to(device)\n",
        "                targets = targets.to(device)\n",
        "            data = data.transpose(0, 1)  # Wrap up by DDP or DataParallel\n",
        "            output = model(data)\n",
        "            output = torch.stack([output[i] for i in range(lm_mask.size(0)) if lm_mask[i]])\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += criterion(output_flat, targets).item()\n",
        "    return total_loss / ((len(data_source) - 1) / args.bptt / args.batch_size)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkB6wQXRUSN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, vocab, train_loss_log, train_data,\n",
        "          optimizer, criterion, ntokens, epoch, scheduler, args, device, rank=None):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    mask_id = vocab.stoi['<MASK>']\n",
        "    cls_id = vocab.stoi['<cls>']\n",
        "    train_loss_log.append(0.0)\n",
        "    dataloader = DataLoader(train_data, batch_size=args.batch_size * args.bptt,\n",
        "                            shuffle=False, collate_fn=lambda b: collate_batch(b, args, mask_id, cls_id))\n",
        "\n",
        "    for batch, (data, lm_mask, targets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        if args.parallel == 'DDP':\n",
        "            data = data.to(device[0])\n",
        "            targets = targets.to(device[0])\n",
        "        else:\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "        data = data.transpose(0, 1)  # Wrap up by DDP or DataParallel\n",
        "        output = model(data)\n",
        "        output = torch.stack([output[i] for i in range(lm_mask.size(0)) if lm_mask[i]])\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if batch % args.log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / args.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            if (rank is None) or rank == 0:\n",
        "                train_loss_log[-1] = cur_loss\n",
        "                print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | ms/batch {:5.2f} | '\n",
        "                      'loss {:5.2f} | ppl {:8.2f}'.format(epoch, batch,\n",
        "                                                          len(train_data) // (args.bptt * args.batch_size),\n",
        "                                                          scheduler.get_last_lr()[0],\n",
        "                                                          elapsed * 1000 / args.log_interval,\n",
        "                                                          cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KwC6hXfUSN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_main(args, rank=None):\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.parallel == 'DDP':\n",
        "        n = torch.cuda.device_count() // args.world_size\n",
        "        device = list(range(rank * n, (rank + 1) * n))\n",
        "    else:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    import torchtext\n",
        "    if args.dataset == 'WikiText103':\n",
        "        from torchtext.experimental.datasets import WikiText103 as WLMDataset\n",
        "    elif args.dataset == 'WikiText2':\n",
        "        from torchtext.experimental.datasets import WikiText2 as WLMDataset\n",
        "    elif args.dataset == 'WMTNewsCrawl':\n",
        "        from data import WMTNewsCrawl as WLMDataset\n",
        "    elif args.dataset == 'EnWik9':\n",
        "        from torchtext.datasets import EnWik9\n",
        "    elif args.dataset == 'BookCorpus':\n",
        "        from data import BookCorpus\n",
        "    else:\n",
        "        print(\"dataset for MLM task is not supported\")\n",
        "\n",
        "    try:\n",
        "        vocab = torch.load(args.save_vocab)\n",
        "    except:\n",
        "        train_dataset, test_dataset, valid_dataset = WLMDataset()\n",
        "        old_vocab = train_dataset.vocab\n",
        "        vocab = torchtext.vocab.Vocab(counter=old_vocab.freqs,\n",
        "                                      specials=['<unk>', '<pad>', '<MASK>'])\n",
        "        with open(args.save_vocab, 'wb') as f:\n",
        "            torch.save(vocab, f)\n",
        "\n",
        "    if args.dataset == 'WikiText103' or args.dataset == 'WikiText2':\n",
        "        train_dataset, test_dataset, valid_dataset = WLMDataset(vocab=vocab)\n",
        "    elif args.dataset == 'WMTNewsCrawl':\n",
        "        from torchtext.experimental.datasets import WikiText2\n",
        "        test_dataset, valid_dataset = WikiText2(vocab=vocab, data_select=('test', 'valid'))\n",
        "        train_dataset, = WLMDataset(vocab=vocab, data_select='train')\n",
        "    elif args.dataset == 'EnWik9':\n",
        "        enwik9 = EnWik9()\n",
        "        idx1, idx2 = int(len(enwik9) * 0.8), int(len(enwik9) * 0.9)\n",
        "        train_data = torch.tensor([vocab.stoi[_id]\n",
        "                                  for _id in enwik9[0:idx1]]).long()\n",
        "        val_data = torch.tensor([vocab.stoi[_id]\n",
        "                                 for _id in enwik9[idx1:idx2]]).long()\n",
        "        test_data = torch.tensor([vocab.stoi[_id]\n",
        "                                 for _id in enwik9[idx2:]]).long()\n",
        "        from torchtext.experimental.datasets import LanguageModelingDataset\n",
        "        train_dataset = LanguageModelingDataset(train_data, vocab)\n",
        "        valid_dataset = LanguageModelingDataset(val_data, vocab)\n",
        "        test_dataset = LanguageModelingDataset(test_data, vocab)\n",
        "    elif args.dataset == 'BookCorpus':\n",
        "        train_dataset, test_dataset, valid_dataset = BookCorpus(vocab)\n",
        "\n",
        "\n",
        "    train_data = process_raw_data(train_dataset.data, args)\n",
        "    if rank is not None:\n",
        "        # Chunk training data by rank for different gpus\n",
        "        chunk_len = len(train_data) // args.world_size\n",
        "        train_data = train_data[(rank * chunk_len):((rank + 1) * chunk_len)]\n",
        "    val_data = process_raw_data(valid_dataset.data, args)\n",
        "    test_data = process_raw_data(test_dataset.data, args)\n",
        "\n",
        "    ntokens = len(train_dataset.get_vocab())\n",
        "    if args.checkpoint != 'None':\n",
        "        model = torch.load(args.checkpoint)\n",
        "    else:\n",
        "        model = MLMTask(ntokens, args.emsize, args.nhead, args.nhid, args.nlayers, args.dropout)\n",
        "    if args.parallel == 'DDP':\n",
        "        model = model.to(device[0])\n",
        "        model = DDP(model, device_ids=device)\n",
        "    else:\n",
        "        model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "    best_val_loss = None\n",
        "    train_loss_log, val_loss_log = [], []\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(model, train_dataset.vocab, train_loss_log, train_data,\n",
        "              optimizer, criterion, ntokens, epoch, scheduler, args, device, rank)\n",
        "        val_loss = evaluate(val_data, model, train_dataset.vocab, ntokens, criterion, args, device)\n",
        "        if (rank is None) or (rank == 0):\n",
        "            val_loss_log.append(val_loss)\n",
        "            print('-' * 89)\n",
        "            print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                  'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                             val_loss, math.exp(val_loss)))\n",
        "            print('-' * 89)\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            if rank is None:\n",
        "                with open(args.save, 'wb') as f:\n",
        "                    torch.save(model, f)\n",
        "            elif rank == 0:\n",
        "                with open(args.save, 'wb') as f:\n",
        "                    torch.save(model.state_dict(), f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            scheduler.step()\n",
        "    if args.parallel == 'DDP':\n",
        "        dist.barrier()\n",
        "        rank0_devices = [x - rank * len(device) for x in device]\n",
        "        device_pairs = zip(rank0_devices, device)\n",
        "        map_location = {'cuda:%d' % x: 'cuda:%d' % y for x, y in device_pairs}\n",
        "        model.load_state_dict(\n",
        "            torch.load(args.save, map_location=map_location))\n",
        "        test_loss = evaluate(test_data, model, train_dataset.vocab, ntokens, criterion, args, device)\n",
        "        if rank == 0:\n",
        "            wrap_up(train_loss_log, val_loss_log, test_loss, args, model.module, 'mlm_loss.txt', 'full_mlm_model.pt')\n",
        "    else:\n",
        "        with open(args.save, 'rb') as f:\n",
        "            model = torch.load(f)\n",
        "        test_loss = evaluate(test_data, model, train_dataset.vocab, ntokens, criterion, args, device)\n",
        "        wrap_up(train_loss_log, val_loss_log, test_loss, args, model, 'mlm_loss.txt', 'full_mlm_model.pt')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9BoKywRUSN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 Transformer Language Model')\n",
        "#args = parser.parse_args()\n",
        "#help(argparse)\n",
        "args = parser.parse_args(args=[])\n",
        "#help(parser)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TLZZXjUUSN_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39f1e8c2-95ea-4741-a723-f6fe63662c4b"
      },
      "source": [
        "import argparse\n",
        "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 Transformer Language Model')\n",
        "print('PyTorch Wikitext-2 Transformer Language Model')\n",
        "args.emsize=768  # size of word embeddings'\n",
        "args.emsize=8  # size of word embeddings'\n",
        "\n",
        "args.nhid=3072  #  number of hidden units per layer\n",
        "args.nhid=12  #  number of hidden units per layer\n",
        "\n",
        "args.nlayers=12  # number of layers'\n",
        "args.nlayers=1  # number of layers'\n",
        "\n",
        "args.nhead=12  # the number of heads in the encoder/decoder of the transformer model\n",
        "args.nhead=2  # the number of heads in the encoder/decoder of the transformer model\n",
        "\n",
        "args.lr=6.  # initial learning rate\n",
        "args.clip=0.1  # gradient clipping\n",
        "\n",
        "args.epochs=8  # upper epoch limit\n",
        "args.epochs=2  # upper epoch limit\n",
        "\n",
        "args.batch_size=32  # batch size\n",
        "args.bptt=128  # sequence length\n",
        "args.dropout=0.2  #  dropout applied to layers (0 = no dropout)\n",
        "args.seed=5431916812  # random seed\n",
        "args.log_interval=10  # report interval\n",
        "args.checkpoint='None'  # path to load the checkpoint\n",
        "\n",
        "args.save='mlm_bert.pt'   # path to save the final model\n",
        "args.save='2020-0809mlm_bert.pt'   # path to save the final model\n",
        "\n",
        "args.save_vocab='torchtext_bert_vocab.pt' # path to save the vocab\n",
        "args.save_vocab='2020-0809torchtext_bert_vocab.pt' # path to save the vocab\n",
        "args.mask_frac=0.15  # the fraction of masked tokens\n",
        "args.dataset='WikiText2'  # dataset used for MLM task\n",
        "args.parallel=None  # Use DataParallel to train model\n",
        "args.world_size=8  # the world size to initiate DPP\n",
        "\n",
        "if args.parallel == 'DDP':\n",
        "    run_demo(run_ddp, run_main, args)\n",
        "else:\n",
        "    run_main(args)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Wikitext-2 Transformer Language Model\n",
            "| epoch   1 |    10/  500 batches | lr 6.00000 | ms/batch 192.56 | loss 10.65 | ppl 42104.02\n",
            "| epoch   1 |    20/  500 batches | lr 6.00000 | ms/batch 176.32 | loss  8.46 | ppl  4703.71\n",
            "| epoch   1 |    30/  500 batches | lr 6.00000 | ms/batch 177.82 | loss  8.11 | ppl  3329.03\n",
            "| epoch   1 |    40/  500 batches | lr 6.00000 | ms/batch 180.51 | loss  8.17 | ppl  3534.03\n",
            "| epoch   1 |    50/  500 batches | lr 6.00000 | ms/batch 175.40 | loss  7.81 | ppl  2472.50\n",
            "| epoch   1 |    60/  500 batches | lr 6.00000 | ms/batch 174.20 | loss  7.88 | ppl  2656.53\n",
            "| epoch   1 |    70/  500 batches | lr 6.00000 | ms/batch 176.90 | loss  7.84 | ppl  2547.20\n",
            "| epoch   1 |    80/  500 batches | lr 6.00000 | ms/batch 180.10 | loss  7.78 | ppl  2393.71\n",
            "| epoch   1 |    90/  500 batches | lr 6.00000 | ms/batch 174.61 | loss  7.79 | ppl  2422.58\n",
            "| epoch   1 |   100/  500 batches | lr 6.00000 | ms/batch 175.20 | loss  7.64 | ppl  2084.21\n",
            "| epoch   1 |   110/  500 batches | lr 6.00000 | ms/batch 174.40 | loss  7.72 | ppl  2260.19\n",
            "| epoch   1 |   120/  500 batches | lr 6.00000 | ms/batch 179.02 | loss  7.71 | ppl  2235.99\n",
            "| epoch   1 |   130/  500 batches | lr 6.00000 | ms/batch 176.55 | loss  7.66 | ppl  2130.34\n",
            "| epoch   1 |   140/  500 batches | lr 6.00000 | ms/batch 174.95 | loss  7.67 | ppl  2138.35\n",
            "| epoch   1 |   150/  500 batches | lr 6.00000 | ms/batch 176.51 | loss  7.56 | ppl  1917.79\n",
            "| epoch   1 |   160/  500 batches | lr 6.00000 | ms/batch 180.11 | loss  7.54 | ppl  1872.92\n",
            "| epoch   1 |   170/  500 batches | lr 6.00000 | ms/batch 177.98 | loss  7.62 | ppl  2046.16\n",
            "| epoch   1 |   180/  500 batches | lr 6.00000 | ms/batch 174.96 | loss  7.51 | ppl  1831.43\n",
            "| epoch   1 |   190/  500 batches | lr 6.00000 | ms/batch 174.66 | loss  7.63 | ppl  2066.29\n",
            "| epoch   1 |   200/  500 batches | lr 6.00000 | ms/batch 175.76 | loss  7.50 | ppl  1803.95\n",
            "| epoch   1 |   210/  500 batches | lr 6.00000 | ms/batch 180.44 | loss  7.45 | ppl  1718.25\n",
            "| epoch   1 |   220/  500 batches | lr 6.00000 | ms/batch 175.78 | loss  7.48 | ppl  1766.51\n",
            "| epoch   1 |   230/  500 batches | lr 6.00000 | ms/batch 175.67 | loss  7.43 | ppl  1683.29\n",
            "| epoch   1 |   240/  500 batches | lr 6.00000 | ms/batch 175.19 | loss  7.63 | ppl  2050.81\n",
            "| epoch   1 |   250/  500 batches | lr 6.00000 | ms/batch 180.67 | loss  7.37 | ppl  1588.52\n",
            "| epoch   1 |   260/  500 batches | lr 6.00000 | ms/batch 176.39 | loss  7.55 | ppl  1899.82\n",
            "| epoch   1 |   270/  500 batches | lr 6.00000 | ms/batch 175.97 | loss  7.44 | ppl  1706.01\n",
            "| epoch   1 |   280/  500 batches | lr 6.00000 | ms/batch 177.11 | loss  7.42 | ppl  1665.43\n",
            "| epoch   1 |   290/  500 batches | lr 6.00000 | ms/batch 181.89 | loss  7.37 | ppl  1589.43\n",
            "| epoch   1 |   300/  500 batches | lr 6.00000 | ms/batch 175.13 | loss  7.39 | ppl  1613.84\n",
            "| epoch   1 |   310/  500 batches | lr 6.00000 | ms/batch 176.41 | loss  7.35 | ppl  1553.69\n",
            "| epoch   1 |   320/  500 batches | lr 6.00000 | ms/batch 175.71 | loss  7.36 | ppl  1570.92\n",
            "| epoch   1 |   330/  500 batches | lr 6.00000 | ms/batch 180.95 | loss  7.35 | ppl  1557.56\n",
            "| epoch   1 |   340/  500 batches | lr 6.00000 | ms/batch 175.64 | loss  7.35 | ppl  1552.04\n",
            "| epoch   1 |   350/  500 batches | lr 6.00000 | ms/batch 177.90 | loss  7.34 | ppl  1535.41\n",
            "| epoch   1 |   360/  500 batches | lr 6.00000 | ms/batch 175.12 | loss  7.35 | ppl  1553.41\n",
            "| epoch   1 |   370/  500 batches | lr 6.00000 | ms/batch 181.93 | loss  7.34 | ppl  1536.66\n",
            "| epoch   1 |   380/  500 batches | lr 6.00000 | ms/batch 175.83 | loss  7.28 | ppl  1451.17\n",
            "| epoch   1 |   390/  500 batches | lr 6.00000 | ms/batch 178.47 | loss  7.30 | ppl  1480.47\n",
            "| epoch   1 |   400/  500 batches | lr 6.00000 | ms/batch 175.23 | loss  7.38 | ppl  1602.10\n",
            "| epoch   1 |   410/  500 batches | lr 6.00000 | ms/batch 180.67 | loss  7.18 | ppl  1309.01\n",
            "| epoch   1 |   420/  500 batches | lr 6.00000 | ms/batch 176.10 | loss  7.27 | ppl  1442.41\n",
            "| epoch   1 |   430/  500 batches | lr 6.00000 | ms/batch 175.15 | loss  7.25 | ppl  1413.16\n",
            "| epoch   1 |   440/  500 batches | lr 6.00000 | ms/batch 177.50 | loss  7.22 | ppl  1369.64\n",
            "| epoch   1 |   450/  500 batches | lr 6.00000 | ms/batch 179.69 | loss  7.30 | ppl  1481.01\n",
            "| epoch   1 |   460/  500 batches | lr 6.00000 | ms/batch 178.25 | loss  7.25 | ppl  1402.19\n",
            "| epoch   1 |   470/  500 batches | lr 6.00000 | ms/batch 176.21 | loss  7.30 | ppl  1475.11\n",
            "| epoch   1 |   480/  500 batches | lr 6.00000 | ms/batch 175.94 | loss  7.31 | ppl  1491.69\n",
            "| epoch   1 |   490/  500 batches | lr 6.00000 | ms/batch 181.19 | loss  7.18 | ppl  1308.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 89.50s | valid loss  7.02 | valid ppl  1123.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    10/  500 batches | lr 6.00000 | ms/batch 192.09 | loss  7.92 | ppl  2739.66\n",
            "| epoch   2 |    20/  500 batches | lr 6.00000 | ms/batch 176.00 | loss  7.26 | ppl  1420.88\n",
            "| epoch   2 |    30/  500 batches | lr 6.00000 | ms/batch 180.62 | loss  7.20 | ppl  1341.47\n",
            "| epoch   2 |    40/  500 batches | lr 6.00000 | ms/batch 175.91 | loss  7.20 | ppl  1335.36\n",
            "| epoch   2 |    50/  500 batches | lr 6.00000 | ms/batch 175.09 | loss  7.09 | ppl  1202.10\n",
            "| epoch   2 |    60/  500 batches | lr 6.00000 | ms/batch 175.46 | loss  7.20 | ppl  1339.64\n",
            "| epoch   2 |    70/  500 batches | lr 6.00000 | ms/batch 181.39 | loss  7.26 | ppl  1417.29\n",
            "| epoch   2 |    80/  500 batches | lr 6.00000 | ms/batch 175.28 | loss  7.25 | ppl  1406.80\n",
            "| epoch   2 |    90/  500 batches | lr 6.00000 | ms/batch 175.12 | loss  7.15 | ppl  1275.31\n",
            "| epoch   2 |   100/  500 batches | lr 6.00000 | ms/batch 177.10 | loss  7.20 | ppl  1337.13\n",
            "| epoch   2 |   110/  500 batches | lr 6.00000 | ms/batch 180.55 | loss  7.23 | ppl  1379.65\n",
            "| epoch   2 |   120/  500 batches | lr 6.00000 | ms/batch 175.78 | loss  7.17 | ppl  1303.28\n",
            "| epoch   2 |   130/  500 batches | lr 6.00000 | ms/batch 175.19 | loss  7.20 | ppl  1335.44\n",
            "| epoch   2 |   140/  500 batches | lr 6.00000 | ms/batch 175.99 | loss  7.32 | ppl  1513.86\n",
            "| epoch   2 |   150/  500 batches | lr 6.00000 | ms/batch 179.56 | loss  7.13 | ppl  1254.90\n",
            "| epoch   2 |   160/  500 batches | lr 6.00000 | ms/batch 175.23 | loss  7.16 | ppl  1290.39\n",
            "| epoch   2 |   170/  500 batches | lr 6.00000 | ms/batch 176.41 | loss  7.22 | ppl  1361.96\n",
            "| epoch   2 |   180/  500 batches | lr 6.00000 | ms/batch 175.76 | loss  7.14 | ppl  1265.14\n",
            "| epoch   2 |   190/  500 batches | lr 6.00000 | ms/batch 180.52 | loss  7.20 | ppl  1342.91\n",
            "| epoch   2 |   200/  500 batches | lr 6.00000 | ms/batch 175.22 | loss  7.16 | ppl  1286.83\n",
            "| epoch   2 |   210/  500 batches | lr 6.00000 | ms/batch 176.56 | loss  7.23 | ppl  1382.61\n",
            "| epoch   2 |   220/  500 batches | lr 6.00000 | ms/batch 175.79 | loss  7.09 | ppl  1199.60\n",
            "| epoch   2 |   230/  500 batches | lr 6.00000 | ms/batch 180.32 | loss  7.10 | ppl  1206.53\n",
            "| epoch   2 |   240/  500 batches | lr 6.00000 | ms/batch 176.31 | loss  7.18 | ppl  1312.88\n",
            "| epoch   2 |   250/  500 batches | lr 6.00000 | ms/batch 176.53 | loss  7.15 | ppl  1268.79\n",
            "| epoch   2 |   260/  500 batches | lr 6.00000 | ms/batch 175.19 | loss  7.19 | ppl  1328.41\n",
            "| epoch   2 |   270/  500 batches | lr 6.00000 | ms/batch 182.03 | loss  7.20 | ppl  1334.92\n",
            "| epoch   2 |   280/  500 batches | lr 6.00000 | ms/batch 175.91 | loss  7.12 | ppl  1232.77\n",
            "| epoch   2 |   290/  500 batches | lr 6.00000 | ms/batch 174.96 | loss  7.04 | ppl  1135.88\n",
            "| epoch   2 |   300/  500 batches | lr 6.00000 | ms/batch 177.53 | loss  7.13 | ppl  1248.10\n",
            "| epoch   2 |   310/  500 batches | lr 6.00000 | ms/batch 180.48 | loss  7.08 | ppl  1184.99\n",
            "| epoch   2 |   320/  500 batches | lr 6.00000 | ms/batch 176.47 | loss  7.10 | ppl  1211.46\n",
            "| epoch   2 |   330/  500 batches | lr 6.00000 | ms/batch 176.55 | loss  7.16 | ppl  1284.65\n",
            "| epoch   2 |   340/  500 batches | lr 6.00000 | ms/batch 174.74 | loss  7.19 | ppl  1332.45\n",
            "| epoch   2 |   350/  500 batches | lr 6.00000 | ms/batch 182.24 | loss  7.08 | ppl  1190.26\n",
            "| epoch   2 |   360/  500 batches | lr 6.00000 | ms/batch 175.30 | loss  7.09 | ppl  1203.78\n",
            "| epoch   2 |   370/  500 batches | lr 6.00000 | ms/batch 175.06 | loss  7.12 | ppl  1237.17\n",
            "| epoch   2 |   380/  500 batches | lr 6.00000 | ms/batch 175.13 | loss  7.06 | ppl  1159.76\n",
            "| epoch   2 |   390/  500 batches | lr 6.00000 | ms/batch 181.46 | loss  7.05 | ppl  1153.12\n",
            "| epoch   2 |   400/  500 batches | lr 6.00000 | ms/batch 175.54 | loss  7.13 | ppl  1243.95\n",
            "| epoch   2 |   410/  500 batches | lr 6.00000 | ms/batch 174.81 | loss  7.09 | ppl  1198.40\n",
            "| epoch   2 |   420/  500 batches | lr 6.00000 | ms/batch 175.56 | loss  7.16 | ppl  1292.98\n",
            "| epoch   2 |   430/  500 batches | lr 6.00000 | ms/batch 175.46 | loss  7.21 | ppl  1357.48\n",
            "| epoch   2 |   440/  500 batches | lr 6.00000 | ms/batch 180.65 | loss  7.12 | ppl  1234.76\n",
            "| epoch   2 |   450/  500 batches | lr 6.00000 | ms/batch 174.94 | loss  7.14 | ppl  1257.74\n",
            "| epoch   2 |   460/  500 batches | lr 6.00000 | ms/batch 176.33 | loss  7.11 | ppl  1223.26\n",
            "| epoch   2 |   470/  500 batches | lr 6.00000 | ms/batch 175.70 | loss  7.15 | ppl  1269.74\n",
            "| epoch   2 |   480/  500 batches | lr 6.00000 | ms/batch 180.24 | loss  7.12 | ppl  1232.80\n",
            "| epoch   2 |   490/  500 batches | lr 6.00000 | ms/batch 177.01 | loss  7.12 | ppl  1234.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 89.43s | valid loss  6.84 | valid ppl   935.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.81743 | test ppl 913.63508\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG74Ii-cW76S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('mlm_bert.pt')\n",
        "files.download('2020-0809mlm_bert.pt')\n",
        "files.download('torchtext_bert_vocab.pt')\n",
        "files.download('2020-0809torchtext_bert_vocab.pt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}