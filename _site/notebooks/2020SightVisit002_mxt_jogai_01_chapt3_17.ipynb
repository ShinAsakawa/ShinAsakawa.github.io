{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "2020SightVisit002_mxt_jogai_01_chapt3-17.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020SightVisit002_mxt_jogai_01_chapt3_17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI67iWnBG-ev",
        "colab_type": "text"
      },
      "source": [
        "- 出典: 2020-0702情報とデータサイエンスpart2\n",
        "\n",
        "# 参考資料\n",
        "文部科学省の[高等学校情報科「情報II」教員研修用教材(本編)](https://www.mext.go.jp/a_menu/shotou/zyouhou/detail/mext_00742.html) に掲載されている情報\n",
        "\n",
        "- [第3章 情報とデータサイエンス 前半](https://www.mext.go.jp/content/20200702-mxt_jogai01-000007843_004.pdf)\n",
        "- [第3章 情報とデータサイエンス 後半](https://www.mext.go.jp/content/20200609-mxt_jogai01-000007843_007.pdf)\n",
        "\n",
        "- page 160 から 引用\n",
        "\n",
        "## ニューラルネットワークとその仕組み\n",
        "\n",
        "## 研修内容\n",
        "* 生徒に「自律性」と「適応性」の観点からAIの定義を考えさせられるようになる。\n",
        "* 機械学習，AI，ディープラーニングの言葉の違いとそれぞれの関連を生徒に示せるようになる。\n",
        "* ニューラルネットワークの概念と仕組みを生徒に理解させられるようになる。特に「学習」について実習を交えた授業を展開し，生徒がより理解を深められる授業をできるようになる。\n",
        "* 生徒とニューラルネットワークによる手書き文字認識等のサンプルプログラムを実行し，仕組みを確認し，ニューラルネットワークの有用性とAI技術の活用を考えられるようになる。\n",
        "\n",
        "### 1. 人工知能（AI）とは何か，AIの活用例\n",
        "\n",
        "この学習項目では， 人工知能（Artificial Intelligence，AI，以後AIと表記）の定義や活用事例から AI がどのようなものかを捉えることを目標とする。\n",
        "AI は人によって想像するものが異なる幅広い意味を含んだ語であり，明確な定義はない。\n",
        "例えば AI のキーワードに自律性 (Autonomy) と適応性 (Adaptivity) がある。\n",
        "以下の活用例から自律性と適応性を確認する。\n",
        "\n",
        "* 自動車の自動運転技術\n",
        "* 写真に写っている顔への自動タグ付け\n",
        "* Webの閲覧状況からのお薦め表示\n",
        "\n",
        "自動車の自動運転では，突然前に障害物が飛び出してきたときやカーブした道等の様々な状況下で，自動車が自分で判断して適切にブレーキを踏みハンドルを切る。\n",
        "写真の自動タグ付けではポインターを指定せずに撮影した画像から顔を認識する。\n",
        "自律性とはこのように，人の判断なしに状況に応じて動作する能力である。\n",
        "適応性とは，大量のデータから特徴を見つけ出し状況判断ができる，あるいは与えられた正解データと新たなデータを照合することで自らのプログラムの精度を上げていくことができる（学習）能力である。\n",
        "前述の例では Web の閲覧状況から閲覧者に合わせた情報を表示する。\n",
        "\n",
        "### 2. ニューラルネットワークの由来\n",
        "\n",
        "この学習項目では，ニューラルネットワーク，AI，ディープラーニング，機械学習の関連を整理する。\n",
        "ニューラルネットワークとは人間の神経細胞とその仕組みに似せて考えられたコンピュータの処理である。\n",
        "人間の神経細胞（ニューロン）は，受容体を持つ樹状突起と軸索，シナプスの組み合わせ 図表３ で構成され，シナプス間を神経伝達物質といわれる化学物質が伝達されることで成り立っている。\n",
        "\n",
        "このモデルを使ったネットワークの原形が 1943 年に発表された。\n",
        "ニューラルネットワークは，膨大な計算量とその計算時間が大量であること，隠れ層の重み付けを決めるアルゴリズムの難しさから長らく非現実的とされた（冬の時代）。\n",
        "その間，人が特徴量を指定して行う機械学習が活用されるようになったが，人によるコンピュータの学習であり，限界が見られていた。\n",
        "その後，計算処理を分散して実行できる GPU の登場と，コンピュータの計算処理速度の向上により重み付けの調整が人の手から離れ，コンピュータが計算を繰り返すことで最適解を探すこと（自律学習）が可能となった。このコンピュータによる学習の繰り返しを多層ニューラルネットワークで表したものをディープラーニング（深層学習）という。\n",
        "これにより AI の予測分類精度がそれまでのものから劇的に上がり，ニューラルネットワークを基にした AI 技術が大きく注目されるようになった図表４ 。\n",
        "\n",
        "### 3. ニューラルネットワークの概念\n",
        "この学習項目では，ニューラルネットワークの概念と仕組みを演習しながら学び，理解することを目標とする。\n",
        "ニューラルネットワークは大きく分けて「入力」「隠れ層（中間層）」「出力層」で構成されている図表５ 。\n",
        "隠れ層を次の入力として別の隠れ層に渡していくことを繰り返すと図表６ ，複数の層の隠れ層をもつ深層学習（Deep Learning）となり，より複雑な出力に対応できる。\n",
        "一般に隠れ層と出力層を合わせてネットワークの層の数を表す。\n",
        "各ニューロンでは下記のように計算を行っている。\n",
        "\n",
        "```\n",
        "（入力）　×　（重み付け）　＋　（バイアス）\n",
        "```\n",
        "\n",
        "その後，各ニューロンの結果を活性化関数に入れて（発火という）出力とする。\n",
        "\n",
        "例えば手書き文字が 4 か 9 か認識したいとする。\n",
        "入力は手書き文字画像のピクセルごとのグレースケールの階調を値にして表した配列である。\n",
        "出力は手書き文字が 4 であれば 0，9 であれば 1 とする。\n",
        "各ピクセルの値に重み付けとバイアスの値を加えることでどちらかの出力値に寄せていく。\n",
        "最初の重み付けとバイアスはコンピュータがランダムに行い，学習の過程で自律的にそれぞれ修整していく。\n",
        "活性化関数は，隠れ層の計算結果が一定の値（閾値）を超えたときに大きな値を得られる関数である。具体的な活性化関数については次節で扱う。\n",
        "\n",
        "### 4. ニューラルネットワークの仕組み\n",
        "この学習項目では，ニューラルネットワークの仕組みを計算とプログラムで表現し，ニューラルネットワークの計算や活性化関数の働きを理解しよう。\n",
        "\n",
        "#### （1）ニューラルネットワークの計算\n",
        "入力値が複数の場合や隠れ層が多層になったときは行列の乗算（内積，ドット積）を行う図表８ 。\n",
        "\n",
        "### 演習 3\n",
        "ニューラルネットワークの計算を確認しましょう。\n",
        "入力（0.3，0.8，1.2，0.7），重み付け（1，2，1，1），バイアス0.2 のときの出力結果を求めましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8UbYrcYG-ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6b33ae2c-63c2-4af7-aa14-96f2588a77ba"
      },
      "source": [
        "# colab では直下行頭の # を削除して実行してください\n",
        "!pip install japanize-matplotlib"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: japanize-matplotlib in /usr/local/lib/python3.6/dist-packages (1.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from japanize-matplotlib) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->japanize-matplotlib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJselEuKG-e2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgYJLns0G-e7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "cf2f4162-5fe7-4625-bec7-d8d71c3ac9d9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([0.3, 0.8, 1.2, 0.7])\n",
        "W = np.array([1, 2, 1, 1])\n",
        "B = 0.2\n",
        "\n",
        "A = np.dot(X,W) + B\n",
        "print(A)  # 出力結果は 4\n",
        "\n",
        "\"\"\"\n",
        "Xは入力，Wは重み付け，Bはバイアスを表す。\n",
        "1 行目では，Pythonで計算処理ができるパッケージ numpy を np として使えるようにしている。\n",
        "2 〜 4行目で演習3と同じ値をそれぞれに用意している。\n",
        "5 行目で numpy パッケージにあるdot関数を使って行列計算をしてバイアスを加え，出力 A としている。\n",
        "最後に print 文で計算結果を出力している。\n",
        "この隠れ層での計算の後，活性化関数を用いる。\n",
        "\"\"\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nXは入力，Wは重み付け，Bはバイアスを表す。\\n1 行目では，Pythonで計算処理ができるパッケージ numpy を np として使えるようにしている。\\n2 〜 4行目で演習3と同じ値をそれぞれに用意している。\\n5 行目で numpy パッケージにあるdot関数を使って行列計算をしてバイアスを加え，出力 A としている。\\n最後に print 文で計算結果を出力している。\\nこの隠れ層での計算の後，活性化関数を用いる。\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG0FAZxdG-fB",
        "colab_type": "text"
      },
      "source": [
        "### （2）活性化関数\n",
        "各ニューロンの出力に作用させる関数を活性化関数という。\n",
        "活性化関数は各ニューロンの結果の正確さを確率で表したものである。\n",
        "論理演算のように入力に対して 0 か 1 を返すような 2 値に分類ができる場合はステップ関数(図表９) を用いる。\n",
        "はっきりとした分類ができる分類器をパーセプトロンという。\n",
        "ニューラルネットワークでは論理演算のように 2 値に分けられるとは限らない。\n",
        "猫か犬かのような2値分類の場合には，シグモイド関数 (図表10) やReLU（ランプ）関数を使い，猫か犬か鳥かといった複数に分類する場合はソフトマックス関数が多く用いられている(図表11)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcZLuqTrG-fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "class step(object):\n",
        "    @staticmethod\n",
        "    def forward(x):\n",
        "        z = np.zeros_like(x)\n",
        "        z = [1. if x_ >= 0 else 0. for x_ in x]\n",
        "        return z\n",
        "\n",
        "\n",
        "class sigmoid(object):\n",
        "    \"\"\"Logistic Sigmoid function, that returns from 0 to 1.\n",
        "\n",
        "    Reference:\n",
        "        David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. williams.\n",
        "        Learning representations by back-propagating errors. Nature,\n",
        "        323(6088):533-536, 1986\n",
        "\n",
        "    Link:\n",
        "        [https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/].\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(x):\n",
        "        if x.any() >= 0:\n",
        "            z = np.exp(x)\n",
        "        else:\n",
        "            z = np.exp(-x)\n",
        "        return z / (1 + z)\n",
        "        #return 1 / (1. + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(y):\n",
        "        #y = sigmoid.forward(x)\n",
        "        #return x * (1. - x)\n",
        "        return y * (1. - y)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0wjhGlMG-fH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "outputId": "e2e42d3f-14eb-415a-e30a-23859e44c122"
      },
      "source": [
        "x = np.linspace(-5,5)\n",
        "\n",
        "plt.title('ステップ関数'); plt.plot(x, step.forward(x)); plt.show()\n",
        "plt.title('シグモイド関数'); plt.plot(x, sigmoid.forward(x));plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEHCAYAAABcCaZFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU5dn/8c9FVpKwhJAQWRI2EVnciCBqFa24VKxarVWrorQPLrXtU2tt+1RtKz+12tpWa1uxrUsrLlVaqYq2dQG1CBgElX1fZA1LNrIn1++PDDZEIANMcjIz3/frldfMnPvMzDUwfHNzn/uc29wdERGJbh2CLkBERA6fwlxEJAYozEVEYoDCXEQkBijMRURigMJcopaZJZjZmUHXIdIeKMwlapjZeWaWELqfAdwIXG5mX2+F9+oS6dcM4z2/Ymb5ZjbbzLqa2fdD2580s/y2rkeii8JcAmFmT5nZd8wsrO+gmZ0M/Nrd6wHcvRw4ElgObI5wbWcBC8ws7QD7zDCzk0L3E83sNDO7r9k+xaH9ZpjZx2b2AzN72MwKm2xf3eQpCcD1oftfAXaE7p8A1JlZbugnJWIfVmJGYtAFSNz6MfBH4ETgyjD2vwfINbOVoccO3AEscveP9+xkZk8CpwAdga7sHfRT3f37B3oTMysAngS+7O4VYX6W84BLga1mdoa7vxXavht4NnR/eJP9XwM+Cd3/buh9jwe+EfpcQ4CrQ9uXA9nAn4ABode8EXgvzNokTijMpU2ZWTcae51/BM4C8pq15wMZ7r6oybabaAy5bnt65mb2ElDdNMgB3H18qH0McKu7jzuI2s4D/gJc5O6zwn2eu78U6qU/26yeNBpDfs/9hTT+kjkDqAlt7xB6jfnAKWb2S+Bo4BXg18CFwNPufouZ/Qr4u7sryOUzFObS1jJoDKgbaOz9Lt/TEBoPf4rGQF0U2nY28H1gVJMgzwZGAZdFoiAzSwXuBi4Bqtz93TCfeoWZLXX3Ynf/0T7at7r7WaH3+BXwBJAE5Ln7daHtS0O3OcBvgX8BK4ClwHM0hv2ef6eDaRxWEvkMjZlLm3L39e5+LvAH/ts73eM+oMTdHwUwsxOAycC57r6lyX7fB55w98qmTzazX5rZJ2b2CfA88Pk9j5v8/KTJ/glmNh74EGgAzjnIj5MNLDSzO82s4/52Co2lnwFMofEX1TFmdm+z3RKBx4B/Arvd/e/A5UA9kBL6H02fZn8OIp8yXWhLghY6CPo7YCQw1t13hLYb0KNpgJnZSOBFYKi77zrAa46hhWGW0EHV/wV+7O5LzKwv8K679w6j5hnAD4D1wENAAXCju7/aZJ+l7j7YzPrT2HG6FtgFTAPq3X1Nk31GAQ8APWg8ELop9DL3An2BC4AV7v7NlmqT+KRhFgmUmZ1I49hwDTDG3Uv3tHljT6NpkA+jscc9/kBBHq7QuHjYY+P7eY1NwKWhHr41bw8d2PxN6GFfoAS4ONT2aTC7+xwzuwz4s7ufZWa/Baa4+6zQ7JW7gIcPp1aJbRpmkTZnZl3M7Aoz+zswncahh883DfJ9POcCGmeB3ODu/w7jbWqBnqH3MjNLas254+7+pLtP38f2+e5+Ko3DLJ/Q2ON+BPhc6KBnUwOBBDN7GDiO/85Y+R9gAXCvmfVsrc8g0U09cwnCbcCpwDPAde5evL8dQwcnXwE6A1909w/CfI8FwLrQbTqNPf/FwNmHUfchMbPTgdtpnPL4DI1DJyeFeuZNO1QLQu3XA2XAXaGhptNorPsc4D0zu1wzWqQ5hbm0uf3M/NjfvlVm9n/A++7ecBDP201oOOMg7KRx/ns45gL7/SXUzBDgtiY98RtDs3SWAG8DmFkyjXPJ3wVOdfdKMxtAY6/8bHevAqaZ2U4ax+lF9qIDoCIiMUBj5iIiMUBhLiISAxTmIiIxIJADoN27d/e+ffsG8dYiIlFr3rx52909e19tgYR53759KSwsDOKtRUSilpmt21+bhllERGKAwlxEJAYozEVEYoDCXEQkBrQY5mZ2qZn91cz2eQqxmV1mZnPNbJ6ZPRD5EkVEpCXh9MyLgJuA5OYNoSW+JgFjabyec28zuySiFYqISItaDHN3n+nu2/fTfC6Ni+SWhK49PRm4KJIFiohIyw53nnkWTRYPoHEl9Jx97WhmE4GJAHl5efvaRUQkatXVN1BWVUdZVR2lVbXsrq6joqae8uo6KmrqKK+up6K6jqyMFK4cFfkMPNww3wr0a/I4N7TtM0LrOj4KUFBQoEs1iki75O7srqlne1k1O3ZXU1RWw47d1RRX1FJcUUNxRS27KmopqWy8X1pVS1lVY3CH4/i8ru0yzKcDr5vZfe5eBkygcX1GEZF2p77B2VJaxabiSraUVDX+lDbebi6pZGtpY4BX1e770vmpSR3ITEumS8ckMtOSGZiTQefUJDqlJtLp09v/3k9PSSQ9OYG0lEQykhPpmJxAcmLrTCI8pDA3s2eBn7n7AjO7B3jbzGqAd9x9akQrFBE5CDV1DazdsZtV28pZt7OC9Tsr2BD62VhcSW393gMDackJ5HZJJbdzKiP7daN7RjLdM1LIykhpcj+ZzLRkUpMSAvpULQs7zN09t8n9y5vcnwJMiXBdIiIHVFvfwMpt5SzeVMqKbeWs3FbO6qLGAK9v+G9gZ6YlkdctjaG9unDe8CPok5lGr8yOHNElldwuqXRKSaRxdb7opmXjRKTdq66rZ/GmUhZuKmXxphIWbixl2dYyauoah0OSEoy+WekclduJ8485ggHZGQzMySA/K41OqUkBV982FOYi0u4UV9Qwb90uCtftonDtTj78pOTT4O7SMYlhvTpz7cl9GdqzM0N7dqFvVhqJCfF9QrvCXEQCV1FTx+zVO5i5rIj3Vu9g+dZyABI7GMN6dWH86HxG5GcyrFcXenXtGBPDIpGmMBeRNufurNhWzsxlRcxcXsTcNTupqW8gNakDI/tl8cVje1LQtxvH9u5Kx+T2e9CxPVGYi0ibcHcWbizl5Y838cpHm/lkVyUAg3pkMP7kfE4flENB38x2PWOkPVOYi0ircXeWbC7jlVCAr91RQWIH49Qju3PTmIGcflQ2vbp2DLrMmKAwF5GIK66o4YV5n/Ds+xtYua2chA7GyQOyuHHMAM4ekktm+meu2yeHSWEuIhHh7nywvpgpc9bx8kebqalr4IS8rtx98TDOHZpLVkZK0CXGNIW5iByWipo6pn6wkSmz17F0SxkZKYl8paAPV47K4+gjOgddXtxQmIvIISmtquUv763jT++uYefuGob16sy9XxrOF4/tSXqKoqWt6U9cRA7Krt01PP6fNTw+ay1lVXWMOSqbm88YSEHfbkGXFtcU5iISlu3l1UyeuYopc9ZTUVPPecNy+cYZAxnWq0vQpQkKcxFpQXVdPU/OWstv3ljJ7po6LjyuFzeNGcCRPToFXZo0oTAXkX1yd/61eCv3TF/Cuh0VnDk4h//7wtEMzMkIujTZB4W5iHzG4k2lTHp5Me+t3sGRORk8OWEkpw/KDrosOQCFuYh8and1Hfe9tpS/zF5H145JTLpwKFeMzIv7KxJGA4W5iADw3qod3Db1Qz7ZVcn40X35zlmD6JIWH9cCjwUKc5E4V1FTx32vLuXJ99bRNyuNv14/mhM1zTDqKMxF4tic1Tv43gsfsX5nBded0pfbzhmsS85GKYW5SByqqWvgvteW8th/1tAnM41nJ57ESf2zgi5LDoPCXCTObCmp4qYp8/hgfTFXn5TPD78wmLRkRUG009+gSByZvXoHNz/9ARU19Tx85fGMO6Zn0CVJhCjMReKAu/PHd9bws9eWkp+VxjP/c5LO4IwxCnORGFdeXcdtL3zI9I+3cO7QXH7+5WPolKoph7FGYS4SwzbsrOC6J95ndVE5PzxvMBNP66+V7WOUwlwkRi3eVMr4x+dSU9fAU18fxckDugddkrQihblIDJq1ajvX/3keGamJPH3DaI2PxwGFuUiMefmjTdzy3If07Z7GkxNGckSXjkGXJG1AYS4SQ574zxp++vJiCvIz+eM1J+raKnFEYS4SA9ydn/9zGb+bsYqzh/TgoSuOJzVJp+XHE4W5SJRzd3760mKemLWWK0flMenCYSR00IyVeKMwF4li7s69ry7liVlr+dqp/bj9/KM19TBOhXXFeTO7zMzmmtk8M3ugWVuCmT1oZrND+/zezDRQJ9IGHvjXch59ezXXjM5XkMe5FsPczPKBScBYoADobWaXNNnlC0Avdz/J3UcCPYCLWqNYEfmvh95YwcNvreSKkX34yQVDFeRxLpye+bnAVHcvcXcHJrN3WH8CJJpZBzPrANQCiyNfqojs8fsZq/jlv5dz6Yje3H3RcDpojDzuhTNmngVsafJ4M5Cz54G7zzezmcDPQptmuPui5i9iZhOBiQB5eXmHXLBIvPvjO6u577WlXHhcT+675BgFuQDh9cy30iS8gdzQNgDM7Bog2d1vc/fbgE5mNqH5i7j7o+5e4O4F2dla5VvkUDwzdz3/75UlfGF4Lg98+VjNWpFPhRPm04GLzWzP+cATgGlN2oeydw8/GTgyMuWJyB5vLdvG7S8u5Iyjsnnw8uNJTAhr/oLEiRa/De6+GbgHeNvM5gBb3X2qmc0ws1zgAWCkmc0ys9nACcAvWrVqkTizaFMJN0/5gMG5nXj4yhNIUpBLM2HNM3f3KcCUZtvGNHl4YQRrEpEmNpdUMuGJ9+ncMYnHrj2R9BSdHiKfpW+FSDtWVlXLdY+/z+7qel64cTQ9OqcGXZK0UwpzkXaqrr6Bm5+ez4pt5Tx+7YkMzu0cdEnSjmngTaQdcnfumLaImcuLuPuiYZw2SDPA5MAU5iLt0B/eWc0zc9dz45gBXD5S52VIyxTmIu3Muyu287NXl3L+8CP43tlHBV2ORAmFuUg7srG4km89O5+BORncf6nO7pTwKcxF2onqunpuemoeNXUNPHLVCE1BlIOib4tIO/HTlxbz4SclPHLVCPpnZwRdjkQZ9cxF2oHnCzfw9Jz13HD6AM4dlht0ORKFFOYiAVu4sYTbX1zIyQOyuPXsQUGXI1FKYS4SoOKKGm6cMo/MtGQeukIXz5JDpzFzkYA0NDjfeW4BW0qqeO760XTPSAm6JIli6gaIBOTxWWt5a1kRt58/hBPyMoMuR6KcwlwkAIs3lXLfq0s56+geXDM6P+hyJAYozEXaWFVtPd9+dj5d0pK475LhWohZIkJj5iJt7O5XlrBiWzl/+dpIsjROLhGinrlIG3p98Vb+MnsdXz+1H587UldClMhRmIu0kW2lVdw29SOGHNGZ752rC2hJZCnMRdpAQ4Pz3ec/pKKmjoeuOI6UxISgS5IYozAXaQOP/WcN76zYzu3nD2FgTqegy5EYpDAXaWVLt5Ry/2vLGDukB18dpYUmpHUozEVaUW19A7c+/yGdUhP52Zc0DVFaj6YmirSiyTNXsXBjKb//6gmahiitSj1zkVaydEspD76xgnHHHMF5w48IuhyJcQpzkVawZ3ilc2oSd104LOhyJA5omEWkFTQdXumWnhx0ORIH1DMXiTANr0gQFOYiEaThFQmKhllEIkjDKxIU9cxFImTZljINr0hgFOYiEVDf4Hx/6kd00vCKBERhLhIBT81ex4INxdw5boiGVyQQYYW5mV1mZnPNbJ6ZPbCP9uFm9k8ze9PMXjazPpEvVaR92lRcyf2vLeVzR3bnwuN6Bl2OxKkWD4CaWT4wCRgJlALPmtkl7j411J4APAxc6u5FZtYbKG7FmkXaDXfnzmmLqHfn7ot07RUJTjg983OBqe5e4u4OTAYuatJ+IrAZuMfM3gVuACojXqlIO/TPRVt4fclWbhk7iLystKDLkTgWTphnAVuaPN4M5DR5nAeMBu4CTgs9Ht/8RcxsopkVmllhUVHRoVcs0k6UVNZy57RFDDmiMxNO6Rd0ORLnwgnzrewd3rmhbXsUAzPdfYO7NwDPAyOav4i7P+ruBe5ekJ2ttQ8l+t3/2lK2l1fzs0uGk5iguQQSrHC+gdOBi81sz/IoE4BpTdrfA44xs+6hx+cACyJXokj78/7anUyZs57rTunHMb27Bl2OSMth7u6bgXuAt81sDrDV3aea2Qwzy3X3MuA7wN/NbBaQAjzeqlWLBKi6rp4f/u1jenXtyC1jBwVdjggQ5un87j4FmNJs25gm998CPhfRykTaqckzV7NyWzmPX3si6Sm6Ioa0DxroEzkIa7fv5uG3VnL+MUdwxuCclp8g0kYU5iJhcnfumLaQ5IQO3DluSNDliOxFYS4Splc+3sw7K7Zz69mD6NE5NehyRPaiMBcJQ1lVLXe9tJhhvTpz9ei+QZcj8hk6eiMShl/+ezlF5dX84ZoCEjrolH1pf9QzF2nBwo0lPDlrLVeNyufYPppTLu2TwlzkAOobnB+9uJBu6Snces5RQZcjsl8Kc5EDeGbuej7cUMwd446mS8ekoMsR2S+Fuch+FJVVc99rSzllYBZfPFbXKZf2TWEush/3TF9CdW0Dd104TNcpl3ZPYS6yD++t2sHf52/k+tP7MyA7I+hyRFqkMBdppqaugTunLaRPt45844yBQZcjEhbNMxdp5rH/rGHFtnL+NL6A1KSEoMsRCYt65iJNbCyu5MHXV3D2kB58/ugeQZcjEjaFuUgTk15ajOPceYEupCXRRWEuEvLWsm28tmgL3/r8kfTO1OLMEl0U5iJAVW09P562iAHZ6Xz91P5BlyNy0HQAVAT4/YxVrN9ZwdP/M4rkRPVxJProWytxb+323fx+5iouPK4nJw/o3vITRNohhbnENXfnx/9YREpCB370haODLkfkkCnMJa69unALM5cXccvZg8jR6kESxRTmErfKqmr56UuLGNqzM1eflB90OSKHRQdAJW796t8r2FZWzeSrC0hMUL9Gopu+wRKXFm4s4YlZa/jqqDyO0+pBEgMU5hJ3Ghqc219cSLf0ZL53zuCgyxGJCIW5xJ1n3l/Pgg3F/Oh8rR4ksUNhLnGlqKya+15dyuj+WVx0XK+gyxGJGIW5xJV7py+hsraeSRdp9SCJLQpziRuzVm3nb/M3cv1pAxiYo9WDJLYozCUu1NQ1cMeLC8nrlsbNZ2r1IIk9mmcuceGRmatYVbSbx687UasHSUxSz1xi3qqich5+cyUXHNuTM47KCbockVYRVpib2WVmNtfM5pnZAwfY709m9kTEqhM5TA0Nzg//9jGpSR24c5xWD5LY1WKYm1k+MAkYCxQAvc3skn3sdxGQHPEKRQ7DXws3MHfNTn50/tFkd0oJuhyRVhNOz/xcYKq7l7i7A5OBi5ruYGY9gFuBuyNfosih2VZWxT3TlzCqXzcuK+gTdDkirSqcMM8CtjR5vBloPvA4mcYwr9rfi5jZRDMrNLPCoqKigy5U5GDd9dJiquoauPdLwzWnXGJeOGG+lb3DOze0DQAzux5Y7O6zD/Qi7v6ouxe4e0F2dvYhFSsSrjeXbuXljzbzzTMG0j9bc8ol9oUT5tOBi82sU+jxBGBak/ZzgGPN7EXgUeBMM/tFZMsUCd/u6jpu//tCjszJ4PrTBwRdjkibaHGeubtvNrN7gLfNrAZ4x92nmtkM4HJ3/9Kefc2sL/ATd7+1leoVadED/1rO5tIqXrhhtBZnlrgR1klD7j4FmNJs25h97LcWuDYCdYkckg83FPPErDVcNSqfEfndgi5HpM2o2yIxo7qunu+98CE5nVL53rlHBV2OSJvS6fwSMx56YwXLt5bz+HUn0jlV1ymX+KKeucSEjz4p5pGZq/nyiN46ZV/iksJcol51XT23Pv8h2Rkp3K5T9iVOaZhFol7T4RUtAyfxSj1ziWoaXhFppDCXqKXhFZH/0jCLRC0Nr4j8l3rmEpU0vCKyN4W5RJ3Kmnq+89wCDa+INKFhFok6k15ZzOrtu5nytVEaXhEJUc9coso/F23h6TnrmXhaf04e2D3ockTaDYW5RI0tJVV8f+pHDOvVme+O1bVXRJpSmEtUaGhwvvv8AqprG3jw8uN1aVuRZvQvQqLCH95ZzX9W7uDOC4YwQCsHiXyGwlzavYUbS/jFv5ZxztAeXH6iFmYW2ReFubRrFTV1fOuZ+WSlp/CzLx2jhZlF9kNTE6Vdu+ulxazZ0TgNMTM9OehyRNot9cyl3Xq+cAPPvr+BG04foGmIIi1QmEu7tGhTCbe/uJDR/bP47thBQZcj0u4pzKXdKamo5Yan5pGZlsxvrjyexAR9TUVaojFzaVcaGpz/fW4+W0qqeO760XTPSAm6JJGooC6PtCu/eXMlby0r4s5xQzghLzPockSihsJc2o0Zy7bx6zeW86Xje3HVSflBlyMSVRTm0i5s2FnBt59dwFE9OnH3xcM1n1zkICnMJXCVNfXcOGUeDe5MvnoEHZMTgi5JJOroAKgEqr7B+faz81m0qZQ/jS8gPys96JJEopJ65hKou19Zwr8Wb+XOcUM4c3CPoMsRiVoKcwnMk7PW8th/1nDdKX257pR+QZcjEtUU5hKI1xdv5acvLWLskB7cfr7W8RQ5XApzaXMff1LCN5+Zz7BeXXjw8uNI6KCZKyKHS2EubWpjcSUTnnyfbunJ/HF8AWnJOgYvEglhhbmZXWZmc81snpk9sI/2b5rZbDN7z8x+Z2b6JSGfUVJZy3WPz6Wqtp7HrzuRnE6pQZckEjNaDF0zywcmAWOBAqC3mV3SpH0ocAFwiruPBrKBca1TrkSrsqpaxj82lzXbd/PIVSMY1KNT0CWJxJRwetDnAlPdvcTdHZgMXLSn0d0XAV909/rQpkSgMuKVStTaXV3HhCfeZ+HGEn575QmcomuTi0RcOGGeBWxp8ngzkNN0B3evMrOuZvY0sMDd/938RcxsopkVmllhUVHRYRUt0aOypp6vP1nIvHW7ePDy4zl7aG7QJYnEpHDCfCt7h3duaNunzGwY8BzwoLv/dF8v4u6PunuBuxdkZ2cfar0SRapq65n4l0Jmr9nBr75yHOcfc0TQJYnErHDCfDpwsZntGeScAEzb02hm2cCvgcvcfU7kS5RoVF1Xz41PzeOdFdu575JjuPC4XkGXJBLTWgxzd98M3AO8bWZzgK3uPtXMZphZLvAVoB8wLbRthplNbN2ypT2rrW/g5qfn89ayIu6+eBiXFfQJuiSRmGeNxzTbVkFBgRcWFrb5+0rrq6yp5+anP+CNpdv4yQVDuFan6YtEjJnNc/eCfbXpjA2JmF27a/jak+8zf0Mxky4cytWj+wZdkkjcUJhLRGwsruSaP81hw65KfnflCZw3XAc7RdqSwlwO29ItpYx/bC4VNfX8ecJITuqfFXRJInFHYS6HZc7qHXz9z4WkJSfw/A2jGZzbOeiSROKSwlwO2SsfbeY7f11An8yOPDlhJL0z04IuSSRuKczloNXVN/Dzfy1j8szVjMjP5I/XFJCZnhx0WSJxTWEuB2V7eTXffHo+763ewVUn5XHHuCGkJGoBZpGgKcwlbB+s38VNT33ArooafvHlY7l0RO+gSxKREIW5tMjdeWrOeu56aRG5XVL5200nM7Rnl6DLEpEmFOZyQCWVtfx42kJeXLCJMwfn8KvLjqNLWlLQZYlIMwpz2a8Zy7bxg6kfU1RezS1jB3HzGQPpoPU6Rdolhbl8RmlVLXe/vITnCjdwZE4Gj14zgmN6dw26LBE5AIW57OXt5UV8f+pHbC2t4sYxA/jfs47UbBWRKKAwF6DxIln3/3Mpz8zdwIDsdKbeeDLH52UGXZaIhElhHudq6xt4avY6fv36Csqqapl4Wn9uGTuI1CT1xkWiicI8Trk7M5YVMemVxawu2s2pA7tzx7ghHJXbqeUni0i7ozCPQyu2ljHplSW8vbyI/t3T+dP4As4cnIOZZqqIRCuFeRxZua2c381YybQFm0hPTuCOcUO4+qR8khPDWQpWRNozhXkcWLSphN+9tYrpCzeTktiBa0/uyzfOGEg3XRxLJGYozGPYB+t38ds3V/LG0m10SknkpjEDmHBKP7IyUoIuTUQiTGEeY6pq63l14WamzF5P4bpddE1L4rtjB3HNyX3p0lGn4YvEKoV5jFhdVM7Tc9bzwgefUFxRS7/u6dx+/tFcMTKP9BT9NYvEOv0rj2Ll1XW8sWQrz72/gVmrdpDYwThnaC5fHZXH6AFZmp0iEkcU5lGmoqaOt5YW8fJHm3hz6Taq6xro1bUj3zvnKL5c0JucTqlBlygiAVCYR4HSqlreXbGd6R9v5o0l26isrSe7UwqXn9iHccf2ZERepq5mKBLnFObtUEODs3hzKTOXFzFzWRHz1u+ivsHJSk/mkhG9OH94T0b260aCAlxEQhTm7YC7s6qonPfX7mLump28s6KI7eU1AAzv1YUbTx/A6Udlc3yfriQm6AQfEfkshXkAKmvqWbSphMJ1uyhcu5PCdbsorqgFICs9mVOP7M7pg7L53JHZZHfSnHARaZnCvJWVVNayeFMpizaVsGhTKQs3lrCqqJwGb2zv3z2ds4f0oKBvNwryM+nXPV2zUETkoCnMI8Dd2bG7hpXbylm5rZxVRaHbbeVsKqn6dL8enVMY1rML5w3LZWivLozIz6S7zsYUkQhQmIdpd3UdW0qr2LCzgg27Khtvd1awPvRTVlX36b5pyQkMyM5gVP8sBuZkMKxXF4b27KzgFpFWE9dh7u6UVtWxvbyaHeU1bC+vbvwpq2ZLaRWbS6rYGrptGtYAyYkd6JPZkT7d0hiRn0nfrHQG5mQwICeDIzqnaqqgiLSpsMLczC4DbgUSgBnu/t1m7d8CrgKSgafc/ReRLnR/auoaqKipo7y68aesqo6yqlrKquoorayltKqO0qpainfXUlxZw66KWkoqatlVUUNxRS019Q2feU0zyOmUQm7nVPp1T2d0/yxyu3Qkt0sKvTPTyOuWRnZGigJbRNqNFsPczPKBScBIoBR41swucfepofZTgCuAU0NPedPMZrh7YaSL3VFezRV/mM3u6np219RRUV2/zzBuLjmhA13TkshMS6ZLWhJ9u6dxXMeudE1PIjsjhe4ZKWRlJNM9dD8zLUlTAEUkqoTTMz8XmOruJQBmNhm4Dpgaah8HPO7uNaH2x4ALgW2YGeoAAAORSURBVIiHecfkBPp3zyA9JZH0lITG2+QE0pITyUhJJCM1kU6piXRKTQrdJtI5NYmUxA6aISIiMS2cMM8CtjR5vBnIadb+XrP2Uc1fxMwmAhMB8vLyDrpQgLTkRB65esQhPVdEJJaFM5awlb3DOze0Ldx2ANz9UXcvcPeC7OzsQ6lVRET2I5wwnw5cbGZ7lm2fAExr0j4NuMbMkswsARgP/COyZYqIyIG0GObuvhm4B3jbzOYAW919qpnNMLPc0IHOfwBzgdnAS61x8FNERPbP3L3N37SgoMALC5X3IiIHw8zmuXvBvto0/05EJAYozEVEYoDCXEQkBijMRURiQCAHQM2sCFjX5m98+LoD24Muoo3pM8cHfebokO/u+zxRJ5Awj1ZmVri/I8mxSp85PugzRz8Ns4iIxACFuYhIDFCYH5xHgy4gAPrM8UGfOcppzFxEJAaoZy4iEgMU5ofAGv3bzH4SdC1twczuNrNZZva+md0RdD2txcwuM7O5ZjbPzB4Iup62EPrM75nZO2b2VzNLC7qmtmJmd5jZjKDriBSF+aH5Nnsv2BGzzOx8INfdTwZOAs43s2MCLivimiyPOBYoAHqb2SXBVtW6zKwbcBtwprt/jsZzP74ebFVtw8wKgH5B1xFJCvODZGZDaFxK77Gga2kL7v4KcFOTTR2AqoDKaU2fLo/ojQeSJgMXBVxTq3L3ncCp7l4Z2pQIVB7gKTHBzDoCvwJ+EHQtkRTOsnFxx8zOBO7cR9PVwO+Ba4H8tqyptR3gM1/u7lvMrBeNR/8fdfflbVtdm2hpecSY5O5VZpYK3AekEB+dlJ8DD7r7tlhaG1hhvg/u/ibwZvPtZnYvMMXd14T+Wx4z9veZAcxsDHArcIu7L2vLutrQVvb+b/c+lz+MNWbWG/gD8JC7vxp0Pa3NzM4BMt39haBriTRNTTwIoZWWNocedg/9PObu9wdXVesys8HA/cCl7l4TdD2txcyOAF4HTnL3MjP7C/Ciu08NuLRWE+qRvwxc5+4bgq6nLZjZb4CjgIrQplOB6e5+TXBVRYbC/BCFeqtj3P0nAZfSqszsF8B5QFGTzb9095hb59XMvkrj/0BqgHfc/daAS2pVZjaOxmMDK5psftPd7wqopDZnZjPcfUzQdUSCwlxEJAZoNouISAxQmIuIxACFuYhIDFCYi4jEAIW5iEgMUJiLiMQAhbmISAxQmIuIxID/D3LwrTFvIHHSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en2wD7XLG-fL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "fafbc533-06d5-4596-f84c-c15a6d86180b"
      },
      "source": [
        "from scipy.special import softmax \n",
        "\n",
        "a = np.array([4, 5, 6])\n",
        "label=['p(4)', 'p(5)', 'p(6)']\n",
        "\n",
        "plt.bar([1,2,3], softmax(a), tick_label=label)\n",
        "plt.title(\"ソフトマックス関数\"); plt.ylim([0,0.8]); plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WyyVPeJG-fQ",
        "colab_type": "text"
      },
      "source": [
        "### 5. ニューラルネットワークの学習\n",
        "ニューラルネットワークで学習する際に必要な技術\n",
        "を確認する。\n",
        "\n",
        "#### （1）損失関数\n",
        "ニューラルネットワークの性能の良し悪しを測る指標を損失関数という。\n",
        "損失関数では予測データと教師データとの誤差を表し，この値が0に近いほど性能がよい。\n",
        "ニューラルネットワークの学習は，この損失関数の結果が最小となるような重みを探していくことである。\n",
        "学習に使われる計算については次項「（2）勾配降下法」で説明する。損失関数には状況に応じて様々な関数を使う。\n",
        "2乗和誤差や交差エントロピー誤差が有名である。2乗和誤差とは学習結果と教師データの差分を2乗した合計を2で割ったものである。\n",
        "\n",
        "$$\n",
        "E=\\frac{1}{2}\\sum_{k=1}^n\\left(y_k-t_k\\right)^2\n",
        "$$\n",
        "\n",
        "#### （2）勾配降下法\n",
        "ニューラルネットワークは学習を繰り返し，損失関数の結果が最も小さくなる重みやバイアスを探している。\n",
        "ニューラルネットワークの学習では，最適な重みやバイアスを勾配降下法（最急降下法）によって探すことが多い(図表12)。\n",
        "勾配降下法は，勾配を使って損失関数の値を最も減らす方向を求める方法である。\n",
        "勾配はある地点での各重みにおける損失関数の傾きである。\n",
        "全ての重みで勾配が 0 となる地点を極値といい，局所的な最小値（極小値）の候補となる(図表13)。\n",
        "ただし極小値が全体の最小値となるとは限らないため，地点を変えながら繰り返し極小値を探り，最小値を求めている。\n",
        "\n",
        "#### （3）バックプロパゲーションによる学習\n",
        "バックプロパゲーション（誤差逆伝播法）は前項（2）勾配降下法をもとにしたニューラルネットワークの学習で最適な重みとバイアスを探す代表的な手法である(図表14)。\n",
        "\n",
        "#### （4）オーバーフィッティングとその防止\n",
        "オーバーフィッティング（過学習）とは学習の際に特定のデータにだけ過剰に対応し，学習に用いていない他のデータでは正しくならない状態のことである。\n",
        "これらの技術を Python 等のプログラミング言語で実装することもできるが，ここでは，ニューラルネットワークを容易に構築するために Neural Network Console（SONY）を利用する。\n",
        "Neural Network Consoleには，Webブラウザで動作するクラウド版\n",
        "\n",
        "ここでは pytorch 版にしてみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kH1IcjvG-fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from https://github.com/dmlc/xgboost/issues/1715\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtGsOUZ4G-fV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk8w3Cb7G-fY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10e3ecd5-6b28-4975-cfef-69aee9059eae"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obdAI9iEG-fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                          transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                          transform=transform)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS7jO0F0Hde2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "650b3b7d-d73b-4fb7-e419-1f629d5e064c"
      },
      "source": [
        "device"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-KZvtm5G-fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAGgk7I6G-fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval=10, dry_run=False):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            if dry_run:\n",
        "                break\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmpYMfVxG-fr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "897d82d0-d371-437c-bcea-da0851ff5bdd"
      },
      "source": [
        "seed = '20200810'\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "use_cuda = False\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "batch_size = 128\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset1)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2)\n",
        "\n",
        "model = Net().to(device)\n",
        "\n",
        "lr = 1.0\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
        "\n",
        "gamma = 0.7\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "\n",
        "epochs = 10\n",
        "save_model = False\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    if save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.338791\n",
            "Train Epoch: 1 [10/60000 (0%)]\tLoss: 2.229356\n",
            "Train Epoch: 1 [20/60000 (0%)]\tLoss: 2.241246\n",
            "Train Epoch: 1 [30/60000 (0%)]\tLoss: 2.040757\n",
            "Train Epoch: 1 [40/60000 (0%)]\tLoss: 2.905559\n",
            "Train Epoch: 1 [50/60000 (0%)]\tLoss: 1.994517\n",
            "Train Epoch: 1 [60/60000 (0%)]\tLoss: 1.930165\n",
            "Train Epoch: 1 [70/60000 (0%)]\tLoss: 1.812054\n",
            "Train Epoch: 1 [80/60000 (0%)]\tLoss: 6.579892\n",
            "Train Epoch: 1 [90/60000 (0%)]\tLoss: 2.217380\n",
            "Train Epoch: 1 [100/60000 (0%)]\tLoss: 2.596457\n",
            "Train Epoch: 1 [110/60000 (0%)]\tLoss: 2.489357\n",
            "Train Epoch: 1 [120/60000 (0%)]\tLoss: 3.015388\n",
            "Train Epoch: 1 [130/60000 (0%)]\tLoss: 1.968655\n",
            "Train Epoch: 1 [140/60000 (0%)]\tLoss: 1.959915\n",
            "Train Epoch: 1 [150/60000 (0%)]\tLoss: 1.693446\n",
            "Train Epoch: 1 [160/60000 (0%)]\tLoss: 5.739815\n",
            "Train Epoch: 1 [170/60000 (0%)]\tLoss: 0.225754\n",
            "Train Epoch: 1 [180/60000 (0%)]\tLoss: 3.540099\n",
            "Train Epoch: 1 [190/60000 (0%)]\tLoss: 2.489274\n",
            "Train Epoch: 1 [200/60000 (0%)]\tLoss: 0.335676\n",
            "Train Epoch: 1 [210/60000 (0%)]\tLoss: 0.028474\n",
            "Train Epoch: 1 [220/60000 (0%)]\tLoss: 7.444034\n",
            "Train Epoch: 1 [230/60000 (0%)]\tLoss: 0.549325\n",
            "Train Epoch: 1 [240/60000 (0%)]\tLoss: 2.641015\n",
            "Train Epoch: 1 [250/60000 (0%)]\tLoss: 5.847663\n",
            "Train Epoch: 1 [260/60000 (0%)]\tLoss: 1.154236\n",
            "Train Epoch: 1 [270/60000 (0%)]\tLoss: 0.034393\n",
            "Train Epoch: 1 [280/60000 (0%)]\tLoss: 0.583452\n",
            "Train Epoch: 1 [290/60000 (0%)]\tLoss: 0.006892\n",
            "Train Epoch: 1 [300/60000 (0%)]\tLoss: 0.663604\n",
            "Train Epoch: 1 [310/60000 (1%)]\tLoss: 0.010871\n",
            "Train Epoch: 1 [320/60000 (1%)]\tLoss: 0.023191\n",
            "Train Epoch: 1 [330/60000 (1%)]\tLoss: 0.049050\n",
            "Train Epoch: 1 [340/60000 (1%)]\tLoss: 2.906173\n",
            "Train Epoch: 1 [350/60000 (1%)]\tLoss: 0.157195\n",
            "Train Epoch: 1 [360/60000 (1%)]\tLoss: 4.397816\n",
            "Train Epoch: 1 [370/60000 (1%)]\tLoss: 4.557905\n",
            "Train Epoch: 1 [380/60000 (1%)]\tLoss: 0.781716\n",
            "Train Epoch: 1 [390/60000 (1%)]\tLoss: 1.062659\n",
            "Train Epoch: 1 [400/60000 (1%)]\tLoss: 0.067764\n",
            "Train Epoch: 1 [410/60000 (1%)]\tLoss: 0.020791\n",
            "Train Epoch: 1 [420/60000 (1%)]\tLoss: 6.404078\n",
            "Train Epoch: 1 [430/60000 (1%)]\tLoss: 0.023119\n",
            "Train Epoch: 1 [440/60000 (1%)]\tLoss: 0.411107\n",
            "Train Epoch: 1 [450/60000 (1%)]\tLoss: 0.001299\n",
            "Train Epoch: 1 [460/60000 (1%)]\tLoss: 1.006853\n",
            "Train Epoch: 1 [470/60000 (1%)]\tLoss: 0.884909\n",
            "Train Epoch: 1 [480/60000 (1%)]\tLoss: 5.049323\n",
            "Train Epoch: 1 [490/60000 (1%)]\tLoss: 0.003493\n",
            "Train Epoch: 1 [500/60000 (1%)]\tLoss: 5.931475\n",
            "Train Epoch: 1 [510/60000 (1%)]\tLoss: 0.094345\n",
            "Train Epoch: 1 [520/60000 (1%)]\tLoss: 0.271971\n",
            "Train Epoch: 1 [530/60000 (1%)]\tLoss: 0.042297\n",
            "Train Epoch: 1 [540/60000 (1%)]\tLoss: 0.506034\n",
            "Train Epoch: 1 [550/60000 (1%)]\tLoss: 0.858107\n",
            "Train Epoch: 1 [560/60000 (1%)]\tLoss: 5.077664\n",
            "Train Epoch: 1 [570/60000 (1%)]\tLoss: 0.326756\n",
            "Train Epoch: 1 [580/60000 (1%)]\tLoss: 0.063285\n",
            "Train Epoch: 1 [590/60000 (1%)]\tLoss: 0.160827\n",
            "Train Epoch: 1 [600/60000 (1%)]\tLoss: 0.005061\n",
            "Train Epoch: 1 [610/60000 (1%)]\tLoss: 1.613103\n",
            "Train Epoch: 1 [620/60000 (1%)]\tLoss: 0.189661\n",
            "Train Epoch: 1 [630/60000 (1%)]\tLoss: 4.799748\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.009206\n",
            "Train Epoch: 1 [650/60000 (1%)]\tLoss: 3.205204\n",
            "Train Epoch: 1 [660/60000 (1%)]\tLoss: 1.884104\n",
            "Train Epoch: 1 [670/60000 (1%)]\tLoss: 12.961743\n",
            "Train Epoch: 1 [680/60000 (1%)]\tLoss: 0.000277\n",
            "Train Epoch: 1 [690/60000 (1%)]\tLoss: 0.707399\n",
            "Train Epoch: 1 [700/60000 (1%)]\tLoss: 0.384193\n",
            "Train Epoch: 1 [710/60000 (1%)]\tLoss: 0.003059\n",
            "Train Epoch: 1 [720/60000 (1%)]\tLoss: 2.518657\n",
            "Train Epoch: 1 [730/60000 (1%)]\tLoss: 0.084084\n",
            "Train Epoch: 1 [740/60000 (1%)]\tLoss: 0.173240\n",
            "Train Epoch: 1 [750/60000 (1%)]\tLoss: 0.000256\n",
            "Train Epoch: 1 [760/60000 (1%)]\tLoss: 2.351615\n",
            "Train Epoch: 1 [770/60000 (1%)]\tLoss: 1.243500\n",
            "Train Epoch: 1 [780/60000 (1%)]\tLoss: 0.010394\n",
            "Train Epoch: 1 [790/60000 (1%)]\tLoss: 0.004074\n",
            "Train Epoch: 1 [800/60000 (1%)]\tLoss: 0.001195\n",
            "Train Epoch: 1 [810/60000 (1%)]\tLoss: 0.028565\n",
            "Train Epoch: 1 [820/60000 (1%)]\tLoss: 0.004305\n",
            "Train Epoch: 1 [830/60000 (1%)]\tLoss: 0.049023\n",
            "Train Epoch: 1 [840/60000 (1%)]\tLoss: 0.980236\n",
            "Train Epoch: 1 [850/60000 (1%)]\tLoss: 12.582771\n",
            "Train Epoch: 1 [860/60000 (1%)]\tLoss: 0.073411\n",
            "Train Epoch: 1 [870/60000 (1%)]\tLoss: 0.973797\n",
            "Train Epoch: 1 [880/60000 (1%)]\tLoss: 0.127107\n",
            "Train Epoch: 1 [890/60000 (1%)]\tLoss: 0.382168\n",
            "Train Epoch: 1 [900/60000 (2%)]\tLoss: 0.199879\n",
            "Train Epoch: 1 [910/60000 (2%)]\tLoss: 0.864287\n",
            "Train Epoch: 1 [920/60000 (2%)]\tLoss: 0.532410\n",
            "Train Epoch: 1 [930/60000 (2%)]\tLoss: 0.022985\n",
            "Train Epoch: 1 [940/60000 (2%)]\tLoss: 0.680593\n",
            "Train Epoch: 1 [950/60000 (2%)]\tLoss: 0.000085\n",
            "Train Epoch: 1 [960/60000 (2%)]\tLoss: 0.064575\n",
            "Train Epoch: 1 [970/60000 (2%)]\tLoss: 2.944817\n",
            "Train Epoch: 1 [980/60000 (2%)]\tLoss: 0.012738\n",
            "Train Epoch: 1 [990/60000 (2%)]\tLoss: 0.000930\n",
            "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 0.109444\n",
            "Train Epoch: 1 [1010/60000 (2%)]\tLoss: 0.001783\n",
            "Train Epoch: 1 [1020/60000 (2%)]\tLoss: 0.000101\n",
            "Train Epoch: 1 [1030/60000 (2%)]\tLoss: 0.519362\n",
            "Train Epoch: 1 [1040/60000 (2%)]\tLoss: 0.514726\n",
            "Train Epoch: 1 [1050/60000 (2%)]\tLoss: 0.024683\n",
            "Train Epoch: 1 [1060/60000 (2%)]\tLoss: 0.014049\n",
            "Train Epoch: 1 [1070/60000 (2%)]\tLoss: 5.672513\n",
            "Train Epoch: 1 [1080/60000 (2%)]\tLoss: 1.003438\n",
            "Train Epoch: 1 [1090/60000 (2%)]\tLoss: 0.001379\n",
            "Train Epoch: 1 [1100/60000 (2%)]\tLoss: 2.320776\n",
            "Train Epoch: 1 [1110/60000 (2%)]\tLoss: 0.194188\n",
            "Train Epoch: 1 [1120/60000 (2%)]\tLoss: 4.954932\n",
            "Train Epoch: 1 [1130/60000 (2%)]\tLoss: 0.157970\n",
            "Train Epoch: 1 [1140/60000 (2%)]\tLoss: 0.972322\n",
            "Train Epoch: 1 [1150/60000 (2%)]\tLoss: 0.638847\n",
            "Train Epoch: 1 [1160/60000 (2%)]\tLoss: 1.633868\n",
            "Train Epoch: 1 [1170/60000 (2%)]\tLoss: 1.895536\n",
            "Train Epoch: 1 [1180/60000 (2%)]\tLoss: 0.002003\n",
            "Train Epoch: 1 [1190/60000 (2%)]\tLoss: 0.058165\n",
            "Train Epoch: 1 [1200/60000 (2%)]\tLoss: 0.057085\n",
            "Train Epoch: 1 [1210/60000 (2%)]\tLoss: 4.156301\n",
            "Train Epoch: 1 [1220/60000 (2%)]\tLoss: 0.708369\n",
            "Train Epoch: 1 [1230/60000 (2%)]\tLoss: 0.002023\n",
            "Train Epoch: 1 [1240/60000 (2%)]\tLoss: 0.056511\n",
            "Train Epoch: 1 [1250/60000 (2%)]\tLoss: 0.171419\n",
            "Train Epoch: 1 [1260/60000 (2%)]\tLoss: 0.364866\n",
            "Train Epoch: 1 [1270/60000 (2%)]\tLoss: 4.460553\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.068385\n",
            "Train Epoch: 1 [1290/60000 (2%)]\tLoss: 1.047068\n",
            "Train Epoch: 1 [1300/60000 (2%)]\tLoss: 0.000066\n",
            "Train Epoch: 1 [1310/60000 (2%)]\tLoss: 0.032866\n",
            "Train Epoch: 1 [1320/60000 (2%)]\tLoss: 0.025564\n",
            "Train Epoch: 1 [1330/60000 (2%)]\tLoss: 2.873044\n",
            "Train Epoch: 1 [1340/60000 (2%)]\tLoss: 0.011697\n",
            "Train Epoch: 1 [1350/60000 (2%)]\tLoss: 0.003864\n",
            "Train Epoch: 1 [1360/60000 (2%)]\tLoss: 0.578288\n",
            "Train Epoch: 1 [1370/60000 (2%)]\tLoss: 0.019807\n",
            "Train Epoch: 1 [1380/60000 (2%)]\tLoss: 0.020522\n",
            "Train Epoch: 1 [1390/60000 (2%)]\tLoss: 0.060050\n",
            "Train Epoch: 1 [1400/60000 (2%)]\tLoss: 0.109073\n",
            "Train Epoch: 1 [1410/60000 (2%)]\tLoss: 0.033173\n",
            "Train Epoch: 1 [1420/60000 (2%)]\tLoss: 1.404907\n",
            "Train Epoch: 1 [1430/60000 (2%)]\tLoss: 0.041347\n",
            "Train Epoch: 1 [1440/60000 (2%)]\tLoss: 0.000502\n",
            "Train Epoch: 1 [1450/60000 (2%)]\tLoss: 0.038495\n",
            "Train Epoch: 1 [1460/60000 (2%)]\tLoss: 0.001663\n",
            "Train Epoch: 1 [1470/60000 (2%)]\tLoss: 0.017082\n",
            "Train Epoch: 1 [1480/60000 (2%)]\tLoss: 0.003354\n",
            "Train Epoch: 1 [1490/60000 (2%)]\tLoss: 0.000062\n",
            "Train Epoch: 1 [1500/60000 (2%)]\tLoss: 1.288597\n",
            "Train Epoch: 1 [1510/60000 (3%)]\tLoss: 0.178108\n",
            "Train Epoch: 1 [1520/60000 (3%)]\tLoss: 0.041067\n",
            "Train Epoch: 1 [1530/60000 (3%)]\tLoss: 0.030922\n",
            "Train Epoch: 1 [1540/60000 (3%)]\tLoss: 0.003245\n",
            "Train Epoch: 1 [1550/60000 (3%)]\tLoss: 0.000807\n",
            "Train Epoch: 1 [1560/60000 (3%)]\tLoss: 1.102667\n",
            "Train Epoch: 1 [1570/60000 (3%)]\tLoss: 0.033822\n",
            "Train Epoch: 1 [1580/60000 (3%)]\tLoss: 2.819968\n",
            "Train Epoch: 1 [1590/60000 (3%)]\tLoss: 0.103928\n",
            "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.923868\n",
            "Train Epoch: 1 [1610/60000 (3%)]\tLoss: 0.028520\n",
            "Train Epoch: 1 [1620/60000 (3%)]\tLoss: 5.420857\n",
            "Train Epoch: 1 [1630/60000 (3%)]\tLoss: 0.063189\n",
            "Train Epoch: 1 [1640/60000 (3%)]\tLoss: 0.001353\n",
            "Train Epoch: 1 [1650/60000 (3%)]\tLoss: 0.186312\n",
            "Train Epoch: 1 [1660/60000 (3%)]\tLoss: 0.169648\n",
            "Train Epoch: 1 [1670/60000 (3%)]\tLoss: 0.104831\n",
            "Train Epoch: 1 [1680/60000 (3%)]\tLoss: 0.000004\n",
            "Train Epoch: 1 [1690/60000 (3%)]\tLoss: 0.001949\n",
            "Train Epoch: 1 [1700/60000 (3%)]\tLoss: 0.000017\n",
            "Train Epoch: 1 [1710/60000 (3%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [1720/60000 (3%)]\tLoss: 0.005490\n",
            "Train Epoch: 1 [1730/60000 (3%)]\tLoss: 0.012437\n",
            "Train Epoch: 1 [1740/60000 (3%)]\tLoss: 0.335147\n",
            "Train Epoch: 1 [1750/60000 (3%)]\tLoss: 0.000031\n",
            "Train Epoch: 1 [1760/60000 (3%)]\tLoss: 0.002034\n",
            "Train Epoch: 1 [1770/60000 (3%)]\tLoss: 0.000093\n",
            "Train Epoch: 1 [1780/60000 (3%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [1790/60000 (3%)]\tLoss: 0.124324\n",
            "Train Epoch: 1 [1800/60000 (3%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [1810/60000 (3%)]\tLoss: 0.007442\n",
            "Train Epoch: 1 [1820/60000 (3%)]\tLoss: 1.146952\n",
            "Train Epoch: 1 [1830/60000 (3%)]\tLoss: 0.000025\n",
            "Train Epoch: 1 [1840/60000 (3%)]\tLoss: 0.001099\n",
            "Train Epoch: 1 [1850/60000 (3%)]\tLoss: 0.248053\n",
            "Train Epoch: 1 [1860/60000 (3%)]\tLoss: 0.000536\n",
            "Train Epoch: 1 [1870/60000 (3%)]\tLoss: 0.000552\n",
            "Train Epoch: 1 [1880/60000 (3%)]\tLoss: 1.182386\n",
            "Train Epoch: 1 [1890/60000 (3%)]\tLoss: 0.009311\n",
            "Train Epoch: 1 [1900/60000 (3%)]\tLoss: 0.256212\n",
            "Train Epoch: 1 [1910/60000 (3%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 3.283491\n",
            "Train Epoch: 1 [1930/60000 (3%)]\tLoss: 0.000060\n",
            "Train Epoch: 1 [1940/60000 (3%)]\tLoss: 2.720685\n",
            "Train Epoch: 1 [1950/60000 (3%)]\tLoss: 0.084833\n",
            "Train Epoch: 1 [1960/60000 (3%)]\tLoss: 0.000905\n",
            "Train Epoch: 1 [1970/60000 (3%)]\tLoss: 0.000045\n",
            "Train Epoch: 1 [1980/60000 (3%)]\tLoss: 0.000102\n",
            "Train Epoch: 1 [1990/60000 (3%)]\tLoss: 0.001416\n",
            "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.667735\n",
            "Train Epoch: 1 [2010/60000 (3%)]\tLoss: 0.177317\n",
            "Train Epoch: 1 [2020/60000 (3%)]\tLoss: 0.003154\n",
            "Train Epoch: 1 [2030/60000 (3%)]\tLoss: 0.190337\n",
            "Train Epoch: 1 [2040/60000 (3%)]\tLoss: 3.939273\n",
            "Train Epoch: 1 [2050/60000 (3%)]\tLoss: 0.497340\n",
            "Train Epoch: 1 [2060/60000 (3%)]\tLoss: 0.002602\n",
            "Train Epoch: 1 [2070/60000 (3%)]\tLoss: 4.094506\n",
            "Train Epoch: 1 [2080/60000 (3%)]\tLoss: 0.787340\n",
            "Train Epoch: 1 [2090/60000 (3%)]\tLoss: 0.004688\n",
            "Train Epoch: 1 [2100/60000 (4%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [2110/60000 (4%)]\tLoss: 0.001308\n",
            "Train Epoch: 1 [2120/60000 (4%)]\tLoss: 1.063659\n",
            "Train Epoch: 1 [2130/60000 (4%)]\tLoss: 0.000081\n",
            "Train Epoch: 1 [2140/60000 (4%)]\tLoss: 0.253437\n",
            "Train Epoch: 1 [2150/60000 (4%)]\tLoss: 0.156132\n",
            "Train Epoch: 1 [2160/60000 (4%)]\tLoss: 2.094598\n",
            "Train Epoch: 1 [2170/60000 (4%)]\tLoss: 0.008394\n",
            "Train Epoch: 1 [2180/60000 (4%)]\tLoss: 0.630268\n",
            "Train Epoch: 1 [2190/60000 (4%)]\tLoss: 0.000013\n",
            "Train Epoch: 1 [2200/60000 (4%)]\tLoss: 4.254245\n",
            "Train Epoch: 1 [2210/60000 (4%)]\tLoss: 0.063996\n",
            "Train Epoch: 1 [2220/60000 (4%)]\tLoss: 1.218473\n",
            "Train Epoch: 1 [2230/60000 (4%)]\tLoss: 0.000812\n",
            "Train Epoch: 1 [2240/60000 (4%)]\tLoss: 0.004122\n",
            "Train Epoch: 1 [2250/60000 (4%)]\tLoss: 0.038148\n",
            "Train Epoch: 1 [2260/60000 (4%)]\tLoss: 0.000107\n",
            "Train Epoch: 1 [2270/60000 (4%)]\tLoss: 0.000018\n",
            "Train Epoch: 1 [2280/60000 (4%)]\tLoss: 0.196116\n",
            "Train Epoch: 1 [2290/60000 (4%)]\tLoss: 0.004317\n",
            "Train Epoch: 1 [2300/60000 (4%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2310/60000 (4%)]\tLoss: 0.006278\n",
            "Train Epoch: 1 [2320/60000 (4%)]\tLoss: 1.900293\n",
            "Train Epoch: 1 [2330/60000 (4%)]\tLoss: 0.014233\n",
            "Train Epoch: 1 [2340/60000 (4%)]\tLoss: 0.000010\n",
            "Train Epoch: 1 [2350/60000 (4%)]\tLoss: 0.140481\n",
            "Train Epoch: 1 [2360/60000 (4%)]\tLoss: 0.010551\n",
            "Train Epoch: 1 [2370/60000 (4%)]\tLoss: 0.094765\n",
            "Train Epoch: 1 [2380/60000 (4%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [2390/60000 (4%)]\tLoss: 0.008054\n",
            "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 0.000013\n",
            "Train Epoch: 1 [2410/60000 (4%)]\tLoss: 11.589177\n",
            "Train Epoch: 1 [2420/60000 (4%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [2430/60000 (4%)]\tLoss: 0.035974\n",
            "Train Epoch: 1 [2440/60000 (4%)]\tLoss: 0.011267\n",
            "Train Epoch: 1 [2450/60000 (4%)]\tLoss: 7.427733\n",
            "Train Epoch: 1 [2460/60000 (4%)]\tLoss: 0.077303\n",
            "Train Epoch: 1 [2470/60000 (4%)]\tLoss: 0.493085\n",
            "Train Epoch: 1 [2480/60000 (4%)]\tLoss: 0.846219\n",
            "Train Epoch: 1 [2490/60000 (4%)]\tLoss: 0.072456\n",
            "Train Epoch: 1 [2500/60000 (4%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2510/60000 (4%)]\tLoss: 0.001061\n",
            "Train Epoch: 1 [2520/60000 (4%)]\tLoss: 0.000028\n",
            "Train Epoch: 1 [2530/60000 (4%)]\tLoss: 0.000190\n",
            "Train Epoch: 1 [2540/60000 (4%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2550/60000 (4%)]\tLoss: 0.900507\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [2570/60000 (4%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2580/60000 (4%)]\tLoss: 0.000705\n",
            "Train Epoch: 1 [2590/60000 (4%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [2600/60000 (4%)]\tLoss: 0.003628\n",
            "Train Epoch: 1 [2610/60000 (4%)]\tLoss: 0.000172\n",
            "Train Epoch: 1 [2620/60000 (4%)]\tLoss: 0.025642\n",
            "Train Epoch: 1 [2630/60000 (4%)]\tLoss: 0.000004\n",
            "Train Epoch: 1 [2640/60000 (4%)]\tLoss: 0.003699\n",
            "Train Epoch: 1 [2650/60000 (4%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2660/60000 (4%)]\tLoss: 0.392674\n",
            "Train Epoch: 1 [2670/60000 (4%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2680/60000 (4%)]\tLoss: 0.167046\n",
            "Train Epoch: 1 [2690/60000 (4%)]\tLoss: 1.876066\n",
            "Train Epoch: 1 [2700/60000 (4%)]\tLoss: 0.000093\n",
            "Train Epoch: 1 [2710/60000 (5%)]\tLoss: 0.000046\n",
            "Train Epoch: 1 [2720/60000 (5%)]\tLoss: 16.432585\n",
            "Train Epoch: 1 [2730/60000 (5%)]\tLoss: 0.010713\n",
            "Train Epoch: 1 [2740/60000 (5%)]\tLoss: 0.003356\n",
            "Train Epoch: 1 [2750/60000 (5%)]\tLoss: 0.000090\n",
            "Train Epoch: 1 [2760/60000 (5%)]\tLoss: 0.001200\n",
            "Train Epoch: 1 [2770/60000 (5%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2780/60000 (5%)]\tLoss: 0.779271\n",
            "Train Epoch: 1 [2790/60000 (5%)]\tLoss: 0.003904\n",
            "Train Epoch: 1 [2800/60000 (5%)]\tLoss: 0.135023\n",
            "Train Epoch: 1 [2810/60000 (5%)]\tLoss: 0.064125\n",
            "Train Epoch: 1 [2820/60000 (5%)]\tLoss: 0.000372\n",
            "Train Epoch: 1 [2830/60000 (5%)]\tLoss: 0.001010\n",
            "Train Epoch: 1 [2840/60000 (5%)]\tLoss: 0.008821\n",
            "Train Epoch: 1 [2850/60000 (5%)]\tLoss: 0.002559\n",
            "Train Epoch: 1 [2860/60000 (5%)]\tLoss: 0.233388\n",
            "Train Epoch: 1 [2870/60000 (5%)]\tLoss: 0.000012\n",
            "Train Epoch: 1 [2880/60000 (5%)]\tLoss: 0.940446\n",
            "Train Epoch: 1 [2890/60000 (5%)]\tLoss: 0.000023\n",
            "Train Epoch: 1 [2900/60000 (5%)]\tLoss: 1.165872\n",
            "Train Epoch: 1 [2910/60000 (5%)]\tLoss: 0.000420\n",
            "Train Epoch: 1 [2920/60000 (5%)]\tLoss: 0.000012\n",
            "Train Epoch: 1 [2930/60000 (5%)]\tLoss: 0.000080\n",
            "Train Epoch: 1 [2940/60000 (5%)]\tLoss: 0.085257\n",
            "Train Epoch: 1 [2950/60000 (5%)]\tLoss: 0.071406\n",
            "Train Epoch: 1 [2960/60000 (5%)]\tLoss: 0.212021\n",
            "Train Epoch: 1 [2970/60000 (5%)]\tLoss: 0.008031\n",
            "Train Epoch: 1 [2980/60000 (5%)]\tLoss: 0.368376\n",
            "Train Epoch: 1 [2990/60000 (5%)]\tLoss: 0.000254\n",
            "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 1.441352\n",
            "Train Epoch: 1 [3010/60000 (5%)]\tLoss: 0.666382\n",
            "Train Epoch: 1 [3020/60000 (5%)]\tLoss: 0.012572\n",
            "Train Epoch: 1 [3030/60000 (5%)]\tLoss: 7.208068\n",
            "Train Epoch: 1 [3040/60000 (5%)]\tLoss: 0.196793\n",
            "Train Epoch: 1 [3050/60000 (5%)]\tLoss: 0.000594\n",
            "Train Epoch: 1 [3060/60000 (5%)]\tLoss: 0.992888\n",
            "Train Epoch: 1 [3070/60000 (5%)]\tLoss: 3.688819\n",
            "Train Epoch: 1 [3080/60000 (5%)]\tLoss: 0.006814\n",
            "Train Epoch: 1 [3090/60000 (5%)]\tLoss: 0.026600\n",
            "Train Epoch: 1 [3100/60000 (5%)]\tLoss: 0.277209\n",
            "Train Epoch: 1 [3110/60000 (5%)]\tLoss: 0.137752\n",
            "Train Epoch: 1 [3120/60000 (5%)]\tLoss: 0.058647\n",
            "Train Epoch: 1 [3130/60000 (5%)]\tLoss: 0.000281\n",
            "Train Epoch: 1 [3140/60000 (5%)]\tLoss: 2.118033\n",
            "Train Epoch: 1 [3150/60000 (5%)]\tLoss: 3.858366\n",
            "Train Epoch: 1 [3160/60000 (5%)]\tLoss: 0.013798\n",
            "Train Epoch: 1 [3170/60000 (5%)]\tLoss: 1.149910\n",
            "Train Epoch: 1 [3180/60000 (5%)]\tLoss: 0.105153\n",
            "Train Epoch: 1 [3190/60000 (5%)]\tLoss: 0.010346\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.007840\n",
            "Train Epoch: 1 [3210/60000 (5%)]\tLoss: 0.012386\n",
            "Train Epoch: 1 [3220/60000 (5%)]\tLoss: 4.990105\n",
            "Train Epoch: 1 [3230/60000 (5%)]\tLoss: 0.000032\n",
            "Train Epoch: 1 [3240/60000 (5%)]\tLoss: 0.000016\n",
            "Train Epoch: 1 [3250/60000 (5%)]\tLoss: 0.000011\n",
            "Train Epoch: 1 [3260/60000 (5%)]\tLoss: 0.790103\n",
            "Train Epoch: 1 [3270/60000 (5%)]\tLoss: 0.000008\n",
            "Train Epoch: 1 [3280/60000 (5%)]\tLoss: 0.000024\n",
            "Train Epoch: 1 [3290/60000 (5%)]\tLoss: 3.886545\n",
            "Train Epoch: 1 [3300/60000 (6%)]\tLoss: 0.002490\n",
            "Train Epoch: 1 [3310/60000 (6%)]\tLoss: 0.027357\n",
            "Train Epoch: 1 [3320/60000 (6%)]\tLoss: 0.008199\n",
            "Train Epoch: 1 [3330/60000 (6%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [3340/60000 (6%)]\tLoss: 0.115120\n",
            "Train Epoch: 1 [3350/60000 (6%)]\tLoss: 0.104251\n",
            "Train Epoch: 1 [3360/60000 (6%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [3370/60000 (6%)]\tLoss: 4.216467\n",
            "Train Epoch: 1 [3380/60000 (6%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [3390/60000 (6%)]\tLoss: 0.008474\n",
            "Train Epoch: 1 [3400/60000 (6%)]\tLoss: 0.000057\n",
            "Train Epoch: 1 [3410/60000 (6%)]\tLoss: 0.018439\n",
            "Train Epoch: 1 [3420/60000 (6%)]\tLoss: 0.000123\n",
            "Train Epoch: 1 [3430/60000 (6%)]\tLoss: 1.555813\n",
            "Train Epoch: 1 [3440/60000 (6%)]\tLoss: 0.012648\n",
            "Train Epoch: 1 [3450/60000 (6%)]\tLoss: 0.000200\n",
            "Train Epoch: 1 [3460/60000 (6%)]\tLoss: 0.002868\n",
            "Train Epoch: 1 [3470/60000 (6%)]\tLoss: 0.001854\n",
            "Train Epoch: 1 [3480/60000 (6%)]\tLoss: 0.000350\n",
            "Train Epoch: 1 [3490/60000 (6%)]\tLoss: 0.003010\n",
            "Train Epoch: 1 [3500/60000 (6%)]\tLoss: 0.835819\n",
            "Train Epoch: 1 [3510/60000 (6%)]\tLoss: 8.217625\n",
            "Train Epoch: 1 [3520/60000 (6%)]\tLoss: 0.439770\n",
            "Train Epoch: 1 [3530/60000 (6%)]\tLoss: 0.011941\n",
            "Train Epoch: 1 [3540/60000 (6%)]\tLoss: 0.000033\n",
            "Train Epoch: 1 [3550/60000 (6%)]\tLoss: 0.000016\n",
            "Train Epoch: 1 [3560/60000 (6%)]\tLoss: 0.008238\n",
            "Train Epoch: 1 [3570/60000 (6%)]\tLoss: 4.334855\n",
            "Train Epoch: 1 [3580/60000 (6%)]\tLoss: 0.152019\n",
            "Train Epoch: 1 [3590/60000 (6%)]\tLoss: 0.109510\n",
            "Train Epoch: 1 [3600/60000 (6%)]\tLoss: 0.000028\n",
            "Train Epoch: 1 [3610/60000 (6%)]\tLoss: 0.019898\n",
            "Train Epoch: 1 [3620/60000 (6%)]\tLoss: 0.000113\n",
            "Train Epoch: 1 [3630/60000 (6%)]\tLoss: 0.004484\n",
            "Train Epoch: 1 [3640/60000 (6%)]\tLoss: 0.000491\n",
            "Train Epoch: 1 [3650/60000 (6%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [3660/60000 (6%)]\tLoss: 0.000094\n",
            "Train Epoch: 1 [3670/60000 (6%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [3680/60000 (6%)]\tLoss: 0.005234\n",
            "Train Epoch: 1 [3690/60000 (6%)]\tLoss: 0.000131\n",
            "Train Epoch: 1 [3700/60000 (6%)]\tLoss: 0.001246\n",
            "Train Epoch: 1 [3710/60000 (6%)]\tLoss: 0.009934\n",
            "Train Epoch: 1 [3720/60000 (6%)]\tLoss: 0.002543\n",
            "Train Epoch: 1 [3730/60000 (6%)]\tLoss: 13.557889\n",
            "Train Epoch: 1 [3740/60000 (6%)]\tLoss: 0.901360\n",
            "Train Epoch: 1 [3750/60000 (6%)]\tLoss: 0.000172\n",
            "Train Epoch: 1 [3760/60000 (6%)]\tLoss: 0.006622\n",
            "Train Epoch: 1 [3770/60000 (6%)]\tLoss: 0.000688\n",
            "Train Epoch: 1 [3780/60000 (6%)]\tLoss: 0.002554\n",
            "Train Epoch: 1 [3790/60000 (6%)]\tLoss: 0.018322\n",
            "Train Epoch: 1 [3800/60000 (6%)]\tLoss: 0.000420\n",
            "Train Epoch: 1 [3810/60000 (6%)]\tLoss: 3.585713\n",
            "Train Epoch: 1 [3820/60000 (6%)]\tLoss: 0.001302\n",
            "Train Epoch: 1 [3830/60000 (6%)]\tLoss: 0.000332\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.014288\n",
            "Train Epoch: 1 [3850/60000 (6%)]\tLoss: 0.005990\n",
            "Train Epoch: 1 [3860/60000 (6%)]\tLoss: 0.010651\n",
            "Train Epoch: 1 [3870/60000 (6%)]\tLoss: 0.085498\n",
            "Train Epoch: 1 [3880/60000 (6%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [3890/60000 (6%)]\tLoss: 0.000514\n",
            "Train Epoch: 1 [3900/60000 (6%)]\tLoss: 0.000322\n",
            "Train Epoch: 1 [3910/60000 (7%)]\tLoss: 0.000849\n",
            "Train Epoch: 1 [3920/60000 (7%)]\tLoss: 0.000524\n",
            "Train Epoch: 1 [3930/60000 (7%)]\tLoss: 0.003987\n",
            "Train Epoch: 1 [3940/60000 (7%)]\tLoss: 0.000003\n",
            "Train Epoch: 1 [3950/60000 (7%)]\tLoss: 0.006874\n",
            "Train Epoch: 1 [3960/60000 (7%)]\tLoss: 3.230838\n",
            "Train Epoch: 1 [3970/60000 (7%)]\tLoss: 1.133074\n",
            "Train Epoch: 1 [3980/60000 (7%)]\tLoss: 0.091742\n",
            "Train Epoch: 1 [3990/60000 (7%)]\tLoss: 0.009704\n",
            "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.073145\n",
            "Train Epoch: 1 [4010/60000 (7%)]\tLoss: 0.000178\n",
            "Train Epoch: 1 [4020/60000 (7%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4030/60000 (7%)]\tLoss: 1.807205\n",
            "Train Epoch: 1 [4040/60000 (7%)]\tLoss: 0.202518\n",
            "Train Epoch: 1 [4050/60000 (7%)]\tLoss: 2.445890\n",
            "Train Epoch: 1 [4060/60000 (7%)]\tLoss: 0.001311\n",
            "Train Epoch: 1 [4070/60000 (7%)]\tLoss: 0.000009\n",
            "Train Epoch: 1 [4080/60000 (7%)]\tLoss: 2.036564\n",
            "Train Epoch: 1 [4090/60000 (7%)]\tLoss: 0.011144\n",
            "Train Epoch: 1 [4100/60000 (7%)]\tLoss: 0.000391\n",
            "Train Epoch: 1 [4110/60000 (7%)]\tLoss: 0.288096\n",
            "Train Epoch: 1 [4120/60000 (7%)]\tLoss: 0.000021\n",
            "Train Epoch: 1 [4130/60000 (7%)]\tLoss: 0.000096\n",
            "Train Epoch: 1 [4140/60000 (7%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [4150/60000 (7%)]\tLoss: 0.091883\n",
            "Train Epoch: 1 [4160/60000 (7%)]\tLoss: 0.000200\n",
            "Train Epoch: 1 [4170/60000 (7%)]\tLoss: 0.012759\n",
            "Train Epoch: 1 [4180/60000 (7%)]\tLoss: 0.029319\n",
            "Train Epoch: 1 [4190/60000 (7%)]\tLoss: 0.021527\n",
            "Train Epoch: 1 [4200/60000 (7%)]\tLoss: 0.801016\n",
            "Train Epoch: 1 [4210/60000 (7%)]\tLoss: 0.003934\n",
            "Train Epoch: 1 [4220/60000 (7%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4230/60000 (7%)]\tLoss: 0.000011\n",
            "Train Epoch: 1 [4240/60000 (7%)]\tLoss: 0.000027\n",
            "Train Epoch: 1 [4250/60000 (7%)]\tLoss: 0.011130\n",
            "Train Epoch: 1 [4260/60000 (7%)]\tLoss: 0.000659\n",
            "Train Epoch: 1 [4270/60000 (7%)]\tLoss: 0.000352\n",
            "Train Epoch: 1 [4280/60000 (7%)]\tLoss: 0.002021\n",
            "Train Epoch: 1 [4290/60000 (7%)]\tLoss: 1.937814\n",
            "Train Epoch: 1 [4300/60000 (7%)]\tLoss: 0.000581\n",
            "Train Epoch: 1 [4310/60000 (7%)]\tLoss: 0.000749\n",
            "Train Epoch: 1 [4320/60000 (7%)]\tLoss: 0.009253\n",
            "Train Epoch: 1 [4330/60000 (7%)]\tLoss: 0.000048\n",
            "Train Epoch: 1 [4340/60000 (7%)]\tLoss: 0.004576\n",
            "Train Epoch: 1 [4350/60000 (7%)]\tLoss: 0.002120\n",
            "Train Epoch: 1 [4360/60000 (7%)]\tLoss: 1.814761\n",
            "Train Epoch: 1 [4370/60000 (7%)]\tLoss: 0.013102\n",
            "Train Epoch: 1 [4380/60000 (7%)]\tLoss: 0.000591\n",
            "Train Epoch: 1 [4390/60000 (7%)]\tLoss: 0.000050\n",
            "Train Epoch: 1 [4400/60000 (7%)]\tLoss: 0.002540\n",
            "Train Epoch: 1 [4410/60000 (7%)]\tLoss: 0.000015\n",
            "Train Epoch: 1 [4420/60000 (7%)]\tLoss: 0.054645\n",
            "Train Epoch: 1 [4430/60000 (7%)]\tLoss: 0.000155\n",
            "Train Epoch: 1 [4440/60000 (7%)]\tLoss: 0.000500\n",
            "Train Epoch: 1 [4450/60000 (7%)]\tLoss: 0.014014\n",
            "Train Epoch: 1 [4460/60000 (7%)]\tLoss: 7.655735\n",
            "Train Epoch: 1 [4470/60000 (7%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.000006\n",
            "Train Epoch: 1 [4490/60000 (7%)]\tLoss: 0.000003\n",
            "Train Epoch: 1 [4500/60000 (8%)]\tLoss: 1.740017\n",
            "Train Epoch: 1 [4510/60000 (8%)]\tLoss: 0.151996\n",
            "Train Epoch: 1 [4520/60000 (8%)]\tLoss: 0.000148\n",
            "Train Epoch: 1 [4530/60000 (8%)]\tLoss: 0.158445\n",
            "Train Epoch: 1 [4540/60000 (8%)]\tLoss: 0.001370\n",
            "Train Epoch: 1 [4550/60000 (8%)]\tLoss: 0.041095\n",
            "Train Epoch: 1 [4560/60000 (8%)]\tLoss: 0.752597\n",
            "Train Epoch: 1 [4570/60000 (8%)]\tLoss: 0.019652\n",
            "Train Epoch: 1 [4580/60000 (8%)]\tLoss: 0.044040\n",
            "Train Epoch: 1 [4590/60000 (8%)]\tLoss: 2.836558\n",
            "Train Epoch: 1 [4600/60000 (8%)]\tLoss: 0.000350\n",
            "Train Epoch: 1 [4610/60000 (8%)]\tLoss: 0.000127\n",
            "Train Epoch: 1 [4620/60000 (8%)]\tLoss: 0.009361\n",
            "Train Epoch: 1 [4630/60000 (8%)]\tLoss: 0.000006\n",
            "Train Epoch: 1 [4640/60000 (8%)]\tLoss: 2.814823\n",
            "Train Epoch: 1 [4650/60000 (8%)]\tLoss: 0.000082\n",
            "Train Epoch: 1 [4660/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4670/60000 (8%)]\tLoss: 0.000058\n",
            "Train Epoch: 1 [4680/60000 (8%)]\tLoss: 0.618827\n",
            "Train Epoch: 1 [4690/60000 (8%)]\tLoss: 0.008712\n",
            "Train Epoch: 1 [4700/60000 (8%)]\tLoss: 0.018919\n",
            "Train Epoch: 1 [4710/60000 (8%)]\tLoss: 0.758822\n",
            "Train Epoch: 1 [4720/60000 (8%)]\tLoss: 0.002347\n",
            "Train Epoch: 1 [4730/60000 (8%)]\tLoss: 2.914337\n",
            "Train Epoch: 1 [4740/60000 (8%)]\tLoss: 0.000848\n",
            "Train Epoch: 1 [4750/60000 (8%)]\tLoss: 0.000005\n",
            "Train Epoch: 1 [4760/60000 (8%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [4770/60000 (8%)]\tLoss: 0.004981\n",
            "Train Epoch: 1 [4780/60000 (8%)]\tLoss: 0.015543\n",
            "Train Epoch: 1 [4790/60000 (8%)]\tLoss: 0.000037\n",
            "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.028615\n",
            "Train Epoch: 1 [4810/60000 (8%)]\tLoss: 1.724630\n",
            "Train Epoch: 1 [4820/60000 (8%)]\tLoss: 0.000238\n",
            "Train Epoch: 1 [4830/60000 (8%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [4840/60000 (8%)]\tLoss: 0.002995\n",
            "Train Epoch: 1 [4850/60000 (8%)]\tLoss: 0.000765\n",
            "Train Epoch: 1 [4860/60000 (8%)]\tLoss: 0.290817\n",
            "Train Epoch: 1 [4870/60000 (8%)]\tLoss: 0.000008\n",
            "Train Epoch: 1 [4880/60000 (8%)]\tLoss: 1.985192\n",
            "Train Epoch: 1 [4890/60000 (8%)]\tLoss: 0.000034\n",
            "Train Epoch: 1 [4900/60000 (8%)]\tLoss: 0.000007\n",
            "Train Epoch: 1 [4910/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4920/60000 (8%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [4930/60000 (8%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [4940/60000 (8%)]\tLoss: 0.205867\n",
            "Train Epoch: 1 [4950/60000 (8%)]\tLoss: 0.000017\n",
            "Train Epoch: 1 [4960/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4970/60000 (8%)]\tLoss: 0.000380\n",
            "Train Epoch: 1 [4980/60000 (8%)]\tLoss: 0.000772\n",
            "Train Epoch: 1 [4990/60000 (8%)]\tLoss: 0.116452\n",
            "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.000048\n",
            "Train Epoch: 1 [5010/60000 (8%)]\tLoss: 0.523877\n",
            "Train Epoch: 1 [5020/60000 (8%)]\tLoss: 0.000033\n",
            "Train Epoch: 1 [5030/60000 (8%)]\tLoss: 0.000108\n",
            "Train Epoch: 1 [5040/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5050/60000 (8%)]\tLoss: 0.002387\n",
            "Train Epoch: 1 [5060/60000 (8%)]\tLoss: 3.349828\n",
            "Train Epoch: 1 [5070/60000 (8%)]\tLoss: 0.000067\n",
            "Train Epoch: 1 [5080/60000 (8%)]\tLoss: 0.131319\n",
            "Train Epoch: 1 [5090/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5100/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5110/60000 (9%)]\tLoss: 0.000781\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5130/60000 (9%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5140/60000 (9%)]\tLoss: 0.442537\n",
            "Train Epoch: 1 [5150/60000 (9%)]\tLoss: 0.007277\n",
            "Train Epoch: 1 [5160/60000 (9%)]\tLoss: 0.000147\n",
            "Train Epoch: 1 [5170/60000 (9%)]\tLoss: 0.930127\n",
            "Train Epoch: 1 [5180/60000 (9%)]\tLoss: 0.354901\n",
            "Train Epoch: 1 [5190/60000 (9%)]\tLoss: 0.000003\n",
            "Train Epoch: 1 [5200/60000 (9%)]\tLoss: 0.000009\n",
            "Train Epoch: 1 [5210/60000 (9%)]\tLoss: 5.398219\n",
            "Train Epoch: 1 [5220/60000 (9%)]\tLoss: 0.001452\n",
            "Train Epoch: 1 [5230/60000 (9%)]\tLoss: 0.000819\n",
            "Train Epoch: 1 [5240/60000 (9%)]\tLoss: 0.000766\n",
            "Train Epoch: 1 [5250/60000 (9%)]\tLoss: 0.026837\n",
            "Train Epoch: 1 [5260/60000 (9%)]\tLoss: 0.179932\n",
            "Train Epoch: 1 [5270/60000 (9%)]\tLoss: 0.000003\n",
            "Train Epoch: 1 [5280/60000 (9%)]\tLoss: 0.026401\n",
            "Train Epoch: 1 [5290/60000 (9%)]\tLoss: 0.000192\n",
            "Train Epoch: 1 [5300/60000 (9%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [5310/60000 (9%)]\tLoss: 0.000678\n",
            "Train Epoch: 1 [5320/60000 (9%)]\tLoss: 0.001371\n",
            "Train Epoch: 1 [5330/60000 (9%)]\tLoss: 0.000003\n",
            "Train Epoch: 1 [5340/60000 (9%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5350/60000 (9%)]\tLoss: 0.000658\n",
            "Train Epoch: 1 [5360/60000 (9%)]\tLoss: 0.004695\n",
            "Train Epoch: 1 [5370/60000 (9%)]\tLoss: 0.866386\n",
            "Train Epoch: 1 [5380/60000 (9%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5390/60000 (9%)]\tLoss: 2.175741\n",
            "Train Epoch: 1 [5400/60000 (9%)]\tLoss: 0.013264\n",
            "Train Epoch: 1 [5410/60000 (9%)]\tLoss: 0.000167\n",
            "Train Epoch: 1 [5420/60000 (9%)]\tLoss: 0.058748\n",
            "Train Epoch: 1 [5430/60000 (9%)]\tLoss: 19.299332\n",
            "Train Epoch: 1 [5440/60000 (9%)]\tLoss: 0.074448\n",
            "Train Epoch: 1 [5450/60000 (9%)]\tLoss: 0.000185\n",
            "Train Epoch: 1 [5460/60000 (9%)]\tLoss: 0.000011\n",
            "Train Epoch: 1 [5470/60000 (9%)]\tLoss: 0.001313\n",
            "Train Epoch: 1 [5480/60000 (9%)]\tLoss: 0.000037\n",
            "Train Epoch: 1 [5490/60000 (9%)]\tLoss: 0.000015\n",
            "Train Epoch: 1 [5500/60000 (9%)]\tLoss: 0.000218\n",
            "Train Epoch: 1 [5510/60000 (9%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [5520/60000 (9%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [5530/60000 (9%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5540/60000 (9%)]\tLoss: 0.000020\n",
            "Train Epoch: 1 [5550/60000 (9%)]\tLoss: 1.120754\n",
            "Train Epoch: 1 [5560/60000 (9%)]\tLoss: 0.081503\n",
            "Train Epoch: 1 [5570/60000 (9%)]\tLoss: 0.000264\n",
            "Train Epoch: 1 [5580/60000 (9%)]\tLoss: 2.371303\n",
            "Train Epoch: 1 [5590/60000 (9%)]\tLoss: 1.080946\n",
            "Train Epoch: 1 [5600/60000 (9%)]\tLoss: 0.106469\n",
            "Train Epoch: 1 [5610/60000 (9%)]\tLoss: 0.003138\n",
            "Train Epoch: 1 [5620/60000 (9%)]\tLoss: 0.004615\n",
            "Train Epoch: 1 [5630/60000 (9%)]\tLoss: 1.233146\n",
            "Train Epoch: 1 [5640/60000 (9%)]\tLoss: 0.001335\n",
            "Train Epoch: 1 [5650/60000 (9%)]\tLoss: 1.151102\n",
            "Train Epoch: 1 [5660/60000 (9%)]\tLoss: 0.000061\n",
            "Train Epoch: 1 [5670/60000 (9%)]\tLoss: 0.002502\n",
            "Train Epoch: 1 [5680/60000 (9%)]\tLoss: 0.001085\n",
            "Train Epoch: 1 [5690/60000 (9%)]\tLoss: 0.000745\n",
            "Train Epoch: 1 [5700/60000 (10%)]\tLoss: 5.919576\n",
            "Train Epoch: 1 [5710/60000 (10%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [5720/60000 (10%)]\tLoss: 0.395241\n",
            "Train Epoch: 1 [5730/60000 (10%)]\tLoss: 0.000881\n",
            "Train Epoch: 1 [5740/60000 (10%)]\tLoss: 8.389215\n",
            "Train Epoch: 1 [5750/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.000008\n",
            "Train Epoch: 1 [5770/60000 (10%)]\tLoss: 0.000445\n",
            "Train Epoch: 1 [5780/60000 (10%)]\tLoss: 0.526593\n",
            "Train Epoch: 1 [5790/60000 (10%)]\tLoss: 3.627792\n",
            "Train Epoch: 1 [5800/60000 (10%)]\tLoss: 0.000004\n",
            "Train Epoch: 1 [5810/60000 (10%)]\tLoss: 0.000114\n",
            "Train Epoch: 1 [5820/60000 (10%)]\tLoss: 0.000091\n",
            "Train Epoch: 1 [5830/60000 (10%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [5840/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5850/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5860/60000 (10%)]\tLoss: 0.000653\n",
            "Train Epoch: 1 [5870/60000 (10%)]\tLoss: 0.000039\n",
            "Train Epoch: 1 [5880/60000 (10%)]\tLoss: 0.000799\n",
            "Train Epoch: 1 [5890/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5900/60000 (10%)]\tLoss: 0.000013\n",
            "Train Epoch: 1 [5910/60000 (10%)]\tLoss: 0.000024\n",
            "Train Epoch: 1 [5920/60000 (10%)]\tLoss: 0.000209\n",
            "Train Epoch: 1 [5930/60000 (10%)]\tLoss: 0.000005\n",
            "Train Epoch: 1 [5940/60000 (10%)]\tLoss: 0.007218\n",
            "Train Epoch: 1 [5950/60000 (10%)]\tLoss: 0.037942\n",
            "Train Epoch: 1 [5960/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5970/60000 (10%)]\tLoss: 0.002769\n",
            "Train Epoch: 1 [5980/60000 (10%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [5990/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6010/60000 (10%)]\tLoss: 0.000017\n",
            "Train Epoch: 1 [6020/60000 (10%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [6030/60000 (10%)]\tLoss: 0.049108\n",
            "Train Epoch: 1 [6040/60000 (10%)]\tLoss: 0.000005\n",
            "Train Epoch: 1 [6050/60000 (10%)]\tLoss: 0.000120\n",
            "Train Epoch: 1 [6060/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6070/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6080/60000 (10%)]\tLoss: 0.000036\n",
            "Train Epoch: 1 [6090/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6100/60000 (10%)]\tLoss: 0.649553\n",
            "Train Epoch: 1 [6110/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6120/60000 (10%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [6130/60000 (10%)]\tLoss: 10.728240\n",
            "Train Epoch: 1 [6140/60000 (10%)]\tLoss: 0.000018\n",
            "Train Epoch: 1 [6150/60000 (10%)]\tLoss: 0.660793\n",
            "Train Epoch: 1 [6160/60000 (10%)]\tLoss: 0.023524\n",
            "Train Epoch: 1 [6170/60000 (10%)]\tLoss: 0.205545\n",
            "Train Epoch: 1 [6180/60000 (10%)]\tLoss: 0.000657\n",
            "Train Epoch: 1 [6190/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6200/60000 (10%)]\tLoss: 0.003238\n",
            "Train Epoch: 1 [6210/60000 (10%)]\tLoss: 0.036925\n",
            "Train Epoch: 1 [6220/60000 (10%)]\tLoss: 0.010667\n",
            "Train Epoch: 1 [6230/60000 (10%)]\tLoss: 0.000013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRscTKv3HhsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}