{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_0211ReadingAloudModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dad8bcd9-b0eb-41a3-a524-238a9efe519b",
      "metadata": {
        "id": "dad8bcd9-b0eb-41a3-a524-238a9efe519b"
      },
      "source": [
        "# 2022_0213 資料\n",
        "\n",
        "## メモ\n",
        "\n",
        "* データセットとして，psylex71, vdrj, onechar, fushimi1999 を用意した。\n",
        "* orth と phon を source と target に指定することで，4 種類の条件を試すことができる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a331efbe-bf75-48c2-8566-98181ce13cd0",
      "metadata": {
        "id": "a331efbe-bf75-48c2-8566-98181ce13cd0"
      },
      "source": [
        "# 1. 必要となるライブラリのインストール\n",
        "\n",
        "注意: Colab 上で実行する場合，Google Drive への接続許可を求めるポップアップウィンドウが開くので，許可する必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b7f33b-fe8a-4638-86ab-186da6be99dc",
      "metadata": {
        "id": "80b7f33b-fe8a-4638-86ab-186da6be99dc"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import torch\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import gzip\n",
        "import json\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "try:\n",
        "    import jaconv\n",
        "except ImportError:\n",
        "    !pip install jaconv\n",
        "    import jaconv\n",
        "    \n",
        "\n",
        "from IPython import get_ipython\n",
        "isColab =  'google.colab' in str(get_ipython())\n",
        "\n",
        "if isColab:\n",
        "\n",
        "    # termcolor を downgrade しないと colab ではテキストに色がつかない\n",
        "    !pip install --upgrade termcolor==1.1\n",
        "    import termcolor    \n",
        "\n",
        "    # 結果を保存するために Google Drive をマウントする\n",
        "    import google.colab\n",
        "    google.colab.drive.mount('/content/drive/')\n",
        "    \n",
        "    # GPU 情報を表示\n",
        "    !nvidia-smi -L\n",
        "\n",
        "    #!pip install ipynbname --upgrade > /dev/null\n",
        "\n",
        "if isColab:\n",
        "    # colab 上で MeCab を動作させるために，C コンパイラを起動して，MeCab の構築を行う\n",
        "    # そのため時間がかかる。\n",
        "    !apt install aptitude\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "    !pip install mecab-python3==0.7\n",
        "    !pip install jaconv\n",
        "    !pip install japanize_matplotlib\n",
        "    \n",
        "    import MeCab\n",
        "    mecab_wakati = MeCab.Tagger('-Owakati').parse\n",
        "    mecab_yomi = MeCab.Tagger('-Oyomi').parse\n",
        "    \n",
        "else:\n",
        "    from ccap.mecab_settings import yomi as mecab_yomi\n",
        "    from ccap.mecab_settings import wakati as mecab_wakati\n",
        "\n",
        "\n",
        "# ここから下は，コード実行に関するバージョン情報などの情報源の取得と表示\n",
        "from termcolor import colored\n",
        "\n",
        "import platform\n",
        "HOSTNAME = platform.node().split('.')[0]\n",
        "\n",
        "import os\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "try:\n",
        "    import ipynbname\n",
        "except ImportError:\n",
        "    !pip install ipynbname\n",
        "    import ipynbname\n",
        "FILEPATH = str(ipynbname.path()).replace(HOME+'/','')\n",
        "\n",
        "import pwd\n",
        "USER=pwd.getpwuid(os.geteuid())[0]\n",
        "\n",
        "from datetime import date\n",
        "TODAY=date.today()\n",
        "\n",
        "import torch\n",
        "TORCH_VERSION = torch.__version__\n",
        "\n",
        "color = 'green'\n",
        "print('日付:',colored(f'{TODAY}', color=color, attrs=['bold']))\n",
        "print('HOSTNAME:',colored(f'{HOSTNAME}', color=color, attrs=['bold']))\n",
        "print('ユーザ名:',colored(f'{USER}', color=color, attrs=['bold']))\n",
        "print('HOME:',colored(f'{HOME}', color=color,attrs=['bold']))\n",
        "print('ファイル名:',colored(f'{FILEPATH}', color=color, attrs=['bold']))\n",
        "print('torch.__version__:',colored(f'{TORCH_VERSION}', color=color, attrs=['bold']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d812d38-d113-4b1c-8058-8474eb6b6a1b",
      "metadata": {
        "id": "4d812d38-d113-4b1c-8058-8474eb6b6a1b"
      },
      "source": [
        "# 3. ライブラリの輸入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5568d129-cf32-4f94-b678-b61fb16a703b",
      "metadata": {
        "id": "5568d129-cf32-4f94-b678-b61fb16a703b"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "if isColab:\n",
        "    !git clone https://github.com/ShinAsakawa/RAM.git \n",
        "\n",
        "from RAM import convert_ids2tensor    \n",
        "from RAM import calc_accuracy    \n",
        "from RAM import asMinutes\n",
        "from RAM import timeSince\n",
        "from RAM import check_vals_performance\n",
        "from RAM import evaluate\n",
        "#from RAM import draw_word_char_histgram\n",
        "from RAM.utils import _train\n",
        "from RAM.utils import _fit\n",
        "# #from RAM import evaluate\n",
        "\n",
        "from RAM.fushimi1999 import _fushimi1999_list\n",
        "from RAM.fushimi1999 import fushimi1999_dict\n",
        "# fushimi1999_list = _fushimi1999_list()\n",
        "# for k, v in fushimi1999_dict.items():\n",
        "#     print(k,v)\n",
        "\n",
        "from RAM.utils import dup_model_with_params\n",
        "\n",
        "from RAM.fushimi1999 import _fushimi1999_list\n",
        "fushimi1999_list = _fushimi1999_list()\n",
        "#print(fushimi1999_list[:120])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e975d51d-a302-4cf5-9f9b-bad8e5be2e70",
      "metadata": {
        "id": "e975d51d-a302-4cf5-9f9b-bad8e5be2e70"
      },
      "source": [
        "# 2. パラメータ設定\n",
        "\n",
        "語彙数を 10K 語から 20K 語に倍増しているのは，Fushimi1999 の語彙リストの未知語が存在したためである。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e06d0fc4-dab5-4c87-8fc0-34579d3c4656",
      "metadata": {
        "id": "e06d0fc4-dab5-4c87-8fc0-34579d3c4656"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# シミュレーションに必要なパラメータの設定\n",
        "params = {\n",
        "    'dataset_name'  : 'psylex71',   # ['pyslex71', 'vdrj', 'onechar', 'fushimi1999']\n",
        "    #'dataset_name'  : 'fushimi1999',   # ['pyslex71', 'vdrj', 'onechar', 'fushimi1999']\n",
        "    #'dataset_name'   : 'onechar',\n",
        "    'traindata_size':  10000,    # 訓練データ (語彙) 数，\n",
        "    #'traindata_size':  20000,   # 訓練データ (語彙) 数，\n",
        "    'traindata_ratio': 0.9,     # 訓練データと検証データを分割する比率。ただし onechar データセットでは無効\n",
        "    \n",
        "    'epochs': 80,               # 学習のためのエポック数\n",
        "    \n",
        "    # 以下 `source` と `rget` を定義することで，別の課題を実行可能\n",
        "    'source': 'orth',          # ['orth', 'phon']\n",
        "    'target': 'phon',          # ['orth', 'phon']\n",
        "\n",
        "    #'hidden_size': 256,        # 中間層のニューロン数\n",
        "    'hidden_size': 128,\n",
        "    #'hidden_size': 32,\n",
        "\n",
        "    'lr' : 0.0001,\n",
        "    #'lr': 1e-4,                       # 学習率\n",
        "    #'lr': 1e-3,                       # 学習率\n",
        "    'dropout_p': 0.0,                 # ドロップアウト率\n",
        "    'teacher_forcing_ratio': 0.5,     # 教師強制を行う確率\n",
        "    'optim_func': torch.optim.Adam,   # 最適化アルゴリズム ['torch.optim.Adam', 'torch.optim.SGD', 'torch.optim.AdamW']\n",
        "    'loss_func' :torch.nn.NLLLoss(),  # 負の対数尤度損失 ['torch.nn.NLLLoss()', or 'torch.nn.CrossEntropyLoss()']\n",
        "\n",
        "    'random_seed': 42,          # 乱数の種。ダグラス・アダムス著「銀河ヒッチハイカーズガイド」\n",
        "\n",
        "    'pretrained': False,       # True であれば訓練済ファイルを読み込む\n",
        "    #'isTrain'   : True,       # True であれば学習する\n",
        "    \n",
        "    'verbose'   : True,\n",
        "    # 学習済のモデルパラメータを保存するファイル名\n",
        "    #'path_saved': '2022_0607lam_o2p_hid32_vocab10k.pt', \n",
        "    'path_saved': '2023_0201RAM.pt',\n",
        "    #'path_saved': False,                      # 保存しない場合\n",
        "    \n",
        "}\n",
        "\n",
        "# データセットの読み込み\n",
        "# 1. Psylex71 は「NTT 日本語の語彙特性」頻度表であり，著作権上の問題があるため配布不可\n",
        "# 2. VDRJ は松下言語学習ラボ，[日本語を読むための語彙データベース（研究用）](http://www17408ui.sakura.ne.jp/tatsum/database.html#vdrj) を加工して作成したデータである\n",
        "# 3. OneChar は一文字の読みについてのおもちゃのデータセットである。\n",
        "#\n",
        "# RAM ディレクトリ直下に，`psylex71_data.gz`, `vdrj_data.gz` がある。\n",
        "# これらは，`RAM/make_psylex71_dict.py`, `RAM/make_vdrj_dict.py` を実行して作成されたデータファイルである。\n",
        "# ここでは，これらのデータファイルが作成済と仮定している。\n",
        "from RAM.dataset import *\n",
        "if params['dataset_name'] == 'psylex71':\n",
        "    psylex71_dataset = Psylex71_Dataset(source=params['source'], \n",
        "                                        target=params['target'], \n",
        "                                        max_words=params['traindata_size'],\n",
        "                                        stop_list=fushimi1999_list[:120])\n",
        "                                        # fushimi1999_list[:120] としているのは 240 以降の単語は非単語である。\n",
        "                                        # このため，検証データとしては不適なため\n",
        "    ds = psylex71_dataset\n",
        "elif params['dataset_name'] == 'vdrj':\n",
        "    vdrj_dataset     = VDRJ_Dataset(source=params['source'], \n",
        "                                    target=params['target'], \n",
        "                                    max_words=params['traindata_size'],\n",
        "                                    stop_list=fushimi1999_list[:120])\n",
        "    ds = vdrj_dataset\n",
        "elif params['dataset_name'] == 'onechar':\n",
        "    onechar_dataset  = OneChar_Dataset(source=params['source'], target=params['target'])\n",
        "    ds = onechar_dataset\n",
        "elif params['dataset_name'] == 'fushimi1999':\n",
        "    fushimi1999_dataset = Fushimi1999_Dataset(source=params['source'], target=params['target'])\n",
        "    ds = fushimi1999_dataset\n",
        "else:\n",
        "    psylex71_dataset = Psylex71_Dataset(source=params['source'], \n",
        "                                        target=params['target'], \n",
        "                                        max_words=params['traindata_size'],\n",
        "                                        stop_list=fushimi1999_list[:120])\n",
        "    ds = psylex71_dataset\n",
        "    \n",
        "\n",
        "# 符号化器-復号化器モデルの定義\n",
        "from RAM.model import EncoderRNN\n",
        "from RAM.model import AttnDecoderRNN\n",
        "from RAM import train_one_seq2seq\n",
        "from RAM import train_epochs\n",
        "encoder = EncoderRNN(\n",
        "    n_inp=len(ds.source_list),                # 符号化器への入力データ次元数の特徴数 (語彙数): int\n",
        "    n_hid=params['hidden_size']).to(device)   # 符号化器の中間層数，埋め込みベクトルとして復号化器へ渡される次元数: int\n",
        "                                              # 復号化器の出力層素子数は，入力層と同一であるので指定しない\n",
        "\n",
        "decoder = AttnDecoderRNN(\n",
        "    n_hid=params['hidden_size'],               # 復号化器の中間層次元数: int\n",
        "    n_out=len(ds.target_list),                 # 復号化器の出力層次元数，入力層の次元と等しいので入力層次元を指定せず: int\n",
        "    dropout_p=params['dropout_p'],\n",
        "    max_length=ds.maxlen).to(device)\n",
        "\n",
        "## 訓練用最適化関数の定義\n",
        "encoder_optimizer = params['optim_func'](encoder.parameters(), lr=params['lr'])\n",
        "decoder_optimizer = params['optim_func'](decoder.parameters(), lr=params['lr'])\n",
        "\n",
        "## データを訓練データと検証データとに分割\n",
        "N_train = int(ds.__len__() * params['traindata_ratio'])   # 訓練データを 90 % に相当する数に\n",
        "N_val   = ds.__len__() - N_train    # 検証データを残り 10 % に相当する数\n",
        "if (params['dataset_name'] == 'onechar') or (params['dataset_name'] == 'fushimi1999'):\n",
        "    train_dataset = ds\n",
        "    val_dataset = None\n",
        "    N_train = len(ds.data_dict)\n",
        "    N_val = 0\n",
        "else:\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset=ds,\n",
        "        lengths=(N_train, N_val),\n",
        "        generator=torch.Generator().manual_seed(params['random_seed']))\n",
        "    \n",
        "color = 'blue'\n",
        "for k, v in params.items():\n",
        "    print(f'{k}:{colored(v, color=color, attrs=[\"bold\"])}')\n",
        "\n",
        "print('train_dataset size:', colored(f'{N_train}', color, attrs=['bold']),\n",
        "      'val_dataset size:', colored(f'{N_val}', color, attrs=['bold']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3f36ea0-bc02-41a6-9f0c-b6c5e78872eb",
      "metadata": {
        "id": "d3f36ea0-bc02-41a6-9f0c-b6c5e78872eb"
      },
      "source": [
        "## 3.1. データの確認とヒストグラム描画"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8bf9857-3907-4681-91e6-f3f2494dcbf6",
      "metadata": {
        "id": "f8bf9857-3907-4681-91e6-f3f2494dcbf6"
      },
      "outputs": [],
      "source": [
        "if params['verbose']:\n",
        "    N = 3\n",
        "    for n in range(N):\n",
        "        idx = np.random.choice(len(ds.data_dict))\n",
        "        wrd = ds.data_dict[idx]['orth']\n",
        "        inp_ids, tgt_ids = ds.__getitem__(idx)\n",
        "        print(f'idx:{idx}, wrd:{wrd}', \n",
        "              f'source_ids:{ds.source_ids2tkn(inp_ids)}', \n",
        "              f'target_ids:{ds.target_ids2tkn(tgt_ids)}')\n",
        "\n",
        "# データセットの頻度情報の視覚化        \n",
        "if params['verbose']:\n",
        "    from RAM.utils import draw_word_char_histgram\n",
        "\n",
        "    draw_word_char_histgram(_dict=ds.data_dict, key='phon', title=f'{params[\"dataset_name\"]} phon', figsize2=(8,3))\n",
        "    draw_word_char_histgram(_dict=ds.data_dict, key='orth', title=f'{params[\"dataset_name\"]} orth', figsize2=(8,3))        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14ed47e3-8b77-406e-b6bb-d8634806c580",
      "metadata": {
        "id": "14ed47e3-8b77-406e-b6bb-d8634806c580"
      },
      "source": [
        "# 4. 訓練の実施"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0962d3f0-0262-40a2-be94-a9b8d4bcf5d8",
      "metadata": {
        "id": "0962d3f0-0262-40a2-be94-a9b8d4bcf5d8"
      },
      "outputs": [],
      "source": [
        "#複数回実行する際には，一旦 `losses` を削除してください\n",
        "#del losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7049eee-c266-4bb7-b279-bc7d02fbdac1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7049eee-c266-4bb7-b279-bc7d02fbdac1",
        "outputId": "e63b9384-bfeb-4c73-9319-2d4590d100ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_name \u001b[1m\u001b[32mpsylex71\u001b[0m\n",
            "traindata_size \u001b[1m\u001b[32m10000\u001b[0m\n",
            "traindata_ratio \u001b[1m\u001b[32m0.9\u001b[0m\n",
            "epochs \u001b[1m\u001b[32m80\u001b[0m\n",
            "source \u001b[1m\u001b[32morth\u001b[0m\n",
            "target \u001b[1m\u001b[32mphon\u001b[0m\n",
            "hidden_size \u001b[1m\u001b[32m128\u001b[0m\n",
            "lr \u001b[1m\u001b[32m0.0001\u001b[0m\n",
            "dropout_p \u001b[1m\u001b[32m0.0\u001b[0m\n",
            "teacher_forcing_ratio \u001b[1m\u001b[32m0.5\u001b[0m\n",
            "optim_func \u001b[1m\u001b[32m<class 'torch.optim.adam.Adam'>\u001b[0m\n",
            "loss_func \u001b[1m\u001b[32mNLLLoss()\u001b[0m\n",
            "random_seed \u001b[1m\u001b[32m42\u001b[0m\n",
            "pretrained \u001b[1m\u001b[32mFalse\u001b[0m\n",
            "verbose \u001b[1m\u001b[32mTrue\u001b[0m\n",
            "path_saved \u001b[1m\u001b[32m2023_0201RAM.pt\u001b[0m\n",
            "\u001b[1m\u001b[34mエポック: 0 損失:2.30\u001b[0m \u001b[1m\u001b[36m経過時間: 3分 27秒 (残り時間 273分 38秒)\u001b[0m \u001b[1m\u001b[34m訓練データ精度:0.001\u001b[0m \u001b[1m\u001b[36m検証データ:['val:0.000']\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "fushimi1999_dataset = Fushimi1999_Dataset(source=params['source'], target=params['target'])\n",
        "\n",
        "#params['epochs'] = 50\n",
        "#params['lr'] = 0.0001  # 100 回以降 lr=0.0001 にしてみる\n",
        "\n",
        "color = 'green'\n",
        "for k, v in params.items():\n",
        "    print(k, colored(v, color, attrs=['bold']))\n",
        "    \n",
        "try:\n",
        "    losses\n",
        "except:\n",
        "    losses = []\n",
        "\n",
        "\n",
        "losses += train_epochs( \n",
        "    epochs=params['epochs'], \n",
        "    lr=params['lr'],\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset={'val': fushimi1999_dataset},\n",
        "    encoder=encoder, decoder=decoder,  \n",
        "    encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer,\n",
        "    source_vocab=ds.source_list, target_vocab=ds.target_list,\n",
        "    source_ids=ds.source, target_ids=ds.target,\n",
        "    criterion=params['loss_func'],\n",
        "    params=params,\n",
        "    device=device,\n",
        "    max_length=ds.maxlen,\n",
        "    n_sample=0,\n",
        "    teacher_forcing_ratio=params['teacher_forcing_ratio'],\n",
        ")\n",
        "\n",
        "plt.plot(losses) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0797a80e-e2a4-4142-8b48-d2b80ee4ea0c",
      "metadata": {
        "id": "0797a80e-e2a4-4142-8b48-d2b80ee4ea0c"
      },
      "source": [
        "# 7. 結果の保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c01730-cc01-4dda-8bfe-1f1fdd94227b",
      "metadata": {
        "id": "13c01730-cc01-4dda-8bfe-1f1fdd94227b"
      },
      "outputs": [],
      "source": [
        "params['path_saved'] = '2023_0213ram_psylex71_10k.pt'\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b15e4ca-00f7-4c2f-9778-2852c97d6fcf",
      "metadata": {
        "id": "1b15e4ca-00f7-4c2f-9778-2852c97d6fcf"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "def save_model_and_params(\n",
        "    params:dict=params, \n",
        "    isColab:bool=isColab,\n",
        "    force:bool=True,\n",
        "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \n",
        "    fname = params['path_saved']\n",
        "    if isColab:\n",
        "        # colab 上では，Gdrive 上に保存\n",
        "        fname = 'drive/MyDrive/' + fname\n",
        "    \n",
        "    if os.path.exists(fname) and (force!=True):\n",
        "        print(f'{fname} というファイルが存在し，かつ，force オプションが {force} であるため保存しません')\n",
        "        return\n",
        "    \n",
        "    timestamp = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "    torch.save({'encoder':encoder.state_dict(), \n",
        "                'decoder':decoder.state_dict(), \n",
        "                'encoder_optimizer': encoder_optimizer.state_dict(),\n",
        "                'decoder_optimizer': decoder_optimizer.state_dict(),\n",
        "                'params':params, \n",
        "                'losses':losses,\n",
        "                'timestamp': timestamp}, fname)\n",
        "    \n",
        "    ret = torch.load(fname, map_location=torch.device(device))\n",
        "    return ret\n",
        "    \n",
        "ret = save_model_and_params(force=True);\n",
        "ret.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41cdd433-bc9e-4bb7-ae70-3183a16080d6",
      "metadata": {
        "id": "41cdd433-bc9e-4bb7-ae70-3183a16080d6"
      },
      "source": [
        "## 議論\n",
        "\n",
        "符号化器-復号化器モデルは，[seq2seq](https://arxiv.org/abs/1409.3215) と呼ばれるモデルでもある。\n",
        "邦訳すれば，系列-2-系列 モデルなのだが，今回のプロジェクトでは，意味表象が，系列ではない。\n",
        "そのため，seq2seq という名称よりも，より一般化した，符号化器-復号化器 (enc-dec model) モデルとしたい。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/komazawa-deep-learning/komazawa-deep-learning.github.io/master/assets/2014Sutskever_S22_Fig1.svg\" width=\"66%\">\n",
        "<br/>    \n",
        "From Sutskever 2014, Figure 1.\n",
        "</center>\n",
        "\n",
        "上図は，seq2seq モデルの概略図である。\n",
        "符号化器と呼ばれる部分は，トークン `<EOS>` が入力された時点までである。\n",
        "それ以降は，復号化器となる。\n",
        "符号化器と復号化器とで，一時刻前の中間層の状態が共有されていることがポイントである。\n",
        "seq2seq は翻訳モデルであり，符号化器と復号化器とで，言語モデルの扱う言語が異なっている。\n",
        "具体的には，フランス語と英語である。\n",
        "\n",
        "\n",
        "オリジナルの，三角モデルにおける o2p については，三層のニューラルネットワークとみなしうる。\n",
        "このため，o2p の中間層は，識別性能を向上する役割と，モダリティ間の結合という２つの異なる役割を担っていたとみなすことができる。\n",
        "理論的には，両者を分離する必要も，統合する必要もない，どちらにしても積極的な理由は存在しないと思われる。\n",
        "駄菓子菓子，計算論的な役割においては，異なるモダリティ間の通信を媒介する役割と，入力モダリティにおける表象を確立するという意味合いを分離すると，役割分担が明確になるのであろうということである。\n",
        "\n",
        "---\n",
        "\n",
        "* 一文字の orth2phon を担保したいために，全角の数字，アルファベット，ひらがな，計 109 文字をデータ先頭に追加した。\n",
        "* Fushimi1999 (Psyc. Rev.) の語彙リストを fushimi1999_list として収録\n",
        "* Fushimi1999_list の扱いに伴い訓練語彙数を 10K から 20K に増加\n",
        "* 学習率 lr は 0.001 だと収束しない。0.0001 であれば良好であり，訓練損失 0.01 程度，訓練精度 0.987 程度までに至る。\n",
        "* ただし，一文字データセット onechar_dataset では lr=0.001 の方が収束が早い。\n",
        "これは，データセットサイズが 20K と 0.1K と 20 倍の差があるためであろう。\n",
        "* 近藤先生が，GPU 上で実行してくださった訓練済モデルのファイル名が `decoder256new.pt` と `encoder256new.pt` である。\n",
        "これは，中間層ユニット数が 256 である orth2phon モデルの訓練済モデルである。\n",
        "* `_train()` 関数内で，正解判定をする際に，GPU から CPU へ転送しなければいけないことを忘れていたので修正した。\n",
        "具体的には， `detach()` と `numpy()` の間に `cpu()` を挿入した。2 箇所\n",
        "```python\n",
        "    ok_flag = (ok_flag) and (decoder_output.argmax() == target_tensor[di].detach().cpu().numpy()[0])\n",
        "```\n",
        "* 近藤先生の GPU で訓練済モデルを CPU 環境で実行する必要がある場合，変更して読み込む必要がある\n",
        "\n",
        "```python\n",
        "encoder_pretrained_fname = 'encoder256new.pt'\n",
        "decoder_pretrained_fname = 'decoder256new.pt'\n",
        "if os.path.exists(encoder_pretrained_fname):\n",
        "    encoder = torch.load(encoder_pretrained_fname, _location=torch.device(device))map\n",
        "    \n",
        "if os.path.exists(decoder_pretrained_fname):\n",
        "    decoder = torch.load(decoder_pretrained_fname, map_location=torch.device(device))\n",
        "```\n",
        "\n",
        "近藤先生の実験によれば，結果は以下の通りである(そうだ)。\n",
        "\n",
        "正答率\n",
        "\n",
        "|   | 条件 | 記述         | 正解率 | \n",
        "|:----|:-----|:------------|:------|\n",
        "|WORD |   HF |1:consistent |　18/20\n",
        "|WORD |   HF |2:typical    |   HF___inconsist  16/20|\n",
        "|WORD |   HF |3:atypical   |   HF___atypical_  8/20 |\n",
        "|WORD |   LF |1:consistent |   LF___consist__  14/20|\n",
        "|WORD |   LF |2:typical    |   LF___inconsist  9/20|\n",
        "|WORD |   LF |3:atypical   |   LF___atypical_  3/20|\n",
        "\n",
        "* 伏見らではでなかったatypical効果だけでなく，\n",
        "　consistent-typicalの差もある程度ある気がします\n",
        " また，LFでも効果ありであり，かつ，頻度効果もあり\n",
        "* **今回，L(legitimate alternative reading of components） マークを付けてみました**\n",
        "  Lm, Lnは，モーラ間違い，一文字間違いと混合\n",
        "\n",
        "アクセプト率\n",
        "\n",
        "|     | 条件 | 記述         | 正解率 | \n",
        "|:----|:----|:------------|:------|\n",
        "|非単語| HF  | 1:consistent|HFNW_consist__  17/20|\n",
        "|非単語| HF  | 2:typical   |HFNW_inconsist　　17/20|\n",
        "|非単語| HF  | 3:ambiguous |HFNW_ambiguous  13/20|\n",
        "|非単語| LF  | 1:consistent|LFNW_consist__  15/20|\n",
        "|非単語| LF  | 2:typical   |LFNW_inconsist  13/20|\n",
        "|非単語| LF  | 3:ambiguous |LFNW_ambiguous  7/20|\n",
        "\n",
        "* かなり読めますね．アクセプトは，どんな読みでもいいので読めそうな読み方ならOKにしています．\n",
        "　単語の L と同じになります．\n",
        "* **結構驚きは，非単語のときに連濁や促音化ができているところ**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96cb1738-bd70-4348-9af7-efbd4d0196fb",
      "metadata": {
        "id": "96cb1738-bd70-4348-9af7-efbd4d0196fb"
      },
      "source": [
        "## 結果の描画"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1546adb-c63a-4421-b7ed-4458668bdbc0",
      "metadata": {
        "id": "d1546adb-c63a-4421-b7ed-4458668bdbc0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "#import matplotlib    \n",
        "#matplotlib.rcParams['text.usetex'] = True    \n",
        "fig, ax = plt.subplots(1,2, figsize=(8,4))\n",
        "\n",
        "fig.suptitle('Fushimi+1999 単語リストの検証 (o2p, hid256)')\n",
        "ax[0].plot((18/20,16/20, 8/20), marker=\"v\", color=\"green\", label=\"高頻度\")\n",
        "ax[0].plot((14/20, 9/20, 3/20), marker=\"^\", color=\"blue\", label=\"低頻度\")\n",
        "ax[0].set_xlim(-0.5,2.5)\n",
        "ax[0].set_ylim(0,1)\n",
        "ax[0].set_xticks(ticks=range(3))\n",
        "ax[0].set_xticklabels(labels=['一貫','非一貫','例外'])\n",
        "ax[0].legend()\n",
        "ax[0].set_title('単語')\n",
        "ax[0].set_ylabel('正解率')\n",
        "\n",
        "ax[1].plot((17/20,17/20,13/20), marker=\"v\", color=\"green\", label=\"高頻度\")\n",
        "ax[1].plot((15/20,13/20, 7/20), marker=\"^\", color=\"blue\", label=\"低頻度\")\n",
        "ax[1].set_xlim(-0.5,2.5)\n",
        "ax[1].set_ylim(0,1)\n",
        "ax[1].set_xticks(ticks=range(3))\n",
        "ax[1].set_xticklabels(labels=['一貫','非一貫','例外'])\n",
        "ax[1].set_title('非単語')\n",
        "ax[1].legend()\n",
        "fig.savefig('2023_0123LAM_o2p_hid256_fushimi1999.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [BERT](https://komazawa-deep-learning.github.io/2021/2021lect18/)"
      ],
      "metadata": {
        "id": "1hASRzbl0O-z"
      },
      "id": "1hASRzbl0O-z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chatGPT との対話\n",
        "\n",
        "* Human: 雪は何色ですか？\n",
        "* chatGPT: Snow is typically white in color. However, the exact shade of white can vary depending on factors such as lighting, the presence of other elements (such as dust or dirt), and the observer's perspective.\n",
        "\n",
        "\n",
        "* Human: Please answer in Japanese.\n",
        "* chatGPT: 雪は通常白色です。ただし、照明や他の要素（塵や汚れなど）の存在、観察者の見方などによって、白色の色合いが異なる場合があります。\n",
        "\n",
        "* Human: 一週間は何日ですか？\n",
        "* chatGPT: 一週間は7日間です。\n",
        "\n",
        "* Human: 看護婦さんはどこに勤めていますか？\n",
        "* chatGPT: I am an AI language model created by OpenAI and do not have personal experiences or a physical form, so I do not work anywhere as a nurse. However, nurses typically work in healthcare settings such as hospitals, clinics, nursing homes, and schools.\n",
        "\n",
        "* Human: I said you would answer in Japanese\n",
        "* chatGPT: 申し訳あります、そうですね。私はOpenAIによって作られたAI言語モデルであり、個人的な経験や身体的な形態を持っていないため、看護婦として働く場所はありません。ただし、看護婦は通常、病院、クリニック、介護施設、学校などの医療設備で働きます。\n",
        "\n",
        "* Human: どんなもので字を書きますか？\n",
        "* chatGPT: 私はコンピュータプログラムで作られたAI言語モデルであり、実際に筆記することはできません。代わりに、人間が入力したテキストを解析し、適切な応答を生成することができます。\n",
        "\n",
        "* Human: どこで切手を買いますか？\n",
        "* chatGPT: 私は人工知能モデルであり、実際に物を買うことはできません。切手はポストオフィスや郵便局、オンラインショッピングサイトなどで購入することができます。\n",
        "\n",
        "* Human: 実は，上記の４つの質問はWAB失語症検査(日本語版)の「会話での応答」の問題だったのです。WAB失語症検査(日本語版) について知っていることがあれば教えて下さい。\n",
        "* chatGPT: 申し訳ございませんが、私はWAB失語症検査(日本語版)に関して特に知識がありません。このテストについての情報は一般に入手可能ですか？\n",
        "\n",
        "* Human: WAB失語症検査そのものは公にされている内容です。質問を変えます。次の問に答えてください。口へんに鳥と書いて何と読みますか？\n",
        "* chatGPT: 「口へんに鳥」と書いて「とり」と読みます。\n",
        "\n",
        "* Human: 糸へんに冬と書いて何と読みますか？\n",
        "* chatGPT: 「いとへんにふゆと書いて、いとと読みます。」\n",
        "\n",
        "* Human: いいえ，糸へんに冬と書いて終わりと読みます。\n",
        "* chatGPT: はい、分かりました。「糸へんに冬」と書いて「終わり」と読みます。\n",
        "\n",
        "* Human: 女へんに家と書いて何と読みますか？\n",
        "* chatGPT: 「女へんに家」と書いて「めいえ」と読みます。\n",
        "\n",
        "* Human: いいえ，女偏に家と書いてヨメと読みます。\n",
        "* chatGPT: はい、その通りです。\n",
        "\n",
        "* Human: 言べんに売と書いて何と読みますか？\n",
        "* chatGPT: 「言べんに売」と書いて、「いべんにうり」と読みます。\n",
        "\n",
        "* Human: さんずいに青と書いて何と読みますか？\n",
        "* chatGPT: いいえ、さん随に青と書いてアオと読みます。\n",
        "\n",
        "* Human: 村という漢字は，どういう「へん」と「つくり」からできていますか？\n",
        "* chatGPT: 「村」という漢字は、「辶」と「木」からできています。「辶」は、歩くことを表していて、「木」は、木を表すことから「村」は、木が多く、住むところを表しています。\n",
        "\n",
        "* Human: 体という漢字は，どういう「へん」と「つくり」からできていますか？\n",
        "* chatGPT: 「体」という漢字は、「身」の「へん」と「亻」の「つくり」からできています。「身」は身体を表しており、「亻」は人を表します。この二つを合わせて「体」という字を形成しました。\n",
        "\n",
        "* Human: 新聞の読みを教えて下さい\n",
        "* chatGPT: 「新聞」は「しんぶん」と読みます。\n",
        "\n",
        "* Human: 毛糸の読みを教えて下さい\n",
        "* chatGPT: 「けいと」と読みます。\n",
        "\n",
        "* Human: 灰皿は何と読みますか？\n",
        "* chatGPT: 灰皿は「はいさら」と読みます。\n",
        "\n",
        "* Human: 時計の読みを教えて下さい。\n",
        "* chatGPT: 「とけい」と読みます。\n",
        "\n",
        "* Human: 切手の読みを教えて下さい。\n",
        "* chatGPT: 切手は「きって」と読みます。\n",
        "\n",
        "* Human: 鉛筆の読みを教えて下さい。\n",
        "* chatGPT: 鉛筆は「えんぴつ」と読みます。\n",
        "\n",
        "* Human: 「えんぴつ」を漢字にしてください。\n",
        "* chatGPT: 「鉛筆」\n",
        "\n",
        "\n",
        "* Human: 「新聞」を漢字にしてください\n",
        "* chatGPT: 「新聞」は「しんぶん」という読み方で、漢字表記は「新聞」です。\n",
        "\n",
        "* Human: 「きって」を漢字にしてください\n",
        "* chatGPT: 「切手」を漢字にすると「切手」です。\n",
        "\n",
        "* Human: 「とけい」を漢字に変換してください。\n",
        "* chatGPT: 「時計」\n",
        "\n",
        "* Human: 「はいざら」を漢字に変換してください。\n",
        "* chatGPT: 「灰皿」\n",
        "\n",
        "* Human: 「けいと」を漢字にして，かつ，英語に翻訳してください。\n",
        "* chatGPT: 「経灯」という漢字があります。\n",
        "英語では「Lamp」になります。\n",
        "\n",
        "* Human: 他に該当する漢字はありますか？\n",
        "* chatGPT: いいえ、「けいと」という読み方の漢字は存在しないか、他の漢字との誤りマッチングの可能性があります。 あなたが望むものを正確に把握するためには、もう少し詳細な情報が必要です。\n",
        "\n"
      ],
      "metadata": {
        "id": "8BUcuibRwSK7"
      },
      "id": "8BUcuibRwSK7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "＃ chatGPT による物語作成\n",
        "\n",
        "* 「鬼の面」という物語を作ってください\n",
        "\n",
        "```\n",
        "はい、「鬼の面」の物語を作ります。\n",
        "\n",
        "ある村で、鬼の面と呼ばれる伝説の男がいました。彼は、時には人間のように、時には鬼のように見えました。\n",
        "村の人々は彼が何者かわからないと恐れていました。\n",
        "\n",
        "ある日、村の町長が鬼の面に助けを求めました。村は、近くの森に住む悪魔に襲われていたからです。町長は\n",
        "鬼の面に、悪魔を倒すことを頼みました。\n",
        "\n",
        "鬼の面は、町長の依頼を受けました。彼は森に向かって歩き始めました。しかし、彼が森に到着すると、彼は\n",
        "悪魔と戦うことができなかったことが判明しました。代わりに、彼は悪魔との取引をしました。鬼の面は、悪\n",
        "魔が村に襲いかからないように、代わりに自分自身を捧げることに同意しました。\n",
        "\n",
        "時が過ぎます。村の人々は鬼の面が亡くなったと信じていましたが、彼は実際には悪魔と共に森に住んでいま\n",
        "した。村の人々は、彼がまだ生きていると知りませんでした。\n",
        "\n",
        "ある日、村は再び悪魔に襲われました。しかし、この時は鬼の面が現れました。彼は悪魔と戦い、村を救いま\n",
        "した。村の人々は彼が本当に鬼の面であることを知りましたが、彼は再び森に戻って行きました。\n",
        "\n",
        "以後、村の人々は鬼の面を信仰し始めました。彼は、村を守ってくれる英雄として敬われました。しかし、彼\n",
        "が実際には何者なのかは、誰も知りませんでした。\n",
        "```\n",
        "\n",
        "* 「女性が山で鬼の面をかぶっている」をテーマにして「鬼の面」という物語を作ってください。\n",
        "\n",
        "```\n",
        "はい、「女性が山で鬼の面をかぶっている」というテーマの物語「鬼の面」を作ります。\n",
        "\n",
        "ある山から来た年輩の女性、マリアが村にやってきました。彼女は村の人々に驚かせました。なぜなら、彼女は\n",
        "鬼の面をかぶっていたからです。\n",
        "\n",
        "村の人々はマリアが何者かわからないと恐れましたが、彼女は彼らに安心させるために、彼女が鬼の面をかぶって\n",
        "いる理由を語りました。彼女は、山で鬼の面を見つけましたが、それが彼女の息子を助けることができると信じて\n",
        "いました。彼女の息子は、山で迷い込んでしまっていました。\n",
        "\n",
        "村の人々は彼女の悲劇的な話を聞きましたが、彼女が鬼の面をかぶって山に戻ることを許せませんでした。代わり\n",
        "に、彼らは自分たちが山に行くことに同意しました。\n",
        "\n",
        "マリアは彼女の息子を助けるために、山に向かって歩き始めました。彼女は、山の中で数え切れないほどのトラブ\n",
        "ルに直面しましたが、彼女は決して諦めませんでした。彼女は彼女の息子を見つけるために、山を探索しました。\n",
        "\n",
        "ついに、彼女は彼女の息子を見つけましたが、彼は重傷を負っていました。彼女は彼を家に連れて帰ろうと努力し\n",
        "ましたが、彼女は鬼の面をかぶっていることに気付きました。彼女は鬼の面を取り、彼女の息子を家に連れて帰り\n",
        "ました。\n",
        "\n",
        "村の人々は彼女の息子が回復するのを見守りましたが、彼女自身も気付きました。彼女は\n",
        "```\n",
        "\n",
        "* 女性が山で鬼の面をかぶっている」をテーマにして「鬼の面」という物語を作ってください。その際，小泉八雲\n",
        "風の妖怪談のスタイルでお願いします。女の名前は「お雪」にしてください。\n",
        "\n",
        "```\n",
        "「鬼の面」\n",
        "\n",
        "山から帰るところだった、お雪は、鬼の面をかぶった姿で村に現れました。村の人々は、彼女が何者かわからず恐\n",
        "れましたが、お雪は彼女が山で鬼の面を見つけ、それが彼女の息子を助けることができると信じているからかぶっ\n",
        "ていると語りました。\n",
        "\n",
        "彼女の息子は、山で迷い込んでしまっていたのです。村の人々は、彼女の悲劇的な話を聞き、彼女が鬼の面をかぶっ\n",
        "て山に戻ることを許せませんでした。代わりに、彼らは自分たちが山に行くことに同意しました。\n",
        "\n",
        "お雪は、山に向かって歩き始めました。彼女は、鬼の面によって助けられ、山の中で多くのトラブルから逃れました\n",
        "が、彼女は決して諦めませんでした。彼女は、彼女の息子を見つけるために山を探索しました。\n",
        "\n",
        "結局、お雪は彼女の息子を見つけましたが、彼は重傷を負っていました。彼女は彼を家に連れて帰ろうと努力しまし\n",
        "たが、彼女は鬼の面をかぶっていることに気付きました。彼女は鬼の面を取り、彼女の息子を家に連れて帰りました。\n",
        "\n",
        "村の人々は、彼女の息子が回復するのを見守りました。彼女自身も気付きました。彼女は、鬼の面が彼女を助け、彼\n",
        "女の息子を助けたのだということを知りました。彼女は村の人々に感謝の意を伝え、平和な生活を送ることができま\n",
        "した。\n",
        "\n",
        "ところで、その後も、山から帰る人々が鬼の面をかぶった\n",
        "```\n",
        "\n",
        "* 小泉八雲の妖怪譚のスタイルで「女性が山で鬼の面をかぶっている」をテーマに「鬼の面」という物語を作ってください。物語の長さは 2000 字程度にしてください。\n",
        "\n",
        "```\n",
        "「鬼の面」\n",
        "\n",
        "ある日、山から帰る途中に、お雪は鬼の面を見つけました。彼女は、このマスクが彼女の息子を助けることができる\n",
        "と信じました。彼女の息子は、山で迷い込んでしまっていたのです。\n",
        "\n",
        "お雪は、鬼の面をかぶり、山に向かいました。彼女は、鬼の面によって助けられ、山の中で多くのトラブルから逃れ\n",
        "ました。しかし、彼女の息子はまだ見つかりませんでした。\n",
        "\n",
        "山を探索していると、お雪は鬼の面に気付きました。彼女は鬼の面を取り、彼女の息子を見つけましたが、彼は重傷\n",
        "を負っていました。お雪は彼を家に連れて帰ろうと努力しましたが、彼女は鬼の面を持っていたことに気付きました。\n",
        "\n",
        "お雪は、鬼の面を捨てましたが、彼女の息子はますます悪化しました。彼女は、鬼の面が彼女を助け、彼女の息子を\n",
        "助けたのだということを知りました。彼女は、鬼の面を再びかぶり、彼女の息子を救いました。\n",
        "\n",
        "村の人々は、彼女の息子が回復するのを見守りました。彼女自身も気付きました。彼女は、鬼の面が彼女を助け、彼\n",
        "女の息子を助けたのだということを知りました。彼女は村の人々に感謝の意を伝え、平和な生活を送ることができま\n",
        "した。\n",
        "\n",
        "ところで、その後も、山から帰る人々は鬼の面をかぶった姿のお雪を見かけました。彼女は、ますます神秘的な存在\n",
        "になっていました。彼女が何を知っているのか、何を望んでいるのか、誰も\n",
        "```\n",
        "\n",
        "* 「女性が山で鬼の面をかぶっている」をテーマに「鬼の面」という物語を作ってください。異世界転生の要素を入れてお願いします。\n",
        "\n",
        "```\n",
        "「鬼の面」\n",
        "\n",
        "お雪は、現実世界では普通の女性でしたが、突然異世界に転生してしまいました。彼女は、異世界で生きることがで\n",
        "きる方法を探していたところ、山で鬼の面を見つけました。\n",
        "\n",
        "お雪は、鬼の面をかぶり、山を探索することにしました。彼女は、鬼の面によって助けられ、異世界のトラブルから\n",
        "逃れました。しかし、彼女はますますこの世界に馴染めないと感じました。\n",
        "\n",
        "彼女は、鬼の面が彼女を助け、彼女がこの世界で生きることができる方法を示すものだと思いました。彼女は、鬼の\n",
        "面を通じてこの世界を理解することに努めました。彼女は、この世界で自分自身を見つけることができました。\n",
        "\n",
        "お雪は、鬼の面を通じて、異世界での彼女の使命を発見することができました。彼女は、この世界を救うことができ\n",
        "る力を持っていたと知りました。彼女は、鬼の面を通じて、異世界を救うために戦いました。\n",
        "\n",
        "お雪は、異世界を救うことができました。彼女は、鬼の面を捨て、彼女自身の力でこの世界を救うことができました。\n",
        "彼女は、異世界での人生を送ることができました。\n",
        "\n",
        "ところで、その後も、異世界の人々は鬼の面をかぶった姿のお雪を見かけました。彼女は、異世界で有名な英雄となっ\n",
        "ていました。彼女が何を知っているのか、何を望んでいるのか、誰も知りませんでした。彼女は、異世界で謎めいた存在\n",
        "```\n",
        "\n",
        "* 上記のような物語を自動生成する LISP のコードを書いてください。\n",
        "\n",
        "```\n",
        "I'm sorry, but I am not able to write LISP code to automatically generate a story based on\n",
        "the theme you've provided. However, as a language model I can provide text-based responses\n",
        "to your questions and requests.\n",
        "```\n",
        "\n",
        "* Okay, please write a Python code to generate Japanese narratives like you did above.\n",
        "\n",
        "```\n",
        "I'm sorry, but I am an AI language model and do not have the ability to write code in Python\n",
        "or any other programming language. I can only respond to natural language inputs. However,\n",
        "I can suggest some resources for learning how to generate narratives using programming\n",
        "languages such as Python. One popular library for generating text with Python is GPT-3,\n",
        "which is based on deep learning algorithms. Another library is NLTK (Natural Language\n",
        "Toolkit), which provides tools for working with human language data.\n",
        "```\n",
        "\n",
        "* How and where can I get such GPT-3 code?\n",
        "\n",
        "```\n",
        "GPT-3 is a proprietary AI language model developed by OpenAI and is not publicly available\n",
        "for individual use. If you want to utilize GPT-3 or similar language models, you'll need to\n",
        "access it through OpenAI's API or through a third-party service that uses the API.\n",
        "\n",
        "You can access OpenAI's API and apply for access at the OpenAI website (https://beta.openai.com/).\n",
        "Once you have access to the API, you can use it to generate text, answer questions, translate\n",
        "languages, and more. Additionally, there are many companies that offer services built on\n",
        "top of the OpenAI API, so you might consider exploring those options as well.\n",
        "\n",
        "Please note that access to OpenAI's API is subject to certain usage restrictions and\n",
        "limitations, and there may be associated costs for using the API.\n",
        "```\n",
        "\n",
        "* BTW, how would you evaluate yourself about your stories on 鬼の面?\n",
        "\n",
        "```\n",
        "As an AI language model, I don't have personal opinions or emotions. However, I can tell you\n",
        "that my responses to the theme \"女性が山で鬼の面をかぶっている\" are generated based on patterns\n",
        "I have learned from the vast amount of text data I was trained on.\n",
        "\n",
        "In terms of the quality of the story I generated, I strive to generate coherent and grammatically\n",
        "correct narratives, but the quality of my response depends on the input prompt and the data\n",
        "I have been trained on. It is possible that my response may contain inaccuracies or may not\n",
        "fully meet your expectations, but I do my best to generate a meaningful and relevant response\n",
        "based on the information I have been trained on.\n",
        "```\n",
        "\n",
        "* Did you mean you and AI models can generate Japanese narratives as the same quality as Dr. Ono and Dr. Ogata, and their colleague's model did?\n",
        "\n",
        "```\n",
        "As an AI language model, I can generate text in the style of an author or in a specific genre,\n",
        "but it is not possible for me or any AI model to generate text of the same quality as a human\n",
        "writer such as Dr. Ono or Dr. Ogata. While AI models like me have been trained on a large amount\n",
        "of text data and can generate coherent and grammatically correct responses, they lack the\n",
        "creativity, intuition, and emotional intelligence that human writers possess.\n",
        "\n",
        "AI language models can be useful tools for generating text quickly and efficiently, but they\n",
        "should not be seen as a replacement for human writers. The strengths of AI models lie in their\n",
        "ability to process vast amounts of information and generate outputs based on patterns in that\n",
        "data, but they still have limitations and cannot fully replicate the unique experiences,\n",
        "perspectives, and styles of human writers.\n",
        "```\n",
        "\n",
        "* No, do not misunderstand. Dr. Ono and Dr. Ogata are not the Japanese writer, but they are researchers who are writing computer programs to generate Japanese narratives automatically. Do you think that you and AI models be able to achieve the same quality of narratives like they have done?\n",
        "\n",
        "```\n",
        "As an AI language model, it's possible for me to generate narratives at a level similar to the\n",
        "one generated by the programs developed by Dr. Ono and Dr. Ogata, or any other AI-powered\n",
        "narrative generation system. The quality of the narratives generated by an AI model depends\n",
        "on several factors, including the quality of the training data, the size and architecture\n",
        "of the model, and the specific algorithms and techniques used to generate the narratives.\n",
        "\n",
        "It's worth noting that AI models are continuously improving and the field of AI narrative\n",
        "generation is rapidly advancing. As a result, the quality of the narratives generated by\n",
        "AI models is likely to improve over time as well. However, it's important to keep in mind\n",
        "that AI models still have limitations and may not be able to fully replicate the creativity,\n",
        "intuition, and emotional intelligence of human writers.\n",
        "```\n",
        "\n",
        "* Do you think the narrative performance generated by AI powered models is the matter of quality of the training data, size and architecture?\n",
        "\n",
        "```\n",
        "Yes, the quality of the narratives generated by AI models is largely influenced by the quality\n",
        "of the training data, size, and architecture of the model.\n",
        "\n",
        "Training data is critical in shaping the behavior of an AI model. The more diverse, high-quality,\n",
        "and relevant training data an AI model is exposed to, the better it will perform in generating\n",
        "narratives. This means that if an AI model is trained on high-quality Japanese narratives, it\n",
        "will generate narratives with a similar quality.\n",
        "\n",
        "The size of the model is also important, as larger models are able to process more information\n",
        "and have a better understanding of the relationships between words, phrases, and concepts.\n",
        "This allows them to generate more coherent and contextually appropriate narratives.\n",
        "\n",
        "The architecture of the model is also important, as different architectures are better suited to\n",
        "different tasks. For example, some architectures are better suited to generating text, while\n",
        "others are better suited to understanding and processing text. The choice of architecture will\n",
        "also influence the quality of the narratives generated by an AI model.\n",
        "\n",
        "In summary, the quality of narratives generated by AI models is dependent on the quality of the\n",
        "training data, size, and architecture of the model, and it's important to consider these factors\n",
        "when evaluating the performance of an AI-powered narrative generation system.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qsNhSN5KxUZZ"
      },
      "id": "qsNhSN5KxUZZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1359e5d-a410-481c-aa7e-a6c144881d2a",
      "metadata": {
        "id": "e1359e5d-a410-481c-aa7e-a6c144881d2a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}