{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOoTjhVZ+IjifAhpXc7XqRE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1027Knd_Ijn_Ask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())"
      ],
      "metadata": {
        "id": "JnIgvHWIv0TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* date: 2023_1110\n",
        "* author: 浅川伸一\n",
        "* filename: 2023_1027Knd_Ijn_Ask.ipynb 名前変更，旧名は `2023_1027Knd_Ijn_Ask_s2p_p2s.ipynb` 理由は orthgraphy も追加したら。\n",
        "\n",
        "# 符号化器‐復号化器 (encoder-decoder a.k.a seq2seq) モデルによる，単語認識過程 beyond triangle\n",
        "\n",
        "`fit_seq2seq()`, `eval_seq2seq()` は，encoder 側が 系列データでも，埋め込みベクトルでも動作する。\n",
        "従って，o2o, o2p, p2o, p2p, s2o, s2p の 6 モデルはこれでよいようだ。\n",
        "残された，o2s, p2s, s2s を開発すれば良い。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2006Kello_fig4.svg\" style=\"width:39%\">\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2006Kello_junction_fig5.svg\" style=\"width:39%\">\n",
        "<!-- <img src=\"2006Kello_fig4.svg\" width=\"39%\"><img src=\"2006Kello_junction_fig5.svg\" style=\"width:39%\"> -->\n",
        "<div style=\"background-color:lavender;text-align:left;width:66%\">\n",
        "左: RNN を用いた符号化器 (encoder) -復号化器 (decoder) モデル。\n",
        "右: Kello らの結節点 (junction) モデル。中央の語彙ノード (lexical nodes) 上の数字 45263 は，交差点モデルが扱うことが可能な語彙数。<br/>\n",
        "左: Kello+2006 Fig.4, 右: Kello+2006 Fig5\n",
        "</div>\n",
        "</center>\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2004Harm_Seidenberg_fig1_extended.svg\" style=\"width:39%\">\n",
        "<!-- <img src=\"2004Harm_Seidenberg_fig1_extended.svg\" width=\"39%\"> -->\n",
        "<div style=\"background-color:lavender;text-align:center;width:44%\">\n",
        "Harm\\&Seidenberg2004 Fig. 1 を改変。\n",
        "</div>\n",
        "</center>\n",
        "\n",
        "1. O(rthgraphy), P(honology), S(emnatics) のそれぞれに対して，ソースとターゲットと見立てた，9 つのデータセット，モデルを用意した。\n",
        "モデル名を下表に示す。\n",
        "表中の x2y は，ソースが x [o,p,s] でターゲットが y [o,p,s] であるモデルを意味する。\n",
        "カッコ内は，ソースとターゲットのそれぞれが，系列データであれば Seq であり，埋め込みベクトルデータであれば Vec である。\n",
        "\n",
        "|source\\target   | O   | P   |  S |\n",
        "|:--:|:--:|:---:|:--:|\n",
        "| O | o2o (Seq2seq)| o2p (Seq2Seq)| o2s (Seq2Vec)|\n",
        "| P | p2o (Seq2Seq)| p2p (Seq2Seq)| p2s (Seq2Vec)|\n",
        "| S | s2o (Vec2Seq)| s2p (Vec2Seq)| s2s (Vec2Vec)|\n",
        "\n",
        "ソースからターゲットへと系列データかベクトル埋め込みデータかによって，モデルは 4 種類に分類できる。\n",
        "\n",
        "1. 系列から系列へ: 4 (o2o, o2p, p2o, p2p)，\n",
        "2. 系列からベクトル埋め込みへ: 2 (o2s, p2s)\n",
        "3. ベクトル埋め込みから系列へ: 2 (s2o, s2p)\n",
        "4. ベクトル埋め込みからベクトル埋め込み 1\n",
        "\n",
        "## メモ\n",
        "\n",
        "1. 2023_1027 に近藤先生には，このコードのプロトタイプをお見せした。\n",
        "すなわち GitHub にアップロード済である。\n",
        "このファイルには，その後の改良が加えられている。\n",
        "ただし，最初のセルで MeCab をソースコードからダウンロードして，コンパイル & インストールに時間を要したため，実施まではお見せしていない。\n",
        "\n",
        "2. MeCab の使用は，未知語が入力として与えられた場合，仮のヨミを得るために使用している。\n",
        "上記の役割を除けば MeCab は不要だと判断し，MeCab の使用を中止した\n",
        "\n",
        "\n",
        "<center>\n",
        "<!-- <img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2004Harm_Seidenberg_fig4c.svg\"><br/>\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2004Harm_Seidenberg_fig4d.svg\"><br/> -->\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2004Harm_Seidenberg_fig4ab.svg\"><br/>\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2004Harm_Seidenberg_fig4c.svg\">\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2004Harm_Seidenberg_fig4d.svg\"><br/>\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/1999Levelt_blueprint.jpg\" width=49%\"><br/>\n",
        "<!-- <img src=\"2004Harm_Seidenberg_fig4ab.svg\"><br/>\n",
        "<img src=\"2004Harm_Seidenberg_fig4c.svg\">\n",
        "<img src=\"2004Harm_Seidenberg_fig4d.svg\"><br/> -->\n",
        "`Harm & Seidenberg (2004)`, Figure 4 c, and d,\n",
        "`Levelt 1999\n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2015Loung_fig1.svg\" width=\"24%\"><br/>\n",
        "ニューラル翻訳モデル。\n",
        "青色がソース言語モデル，赤がターゲット言語モデルである。\n",
        "ソース言語モデルの，最終時刻の中間層状態を，ターゲット言語モデルの開始時の中間層状態として用いる。\n",
        "Loung+2015 Fig.1 より。    \n",
        "</center>\n",
        "\n",
        "\n",
        "* 文献\n",
        "    * Harm & Seidenberg (2004) Computing the Meanings of Words in Reading: Cooperative Division of Labor Between Visual and Phonological Processes, Psychological Review, DOI:10.1037/0033-295X.111.3.662\n",
        "    * Seq2seq 翻訳モデル: Sutskever+ (2014) Sequence to Sequence Learning with Neural Networks, [arXiv:1409.3215](https://arxiv.org/abs/1409.3215)\n",
        "    * 注意つき符号化器‐復号化器モデル: Bahdanau, Cho, & Bengio (2015) NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE, [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)\n",
        "    * もう一つの注意つき符号化器‐復号化器モデル Luong+ (2015) Effective Approaches to Attention-based Neural Machine Translation, [arXiv:1508.04025](https://arxiv.org/abs/1508.04025)\n"
      ],
      "metadata": {
        "id": "vrmtA8smwBXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 準備\n"
      ],
      "metadata": {
        "id": "WlVdHvfCwNoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 必要なライブラリの輸入"
      ],
      "metadata": {
        "id": "Jz7axj-IwR1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import jaconv\n",
        "except ImportError:\n",
        "    !pip install jaconv\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "if isColab:\n",
        "    !pip install --upgrade termcolor==1.1\n",
        "from termcolor import colored\n",
        "\n",
        "try:\n",
        "    import RAM\n",
        "except ImportError:\n",
        "    !git clone https://github.com/ShinAsakawa/RAM.git\n",
        "    import RAM\n",
        "\n",
        "# 近藤先生との議論から音韻情報の代替案として，ローマ字表記を採用することとした。\n",
        "# このとき，訓令式の表記にすることとした。ヘボン式，パスポート式ではないことに注意\n",
        "try:\n",
        "    from kunrei import kunrei\n",
        "except ImportError:\n",
        "    !wget https://shinasakawa.github.io/2023notebooks/kunrei.py -O kunrei.py\n",
        "    from kunrei import kunrei"
      ],
      "metadata": {
        "id": "UK5wIB96wUtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 意味表現として word2vec による意味埋め込みベクトルを使う"
      ],
      "metadata": {
        "id": "a1iRUQM-wZ-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# word2vec のため gensim を使う\n",
        "import requests\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "import os\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "w2v_2017 = {\n",
        "    'cbow200': 'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz',\n",
        "    'sgns200': 'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_sgns.bin.gz',\n",
        "    'cbow300': 'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid300_win20_neg20_sgns.bin.gz',\n",
        "    'sgns300': 'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz'\n",
        "}\n",
        "\n",
        "w2v_2021 = {\n",
        "    'cbow128': { 'id': '1B9HGhLZOja4Xku5c_d-kMhCXn1LBZgDb',\n",
        "                'outfile': '2021_05jawiki_hid128_win10_neg10_cbow.bin.gz'},\n",
        "    'sgns128': { 'id': '1OWmFOVRC6amCxsomcRwdA6ILAA5s4y4M',\n",
        "                'outfile': '2021_05jawiki_hid128_win10_neg10_sgns.bin.gz'},\n",
        "    'cbow200': { 'id': '1JTkU5SUBU2GkURCYeHkAWYs_Zlbqob0s',\n",
        "                'outfile': '2021_05jawiki_hid200_win20_neg20_sgns.bin.gz'}\n",
        "}\n",
        "\n",
        "is2017=True\n",
        "\n",
        "if isColab:\n",
        "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "    if is2017:\n",
        "        response = requests.get(w2v_2017['cbow200'])\n",
        "        fname = w2v_2017['cbow200'].split('/')[-1]\n",
        "        with open(fname, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    else:\n",
        "        #訓練済 word2vec ファイルの取得\n",
        "        (f_id, outfile) = w2v_2021['sgns128']['id'], w2v_2021['sgns128']['outfile']\n",
        "        gdd.download_file_from_google_drive(file_id=f_id,\n",
        "                                            dest_path=outfile,\n",
        "                                            unzip=False,\n",
        "                                            showsize=True)\n",
        "\n",
        "if is2017:\n",
        "    w2v_base = os.path.join(HOME, 'study/2016wikipedia/') if not isColab else '.'\n",
        "    w2v_file = '2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz'\n",
        "    w2v_file = os.path.join(w2v_base, w2v_file)\n",
        "else:\n",
        "    w2v_base = os.path.join(HOME, 'study/2019attardi_wikiextractor.git/wiki_texts/AA') if isMac else '.'\n",
        "    w2v_file = '2021_05jawiki_hid128_win10_neg10_sgns.bin'\n",
        "\n",
        "w2v = KeyedVectors.load_word2vec_format(\n",
        "    w2v_file,\n",
        "    encoding='utf-8',\n",
        "    unicode_errors='replace',\n",
        "    binary=True)"
      ],
      "metadata": {
        "id": "3wXG9YTqwdQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 データセット Psylex71_Dataset の読み込み"
      ],
      "metadata": {
        "id": "gVnLx6wqwlOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットとしての Psylex71_Dataset の読み込み\n",
        "from RAM import Psylex71_Dataset\n",
        "\n",
        "psylex71_ds = Psylex71_Dataset(max_words=30000)\n",
        "print(f'psylex71_ds の単語数:{psylex71_ds.__len__()}')"
      ],
      "metadata": {
        "id": "GNyXIDwjwnAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセットのヒストグラム描画"
      ],
      "metadata": {
        "id": "c0NNSrTwwqD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from RAM import draw_word_char_histgram\n",
        "draw_word_char_histgram(_dict=psylex71_ds.data_dict, key='phon', title='音韻', figsize2=(8,3))"
      ],
      "metadata": {
        "id": "VffMdJjgwpng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.4 psylex71_ds に存在する全単語を word2vec の埋め込みベクトル行列にする"
      ],
      "metadata": {
        "id": "qGynR8Rzw20P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# psylex71_ds データから word2vec の埋め込みベクトル行列を得る\n",
        "_words = [dct['orth'] for dct in psylex71_ds.data_dict.values()]\n",
        "\n",
        "# gensim() の `vectors_for_all()` 関数を持ちて，望む語彙で構成される word2vec 単語埋め込みモデルを作成\n",
        "w2v_psylex71 = w2v.vectors_for_all(_words)\n",
        "\n",
        "# NaN データが入っている可能性がるので変換\n",
        "w2v_psylex71.vectors = np.nan_to_num(w2v_psylex71.vectors)\n",
        "print(f'w2v_psylex71.vectors.shape:{w2v_psylex71.vectors.shape}')\n",
        "words = w2v_psylex71.index_to_key\n",
        "#len(words)"
      ],
      "metadata": {
        "id": "D8fxWyysw5UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.5 psylex71 データセット中の単語における w2v の表示テスト"
      ],
      "metadata": {
        "id": "HyrJQVUqw76D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Wrd = input('単語を入力してください:')\n",
        "color = 'blue'\n",
        "while (Wrd != \"\"):\n",
        "    if Wrd in w2v_psylex71:\n",
        "        Idx = w2v_psylex71.key_to_index[Wrd]\n",
        "        print(f'入力単語 Wrd:{colored(Wrd, color, attrs=[\"bold\"])},',\n",
        "              f'対応する単語番号 Idx:{colored(Idx, color, attrs=[\"bold\"])},',\n",
        "              f'w2v_psylex71.get_index({Wrd}):{colored(w2v_psylex71.get_index(Wrd), color, attrs=[\"bold\"])}')\n",
        "    else:\n",
        "        print(colored(f'{Wrd} という単語はありません。','red', attrs=['bold']))\n",
        "    Wrd = input('単語を入力してください (終了するには改行のみを入力):')"
      ],
      "metadata": {
        "id": "leVIhNYmw-NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.6 書記素リストの作成"
      ],
      "metadata": {
        "id": "oYPix-o1xDTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import RAM\n",
        "\n",
        "def _grapheme(words=words):\n",
        "    \"\"\"必要と思われる書記素リストを返す\"\"\"\n",
        "\n",
        "    num_alpha='０１２３４５６７８９ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ'\n",
        "    hira = 'あいうえおかがきぎくぐけげこごさざしじすずせぜそぞただちぢつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもやゆよらりるれろわゐゑをんぁぃぅぇっゃゅょゎ'+'ゔ'\n",
        "    kata = 'アイウエオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモヤユヨラリルレロワヰヱヲン'+'ヴヷヸヹヺァィゥヵヶェォッャョュヮ'\n",
        "    symbols='、。，．・：；？！゛゜´｀¨＾‾＿ヽヾゝゞ〃仝々〆〇ー—‐／＼〜‖｜…‥‘’“”（）〔〕［］｛｝〈〉《》「」『』【】＋−±×÷＝≠＜＞≦≧∞∴♂♀°′″℃¥＄¢£％＃＆＊＠§☆★○●◎◇' + '◆□■△▲▽▼※〒→←↑↓〓∈∋⊆⊇⊂⊃∪∩∧∨¬⇒⇔∀∃∠⊥⌒∂∇≡≒≪≫√∽∝∵∫∬Å‰♯♭♪†‡¶◯'\n",
        "    #greek='ΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩαβγδεζηθικλμνξοπρστυφχψω'\n",
        "    #rosian='АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
        "    #digit_symbols='①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳⑴⑵⑶⑷⑸⑹⑺⑻⑼⑽⑾⑿⒀⒁⒂⒃⒄⒅⒆⒇❶❷❸❹❺❻❼❽❾⒈⒉⒊⒋⒌⒍⒎⒏⒐'\n",
        "    #alpha_symbols='ⅠⅡⅢⅣⅤⅥⅦⅧⅨⅩⅪⅫⅰⅱⅲⅳⅴⅵⅶⅷⅸⅹⅺⅻ⒜⒝⒞⒟⒠⒡⒢⒣⒤⒥⒦⒧⒨⒩⒪⒫⒬⒭⒮⒯⒰⒱⒲⒳⒴⒵'\n",
        "    #units='㎜㎟㎝㎠㎤㎡㎥㎞㎢㎎㎏㏄㎖㎗ℓ㎘㎳㎲㎱㎰℉㏔㏋㎐㎅㎆㎇№㏍℡'\n",
        "    #suits='♤♧♡♢♠♣♥♦〠☎〄☞☜☝☟⇆⇄⇅⇨⇦⇧⇩'\n",
        "    #etc='①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳ⅠⅡⅢⅣⅤⅥⅦⅧⅨⅩ㍉㌔㌢㍍㌘㌧㌃㌶㍑㍗㌍㌦㌣㌫㍊㌻㎜㎝㎞㎎㎏㏄㎡㍻〝〟№㏍℡㊤㊥㊦㊧㊨㈱㈲㈹㍾㍽㍼≒≡∫∮∑√⊥∠∟⊿∵∩∪㊙'\n",
        "    #etc2='㍉㌢㍍㌔㌖㌅㌳㍎㌃㌶㌘㌕㌧㍑㍊㌹㍗㌍㍂㌣㌦㌻㌫㌀㌞㌪㌱㍇㍾㍽㍼㍻㍿∮∟⊿〝'\n",
        "\n",
        "    # RAM で作成済の常用漢字リストを用いて単漢字リストを作成\n",
        "    # 平成 22 年の改定により常用漢字は 2136 文字ある\n",
        "    chars_list = [ch for ch in num_alpha+hira+kata+symbols]+ RAM.chars_joyo().char_list\n",
        "    not_chars_list = []\n",
        "    for wrd in tqdm(words):\n",
        "        for ch in wrd:\n",
        "            if (ch not in chars_list) and (ch not in not_chars_list):\n",
        "                not_chars_list.append(ch)\n",
        "    not_chars_list = sorted(not_chars_list)\n",
        "    grapheme = chars_list + not_chars_list\n",
        "    # 上記の処理により grapheme には 2768 文字である。\n",
        "    # これに特殊トークン 4 つ ['<PAD>', '<SOW>', '<EOW>', '<UNK>'] を加えたリストを返す\n",
        "\n",
        "    return ['<PAD>', '<SOW>', '<EOW>', '<UNK>'] + grapheme\n",
        "\n",
        "grapheme = _grapheme()"
      ],
      "metadata": {
        "id": "SUKAKng6xFl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.7 データセット定義"
      ],
      "metadata": {
        "id": "tpda1ya7xIlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import gensim\n",
        "\n",
        "class psylex71_w2v_Dataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 #direction='s2p',  # ['s2p', 'p2s']\n",
        "                 source='seme',    # エンコーダ用 入力データ, ['orth', seme', 'phon'] のいずれか一つ\n",
        "                 target='phon',    # デコーダ用 出力データ ,  ['orth', seme', 'phon'] のいずれか一つ\n",
        "                 w2v:gensim.models.keyedvectors.KeyedVectors=w2v_psylex71,\n",
        "                 old_ds:RAM.dataset.Psylex71_Dataset=psylex71_ds,\n",
        "                 #mecab_yomi=yomi,\n",
        "                 grapheme:list=grapheme,\n",
        "                ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.ds_name = 'psylex71_'+source+\"2\"+target\n",
        "        self.source, self.target = source, target\n",
        "\n",
        "        self.w2v = w2v\n",
        "        self.old_ds = old_ds\n",
        "        #self.mecab_yomi = yomi         # 未知の単語が入力された場合 MeCab を使って読みをえるため\n",
        "        self.grapheme = grapheme\n",
        "\n",
        "        self.words = w2v.index_to_key  # gensim の KeyedVectors を利用して単語リストとする\n",
        "        self.W = w2v.vectors\n",
        "\n",
        "        # 訓令式に従った日本語ローマ字表記 `kurei.py` 参照\n",
        "        self.phoneme = ['<PAD>', '<SOW>', '<EOW>', '<UNK>', # 特殊トークン，純に，埋め草，語頭，語末，未知\n",
        "                        'a', 'i', 'u', 'e', 'o',            # 母音\n",
        "                        'a:', 'i:', 'u:', 'e:', 'o:',       # 長母音\n",
        "                        'N', 'Q',                           # 撥音，拗音\n",
        "                        'b', 'by', 'ch', 'd', 'dy', 'f', 'g', 'gy', 'h', 'hy', # 子音\n",
        "                        'j', 'k', 'ky', 'm', 'my', 'n', 'ny',  'p', 'py', 'r', # 子音\n",
        "                        'ry', 's', 'sy', 't', 'ty', 'w', 'y', 'z', 'zy']       # 子音\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx:int):\n",
        "        wrd = self.words[idx]\n",
        "\n",
        "        if self.source == 'phon':\n",
        "            src = torch.LongTensor(self.wrd2phon_ids(wrd))\n",
        "        elif self.source == 'seme':\n",
        "            src = torch.tensor(self.w2v.get_vector(idx))\n",
        "        elif self.source == 'orth':\n",
        "            src = torch.LongTensor(self.wrd2orth_ids(wrd))\n",
        "        else:\n",
        "            src = None\n",
        "\n",
        "        if self.target == 'phon':\n",
        "            tgt = torch.LongTensor(self.wrd2phon_ids(wrd))\n",
        "        elif self.target == 'seme':\n",
        "            tgt = torch.tensor(self.w2v.get_vector(idx))\n",
        "        elif self.target == 'orth':\n",
        "            tgt = torch.LongTensor(self.wrd2orth_ids(wrd))\n",
        "        else:\n",
        "            tgt = None\n",
        "\n",
        "        return src, tgt\n",
        "\n",
        "#     def __getitem__saved(self,  # 旧バージョン\n",
        "#                     idx:int,\n",
        "#                     direction:str=None):\n",
        "#         wrd = self.words[idx]\n",
        "#         if direction == None:\n",
        "#             direction = self.direction\n",
        "#         if direction == 'p2s':\n",
        "#             X = torch.LongTensor(self.wrd2phon_ids(wrd))\n",
        "#             y = torch.tensor(self.w2v.get_vector(idx), dtype=torch.float16)\n",
        "#         else:\n",
        "#             y = torch.LongTensor(self.wrd2phon_ids(wrd))\n",
        "#             X = torch.tensor(self.w2v.get_vector(idx), dtype=torch.float16)\n",
        "\n",
        "#         return X, y\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.w2v)\n",
        "\n",
        "    def getitem(self,\n",
        "                idx:int):\n",
        "        wrd = self.words[idx]\n",
        "        _yomi = self.wrd2yomi(wrd)\n",
        "        _yomi = kunrei(_yomi).split(' ')\n",
        "        phon_ids = [self.phoneme.index(idx) for idx in _yomi]\n",
        "        orth_ids = [self.grapheme.index(idx) for idx in wrd]\n",
        "        return wrd, _yomi, phon_ids, orth_ids\n",
        "\n",
        "    def source_ids2source(self, ids:list):\n",
        "\n",
        "        if self.source == 'phon':\n",
        "            return self.phon_ids2phn(ids)\n",
        "        elif self.source == 'orth':\n",
        "            return self.orth_ids2orth(ids)\n",
        "        elif self.source == 'seme':\n",
        "            wrd = self.getitem(ids)[0]\n",
        "            return w2v.similar_by_word(wrd)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "    def target_ids2target(self, ids:list):\n",
        "\n",
        "        if self.target == 'phon':\n",
        "            return self.phon_ids2phn(ids)\n",
        "        elif self.target == 'orth':\n",
        "            return self.orth_ids2orth(ids)\n",
        "        elif self.target == 'seme':\n",
        "            wrd = self.getitem(ids)[0]\n",
        "            return w2v.similar_by_word(wrd)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "    def wrd2orth_ids(self, wrd:str)->list:\n",
        "        ids = [self.grapheme.index(ch) for ch in wrd]\n",
        "        ids = [self.grapheme.index('<SOW>')] + ids + [self.grapheme.index('<EOW>')]\n",
        "        #ids = [[self.grapheme.index('<SOW>')] + ids + [self.grapheme.index('<EOW>')]]\n",
        "        return ids\n",
        "\n",
        "    def wrd2phon_ids(self, wrd:str)->list:\n",
        "        _yomi = self.wrd2yomi(wrd)\n",
        "        _yomi = kunrei(_yomi).split(' ')\n",
        "        ids = [self.phoneme.index(idx) for idx in _yomi]\n",
        "        ids = [self.phoneme.index('<SOW>')] + ids + [self.phoneme.index('<EOW>')]\n",
        "        return ids\n",
        "\n",
        "    def get_wrdidx_from_word(self, wrd:str):\n",
        "        if wrd in self.words:\n",
        "            wrd_idx = self.w2v.get_index(wrd)\n",
        "        else:\n",
        "            wrd_idx = -1\n",
        "        return wrd_idx\n",
        "\n",
        "    def wrd2emb(self, wrd:str)->np.ndarray:\n",
        "        if wrd in self.words:\n",
        "            return self.w2v.get_vector(wrd)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def wrd2wrd_ids(self, wrd:str)->int:\n",
        "        if wrd in self.words:\n",
        "            return self.words.index(wrd)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def orth_ids2orth(self,\n",
        "                      ids:np.ndarray)->str:\n",
        "    #def orth_ids2orth(self, ids:list)->str:\n",
        "        ret = [self.grapheme[idx] for idx in ids]\n",
        "        return ret\n",
        "\n",
        "    def wrd_idx2wrd(self, idx:int)->str:\n",
        "        if 0 <= idx and idx < len(self.words):\n",
        "            return self.words[idx]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def wrd2onehot(self, wrd:str)->np.ndarray:\n",
        "        ret = np.zeros((self.W.shape[0],), dtype=np.int32)\n",
        "        if wrd in self.words:\n",
        "            ret[self.w2v.get_index(wrd)] = 1\n",
        "            return ret\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def phon_ids2phn(self, ids:np.ndarray):\n",
        "        ret = \"\".join([self.phoneme[idx] for idx in ids])\n",
        "        return ret\n",
        "\n",
        "    def wrd2yomi(self, wrd:str)->list:\n",
        "        if wrd in self.words:\n",
        "            _yomi = self.old_ds.orth2info_dict[wrd]['ヨミ']\n",
        "        else:\n",
        "            _yomi = self.mecab_yomi(wrd).strip().split()[0]\n",
        "        return _yomi\n",
        "\n",
        "    def wrd2info(self, wrd:str)->dict:\n",
        "        if wrd in self.words:\n",
        "            return self.old_ds.orth2info_dict[wrd]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "# 全部で 9 通りのデータセットを定義\n",
        "psylex71_ds_o2o = psylex71_w2v_Dataset(source='orth', target='orth')\n",
        "psylex71_ds_o2p = psylex71_w2v_Dataset(source='orth', target='phon')\n",
        "psylex71_ds_o2s = psylex71_w2v_Dataset(source='orth', target='seme')\n",
        "\n",
        "psylex71_ds_p2o = psylex71_w2v_Dataset(source='phon', target='orth')\n",
        "psylex71_ds_p2p = psylex71_w2v_Dataset(source='phon', target='phon')\n",
        "psylex71_ds_p2s = psylex71_w2v_Dataset(source='phon', target='seme')\n",
        "\n",
        "psylex71_ds_s2o = psylex71_w2v_Dataset(source='seme', target='orth')\n",
        "psylex71_ds_s2p = psylex71_w2v_Dataset(source='seme', target='phon')\n",
        "psylex71_ds_s2s = psylex71_w2v_Dataset(source='seme', target='seme')"
      ],
      "metadata": {
        "id": "CqnS9IzvxML6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader の設定"
      ],
      "metadata": {
        "id": "WzD4qUKwZ8sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 モデルの定義\n",
        "\n",
        "<div style=\"font-family:serif;font-size:14pt;color:purple;font-weight:900\">\n",
        "\n",
        "PyTorch RNN モデルの実装に対する注意メモ\n",
        "    \n",
        "* Encoder 側のデータと Decoder 側のデータそれぞれに対して Padding の処理を行う。\n",
        "* Encoder 側のデータには Padding 値として `0` で埋める。\n",
        "* Decoder 側のデータをモデルの forward で使う場合には、Padding 値は `0` を埋める。\n",
        "* ただし，Decoder 側のデータを教師データとして使う場合には，Padding 値には -1 を用いて，埋めることに注意。\n",
        "* `nn.Embedding()` のオプションに `padding_idx=O` を付け，`CrosEntropyLoss` のオプションに `ignore_index=-1` を付ける。\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "5iX-OrmXzIwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 全モデル共通使用するライブラリの輸入\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "XRDoaDBUzThi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0 共通のハイパーパラメータ宣言"
      ],
      "metadata": {
        "id": "BpnXWJ5GzcCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper parameters\n",
        "n_hid = 128\n",
        "n_layers = 1\n",
        "bidirectional=False\n",
        "batch_size = 1024\n",
        "\n",
        "def _collate_fn(batch):\n",
        "    inps, tgts = list(zip(*batch))\n",
        "    inps = list(inps)\n",
        "    tgts = list(tgts)\n",
        "    return inps, tgts"
      ],
      "metadata": {
        "id": "PFxY8fyezdKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Seq2Seq model"
      ],
      "metadata": {
        "id": "NKFH1V7NOkCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq_wAtt(nn.Module):\n",
        "    \"\"\" 注意つき符号化器‐復号化器モデル\n",
        "    Bahdanau, Cho, & Bengio (2015) NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE, arXiv:1409.0473\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 enc_vocab_size:int,\n",
        "                 dec_vocab_size:int,\n",
        "                 n_hid:int,\n",
        "                 n_layers:int=2,\n",
        "                 bidirectional:bool=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder 側の入力トークン id を多次元ベクトルに変換\n",
        "        self.encoder_emb = nn.Embedding(num_embeddings=enc_vocab_size,\n",
        "                                        embedding_dim=n_hid,\n",
        "                                        padding_idx=0)\n",
        "\n",
        "        # Decoder 側の入力トークン id を多次元ベクトルに変換\n",
        "        self.decoder_emb = nn.Embedding(num_embeddings=dec_vocab_size,\n",
        "                                        embedding_dim=n_hid,\n",
        "                                        padding_idx=0)\n",
        "\n",
        "        # Encoder LSTM 本体\n",
        "        self.encoder = nn.LSTM(input_size=n_hid,\n",
        "                               hidden_size=n_hid,\n",
        "                               num_layers=n_layers,\n",
        "                               batch_first=True,\n",
        "                               bidirectional=bidirectional)\n",
        "\n",
        "        # Decoder LSTM 本体\n",
        "        self.decoder = nn.LSTM(input_size=n_hid,\n",
        "                               hidden_size=n_hid,\n",
        "                               num_layers=n_layers,\n",
        "                               batch_first=True,\n",
        "                               bidirectional=bidirectional)\n",
        "\n",
        "        # 文脈ベクトルと出力ベクトルの合成を合成する層\n",
        "        bi_fact = 2 if bidirectional else 1\n",
        "        self.combine_layer = nn.Linear(bi_fact * 2 * n_hid, n_hid)\n",
        "\n",
        "        # 最終出力層\n",
        "        self.out_layer = nn.Linear(n_hid, dec_vocab_size)\n",
        "\n",
        "    def forward(self, enc_inp, dec_inp):\n",
        "\n",
        "        enc_emb = self.encoder_emb(enc_inp)\n",
        "        enc_out, (hnx, cnx) = self.encoder(enc_emb)\n",
        "\n",
        "        dec_emb = self.decoder_emb(dec_inp)\n",
        "        dec_out, (hny, cny) = self.decoder(dec_emb,(hnx, cnx))\n",
        "\n",
        "        # enc_out は (バッチサイズ，ソースの単語数，中間層の次元数)\n",
        "        # ソース側 (enc_out) の各単語とターゲット側 (dec_out) の各単語との類似度を測定するため\n",
        "        # 両テンソルの内積をとるため ソース側 (enc_out) の軸を入れ替え\n",
        "        enc_outP = enc_out.permute(0,2,1)\n",
        "\n",
        "        # sim の形状は (バッチサイズ, 中間層の次元数，ソースの単語数)\n",
        "        sim = torch.bmm(dec_out, enc_outP)\n",
        "\n",
        "        # sim の各次元のサイズを記録\n",
        "        batch_size, dec_word_size, enc_word_size = sim.shape\n",
        "\n",
        "        # sim に対して，ソフトマックスを行うため形状を変更\n",
        "        simP = sim.reshape(batch_size * dec_word_size, enc_word_size)\n",
        "\n",
        "        # simP のソフトマックスを用いて注意の重み alpha を算出\n",
        "        alpha = F.softmax(simP,dim=1).reshape(batch_size, dec_word_size, enc_word_size)\n",
        "\n",
        "        # 注意の重み alpha に encoder の出力を乗じて，文脈ベクトル c_t とする\n",
        "        c_t = torch.bmm(alpha, enc_out)\n",
        "\n",
        "        # torch.cat だから c_t と dec_out とで合成\n",
        "        dec_out_ = torch.cat([c_t, dec_out], dim=2)\n",
        "        dec_out_ = self.combine_layer(dec_out_)\n",
        "\n",
        "        return self.out_layer(dec_out_)\n",
        "\n",
        "\n",
        "# 以下確認作業\n",
        "ds = psylex71_ds_o2p\n",
        "o2p = Seq2Seq_wAtt(enc_vocab_size=len(ds.grapheme),\n",
        "                   dec_vocab_size=len(ds.phoneme),\n",
        "                   n_layers=n_layers,\n",
        "                   bidirectional=bidirectional,\n",
        "                   n_hid=n_hid).to(device)\n",
        "print(o2p.eval())"
      ],
      "metadata": {
        "id": "ZBet5d87aM0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Vec2Seq model"
      ],
      "metadata": {
        "id": "3wg61KvMagRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vec2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 sem_dim:int,\n",
        "                 dec_vocab_size:int,\n",
        "                 n_hid:int,\n",
        "                 n_layers:int=2,\n",
        "                 bidirectional:bool=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # 単語の意味ベクトル a.k.a 埋め込み表現 を decoder の中間層に接続するための変換層\n",
        "        # 別解としては，入力層に接続する方法があるが，それはまた別実装にする\n",
        "        self.enc_transform_layer = nn.Linear(\n",
        "            in_features=sem_dim,\n",
        "            out_features=n_hid)\n",
        "        self.decoder_emb = nn.Embedding(\n",
        "            num_embeddings=dec_vocab_size,\n",
        "            embedding_dim=n_hid,\n",
        "            padding_idx=0)\n",
        "\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=n_hid,\n",
        "            hidden_size=n_hid,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional)\n",
        "\n",
        "        # 最終出力層\n",
        "        self.bi_fact = 2 if bidirectional else 1\n",
        "        self.out_layer = nn.Linear(self.bi_fact * n_hid, dec_vocab_size)\n",
        "\n",
        "    def forward(self, enc_inp, dec_inp):\n",
        "        enc_emb = self.enc_transform_layer(enc_inp)\n",
        "        hnx, cnx = enc_emb.clone(), enc_emb.clone()\n",
        "        hnx = hnx.unsqueeze(0)\n",
        "        cnx = cnx.unsqueeze(0)\n",
        "\n",
        "        if self.bi_fact == 2:\n",
        "            hnx = hnx.repeat(2)\n",
        "            cnx = cnx.repeat(2)\n",
        "\n",
        "        dec_emb = self.decoder_emb(dec_inp)\n",
        "\n",
        "        batch_size = enc_inp.size(0)\n",
        "        exp_hid_size = self.decoder.get_expected_hidden_size(enc_inp, batch_sizes=[batch_size])\n",
        "        dec_out, (hny, cny) = self.decoder(dec_emb,(hnx, cnx))\n",
        "\n",
        "        return self.out_layer(dec_out)\n",
        "\n",
        "# 以下確認作業\n",
        "ds = psylex71_ds_s2p\n",
        "s2p = Vec2Seq(\n",
        "    sem_dim=ds.w2v.vector_size,\n",
        "    dec_vocab_size=len(ds.phoneme),\n",
        "    n_hid=n_hid,\n",
        "    n_layers=n_layers,\n",
        "    bidirectional=bidirectional).to(device)\n",
        "print(s2p.eval())"
      ],
      "metadata": {
        "id": "iVc-Hlkpaisi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Seq2Vec model"
      ],
      "metadata": {
        "id": "8voQ8m1aO5V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Vec(nn.Module):\n",
        "    \"\"\" 系列データを符号化器に与え，埋め込みデータ (ベクトル) を復号化するモデル\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        sem_dim:int,\n",
        "        enc_vocab_size:int,\n",
        "        n_hid:int,\n",
        "        n_layers:int=2,\n",
        "        bidirectional:bool=False):\n",
        "\n",
        "        super().__init__()\n",
        "        self.encoder_emb = nn.Embedding(\n",
        "            num_embeddings=enc_vocab_size,\n",
        "            embedding_dim=n_hid,\n",
        "            padding_idx=0)\n",
        "\n",
        "        self.encoder = nn.LSTM(\n",
        "            input_size=n_hid,\n",
        "            hidden_size=n_hid,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional)\n",
        "\n",
        "        bi_fact = 2 if bidirectional else 1\n",
        "\n",
        "        self.encoder_out = nn.Linear(\n",
        "            in_features=n_hid * bi_fact,\n",
        "            out_features=enc_vocab_size)\n",
        "\n",
        "        self.out_layer = nn.Linear(\n",
        "            in_features=n_hid * bi_fact,\n",
        "            out_features=sem_dim)\n",
        "\n",
        "    def forward(self, enc_inp):\n",
        "        enc_emb = self.encoder_emb(enc_inp)\n",
        "        enc_out, (hid, cel) = self.encoder(enc_emb)\n",
        "        _enc_out = self.encoder_out(enc_out)\n",
        "        _sem = self.out_layer(hid)\n",
        "        return _sem, _enc_out\n",
        "\n",
        "# 以下確認作業\n",
        "ds = psylex71_ds_o2s\n",
        "o2s = Seq2Vec(sem_dim=ds.w2v.vector_size,\n",
        "              enc_vocab_size=len(ds.grapheme),\n",
        "              n_layers=n_layers,\n",
        "              bidirectional=bidirectional,\n",
        "              n_hid=n_hid).to(device)\n",
        "print(o2s.eval())"
      ],
      "metadata": {
        "id": "SyHYiBkdO859"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Vec2Vec model"
      ],
      "metadata": {
        "id": "1jdRrg59amxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vec2Vec(nn.Module):\n",
        "    \"\"\" ベクトル埋め込み表現をベクトル埋め込み表現へと変換\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 sem_dim:int,\n",
        "                 n_hid:int):\n",
        "\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(sem_dim, n_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hid, n_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hid, sem_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "# 以下確認作業\n",
        "ds = psylex71_ds_s2s\n",
        "s2s = Vec2Vec(sem_dim=ds.w2v.vector_size,\n",
        "              n_hid=n_hid).to(device)\n",
        "print(s2s.eval())"
      ],
      "metadata": {
        "id": "Dbpmmy9lz8VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 各モデルの宣言"
      ],
      "metadata": {
        "id": "bsm_xR_GPAZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(colored('# 1 写字モデル o2o', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_o2o\n",
        "o2o = Seq2Seq_wAtt(\n",
        "    enc_vocab_size=len(ds.grapheme),\n",
        "    dec_vocab_size=len(ds.grapheme),\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(o2o.eval())\n",
        "\n",
        "print(colored('# 2 o2p 音読モデル意味関与なし', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_o2p\n",
        "o2p = Seq2Seq_wAtt(\n",
        "    enc_vocab_size=len(ds.grapheme),\n",
        "    dec_vocab_size=len(ds.phoneme),\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(o2p.eval())\n",
        "\n",
        "print(colored('# 3 o2s 印字理解モデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_o2s\n",
        "o2s = Seq2Vec(\n",
        "    enc_vocab_size=len(ds.grapheme),\n",
        "    sem_dim=ds.w2v.vector_size,\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(o2s.eval())\n",
        "\n",
        "print(colored('# 4 p2o 聞き書き ディクテーションモデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_p2o\n",
        "p2o = Seq2Seq_wAtt(\n",
        "    enc_vocab_size=len(ds.phoneme),\n",
        "    dec_vocab_size=len(ds.grapheme),\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(p2o.eval())\n",
        "\n",
        "print(colored('# 5 p2p 復唱モデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_p2p\n",
        "p2p = Seq2Seq_wAtt(\n",
        "    enc_vocab_size=len(ds.phoneme),\n",
        "    dec_vocab_size=len(ds.phoneme),\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(p2p.eval())\n",
        "\n",
        "print(colored('# 6 p2s 聴理解モデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_p2s\n",
        "p2s = Seq2Vec(\n",
        "    enc_vocab_size=len(ds.phoneme),\n",
        "    sem_dim=ds.w2v.vector_size,\n",
        "    n_hid=n_hid, n_layers=n_layers, bidirectional=bidirectional).to(device)\n",
        "print(p2s.eval())\n",
        "\n",
        "print(colored('# 7 s2o 書字モデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_s2o\n",
        "s2o = Vec2Seq(\n",
        "    sem_dim=ds.w2v.vector_size,\n",
        "    dec_vocab_size=len(ds.grapheme),\n",
        "    n_hid=n_hid, n_layers=n_layers, bidirectional=bidirectional).to(device)\n",
        "print(s2o.eval())\n",
        "\n",
        "print(colored('# 8 s2p 発話モデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_s2p\n",
        "s2p = Vec2Seq(\n",
        "    sem_dim=ds.w2v.vector_size,\n",
        "    dec_vocab_size=len(ds.phoneme),\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(s2p.eval())\n",
        "\n",
        "print(colored('# 9 s2s 意味理解モデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_s2s\n",
        "s2s = Vec2Vec(\n",
        "    sem_dim=len(ds.grapheme),\n",
        "    n_hid=n_hid).to(device)\n",
        "print(s2s.eval())"
      ],
      "metadata": {
        "id": "naU-W9KuaqKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 訓練手続きの定義"
      ],
      "metadata": {
        "id": "BhHtxVeXa2kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `fit_seq2seq()`"
      ],
      "metadata": {
        "id": "08U8pWoe0RPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "id": "n7bmAkSl4t1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_seq2seq(\n",
        "    model:torch.nn.modules.module.Module=o2p,\n",
        "    epochs:int=10,\n",
        "    ds:Dataset=psylex71_ds_o2p,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=_collate_fn,\n",
        "    #dataloader:torch.utils.data.dataloader.DataLoader=dl_o2p,\n",
        "    optimizer:torch.optim=None,\n",
        "    lr:float=0.001,\n",
        "    criterion:torch.nn.modules.loss=nn.CrossEntropyLoss(ignore_index=-1),\n",
        "    interval:int=None,\n",
        "    isPrint:bool=False,\n",
        "    losses:list=None,\n",
        "    isDraw:bool=True,):\n",
        "    \"\"\" Seq2seq の訓練に用いる関数\"\"\"\n",
        "\n",
        "    start_time = time.time()   # 開始時刻の保存\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset=ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=collate_fn)\n",
        "\n",
        "    if losses == None:\n",
        "        losses = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    if optimizer == None:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    if interval == None:\n",
        "        interval = int(ds.__len__()/batch_size) >> 2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        i = 0\n",
        "        for _inp, _tch in dataloader:\n",
        "            enc_inp = pad_sequence(_inp, batch_first=True).to(device)\n",
        "            dec_inp = pad_sequence(_tch, batch_first=True).to(device)\n",
        "            tch = pad_sequence(_tch, batch_first=True, padding_value=-1.0).to(device)\n",
        "            out = model(enc_inp, dec_inp)\n",
        "            loss = criterion(out[0], tch[0])\n",
        "            for h in range(1,len(tch)):\n",
        "                loss += criterion(out[h], tch[h])\n",
        "            losses.append(loss.item()/len(_inp))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            i += 1\n",
        "            if (i % interval) == 0:\n",
        "                print(f'epoch:{epoch+1:2d}',\n",
        "                      f'batch:{i:2d}',\n",
        "                      f'loss:{loss.item()/batch_size:.5f}')\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "\n",
        "    if isDraw:\n",
        "        plt.plot(losses)\n",
        "        plt.title(f'epochs:{epochs}, batch_size:{batch_size}, n_hid:{n_hid}, n_layers:{n_layers}, time collapsed:{total_time_str}')\n",
        "        plt.show()\n",
        "\n",
        "    return {'Training time':total_time_str,\n",
        "            'losses': losses,\n",
        "            'optimizer': optimizer,\n",
        "            'time': total_time\n",
        "           }\n",
        "\n",
        "#fit_seq2seq(model=o2p, dataloader=dl_o2p) # 音読モデル\n",
        "fit_seq2seq(epochs=1, model=o2p, ds=psylex71_ds_o2p); # 音読モデル\n",
        "fit_seq2seq(epochs=1, model=p2p, ds=psylex71_ds_p2p); # 復唱モデル\n",
        "fit_seq2seq(epochs=1, model=p2o, ds=psylex71_ds_p2o); # ディクテーションモデル\n",
        "fit_seq2seq(epochs=1, model=o2o, ds=psylex71_ds_o2o); # 写字モデル"
      ],
      "metadata": {
        "id": "tlvDjxjeP9DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `eval_seq2seq()`"
      ],
      "metadata": {
        "id": "fxRC3dSg-LQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_seq2seq(\n",
        "    model:torch.nn.modules.module.Module=o2p,\n",
        "    ds:Dataset=psylex71_ds_o2p,\n",
        "    isPrint:bool=False,\n",
        "    errors:list=None):\n",
        "\n",
        "    model.eval()\n",
        "    if errors == None:\n",
        "        errors=[]\n",
        "\n",
        "    for N in tqdm(range(ds.__len__())):\n",
        "        x, y = ds.__getitem__(N)\n",
        "        enc_inp, dec_inp = x.unsqueeze(0).to(device), y.unsqueeze(0).to(device)\n",
        "        grand_truth = y.detach().numpy()[1:-1]\n",
        "        y_hat = model(enc_inp, dec_inp).to('cpu')\n",
        "        y_hat = np.argmax(y_hat.squeeze(0).detach().numpy(), axis=1)[1:-1]\n",
        "\n",
        "        if len(y_hat) == len(grand_truth):\n",
        "            n_correct = np.array((y_hat == grand_truth).sum())\n",
        "            isOK = n_correct == len(grand_truth)\n",
        "        else:\n",
        "            isOK = False\n",
        "\n",
        "        if not isOK:\n",
        "            wrd = ds.getitem(N)[0]\n",
        "            _out = ds.target_ids2target(y_hat)\n",
        "            errors.append((N, wrd, _out,y_hat))\n",
        "            #errors.append((N,y_hat))\n",
        "            if isPrint:\n",
        "                color = 'grey' if isOK else 'red'\n",
        "                wrd = ds.getitem(N)[0]\n",
        "                print(colored(f'{N:05d}', color),\n",
        "                      colored(wrd, color='grey'), # , attrs=[\"bold\"]),\n",
        "                      colored(y_hat,color,attrs=[\"bold\"]),\n",
        "                      colored(ds.target_ids2target(y_hat), color, attrs=[\"bold\"]),\n",
        "                      f'<-{ds.target_ids2target(grand_truth)}')\n",
        "\n",
        "    cr = len(errors) / N\n",
        "    return {'エラー':errors,\n",
        "            '正解率': (1.-cr) * 100}"
      ],
      "metadata": {
        "id": "30yf61aw-NGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit_seq2seq(epochs=3, model=s2p, ds=psylex71_ds_s2p)\n",
        "eval_seq2seq(model=s2p, ds=psylex71_ds_s2p)\n",
        "\n",
        "fit_seq2seq(epochs=3, model=s2o, ds=psylex71_ds_s2o)\n",
        "eval_seq2seq(model=s2o, ds= psylex71_ds_s2o)"
      ],
      "metadata": {
        "id": "XUI2TaOm-Q5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit_seq2seq(epochs=15, model=s2o, ds=psylex71_ds_s2o)\n",
        "out = eval_seq2seq(model=s2o, ds= psylex71_ds_s2o, isPrint=True)"
      ],
      "metadata": {
        "id": "PdMqFzNIfrjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "id": "5GTaV4fDjptP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `fit_vec2sec()` の定義\n",
        "実際には `fit_seq2seq()` を用いるため何もしない。"
      ],
      "metadata": {
        "id": "fvyxlXRa-u6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `eval_vec2sec()` は `eval_seq2seq()` と同じ"
      ],
      "metadata": {
        "id": "MTxWEET2-zwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `fit_seq2vec()` の定義"
      ],
      "metadata": {
        "id": "bkln5eby-2b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_seq2vec(\n",
        "    model:torch.nn.modules.module.Module=o2p,\n",
        "    epochs:int=10,\n",
        "    ds:Dataset=psylex71_ds_o2s,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=_collate_fn,\n",
        "    #dataloader:torch.utils.data.dataloader.DataLoader=dl_o2s,\n",
        "    optimizer:torch.optim=None,\n",
        "    lr:float=0.001,\n",
        "    criterion_dec:torch.nn.modules.loss=nn.MSELoss(),\n",
        "    criterion_enc:torch.nn.modules.loss=nn.CrossEntropyLoss(ignore_index=-1),\n",
        "    interval:int=None,\n",
        "    isPrint:bool=False,\n",
        "    losses:list=None,\n",
        "    isDraw:bool=True,):\n",
        "    \"\"\" Seq2vec の訓練に用いる関数\"\"\"\n",
        "\n",
        "    start_time = time.time()   # 開始時刻の保存\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset=ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "    if losses == None:\n",
        "        losses = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    if optimizer == None:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    criterion_dec = nn.MSELoss()\n",
        "    criterion_enc = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "    #criterion_enc = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    if interval == None:\n",
        "        interval = int(ds.__len__()/batch_size) >> 2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        i = 0\n",
        "        for _inp, _tch in dataloader:\n",
        "            enc_inp = pad_sequence(_inp, batch_first=True).to(device)\n",
        "            tch = pad_sequence(_tch, batch_first=True, padding_value=-1.0).to(device)\n",
        "            out, enc_out = model(enc_inp)\n",
        "\n",
        "            out = out.squeeze(0)\n",
        "            loss = criterion_dec(out, tch)\n",
        "            for _x, _y in zip(enc_out[:,:-1,:], enc_inp[:,1:]):\n",
        "                loss += criterion_enc(_x, _y)\n",
        "            losses.append(loss.item()/len(_inp))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            i += 1\n",
        "            if (i % interval) == 0:\n",
        "                print(f'epoch:{epoch+1:2d}',\n",
        "                      f'batch:{i:2d}',\n",
        "                      f'loss:{loss.item()/batch_size:.5f}')\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "\n",
        "    if isDraw:\n",
        "        plt.plot(losses)\n",
        "        plt.title(f'epochs:{epochs}, batch_size:{batch_size}, n_hid:{n_hid}, n_layers:{n_layers}, time collapsed:{total_time_str}')\n",
        "        plt.show()\n",
        "\n",
        "    return {'Training time': total_time_str,\n",
        "            'losses': losses,\n",
        "            'optimizer': optimizer,\n",
        "            'time': total_time }\n",
        "\n",
        "#fit_seq2vec(epochs=10, model=p2s, lr=0.0001, ds=psylex71_ds_p2s, dataloader=dl_p2s); # 聴理解モデル\n",
        "#fit_seq2vec(epochs=50, model=o2s, lr=0.0001, ds=psylex71_ds_o2s, dataloader=dl_o2s); # 印字理解モデル\n",
        "#fit_seq2vec(epochs=20, model=o2s, lr=0.00001, ds=psylex71_ds_o2s, dataloader=dl_o2s); # 印字理解モデル\n",
        "fit_seq2vec(epochs=20, model=o2s, lr=0.0001, ds=psylex71_ds_o2s); # 印字理解モデル"
      ],
      "metadata": {
        "id": "RKYtsrMO-4fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "n_hid = 64\n",
        "#n_hid = 32\n",
        "n_layers = 1\n",
        "epochs = 10\n",
        "bidirectional=False\n",
        "model = EncDec_s2p(sem_dim=_psylex71_ds.w2v.vector_size,\n",
        "                   dec_vocab_size=len(_psylex71_ds.phoneme),\n",
        "                   n_layers=n_layers,\n",
        "                   bidirectional=bidirectional,\n",
        "                   n_hid=n_hid).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "model.train()\n",
        "interval = int(_psylex71_ds.__len__()/batch_size) >> 2\n",
        "losses = []\n",
        "for epoch in range(epochs):\n",
        "    i = 0\n",
        "    for x, y in dataloader:\n",
        "        print(type(x), len(x))\n",
        "        sys.exit()\n",
        "        enc_inp = torch.from_numpy(np.array(x)).float().to(device).unsqueeze(0)\n",
        "        #enc_inp = torch.from_numpy(np.array(x)).to(device).unsqueeze(0)\n",
        "        dec_inp = pad_sequence(y, batch_first=True).to(device)[:,1:]\n",
        "        tch = pad_sequence(y, batch_first=True, padding_value=-1.0).to(device)\n",
        "        tch = tch[:,1:]\n",
        "        out = model(enc_inp, dec_inp)\n",
        "        loss = criterion(out[0], tch[0])\n",
        "        for h in range(1,len(tch)):\n",
        "            loss += criterion(out[h], tch[h])\n",
        "        losses.append(loss.item()/len(x))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        i += 1\n",
        "        if (i % interval) == 0:\n",
        "            print(f'epoch:{epoch:2d}, batch:{i:2d}, loss:{loss.item()/len(x):.3f}')\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "print(f'Training time {total_time_str}')\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title(f'epochs:{epochs}, batch_size:{batch_size}, n_hid:{n_hid}, n_layers:{n_layers}, time collapsed:{total_time_str}')\n",
        "plt.show()\n",
        "\n",
        "#outfile = \"attnmt2-\" + str(epoch) + \".model\"\n",
        "#torch.save(net.state_dict(),outfile)"
      ],
      "metadata": {
        "id": "Z-Gys8WXa6mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 学習結果の評価"
      ],
      "metadata": {
        "id": "Z2eFYp2Na-dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errors = []\n",
        "model.eval()\n",
        "for N in range(_psylex71_ds.__len__()):\n",
        "#for N in np.random.permutation(_psylex71_ds.__len__())[:100]:\n",
        "    x, y = _psylex71_ds.__getitem__(N)\n",
        "    enc_inp = torch.from_numpy(np.array(x)).to(device).unsqueeze(0)\n",
        "    enc_emb = model.enc_transform_layer(enc_inp)\n",
        "    hnx, cnx = enc_emb.clone(), enc_emb.clone()\n",
        "    dec_inp = y\n",
        "    dec_emb = model.decoder_emb(dec_inp)\n",
        "    dec_out, (hny, cny) = model.decoder(dec_emb,(hnx, cnx))\n",
        "    dec_out = model.out_layer(dec_out)\n",
        "    y_ids = np.argmax(dec_out.detach().numpy(),axis=1)\n",
        "\n",
        "    n_correct = np.array((y_ids[1:-1] == _psylex71_ds.getitem(N)[2]).sum())\n",
        "    isOK = n_correct == len(_psylex71_ds.getitem(N)[2])\n",
        "    color = 'grey' if isOK else 'red'\n",
        "\n",
        "    if not isOK:\n",
        "        errors.append((N,y_ids))\n",
        "        print(colored((f'{N:05d}', #y_ids,\n",
        "                       \"\".join(p for p in _psylex71_ds.phon_ids2phn(y_ids[1:-1]))),color,attrs=[\"bold\"]), end=\" \")\n",
        "        print(_psylex71_ds.getitem(N))\n",
        "\n",
        "cr = len(errors) / _psylex71_ds.__len__()\n",
        "print(f'総エラー数:{len(errors)}',\n",
        "      f'正解率:{(1.-cr)*100:.3f}')\n"
      ],
      "metadata": {
        "id": "27BxVraEbB77"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}