{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOdMeJf4FR/YDqkCCxdBbJw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_0113lam_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w5JxNk5Fq3E"
      },
      "outputs": [],
      "source": [
        "# ここはお遊びなので，スキップしても良い\n",
        "import IPython\n",
        "#IPython.display.Image(url=\"https://livedoor.blogimg.jp/ftb001/imgs/b/4/b4629a79.jpg\")\n",
        "IPython.display.Image(url=\"https://uy-allstars.com/_assets/images/pages/char/detail/webp/lum@pc.webp\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "try:\n",
        "    import bit\n",
        "except ImportError:\n",
        "    !pip install ipynbname --upgrade > /dev/null 2>&1 \n",
        "    !git clone https://github.com/ShinAsakawa/bit.git\n",
        "import bit\n",
        "\n",
        "isColab = bit.isColab\n",
        "HOME = bit.HOME\n",
        "\n",
        "if isColab:\n",
        "    !apt install aptitude\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "    !pip install mecab-python3==0.7\n",
        "    !pip install jaconv\n",
        "    \n",
        "    import MeCab\n",
        "    wakati = MeCab.Tagger('-Owakati').parse\n",
        "    yomi = MeCab.Tagger('-Oyomi').parse\n",
        "else:\n",
        "    from ccap.mecab_settings import yomi\n",
        "    from ccap.mecab_settings import wakati\n",
        "\n",
        "# 自作ライブラリ LAM の読み込み\n",
        "if isColab:\n",
        "    !git clone https://github.com/ShinAsakawa/ccap.git\n",
        "    !git clone https://github.com/ShinAsakawa/lam.git"
      ],
      "metadata": {
        "id": "4AKzWYhyFzNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 準備作業\n"
      ],
      "metadata": {
        "id": "vfXSpjSFF3TI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1.1 ライブラリのインポート\n",
        "\n",
        "1.   直下セルは，mecab をコンパイルするので時間がかかるので注意\n",
        "\n"
      ],
      "metadata": {
        "id": "jSUR1aPNF9Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Colab 上で実行する場合，必要なファイルのアップロード"
      ],
      "metadata": {
        "id": "3Uo56ZcOGFrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload `pslex71utf8.txt`, NTT 日本語の語彙特性 頻度データ\n",
        "# upload `lam/2022_0508SALA_TLPA.json` SALA と TLPA のデータが必要になるかもしれない。\n",
        "if isColab:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()"
      ],
      "metadata": {
        "id": "b2HdzvD9F_Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 パラメータ設定"
      ],
      "metadata": {
        "id": "F6zaoXdEGPK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import lam\n",
        "device = lam.device  # CPU or GPU の選択\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "# シミュレーションに必要なパラメータの設定\n",
        "params = {\n",
        "    'traindata_size':  20000,   # 訓練データ数，NTT 日本語語彙特性の高頻度語を上位から抽出\n",
        "    #'traindata_size': 301612,  # 訓練データ数，NTT 日本語語彙特性の高頻度語を上位から抽出\n",
        "    'epochs': 20,               # 学習のためのエポック数\n",
        "    'hidden_size': 128,         # 中間層のニューロン数\n",
        "    'random_seed': 42,          # 乱数の種。ダグラス・アダムス著「銀河ヒッチハイカーズガイド」\n",
        "\n",
        "    # 以下 `source` と `target` を定義することで，別の課題を実行可能\n",
        "    'source': 'orth',          # ['orth', 'phon', 'mora', 'mora_p', 'mora_p_r']\n",
        "    'target': 'phon',         # ['orth', 'phon', 'mora', 'mora_p', 'mora_p_r']\n",
        "    #'target': 'mora_p_r',      # ['orth', 'phon', 'mora', 'mora_p', 'mora_p_r']\n",
        "    # 'orthography': 書記素, \n",
        "    # 'phonology': 音韻, \n",
        "    # 'mora': モーラ\n",
        "    # 'mora_p': モーラを silius による音分解\n",
        "    # 'mora_p_r': モーラの silius 音分解の逆\n",
        "    'pretrained': False,          # True であれば訓練済ファイルを読み込む\n",
        "    #'pretrained': True,          # True であれば訓練済ファイルを読み込む\n",
        "    #'isTrain'   : True,          # True であれば学習する\n",
        "    \n",
        "    # 学習済のモデルパラメータを保存するファイル名\n",
        "    #'path_saved': '2022_0607lam_o2p_hid32_vocab10k.pt', \n",
        "    #'path_saved': '2022_0829lam_p2p_hid24_vocab10k.pt',\n",
        "    'path_saved': False,                      # 保存しない場合\n",
        "    \n",
        "    # 結果の散布図を保存するファイル名    \n",
        "    #'path_graph': '2022_0829lam_p2p_hid24_vocab10k.pdf',\n",
        "    'path_graph': False,                     # 保存しない場合\n",
        "\n",
        "    'lr': 0.0001,                              # 学習率\n",
        "    'dropout_p': 0.0,                         # ドロップアウト率\n",
        "    'teacher_forcing_ratio': 0.5,             # 教師強制を行う確率\n",
        "    'optim_func': torch.optim.Adam,           # 最適化アルゴリズム ['torch.optim.Adam', 'torch.optim.SGD', 'torch.optim.AdamW']\n",
        "    'loss_func' :torch.nn.CrossEntropyLoss(), # 交差エントロピー損失 ['torch.nn.NLLLoss()', or 'torch.nn.CrossEntropyLoss()']\n",
        "}"
      ],
      "metadata": {
        "id": "wNfdyAqrGP6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 データセットの設定"
      ],
      "metadata": {
        "id": "u_ZJ76frGpe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_vocab = lam.VOCAB(traindata_size=params['traindata_size'], \n",
        "                   w2v=None, \n",
        "                   yomi=yomi) \n",
        "\n",
        "source = params['source']\n",
        "target = params['target']\n",
        "\n",
        "# _max_len はアテンション機構のデコーダで必要になるため，全条件で最長の長さを指定する必要がある\n",
        "_max_len = _vocab.max_ortho_length\n",
        "_max_len = _max_len if _max_len > _vocab.max_phone_length else _vocab.max_phone_length\n",
        "_max_len = _max_len if _max_len > _vocab.max_mora_length else _vocab.max_mora_length\n",
        "_max_len = _max_len if _max_len > _vocab.max_mora_p_length else _vocab.max_mora_p_length\n",
        "_vocab.max_length = _max_len + 1\n",
        "print(colored(f'_vocab.max_length: {_vocab.max_length}', 'blue', attrs=['bold']))\n",
        "\n",
        "# ソース，すなわち encoder 側の，項目番号，項目 ID，decoder 側の項目，項目 ID を設定\n",
        "source_vocab, source_ids, target_vocab, target_ids = lam.get_soure_and_target_from_params(\n",
        "    params=None,\n",
        "    _vocab=_vocab,\n",
        "    source=source,\n",
        "    target=target,\n",
        "    is_print=True)\n",
        "\n",
        "print(colored(f'source:{source}','blue', attrs=['bold']), f'{source_vocab}')\n",
        "print(colored(f'target:{target}','cyan', attrs=['bold']), f'{target_vocab}')\n",
        "print(colored(f'source_ids:{source_ids}','blue', attrs=['bold']), f'{source_ids}')\n",
        "print(colored(f'target_ids:{target_ids}','cyan', attrs=['bold']), f'{target_ids}')\n",
        "\n",
        "# 検証データとして，TLPA と SALA のデータを用いる\n",
        "tlpa1, tlpa2, tlpa3, tlpa4, sala_r29, sala_r30, sala_r31 = lam.read_json_tlpa1234_sala_r29_30_31(\n",
        "    json_fname='lam/2022_0508SALA_TLPA.json')\n",
        "\n",
        "_dataset = {}\n",
        "_data_names = ['tlpa2', 'tlpa3', 'tlpa4', 'sala_r29', 'sala_r30', 'sala_r31']\n",
        "for data in _data_names:\n",
        "    _dataset[data] = {'rawdata':eval(data),\n",
        "                      'pdata': lam.make_vocab_dataset(eval(data),vocab=_vocab)}\n",
        "\n",
        "# 以下は後から付け足したので，コードが汚くなっている。\n",
        "# 時間ができたらコードの整理をすること\n",
        "# X_vals = lam.make_X_vals(_dataset=_dataset,\n",
        "#                          source_vocab=source_vocab,\n",
        "#                          target_vocab=target_vocab,\n",
        "#                          source_ids=source_ids,\n",
        "#                          target_ids=target_ids\n",
        "#                         )"
      ],
      "metadata": {
        "id": "m3N6-wUoGRxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_wordlist = [v['orig'] for k, v in _vocab.train_data.items()]\n",
        "print(len(train_wordlist))"
      ],
      "metadata": {
        "id": "lXta-tEBGs60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 任意の単語 orthography を変換するための関数"
      ],
      "metadata": {
        "id": "L-1QLzhrGxfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_ids_from_orth(orth_wrd:str='てれび',\n",
        "                       __vocab:lam.lam.VOCAB=_vocab):\n",
        "    _yomi, _phon, _phon_r, _orth, _orth_r, _mora, _mora_r = __vocab.get7lists_from_orth(orth=orth_wrd)\n",
        "    phon_ids, phon_ids_r, orth_ids, orth_ids_r, mora_ids, mora_ids_r = __vocab.get6ids(yomi=_yomi, _phon=_phon, _orth=_orth)\n",
        "    \n",
        "    return {'_yomi':_yomi,\n",
        "            '_phon':_phon,\n",
        "            '_phon_r':_phon_r,\n",
        "            '_orth':_orth,\n",
        "            '_orth_r':_orth_r,\n",
        "            '_mora':_mora,\n",
        "            '_mora_r':_mora_r,\n",
        "            'phon_ids':phon_ids,\n",
        "            'phon_ids_r':phon_ids_r,\n",
        "            'orth_ids':orth_ids,\n",
        "            'orth_ids_r':orth_ids_r,\n",
        "            'mora_ids':mora_ids,\n",
        "            'mora_ids_r':mora_ids_r,\n",
        "           }\n",
        "    \n",
        "print(_get_ids_from_orth())\n",
        "\n",
        "\n",
        "def orth_ids2tkn(ids:list):\n",
        "    return [_vocab.ortho_vocab[idx] for idx in ids]\n",
        "\n",
        "def orth_tkn2ids(tkn:list):\n",
        "    return [_vocab.ortho_vocab.index(_tkn) if _tkn in _vocab.ortho_vocab else _vocab.ortho_vocab.index('<UNK>') for _tkn in tkn]\n",
        "\n",
        "def mora_p_ids2tkn(ids:list):\n",
        "    return [_vocab.mora_p_vocab[idx] for idx in ids]\n",
        "\n",
        "def mora_p_tkn2ids(tkn:list):\n",
        "    return [_vocab.mora_p_vocab.index(_tkn) if _tkn in _vocab.mora_p_vocab else _vocab.mora_p_vocab('<UNK>') for _tkn in tkn]\n",
        "\n",
        "def mora_ids2tkn(ids:list):\n",
        "    return [_vocab.mora_vocab[idx] for idx in ids]\n",
        "\n",
        "def mora_tkn2ids(tkn:list):\n",
        "    return [_vocab.mora_vocab.index(_tkn) if _tkn in _vocab.mora_vocab else _vocab.mora_vocab('<UNK>') for _tkn in tkn]\n",
        "    #return [_vocab.mora_vocab.index(_tkn) for _tkn in tkn]\n",
        "\n",
        "\n",
        "_ids = [111, 298]\n",
        "print(orth_ids2tkn(_ids))\n",
        "print(orth_tkn2ids(orth_ids2tkn(_ids)))\n",
        "\n",
        "print(orth_tkn2ids('新しい'))\n",
        "print(orth_tkn2ids('神経心理学'))\n",
        "print(orth_ids2tkn(orth_tkn2ids('神経心理学')))\n",
        "print(mora_p_ids2tkn([17,19,11,4,32,17]))\n",
        "print(mora_p_tkn2ids(mora_p_ids2tkn([17,19,11,4,32,17])))\n",
        "print(mora_ids2tkn([37,139,31,7]))\n",
        "print(mora_tkn2ids(mora_ids2tkn([37,139,31,7])))"
      ],
      "metadata": {
        "id": "KPsp7Xd4Gx3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SALA and TLPA dataset"
      ],
      "metadata": {
        "id": "u7M0qKRVG2nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 検証データとして，TLPA と SALA のデータを用いる\n",
        "tlpa1, tlpa2, tlpa3, tlpa4, sala_r29, sala_r30, sala_r31 = lam.read_json_tlpa1234_sala_r29_30_31(\n",
        "    json_fname='lam/2022_0508SALA_TLPA.json')\n",
        "\n",
        "_dataset = {}\n",
        "_data_names = ['tlpa2', 'tlpa3', 'tlpa4', 'sala_r29', 'sala_r30', 'sala_r31']\n",
        "for data in _data_names:\n",
        "    _dataset[data] = {'rawdata':eval(data),\n",
        "                      'pdata': lam.make_vocab_dataset(eval(data), vocab=_vocab)}\n",
        "\n",
        "# 以下は後から付け足したので，コードが汚くなっている。\n",
        "# 時間ができたらコードの整理をすること\n",
        "# X_vals = lam.make_X_vals(_dataset=_dataset,\n",
        "#                          source_vocab=source_vocab,\n",
        "#                          target_vocab=target_vocab,\n",
        "#                          source_ids=source_ids,\n",
        "#                          target_ids=target_ids)\n",
        "\n",
        "_data_names = ['tlpa2', 'tlpa3', 'tlpa4', 'sala_r29', 'sala_r30', 'sala_r31']    \n",
        "for data in _data_names:\n",
        "    print(colored(data, 'blue', attrs=['bold']), eval(data))"
      ],
      "metadata": {
        "id": "ay7zOCqQG26m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wCHf_X31G_Of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fushimi1999 データセット"
      ],
      "metadata": {
        "id": "d4Hpf_VFG-Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fushimi1999 = {\n",
        "    'HF___consist__': ['戦争', '倉庫', '医学', '注意', '記念', '番号', '料理', '完全', '開始', '印刷',\n",
        "                       '連続', '予約', '多少', '教員', '当局', '材料', '夕刊', '労働', '運送', '電池' ], # consistent, 'high-frequency words\n",
        "    'HF___inconsist': ['反対', '失敗', '作品', '指定', '実験', '決定', '独占', '独身', '固定', '食品',\n",
        "                       '表明', '安定', '各種', '役所', '海岸', '決算', '地帯', '道路', '安打', '楽団' ], # inconsistent, 'high-frequency words\n",
        "    'HF___atypical_': ['仲間', '夫婦', '人間', '神経', '相手', '反発', '化粧', '建物', '彼女', '毛糸', \n",
        "                       '場合', '台風', '夜間', '人形', '東西', '地元', '松原', '競馬', '大幅', '貸家' ], # inconsistent atypical, 'high-frequency words\n",
        "    'LF___consist__': ['集計', '観察', '予告', '動脈', '理学', '信任', '任務', '返信', '医局', '低温', \n",
        "                       '区別', '永続', '持続', '試練', '満開', '軍備', '製材', '銀貨', '急送', '改選' ], # consistent, 'low-frequecy words\n",
        "    'LF___inconsist': ['表紙', '指針', '熱帯', '作詞', '決着', '食費', '古代', '地形', '役場', '品種', \n",
        "                       '祝福', '金銭', '根底', '接種', '経由', '郷土', '街路', '宿直', '曲折', '越境' ], # inconsistent, 'low-frequency words\n",
        "    'LF___atypical_': ['強引', '寿命', '豆腐', '出前', '歌声', '近道', '間口', '風物', '面影', '眼鏡', \n",
        "                       '居所', '献立', '小雨', '毛皮', '鳥居', '仲買', '頭取', '極上', '奉行', '夢路' ], # inconsistent atypical, 'low-frequncy words\n",
        "    'HFNW_consist__': ['集学', '信別', '製信', '運学', '番送', '電続', '完意', '軍開', '動選', '当働', \n",
        "                       '予続', '倉理', '予少', '教池', '理任', '銀務', '連料', '開員', '注全', '記争' ], # consistent, 'high-character-frequency nonwords\n",
        "    'HFNW_inconsist': ['作明', '風行', '失定', '指団', '決所', '各算', '海身', '東発', '楽験', '作代',\n",
        "                       '反原', '独対', '歌上', '反定', '独定', '場家', '安種', '経着', '決土', '松合' ], # inconsistent biased, 'high-character-frequency nonwords\n",
        "    'HFNW_ambiguous': ['表品', '実定', '人風', '神間', '相経', '人元', '小引', '指場', '毛所', '台手',\n",
        "                       '間物', '道品', '出取', '建馬', '大婦', '地打', '化間', '面口', '金由', '彼間' ], # inconsistent ambigous, 'high-character-frequency nonwords\n",
        "    'LFNW_consist__': ['急材', '戦刊', '返計', '印念', '低局', '労号', '満送', '永告', '試脈', '観備',\n",
        "                       '材約', '夕局', '医庫', '任続', '医貨', '改練', '区温', '多始', '材刷', '持察' ], # consistent, 'low-character-frequency nonwords\n",
        "    'LFNW_inconsist': ['食占', '表底', '宿帯', '決帯', '古費', '安敗', '役針', '近命', '眼道', '豆立',\n",
        "                       '街直', '固路', '郷種', '品路', '曲銭', '献居', '奉買', '根境', '役岸', '祝折' ], # inconsistent biased, 'low-character-frequency nonwords\n",
        "    'LFNW_ambiguous': ['食形', '接紙', '競物', '地詞', '強腐', '頭路', '毛西', '夜糸', '仲影', '熱福',\n",
        "                       '寿前', '鳥雨', '地粧', '越種', '仲女', '極鏡', '夢皮', '居声', '貸形', '夫幅' ], # inconsistent ambigous, 'low-character-frequency nonwords\n",
        "}\n",
        "\n",
        "for k, v in fushimi1999.items():\n",
        "    print(colored(k, 'blue', attrs=['bold']), v)\n",
        "\n",
        "fushimi1999_list = []\n",
        "for k, v in fushimi1999.items():\n",
        "    for _v in v:\n",
        "        fushimi1999_list.append(_v)\n",
        "\n",
        "train_wordlist = [v['orig'] for k, v in _vocab.train_data.items()]\n",
        "\n",
        "for i, wrd in enumerate(fushimi1999_list):\n",
        "    if wrd in train_wordlist:\n",
        "        idx = train_wordlist.index(wrd)\n",
        "        print(f'{i:3d} {wrd}:{idx:5d}')\n",
        "        #print(i, wrd, idx, _vocab.train_data[idx])        "
      ],
      "metadata": {
        "id": "0SW8VElBHATs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, w in enumerate(fushimi1999_list):\n",
        "    ids = orth_tkn2ids(w)\n",
        "    tnk = orth_ids2tkn(ids)\n",
        "    if i > 125:\n",
        "        print(i, w, ids, tnk)\n",
        "        o = _get_ids_from_orth(orth_wrd=w)\n",
        "        print(colored((i,w),'blue',attrs=['bold']), o)"
      ],
      "metadata": {
        "id": "t0FDRk9UHJaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テストのための一文字の書記素データセットの作成"
      ],
      "metadata": {
        "id": "8dF3M3soHPHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_chars0 = '０１２３４５６７８９ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ'\n",
        "one_chars1 = 'あいうえおかがきぎくぐけげこごさざしじすずせぜそぞただちぢつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもやゆよらりるれろわをん'\n",
        "one_chars2 = 'アイウエオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモヤユヨラリルレロワヲン'\n",
        "one_chars = one_chars0 + one_chars1 + one_chars2\n",
        "for ch in one_chars:\n",
        "    tmp = _get_ids_from_orth(ch)\n",
        "    __src, __tgt, __yomi = tmp[_src], tmp[_tgt], tmp['_yomi']\n",
        "    print(colored(ch, 'blue', attrs=['bold']), f'__src:{__src}, __tgt:{__tgt}, __yomi:{__yomi}')"
      ],
      "metadata": {
        "id": "ukgmbcx_HOlj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}