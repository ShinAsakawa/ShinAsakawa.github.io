{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP9WphWcT7SNlbTMOh3IauR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_0421s2p.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 意味 s2p, p2s の実装\n",
        "\n",
        "* see Harm&Seidenberg(2004)\n",
        "* and also see doi: https://doi.org/10.1101/2021.04.15.440047, and doi:https://doi.org/10.1101/708156\n",
        "\n",
        "<!-- <img src=\"2004Harm_Seidenberg_fig4c.svg\">\n",
        "<img src=\"2004Harm_Seidenberg_fig4d.svg\"> -->"
      ],
      "metadata": {
        "id": "n052dEJsUVor"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvWM0ffWUTmm"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from IPython import get_ipython\n",
        "isColab =  'google.colab' in str(get_ipython())\n",
        "\n",
        "if isColab:\n",
        "\n",
        "    # termcolor を downgrade しないと colab ではテキストに色がつかない\n",
        "    !pip install --upgrade termcolor==1.1\n",
        "    import termcolor    \n",
        "\n",
        "    # 結果を保存するために Google Drive をマウントする\n",
        "    import google.colab\n",
        "    google.colab.drive.mount('/content/drive/')\n",
        "    \n",
        "    # GPU 情報を表示\n",
        "    !nvidia-smi -L\n",
        "\n",
        "    #!pip install ipynbname --upgrade > /dev/null\n",
        "\n",
        "if isColab:\n",
        "    # colab 上で MeCab を動作させるために，C コンパイラを起動して，MeCab の構築を行う\n",
        "    # そのため時間がかかる。\n",
        "    !apt install aptitude\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "    !pip install mecab-python3==0.7\n",
        "    !pip install jaconv\n",
        "    \n",
        "    import MeCab\n",
        "    mecab_wakati = MeCab.Tagger('-Owakati').parse\n",
        "    mecab_yomi = MeCab.Tagger('-Oyomi').parse\n",
        "    \n",
        "else:\n",
        "    from ccap.mecab_settings import yomi as mecab_yomi\n",
        "    from ccap.mecab_settings import wakati as mecab_wakati\n",
        "\n",
        "\n",
        "# ここから下は，コード実行に関するバージョン情報などの情報源の取得と表示\n",
        "from termcolor import colored\n",
        "\n",
        "import platform\n",
        "HOSTNAME = platform.node().split('.')[0]\n",
        "\n",
        "import os\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "try:\n",
        "    import ipynbname\n",
        "except ImportError:\n",
        "    !pip install ipynbname\n",
        "    import ipynbname\n",
        "FILEPATH = str(ipynbname.path()).replace(HOME+'/','')\n",
        "\n",
        "import pwd\n",
        "USER=pwd.getpwuid(os.geteuid())[0]\n",
        "\n",
        "from datetime import date\n",
        "TODAY=date.today()\n",
        "\n",
        "import torch\n",
        "TORCH_VERSION = torch.__version__\n",
        "\n",
        "color = 'green'\n",
        "print('日付:',colored(f'{TODAY}', color=color, attrs=['bold']))\n",
        "print('HOSTNAME:',colored(f'{HOSTNAME}', color=color, attrs=['bold']))\n",
        "print('ユーザ名:',colored(f'{USER}', color=color, attrs=['bold']))\n",
        "print('HOME:',colored(f'{HOME}', color=color,attrs=['bold']))\n",
        "print('ファイル名:',colored(f'{FILEPATH}', color=color, attrs=['bold']))\n",
        "print('torch.__version__:',colored(f'{TORCH_VERSION}', color=color, attrs=['bold']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b2nRprJYU5-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "# 語彙データは vdrj，意味データは word2vec を使用\n",
        "try:\n",
        "    from ccap import ccap_w2v\n",
        "except ImportError:\n",
        "    !git clone https://github.com/project-ccap/ccap.git\n",
        "    from ccap import ccap_w2v\n",
        "\n",
        "try:\n",
        "    from RAM import VDRJ_Dataset\n",
        "except ImportError:\n",
        "    !git clone https://github.com/ShinAsakawa/RAM.git\n",
        "    from RAM import VDRJ_Dataset\n",
        "vdrj_ds = VDRJ_Dataset(max_words=30000)\n",
        "w2v = ccap_w2v(isColab=False).w2v\n",
        "\n",
        "# vdrj_ds (dataset) から派生させたデータセットを定義\n",
        "vdrj_w2v = vdrj_ds\n",
        "#vdrj_w2v = deepcopy(vdrj_ds)   # deepcopy が動作しないのは何故？\n",
        "for k, v in tqdm(vdrj_ds.data_dict.items()):\n",
        "    lex = v['lexeme']\n",
        "    orth = v['orth']\n",
        "    vdrj_w2v.data_dict[k]['lex_w2v'] = lex in w2v\n",
        "    vdrj_w2v.data_dict[k]['orth_w2v'] = orth in w2v\n"
      ],
      "metadata": {
        "id": "snNKOrCpUZLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#type(w2v)\n",
        "import gensim"
      ],
      "metadata": {
        "id": "iguyJXjGVLSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#help(VDRJ_Dataset)\n",
        "class vdrj_w2v_Dataset(VDRJ_Dataset):\n",
        "    def __init__(self,\n",
        "                 source:str=\"orth\", # ['orth', 'sem', 'phon']\n",
        "                 target:str=\"sem\",  # ['orth', 'sem', 'phone']\n",
        "                 vdrj_ds:VDRJ_Dataset=vdrj_ds,\n",
        "                 w2v:gensim.models.keyedvectors.KeyedVectors=w2v):\n",
        "        super().__init__()\n",
        "        self.w2v = w2v\n",
        "        \n",
        "        self.source = source\n",
        "        self.target = target\n",
        "        \n",
        "        #self.vdrj_ds = vdrj_ds\n",
        "        self.vdrj_ds = vdrj_ds  # vdrj_ds を拡張して新しいデータセット `ds` を作成\n",
        "        #self.vdrj_ds = deepcopy(vdrj_ds)  # vdrj_ds を拡張して新しいデータセット `ds` を作成\n",
        "        data_dict = {}\n",
        "        orth_list = []\n",
        "        for k, v in tqdm(vdrj_ds.data_dict.items()):\n",
        "            lex = v['lexeme']\n",
        "            orth = v['orth']\n",
        "            idx = len(data_dict)\n",
        "            if (orth in w2v):\n",
        "                data_dict[idx] = deepcopy(v)\n",
        "                data_dict[idx]['defined'] = 'orth'\n",
        "                data_dict[idx]['sem'] = w2v[orth]\n",
        "                orth_list.append(orth)\n",
        "            if (lex in w2v):\n",
        "                data_dict[idx] = deepcopy(v)\n",
        "                data_dict[idx]['defined'] = 'lex'\n",
        "                data_dict[idx]['sem'] = w2v[lex]\n",
        "                if not lex in orth_list:\n",
        "                    orth_list.append(lex)\n",
        "                    \n",
        "        self.data_dict = data_dict\n",
        "        self.orth_list = orth_list\n",
        "        #super().set_source_and_target_from_params(source=source, target=target)\n",
        "        self.set_source_and_target_from_params(source=source, target=target)\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data_dict)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if self.source == 'sem':\n",
        "            x = torch.tensor(self.data_dict[idx]['sem'], dtype=torch.float32)\n",
        "        else:\n",
        "            x = self.data_dict[idx][self.source]\n",
        "            #src_ids = self.source_tkn2ids(x) + [self.source_list.index('<EOW>')]\n",
        "            #y = src_ids\n",
        "        \n",
        "        if self.target != 'sem':\n",
        "            y_tkn = self.data_dict[idx][self.target]\n",
        "            y_ids = self.target_tkn2ids(y_tkn)\n",
        "            print(f'type(y_tkn):{type(y_tkn)}, type(y_ids):{type(y_ids)}, y_ids:{y_ids}')\n",
        "            tgt_ids = y_ids + [self.target_list.index('<EOW>')]\n",
        "            y = tgt_ids\n",
        "        else:\n",
        "            y = self.data_dict[idx][self.source]\n",
        "            #src_ids = self.source_tkn2ids(x) + [self.source_list.index('<EOW>')]\n",
        "            #y = torch.tensor(self.data_dict[idx]['sem'], dtype=torch.float32)\n",
        "        return x, y\n",
        "        #return x, tgt_ids\n",
        "            \n",
        "        if self.target == 'sem':\n",
        "            x = self.data_dict[idx][self.source]\n",
        "            #src_ids = self.source_tkn2ids(x) + [self.source_list.index('<EOW>')]\n",
        "            y = torch.tensor(self.data_dict[idx]['sem'], dtype=torch.float32)\n",
        "            return x, y\n",
        "        \n",
        "    def set_source_and_target_from_params(self, source:str='orth', target:str='phon'):\n",
        "        # ソースとターゲットを設定\n",
        "\n",
        "        if source == 'orth':\n",
        "            self.source_list = self.orth_list\n",
        "            self.source_maxlen = self.orth_maxlen\n",
        "            self.source_ids2tkn = self.orth_ids2tkn\n",
        "            self.source_tkn2ids = self.orth_tkn2ids\n",
        "        elif source == 'phon':\n",
        "            self.source_list = self.phon_list\n",
        "            self.source_maxlen = self.phon_maxlen\n",
        "            self.source_ids2tkn = self.phon_ids2tkn\n",
        "            self.source_tkn2ids = self.phon_tkn2ids\n",
        "        elif source == 'sem':\n",
        "            self.source_list = self.orth_list\n",
        "            self.source_max_len = self.w2v.vector_size\n",
        "            self.source_ids2tkn = None\n",
        "            self.source_tkn2ids = None\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        if target == 'orth':\n",
        "            self.target_list = self.orth_list\n",
        "            self.target_maxlen = self.orth_maxlen\n",
        "            self.target_ids2tkn = self.orth_ids2tkn\n",
        "            self.target_tkn2ids = self.orth_tkn2ids\n",
        "        elif target == 'phon':\n",
        "            self.target_list = self.phon_list\n",
        "            self.target_maxlen = self.phon_maxlen\n",
        "            self.target_ids2tkn = self.phon_ids2tkn\n",
        "            self.target_tkn2ids = self.phon_tkn2ids\n",
        "        elif target == 'sem':\n",
        "            self.target_list = self.orth_list\n",
        "            self.target_max_len = self.w2v.vector_size\n",
        "            self.target_ids2tkn = None\n",
        "            self.target_tkn2ids = None\n",
        "        \n",
        "o2s_ds = vdrj_w2v_Dataset(vdrj_ds=vdrj_ds, source='orth', target='sem')\n",
        "o2o_ds = vdrj_w2v_Dataset(vdrj_ds=vdrj_ds, source='orth', target='orth')\n",
        "o2p_ds = vdrj_w2v_Dataset(vdrj_ds=vdrj_ds, source='orth', target='phon')\n",
        "s2o_ds = vdrj_w2v_Dataset(vdrj_ds=vdrj_ds, source='sem', target='orth')\n",
        "s2s_ds = vdrj_w2v_Dataset(vdrj_ds=vdrj_ds, source='sem', target='sem')\n",
        "s2p_ds = vdrj_w2v_Dataset(vdrj_ds=vdrj_ds, source='sem', target='phon')\n",
        "p2o_ds = vdrj_w2v_Dataset(vdrj_ds=vdrj_ds, source='phon', target='orth')\n",
        "p2s_ds = vdrj_w2v_Dataset(vdrj_ds=vdrj_ds, source='phon', target='sem')\n",
        "p2p_ds = vdrj_w2v_Dataset(vdrj_ds=vdrj_ds, source='phon', target='phon')"
      ],
      "metadata": {
        "id": "XgTN5QtuXDBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ds in [o2s_ds, o2o_ds, o2p_ds, s2o_ds, s2s_ds, s2p_ds, p2o_ds, p2s_ds, p2p_ds]:\n",
        "    print(ds.__len__(), str(o2s_ds))\n",
        "    print(ds.target)"
      ],
      "metadata": {
        "id": "2cpcGerIXJ_A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}