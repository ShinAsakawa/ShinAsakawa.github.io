{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_0804vit_example_Vision_Transformers_versus_Convolutional_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/RustamyF/vision-transformer/blob/master/vit_example.ipynb)\n"
      ],
      "metadata": {
        "id": "6ORCnWA_30f2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- source: `https://github.com/RustamyF/vision-transformer/blob/master/vit_example.ipynb`\n",
        "- colab_source:  `https://colab.research.google.com/github/RustamyF/vision-transformer/blob/master/vit_example.ipynb`\n",
        "- blog: `https://medium.com/@faheemrustamy/vision-transformers-vs-convolutional-neural-networks-5fe8f9e18efc`\n",
        "\n",
        "# Vision Transformers vs. Convolutional Neural Networks\n",
        "\n",
        "Fahim Rustamy, PhD\n",
        "\n",
        "このブログ記事は google の研究チームによる [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929.pdf) 論文に触発された。\n",
        "この論文では，画像分類課題に，画像パッチに直接適用される純粋な Transformer の使用を提案している。\n",
        "Vision Transformer (ViT) は，大量のデータで事前に訓練された後，訓練に必要な計算リソースが少ない一方で，複数のベンチマークにおいて最先端の畳み込みネットワークを凌駕する。\n",
        "<!-- This blog post is inspired by the paper titled [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929.pdf) from google’s research team.\n",
        "The paper proposes using a pure Transformer applied directly to image patches for image classification tasks.\n",
        "The Vision Transformer (ViT) outperforms state-of-the-art convolutional networks in multiple benchmarks while requiring fewer computational resources to train, after being pre-trained on large amounts of data.-->\n",
        "\n",
        "Transformer は，その計算効率とスケーラビリティにより，自然言語処理において選択されるモデルとなっている。\n",
        "コンピュータビジョンでは，畳み込みニューラルネットワーク (CNN) アーキテクチャが依然として主流であるが，CNN と自己注意を組み合わせることを試みた研究者もいる。\n",
        "著者らは，標準的な Transformerを画像に直接適用する実験を行い，中規模のデータセットで訓練した場合，ResNetのようなアーキテクチャと比較して，モデルの精度が控えめであることを発見した。\n",
        "しかし，より大きなデータセットで訓練した場合，Vision Transformer (ViT) は優れた結果を達成し，複数の画像認識ベンチマークにおいて，最先端の技術に近づいたり，凌駕したりした。\n",
        "<!--Transformers have become the model of choice in NLP due to their computational efficiency and scalability.\n",
        "In computer vision, convolutional neural network (CNN) architectures remain dominant, but some researchers have tried combining CNNs with self-attention.\n",
        "The authors experimented with applying a standard Transformer directly to images and found that when trained on mid-sized datasets, the models had modest accuracy compared to ResNet-like architectures.\n",
        "However, when trained on larger datasets, the Vision Transformer (ViT) achieved excellent results and approached or surpassed the state of the art on multiple image recognition benchmarks. -->\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*brmcPLvJpiQWjZpY\">\n",
        "\n",
        "図 1 (原著論文より引用) は，2D 画像を平坦化された 2D パッチの系列に変換することで処理するモデルを記述している。\n",
        "パッチは次に，学習可能な線形射影を用いて一定の潜在ベクトルサイズに写像される。\n",
        "学習可能な埋め込みがパッチの系列に付加され，Transformer 符号化器の出力におけるその状態が画像表現となる。\n",
        "この画像表現は，事前学習または微調整のために分類ヘッドに渡される。\n",
        "位置情報を保持するために位置埋め込みが追加され，埋め込みベクトルの系列が Transformer 符号化器の入力となる。\n",
        "<!-- Figure 1 (taken from the original paper) describes a model that processes 2D images by transforming them into sequences of flattened 2D patches.\n",
        "The patches are then mapped to a constant latent vector size with a trainable linear projection.\n",
        "A learnable embedding is prepended to the sequence of patches and its state at the output of the Transformer encoder serves as the image representation.\n",
        "The image representation is then passed through a classification head for either pre-training or fine-tuning.\n",
        "Position embeddings are added to retain positional information and the sequence of embedding vectors serves as input to the Transformer encoder, which consists of alternating layers of multiheaded self-attention and MLP blocks. -->\n",
        "\n",
        "過去，CNN は長い間，画像処理課題の有力な選択肢であった。\n",
        "CNN は畳み込み層を通して局所的な空間パターンを捉えることに優れており，階層的な特徴抽出を可能にする。\n",
        "CNN は大量の画像データから学習することに長けており，画像分類，物体検出，切り分けなどの課題で目覚ましい成功を収めている。\n",
        "<!-- In the past, CNNs have been the go-to choice for image processing tasks for a long time.\n",
        "They excel at capturing local spatial patterns through convolutional layers, enabling hierarchical feature extraction.\n",
        "CNNs are adept at learning from large amounts of image data and have achieved remarkable success in tasks like image classification, object detection, and segmentation.-->\n",
        "\n",
        "CNN は様々なコンピュータビジョン課題で実績があり，大規模なデータセットを効率的に扱うが，Vision Transformer は大域的な依存関係や文脈の理解が重要なシナリオで優位性を発揮する。\n",
        "しかし，Vision Transformer は通常，CNN と同等の性能を達成するために，より大量の学習データを必要とする。\n",
        "また，CNN は並列処理が可能なため計算効率が高く，実時間でリソースに制約のある応用ではより実用的である。\n",
        "<!-- While CNNs have a proven track record in various computer vision tasks and handle large-scale datasets efficiently, Vision Transformers offer advantages in scenarios where global dependencies and contextual understanding are crucial.\n",
        "However, Vision Transformers typically require larger amounts of training data to achieve comparable performance to CNNs.\n",
        "Also, CNNs are computationally efficient due to their parallelizable nature, making them more practical for real-time and resource-constrained applications. -->\n",
        "\n",
        "### 例 CNN と視覚 Transformer の比較 <!-- ### Example: CNN vs. Vision Transformer-->\n",
        "\n",
        "本節では，Kaggle で公開されている cats and dogs データセットに対して，CNN と vision Transformer の両方のアプローチを使って視覚分類器を学習する。\n",
        "まず，Kaggle から 25000 枚の RGB 画像を含む cats and dogs データセットをダウンロードする。\n",
        "まだの方は，こちらの説明を読んで，Kaggle API クレデンシャルの設定方法を学んで欲しい。\n",
        "以下の Python コードはデータセットを現在の作業ディレクトリにダウンロードする。\n",
        "<!-- In this section, we will train a vision classifier on the cats and dogs dataset available in Kaggle, using both CNN and vision transformer approaches.\n",
        "First, we will download the cats and dogs dataset from Kaggle with 25000 RGB images.\n",
        "If you haven’t already, you can read the instructions here to learn how to get your Kaggle API credential set up.\n",
        "The following Python code will download the dataset into your current working directory. -->"
      ],
      "metadata": {
        "id": "fwlUo5S6Lymj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "\n",
        "#api_token = {\"username\":\"username\",\"key\":\"api-key\"}\n",
        "api_token = {\"username\":\"turingcomplete\",\"key\":\"a49cdd9a6452346d9fdacca035bde21a\"}\n",
        "\n",
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "5_XXQqnFKrPw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "NM_jcZ0myOQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle einops"
      ],
      "metadata": {
        "id": "5QWVOKRbxwkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "# we write to the current directory with './'\n",
        "api.dataset_download_files('karakaggle/kaggle-cat-vs-dog-dataset', path='./')"
      ],
      "metadata": {
        "id": "w5qnikB9xu3v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq kaggle-cat-vs-dog-dataset.zip\n",
        "!rm -r kaggle-cat-vs-dog-dataset.zip"
      ],
      "metadata": {
        "id": "BVokL2zoyGn3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8VbaIvlyHVB"
      },
      "outputs": [],
      "source": [
        "# !wget --no-check-certificate \\\n",
        "#     https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "#     -O pets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/RustamyF/vision-transformer.git\n",
        "!mv vision-transformer/vision_tr ."
      ],
      "metadata": {
        "id": "AfnUeUsMibys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip pets\n",
        "# !rm pets\n",
        "# !mv -v cats_and_dogs_filtered/validation/dogs/* cats_and_dogs_filtered/train/dogs/\n",
        "# !mv -v cats_and_dogs_filtered/validation/cats/* cats_and_dogs_filtered/train/cats/"
      ],
      "metadata": {
        "id": "q9AFhZQKkfI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vision_tr.simple_vit import Transformer"
      ],
      "metadata": {
        "id": "KrYTcGVxjYE_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN アプローチ <!-- ### CNN Approach-->\n",
        "\n",
        "\n",
        "この画像分類器の CNN モデルは、カーネル・サイズ 3，ストライド 2，最大プーリング層  2 の 3 層の 2 次元畳み込みで構成される。\n",
        "畳み込み層に続いて、2 つの完全連結層があり，それぞれ 10 ノードで構成される。\n",
        "以下はこの構造を示すコード・スニペットである：\n",
        "<!-- The CNN model for this image classifier consists of three layers of 2D convolutions, with a kernel size of 3, stride of 2, and a max pooling layer of 2.\n",
        "Following the convolution layers, there are two fully connected layers, each composed of 10 nodes.\n",
        "Here is a code snippet that illustrates this structure: -->"
      ],
      "metadata": {
        "id": "6DgcZcdhQC9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練は Tesla T4  (g4dn-xlarge) GPU マシンで 10 訓練エポック行った。\n",
        "Jupyter notebook はプロジェクトの GitHub リポジトリで公開されており，訓練ループのコードが含まれている。\n",
        "以下は，各エポックのトレーニングループの結果である。\n",
        "<!-- The training was performed with a Tesla T4 (g4dn-xlarge) GPU machine for 10 training epochs.\n",
        "The Jupyter Notebook is available in the project’s GitHub repository and contains the code for the training loop.\n",
        "The following are the results of training loops for each epoch. -->\n"
      ],
      "metadata": {
        "id": "SzmqFrwlQZlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "class LoadData:\n",
        "    def __init__(self):\n",
        "        self.cat_path = 'kagglecatsanddogs_3367a/PetImages/Cat'\n",
        "        self.dog_path = 'kagglecatsanddogs_3367a/PetImages/Dog'\n",
        "\n",
        "    def delete_non_jpeg_files(self, directory):\n",
        "        for filename in os.listdir(directory):\n",
        "            if not filename.endswith('.jpg') and not filename.endswith('.jpeg'):\n",
        "                file_path = os.path.join(directory, filename)\n",
        "                try:\n",
        "                    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                        os.unlink(file_path)\n",
        "                    elif os.path.isdir(file_path):\n",
        "                        shutil.rmtree(file_path)\n",
        "                    print('deleted', file_path)\n",
        "                except Exception as e:\n",
        "                    print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "    def data(self):\n",
        "        self.delete_non_jpeg_files(self.dog_path)\n",
        "        self.delete_non_jpeg_files(self.cat_path)\n",
        "\n",
        "        dog_list = os.listdir(self.dog_path)\n",
        "        dog_list = [(os.path.join(self.dog_path, i), 1) for i in dog_list]\n",
        "\n",
        "        cat_list = os.listdir(self.cat_path)\n",
        "        cat_list = [(os.path.join(self.cat_path, i), 0) for i in cat_list]\n",
        "\n",
        "        total_list = cat_list + dog_list\n",
        "\n",
        "        train_list, test_list = train_test_split(total_list, test_size=0.2)\n",
        "        train_list, val_list = train_test_split(train_list, test_size=0.2)\n",
        "        print('train list', len(train_list))\n",
        "        print('test list', len(test_list))\n",
        "        print('val list', len(val_list))\n",
        "        return train_list, test_list, val_list\n",
        "\n",
        "\n",
        "# data Augumentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "class dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    # dataset length\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    # load an one of images\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.file_list[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img_transformed = self.transform(img)\n",
        "        return img_transformed, label\n",
        "\n",
        "\n",
        "class Cnn(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Cnn, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=0, stride=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=0, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=0, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(3 * 3 * 64, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(10, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    lr = 0.001  # learning_rate\n",
        "    batch_size = 800  # we will use mini-batch method\n",
        "    epochs = 10  # How much to train a model\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    torch.manual_seed(1234)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.manual_seed_all(1234)\n",
        "\n",
        "    print(device)\n",
        "\n",
        "    load_data = LoadData()\n",
        "\n",
        "    train_list, test_list, val_list = load_data.data()\n",
        "\n",
        "    train_data = dataset(train_list, transform=transform)\n",
        "    test_data = dataset(test_list, transform=transform)\n",
        "    val_data = dataset(val_list, transform=transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = Cnn().to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.Adam(params=model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_accuracy = 0\n",
        "\n",
        "        for data, label in train_loader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            acc = ((output.argmax(dim=1) == label).float().mean())\n",
        "            epoch_accuracy += acc / len(train_loader)\n",
        "            epoch_loss += loss / len(train_loader)\n",
        "\n",
        "        print(f'Epoch : {epoch + 1:2d},',\n",
        "              f'train accuracy : {epoch_accuracy:.3f},',\n",
        "              f'train loss : {epoch_loss:.3f}')\n",
        "        #print('Epoch : {}, train accuracy : {}, train loss : {}'.format(epoch + 1, epoch_accuracy, epoch_loss))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            epoch_val_accuracy = 0\n",
        "            epoch_val_loss = 0\n",
        "            for data, label in val_loader:\n",
        "                data = data.to(device)\n",
        "                label = label.to(device)\n",
        "\n",
        "                val_output = model(data)\n",
        "                val_loss = criterion(val_output, label)\n",
        "\n",
        "                acc = ((val_output.argmax(dim=1) == label).float().mean())\n",
        "                epoch_val_accuracy += acc / len(val_loader)\n",
        "                epoch_val_loss += val_loss / len(val_loader)\n",
        "\n",
        "            print(f'Epoch : {epoch + 1:2d},',\n",
        "                  f'val_accuracy : {epoch_val_accuracy:.3f},',\n",
        "                  f'val_loss : {epoch_val_loss:.3f}')\n",
        "            #print('Epoch : {}, val_accuracy : {}, val_loss : {}'.format(epoch + 1, epoch_val_accuracy, epoch_val_loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TChHpDa1jtRD",
        "outputId": "00bc7d3a-dd97-490e-d3f9-322fa3c9cddb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "train list 15973\n",
            "test list 4992\n",
            "val list 3994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:858: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch :  1, train accuracy : 0.586, train loss : 0.676\n",
            "Epoch :  1, val_accuracy : 0.625, val_loss : 0.654\n",
            "Epoch :  2, train accuracy : 0.633, train loss : 0.647\n",
            "Epoch :  2, val_accuracy : 0.637, val_loss : 0.640\n",
            "Epoch :  3, train accuracy : 0.666, train loss : 0.624\n",
            "Epoch :  3, val_accuracy : 0.672, val_loss : 0.608\n",
            "Epoch :  4, train accuracy : 0.688, train loss : 0.587\n",
            "Epoch :  4, val_accuracy : 0.701, val_loss : 0.573\n",
            "Epoch :  5, train accuracy : 0.713, train loss : 0.560\n",
            "Epoch :  5, val_accuracy : 0.724, val_loss : 0.549\n",
            "Epoch :  6, train accuracy : 0.725, train loss : 0.540\n",
            "Epoch :  6, val_accuracy : 0.720, val_loss : 0.537\n",
            "Epoch :  7, train accuracy : 0.729, train loss : 0.534\n",
            "Epoch :  7, val_accuracy : 0.732, val_loss : 0.534\n",
            "Epoch :  8, train accuracy : 0.742, train loss : 0.517\n",
            "Epoch :  8, val_accuracy : 0.740, val_loss : 0.528\n",
            "Epoch :  9, train accuracy : 0.748, train loss : 0.507\n",
            "Epoch :  9, val_accuracy : 0.752, val_loss : 0.519\n",
            "Epoch : 10, train accuracy : 0.750, train loss : 0.505\n",
            "Epoch : 10, val_accuracy : 0.737, val_loss : 0.522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vision Transformer Approach\n",
        "\n",
        "The Vision Transformer architecture is designed with customizable dimensions that can be adjusted according to specific requirements.\n",
        "For this size of image dataset, this architecture is still big.\n",
        "\n",
        "```python\n",
        "from vision_tr.simple_vit import ViT\n",
        "model = ViT(\n",
        "    image_size=224,\n",
        "    patch_size=32,\n",
        "    num_classes=2,\n",
        "    dim=128,\n",
        "    depth=12,\n",
        "    heads=8,\n",
        "    mlp_dim=1024,\n",
        "    dropout=0.1,\n",
        "    emb_dropout=0.1,\n",
        ").to(device)\n",
        "```\n",
        "\n",
        "Each parameter in the vision transformer plays a key role and is described here:\n",
        "\n",
        "* `image_size=224`: This parameter specifies the desired size (width and height) of the input images to the model. In this case, the images are expected to be of size 224x224 pixels.\n",
        "* `patch_size=32`: The images are divided into smaller patches, and this parameter defines the size (width and height) of each patch. In this case, each patch is 32x32 pixels.\n",
        "* `num_classes=2`: This parameter indicates the number of classes in the classification task. In this example, the model is designed to classify inputs into two classes (cats and dogs).\n",
        "* `dim=128`: It specifies the dimensionality of the embedding vectors in the model. The embeddings capture the representation of each image patch.\n",
        "* `depth=12`: This parameter defines the depth or number of layers in the Vision Transformer model (encoder model). A higher depth allows for more complex feature extraction.\n",
        "* `heads=8`: This parameter represents the number of attention heads in the self-attention mechanism of the model.\n",
        "* `mlp_dim=1024`: It specifies the dimensionality of the Multi-Layer Perceptron (MLP) hidden layers in the model. The MLP is responsible for transforming the token representations after self-attention.\n",
        "* `dropout=0.1`: This parameter controls the dropout rate, which is a regularization technique used to prevent overfitting. It randomly sets a fraction of input units to 0 during training.\n",
        "* `emb_dropout=0.1`: It defines the dropout rate specifically applied to the token embeddings. This dropout helps prevent over-reliance on specific tokens during training.\n",
        "\n",
        "The training of the vision transformer for the classification task was performed with the Tesla T4 (g4dn-xlarge) GPU machine for 20 training epochs.\n",
        "The training was conducted for 20 epochs (instead of 10 epochs used for CNN) because the training loss’s convergence was slow. The following are the results of training loops for each epoch.\n",
        "\n",
        "The CNN approach reached 75% accuracy in 10 epochs, while the vision transformer model reached 69% accuracy and took significantly longer to train.\n",
        "\n",
        "### 結論 <!-- ### Conclusion-->\n",
        "\n",
        "結論として，CNN モデルと Vision Transformer モデルを比較すると，モデルサイズ，メモリ要件，精度，性能の点で顕著な違いがある。\n",
        "CNN モデルは伝統的に，そのコンパクトなサイズと効率的なメモリ利用で知られており，リソースに制約のある環境に適している。\n",
        "画像処理課題において非常に効果的であることが証明されており，様々なコンピュータビジョン応用において優れた精度を示している。\n",
        "一方，Vision Transformers は，画像の大域的な依存関係や文脈的な理解を捉えるための強力なアプローチを提供し，特定の課題における性能向上をもたらす。\n",
        "しかし，Vision Transformer は CNN に比べてモデルサイズが大きく，メモリ要件が高い傾向がある。\n",
        "特に大規模なデータセットを扱う場合，素晴らしい精度を達成できるかもしれないが，計算上の要求が，リソースが限られたシナリオでの実用性を制限する可能性がある。\n",
        "最終的に，CNN モデルと Vision Transformer モデルのどちらを選択するかは，利用可能なリソース，データセットサイズ，モデルの複雑さ，精度，性能のトレードオフなどの要因を考慮し，目の前の課題の特定の要件に依存する。\n",
        "コンピュータビジョンの分野が進化し続けるにつれて，両アーキテクチャのさらなる進歩が期待され，研究者や実務家が特定のニーズや制約に基づいて，より多くの情報に基づいた選択を行うことができるようになる。\n",
        "<!--In conclusion, when comparing CNN and Vision Transformer models, there are notable differences in terms of model size, memory requirements, accuracy, and performance.\n",
        "CNN models are traditionally known for their compact size and efficient memory utilization, making them suitable for resource-constrained environments.\n",
        "They have proven to be highly effective in image processing tasks and exhibit excellent accuracy in various computer vision applications.\n",
        "On the other hand, Vision Transformers offer a powerful approach to capture global dependencies and contextual understanding in images, resulting in improved performance in certain tasks. However, Vision Transformers tend to have larger model sizes and higher memory requirements compared to CNNs. While they may achieve impressive accuracy, especially when dealing with larger datasets, the computational demands can limit their practicality in scenarios with limited resources. Ultimately, the choice between CNN and Vision Transformer models depends on the specific requirements of the task at hand, considering factors such as available resources, dataset size, and the trade-off between model complexity, accuracy, and performance.\n",
        "As the field of computer vision continues to evolve, further advancements in both architectures are expected, enabling researchers and practitioners to make more informed choices based on their specific needs and constraints. -->\n",
        "\n"
      ],
      "metadata": {
        "id": "lXhPehSbRcmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from vision_tr.simple_vit import ViT\n",
        "# from vit_pytorch.efficient import ViT\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "\n",
        "# from linformer import Linformer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "# from vit_pytorch.efficient import ViT\n",
        "\n",
        "\n",
        "class LoadData:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.cat_path = 'kagglecatsanddogs_3367a/PetImages/Cat'\n",
        "        self.dog_path = 'kagglecatsanddogs_3367a/PetImages/Dog'\n",
        "\n",
        "    def delete_non_jpeg_files(self, directory):\n",
        "        for filename in os.listdir(directory):\n",
        "            if not filename.endswith('.jpg') and not filename.endswith('.jpeg'):\n",
        "                file_path = os.path.join(directory, filename)\n",
        "                try:\n",
        "                    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                        os.unlink(file_path)\n",
        "                    elif os.path.isdir(file_path):\n",
        "                        shutil.rmtree(file_path)\n",
        "                    print('deleted', file_path)\n",
        "                except Exception as e:\n",
        "                    print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "    def data(self):\n",
        "        self.delete_non_jpeg_files(self.dog_path)\n",
        "        self.delete_non_jpeg_files(self.cat_path)\n",
        "\n",
        "        dog_list = os.listdir(self.dog_path)\n",
        "        dog_list = [(os.path.join(self.dog_path, i), 1) for i in dog_list]\n",
        "\n",
        "        cat_list = os.listdir(self.cat_path)\n",
        "        cat_list = [(os.path.join(self.cat_path, i), 0) for i in cat_list]\n",
        "\n",
        "        total_list = cat_list + dog_list\n",
        "\n",
        "        train_list, test_list = train_test_split(total_list, test_size=0.2)\n",
        "        train_list, val_list = train_test_split(train_list, test_size=0.2)\n",
        "        print('train list', len(train_list))\n",
        "        print('test list', len(test_list))\n",
        "        print('val list', len(val_list))\n",
        "        return train_list, test_list, val_list\n",
        "\n",
        "\n",
        "# data Augumentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "class dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    # dataset length\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    # load an one of images\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.file_list[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img_transformed = self.transform(img)\n",
        "        return img_transformed, label\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Training settings\n",
        "    batch_size = 64\n",
        "    epochs = 20\n",
        "    lr = 3e-5\n",
        "    gamma = 0.7\n",
        "    seed = 42\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    torch.manual_seed(1234)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.manual_seed_all(1234)\n",
        "\n",
        "    print(device)\n",
        "\n",
        "    load_data = LoadData()\n",
        "\n",
        "    train_list, test_list, val_list = load_data.data()\n",
        "\n",
        "    train_data = dataset(train_list, transform=transform)\n",
        "    test_data = dataset(test_list, transform=transform)\n",
        "    val_data = dataset(val_list, transform=transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
        "    model = ViT(\n",
        "        image_size=224,\n",
        "        patch_size=32,\n",
        "        num_classes=2,\n",
        "        dim=128,\n",
        "        depth=12,\n",
        "        heads=8,\n",
        "        mlp_dim=1024,\n",
        "        dropout=0.1,\n",
        "        emb_dropout=0.1,\n",
        "    ).to(device)\n",
        "\n",
        "    # loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    # scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "\n",
        "    epochs = 20\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_accuracy = 0\n",
        "\n",
        "        for data, label in train_loader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            acc = ((output.argmax(dim=1) == label).float().mean())\n",
        "            epoch_accuracy += acc / len(train_loader)\n",
        "            epoch_loss += loss / len(train_loader)\n",
        "\n",
        "        print('Epoch : {}, train accuracy : {}, train loss : {}'.format(epoch + 1, epoch_accuracy, epoch_loss))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            epoch_val_accuracy = 0\n",
        "            epoch_val_loss = 0\n",
        "            for data, label in val_loader:\n",
        "                data = data.to(device)\n",
        "                label = label.to(device)\n",
        "\n",
        "                val_output = model(data)\n",
        "                val_loss = criterion(val_output, label)\n",
        "\n",
        "                acc = ((val_output.argmax(dim=1) == label).float().mean())\n",
        "                epoch_val_accuracy += acc / len(val_loader)\n",
        "                epoch_val_loss += val_loss / len(val_loader)\n",
        "\n",
        "            print('Epoch : {}, val_accuracy : {}, val_loss : {}'.format(epoch + 1, epoch_val_accuracy, epoch_val_loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3CfkEoQjwfr",
        "outputId": "8ad8f0dd-d016-42e7-ce0c-232f5084682b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "train list 15973\n",
            "test list 4992\n",
            "val list 3994\n",
            "Epoch : 1, train accuracy : 0.5329695343971252, train loss : 0.6937241554260254\n",
            "Epoch : 1, val_accuracy : 0.5617369413375854, val_loss : 0.6789079904556274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2gNOGSRX5N5l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}