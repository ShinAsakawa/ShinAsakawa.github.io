{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPJ7FbQpWXZOG8qN0bd8pa9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2022_0123ccap_lam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* date: 2023_0123\n",
        "* fname: 2022_0123ccap_lam.ipynb\n",
        "\n",
        "\n",
        "* 一文字の orth2phon を担保したいために，全角の数字，アルファベット，ひらがな，計 109 文字をデータ先頭に追加した。\n",
        "* Fushimi1999 (Psyc. Rev.) の語彙リストを fushimi1999_list として収録\n",
        "* Fushimi1999_list の扱いに伴い訓練語彙数を 10K から 20K に増加\n",
        "* 学習率 lr は 0.001 だと収束しない。0.0001 であれば良好であり，訓練損失 0.01 程度，訓練精度 0.987 程度までに至る。\n",
        "* ただし，一文字データセット onechar_dataset では lr=0.001 の方が収束が早い。\n",
        "これは，データセットサイズが 20K と 0.1K と 20 倍の差があるためであろう。\n",
        "* 近藤先生が，GPU 上で実行してくださった訓練済モデルのファイル名が `decoder256new.pt` と `encoder256new.pt` である。\n",
        "これは，中間層ユニット数が 256 である orth2phon モデルの訓練済モデルである。\n",
        "* `_train()` 関数内で，正解判定をする際に，GPU から CPU へ転送しなければいけないことを忘れていたので修正した。\n",
        "具体的には， `detach()` と `numpy()` の間に `cpu()` を挿入した。2 箇所\n",
        "```python\n",
        "    ok_flag = (ok_flag) and (decoder_output.argmax() == target_tensor[di].detach().cpu().numpy()[0])\n",
        "```\n",
        "\n",
        "近藤先生の実験によれば，結果は以下の通りである(そうだ)。\n",
        "\n",
        "正答率\n",
        "\n",
        "|   | 条件 | 記述         | 正解率 | \n",
        "|:----|:-----|:------------|:------|\n",
        "|WORD |   HF |1:consistent |　18/20\n",
        "|WORD |   HF |2:typical    |   HF___inconsist  16/20|\n",
        "|WORD |   HF |3:atypical   |   HF___atypical_  8/20 |\n",
        "|WORD |   LF |1:consistent |   LF___consist__  14/20|\n",
        "|WORD |   LF |2:typical    |   LF___inconsist  9/20|\n",
        "|WORD |   LF |3:atypical   |   LF___atypical_  3/20|\n",
        "\n",
        "* 伏見らではでなかったatypical効果だけでなく，\n",
        "　consistent-typicalの差もある程度ある気がします\n",
        " また，LFでも効果ありであり，かつ，頻度効果もあり\n",
        "* **今回，L(legitimate alternative reading of components） マークを付けてみました**\n",
        "  Lm, Lnは，モーラ間違い，一文字間違いと混合\n",
        "\n",
        "\n",
        "アクセプト率\n",
        "\n",
        "|     | 条件 | 記述         | 正解率 | \n",
        "|:----|:----|:------------|:------|\n",
        "|非単語| HF  | 1:consistent|HFNW_consist__  17/20|\n",
        "|非単語| HF  | 2:typical   |HFNW_inconsist　　17/20|\n",
        "|非単語| HF  | 3:ambiguous |HFNW_ambiguous  13/20|\n",
        "|非単語| LF  | 1:consistent|LFNW_consist__  15/20|\n",
        "|非単語| LF  | 2:typical   |LFNW_inconsist  13/20|\n",
        "|非単語| LF  | 3:ambiguous |LFNW_ambiguous  7/20|\n",
        "\n",
        "* かなり読めますね．アクセプトは，どんな読みでもいいので読めそうな読み方ならOKにしています．\n",
        "　単語の L と同じになります．\n",
        "* **結構驚きは，非単語のときに連濁や促音化ができているところ**\n",
        "\n",
        "* 2023_0116 関係者にわかりやうように，コメントを多用して，問題点を共有するように務めること"
      ],
      "metadata": {
        "id": "ZjnXViNeh-s1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9CGP0GGh8qn"
      },
      "outputs": [],
      "source": [
        "# ここはお遊びなので，スキップしても良い\n",
        "#import IPython\n",
        "#IPython.display.Image(url=\"https://livedoor.blogimg.jp/ftb001/imgs/b/4/b4629a79.jpg\")\n",
        "#IPython.display.Image(url=\"https://uy-allstars.com/_assets/images/pages/char/detail/webp/lum@pc.webp\")\n",
        "\n",
        "import os\n",
        "lum_img_fname = 'lum@pc.webp'\n",
        "if not os.path.exists(lum_img_fname):\n",
        "    !wget \"https://uy-allstars.com/_assets/images/pages/char/detail/webp/lum@pc.webp\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "x = plt.imread('lum@pc.webp')\n",
        "plt.figure(figsize=(4,8))\n",
        "plt.imshow(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 準備作業\n"
      ],
      "metadata": {
        "id": "Uz-jkdnKiVw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 ライブラリのインポート"
      ],
      "metadata": {
        "id": "tHr0RtFwiXY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "try:\n",
        "    import bit\n",
        "except ImportError:\n",
        "    !pip install ipynbname --upgrade > /dev/null 2>&1 \n",
        "    !git clone https://github.com/ShinAsakawa/bit.git 2>&1\n",
        "    \n",
        "    # `import bit` する前に termcolor を downgrade しないと colab では色付きテキスト不能\n",
        "    !pip install --upgrade termcolor==2.0 2>&1  \n",
        "    import termcolor    \n",
        "    import bit\n",
        "\n",
        "isColab = bit.isColab\n",
        "HOME = bit.HOME\n",
        "\n",
        "if isColab:\n",
        "    # colab 上で MeCab を動作させるために，C コンパイラを起動して，MeCab の構築を行う\n",
        "    # そのため時間がかかる。\n",
        "    !apt install aptitude\n",
        "    !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "    !pip install mecab-python3==0.7\n",
        "    !pip install jaconv\n",
        "    \n",
        "    import MeCab\n",
        "    wakati = MeCab.Tagger('-Owakati').parse\n",
        "    yomi = MeCab.Tagger('-Oyomi').parse\n",
        "    \n",
        "else:\n",
        "    from ccap.mecab_settings import yomi\n",
        "    from ccap.mecab_settings import wakati\n",
        "\n",
        "# 自作ライブラリ LAM の読み込み\n",
        "if isColab:\n",
        "    !git clone https://github.com/ShinAsakawa/ccap.git\n",
        "    !git clone https://github.com/ShinAsakawa/lam.git\n",
        "\n",
        "if isColab:\n",
        "    # colab 上で termcolor の色制御が動作しないので，バージョンを下げる必要がある\n",
        "    !pip install --upgrade termcolor==2.0.1 2>&1\n",
        "    \n",
        "    !pip install jupyter_contrib_nbextensions 2>&1 \n",
        "    !jupyter nbextension enable codefolding/main 2>&1"
      ],
      "metadata": {
        "id": "ni182CeIiS5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 パラメータ設定\n",
        "\n",
        "語彙数を 10K 語から 20K 語に倍増しているのは，Fushimi1999 の語彙リストの未知語が存在したためである。"
      ],
      "metadata": {
        "id": "99MVIY5Siwl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import lam\n",
        "#device = lam.device  # CPU or GPU の選択\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "# シミュレーションに必要なパラメータの設定\n",
        "params = {\n",
        "    'traindata_size':  20000,   # 訓練データ数，NTT 日本語語彙特性の高頻度語を上位から抽出\n",
        "    #'traindata_size': 301612,  # 訓練データ数，NTT 日本語語彙特性の高頻度語を上位から抽出\n",
        "    'epochs': 30,               # 学習のためのエポック数\n",
        "    'hidden_size': 64,          # 中間層のニューロン数\n",
        "    #'hidden_size': 128,         # 中間層のニューロン数\n",
        "    'random_seed': 42,          # 乱数の種。ダグラス・アダムス著「銀河ヒッチハイカーズガイド」\n",
        "\n",
        "    # 以下 `source` と `target` を定義することで，別の課題を実行可能\n",
        "    'source': 'orth',          # ['orth', 'phon', 'mora', 'mora_p', 'mora_p_r']\n",
        "    'target': 'phon',          # ['orth', 'phon', 'mora', 'mora_p', 'mora_p_r']\n",
        "    #'target': 'mora_p_r',     # ['orth', 'phon', 'mora', 'mora_p', 'mora_p_r']\n",
        "    # 'orth': 書記素, \n",
        "    # 'phon': 音韻, \n",
        "    # 'mora': モーラ\n",
        "    # 'mora_p': モーラを silius による音分解\n",
        "    # 'mora_p_r': モーラの silius 音分解の逆\n",
        "    'pretrained': False,          # True であれば訓練済ファイルを読み込む\n",
        "    #'pretrained': True,          # True であれば訓練済ファイルを読み込む\n",
        "    #'isTrain'   : True,          # True であれば学習する\n",
        "    \n",
        "    # 学習済のモデルパラメータを保存するファイル名\n",
        "    'path_saved': '2023_0120lam_o2p_hid64_nttfreq20k.pt', \n",
        "    #'path_saved': '2022_0829lam_p2p_hid24_vocab10k.pt',\n",
        "    #'path_saved': False,                      # 保存しない場合\n",
        "    \n",
        "    # 結果の散布図を保存するファイル名    \n",
        "    'path_graph': '2023_0120lam_p2p_hid64_nttfreq20k.pdf',\n",
        "    #'path_graph': False,                     # 保存しない場合\n",
        "\n",
        "    'lr': 0.0001,                              # 学習率\n",
        "    'dropout_p': 0.0,                         # ドロップアウト率\n",
        "    'teacher_forcing_ratio': 0.5,             # 教師強制を行う確率\n",
        "    'optim_func': torch.optim.Adam,           # 最適化アルゴリズム ['torch.optim.Adam', 'torch.optim.SGD', 'torch.optim.AdamW']\n",
        "    'loss_func' :torch.nn.CrossEntropyLoss(), # 交差エントロピー損失 ['torch.nn.NLLLoss()', or 'torch.nn.CrossEntropyLoss()']\n",
        "}\n",
        "\n",
        "source = params['source']\n",
        "target = params['target']\n",
        "_src, _tgt = source+'_ids', target+'_ids'"
      ],
      "metadata": {
        "id": "42F0wuFgis8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Fushimi1999 データセット"
      ],
      "metadata": {
        "id": "uNT_SU68i9tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "import sys\n",
        "\n",
        "verbose = False\n",
        "\n",
        "fushimi1999 = {\n",
        "    'HF___consist__': ['戦争', '倉庫', '医学', '注意', '記念', '番号', '料理', '完全', '開始', '印刷',\n",
        "                       '連続', '予約', '多少', '教員', '当局', '材料', '夕刊', '労働', '運送', '電池' ], # consistent, 'high-frequency words\n",
        "    'HF___inconsist': ['反対', '失敗', '作品', '指定', '実験', '決定', '独占', '独身', '固定', '食品',\n",
        "                       '表明', '安定', '各種', '役所', '海岸', '決算', '地帯', '道路', '安打', '楽団' ], # inconsistent, 'high-frequency words\n",
        "    'HF___atypical_': ['仲間', '夫婦', '人間', '神経', '相手', '反発', '化粧', '建物', '彼女', '毛糸', \n",
        "                       '場合', '台風', '夜間', '人形', '東西', '地元', '松原', '競馬', '大幅', '貸家' ], # inconsistent atypical, 'high-frequency words\n",
        "    'LF___consist__': ['集計', '観察', '予告', '動脈', '理学', '信任', '任務', '返信', '医局', '低温', \n",
        "                       '区別', '永続', '持続', '試練', '満開', '軍備', '製材', '銀貨', '急送', '改選' ], # consistent, 'low-frequecy words\n",
        "    'LF___inconsist': ['表紙', '指針', '熱帯', '作詞', '決着', '食費', '古代', '地形', '役場', '品種', \n",
        "                       '祝福', '金銭', '根底', '接種', '経由', '郷土', '街路', '宿直', '曲折', '越境' ], # inconsistent, 'low-frequency words\n",
        "    'LF___atypical_': ['強引', '寿命', '豆腐', '出前', '歌声', '近道', '間口', '風物', '面影', '眼鏡', \n",
        "                       '居所', '献立', '小雨', '毛皮', '鳥居', '仲買', '頭取', '極上', '奉行', '夢路' ], # inconsistent atypical, 'low-frequncy words\n",
        "    'HFNW_consist__': ['集学', '信別', '製信', '運学', '番送', '電続', '完意', '軍開', '動選', '当働', \n",
        "                       '予続', '倉理', '予少', '教池', '理任', '銀務', '連料', '開員', '注全', '記争' ], # consistent, 'high-character-frequency nonwords\n",
        "    'HFNW_inconsist': ['作明', '風行', '失定', '指団', '決所', '各算', '海身', '東発', '楽験', '作代',\n",
        "                       '反原', '独対', '歌上', '反定', '独定', '場家', '安種', '経着', '決土', '松合' ], # inconsistent biased, 'high-character-frequency nonwords\n",
        "    'HFNW_ambiguous': ['表品', '実定', '人風', '神間', '相経', '人元', '小引', '指場', '毛所', '台手',\n",
        "                       '間物', '道品', '出取', '建馬', '大婦', '地打', '化間', '面口', '金由', '彼間' ], # inconsistent ambigous, 'high-character-frequency nonwords\n",
        "    'LFNW_consist__': ['急材', '戦刊', '返計', '印念', '低局', '労号', '満送', '永告', '試脈', '観備',\n",
        "                       '材約', '夕局', '医庫', '任続', '医貨', '改練', '区温', '多始', '材刷', '持察' ], # consistent, 'low-character-frequency nonwords\n",
        "    'LFNW_inconsist': ['食占', '表底', '宿帯', '決帯', '古費', '安敗', '役針', '近命', '眼道', '豆立',\n",
        "                       '街直', '固路', '郷種', '品路', '曲銭', '献居', '奉買', '根境', '役岸', '祝折' ], # inconsistent biased, 'low-character-frequency nonwords\n",
        "    'LFNW_ambiguous': ['食形', '接紙', '競物', '地詞', '強腐', '頭路', '毛西', '夜糸', '仲影', '熱福',\n",
        "                       '寿前', '鳥雨', '地粧', '越種', '仲女', '極鏡', '夢皮', '居声', '貸形', '夫幅' ], # inconsistent ambigous, 'low-character-frequency nonwords\n",
        "}\n",
        "\n",
        "\n",
        "for k, v in fushimi1999.items():\n",
        "    # 上のデータを表示\n",
        "    print(colored(k, 'blue', attrs=['bold']), v)\n",
        "\n",
        "fushimi1999_list = [] # 上のデータをリスト化\n",
        "for k, v in fushimi1999.items():\n",
        "    for _v in v:\n",
        "        fushimi1999_list.append(_v)\n",
        "\n",
        "if verbose:\n",
        "    print(colored('# Fushimi1999 データから，訓練データに含まれているデータを表示する', 'blue', attrs=['bold']))\n",
        "    for i, wrd in enumerate(fushimi1999_list):\n",
        "        \n",
        "        if wrd in train_wordlist:\n",
        "            color = 'blue'\n",
        "            idx = train_wordlist.index(wrd)\n",
        "        else:\n",
        "            color = 'red'\n",
        "            idx = -1\n",
        "        print(colored((f'{i:3d} wrd:{wrd},idx:{idx:5d}',\n",
        "              f'orth_tkn2ids:{orth_tkn2ids(wrd)}', #o[_tgt]\n",
        "                 ),color=color, attrs=['bold']))\n",
        "\n",
        "print(f'fushimi1999_list:{fushimi1999_list}')        "
      ],
      "metadata": {
        "id": "SbFzqU6ki-Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, wrd in enumerate(fushimi1999_list):\n",
        "    _end = \"\\n\" if (i+1) % 10 == 0 else ' '\n",
        "    print(f'{i+1:3d}: {wrd}', end=\"\") #\n",
        "    for ch in wrd:\n",
        "        print(f'({ch}:{_vocab.orth_vocab.index(ch):4d})', end=\"\")\n",
        "    if (i+1) % 5 == 0:\n",
        "        print()\n",
        "    else:\n",
        "        print(' ', end=\"\")"
      ],
      "metadata": {
        "id": "lgAlcguYkPLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 データセットの設定"
      ],
      "metadata": {
        "id": "usNDYcXpjMAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm  #jupyter で実行時\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import gzip\n",
        "import jaconv\n",
        "\n",
        "class VOCAB():\n",
        "    '''\n",
        "    訓練データとしては，NTT 日本語語彙特性 (天野，近藤, 1999, 三省堂) の頻度データ，実際のファイル名としては `pslex71.txt` から頻度データを読み込んで，高頻度語を訓練データとする。\n",
        "    ただし，検証データに含まれる単語は訓練データとして用いない。\n",
        "\n",
        "    検証データとして，以下のいずれかを考える\n",
        "    1. TLPA (藤田 他, 2000, 「失語症語彙検査」の開発，音声言語医学 42, 179-202)\n",
        "    2. SALA 上智大学失語症語彙検査\n",
        "\n",
        "    このオブジェクトクラスでは，\n",
        "    `phon_vocab`, `orth_vocab`, `ntt_freq`, に加えて，単語の読みについて ntt_orth2hira によって読みを得ることにした。\n",
        "\n",
        "    * `train_data`, `test_data` という辞書が本体である。\n",
        "    各辞書の項目には，さらに\n",
        "    `Vocab_ja.test_data[0].keys() = dict_keys(['orig', 'orth', 'phon', 'orth_ids', 'phon_ids', 'semem'])`\n",
        "\n",
        "    各モダリティ共通トークンとして以下を設定した\n",
        "    * <PAD>: 埋め草トークン\n",
        "    * <EQW>: 単語終端トークン\n",
        "    * <SOW>: 単語始端トークン\n",
        "    * <UNK>: 未定義トークン\n",
        "\n",
        "    このクラスで定義されるデータは 2 つの辞書である。すなわち 1. train_data, 2. tlpa_data である。\n",
        "    各辞書は，次のような辞書項目を持つ。\n",
        "    ```\n",
        "    {0: {'orig': 'バス',\n",
        "    'yomi': 'ばす',\n",
        "    'orth': ['バ', 'ス'],\n",
        "    'orth_ids': [695, 514],\n",
        "    'orth_r': ['ス', 'バ'],\n",
        "    'orth_ids_r': ['ス', 'バ'],\n",
        "    'phon': ['b', 'a', 's', 'u'],\n",
        "    'phon_ids': [23, 7, 19, 12],\n",
        "    'phon_r': ['u', 's', 'a', 'b'],\n",
        "    'phon_ids_r': [12, 19, 7, 23],\n",
        "    'mora': ['ば', 'す'],\n",
        "    'mora_r': ['す', 'ば'],\n",
        "    'mora_ids': [87, 47],\n",
        "    'mora_p': ['b', 'a', 's', 'u'],\n",
        "    'mora_p_r': ['s', 'u', 'b', 'a'],\n",
        "    'mora_p_ids': [6, 5, 31, 35],\n",
        "    'mora_p_ids_r': [31, 35, 6, 5]},\n",
        "    ```\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 traindata_size = 10000,  # デフォルト語彙数\n",
        "                 w2v=None,                # word2vec (gensim)\n",
        "                 yomi=None,               # MeCab を用いた `読み` の取得のため`\n",
        "                 ps71_fname:str=None,     # NTT 日本語語彙特性の頻度データファイル名\n",
        "                 stop_list:list=[],       # ストップ単語リスト：訓練データから排除する単語リスト\n",
        "                 #test_name='TLPA',  # or 'SALA',\n",
        "                ):\n",
        "\n",
        "        if yomi != None:\n",
        "            self.yomi = yomi\n",
        "        else:\n",
        "            #from mecab_settings import yomi\n",
        "            from ccap.mecab_settings import yomi\n",
        "            self.yomi = yomi\n",
        "\n",
        "        # 訓練語彙数の上限 `training_size` を設定\n",
        "        self.traindata_size = traindata_size\n",
        "\n",
        "        # `self.moraWakachi()` で用いる正規表現のあつまり 各条件を正規表現で表す\n",
        "        self.c1 = '[うくすつぬふむゆるぐずづぶぷゔ][ぁぃぇぉ]' #ウ段＋「ァ/ィ/ェ/ォ」\n",
        "        self.c2 = '[いきしちにひみりぎじぢびぴ][ゃゅぇょ]' #イ段（「イ」を除く）＋「ャ/ュ/ェ/ョ」\n",
        "        self.c3 = '[てで][ぃゅ]' #「テ/デ」＋「ャ/ィ/ュ/ョ」\n",
        "        self.c4 = '[ぁ-ゔー]' #カタカナ１文字（長音含む）\n",
        "        self.c5 = '[ふ][ゅ]'\n",
        "        ## self.c1 = '[ウクスツヌフムユルグズヅブプヴ][ァィェォ]' #ウ段＋「ァ/ィ/ェ/ォ」\n",
        "        ## self.c2 = '[イキシチニヒミリギジヂビピ][ャュェョ]' #イ段（「イ」を除く）＋「ャ/ュ/ェ/ョ」\n",
        "        ## self.c3 = '[テデ][ィュ]' #「テ/デ」＋「ャ/ィ/ュ/ョ」\n",
        "        ## self.c4 = '[ァ-ヴー]' #カタカナ１文字（長音含む）\n",
        "        ##cond = '('+c1+'|'+c2+'|'+c3+'|'+c4+')'\n",
        "        self.cond = '('+self.c5+'|'+self.c1+'|'+self.c2+'|'+self.c3+'|'+self.c4+')'\n",
        "        self.re_mora = re.compile(self.cond)\n",
        "        ## 以上 `self.moraWakachi()` で用いる正規表現の定義\n",
        "\n",
        "        self.orth_vocab, self.orth_freq = ['<PAD>', '<EOW>','<SOW>','<UNK>'], {}\n",
        "        self.phon_vocab, self.phone_freq = ['<PAD>', '<EOW>','<SOW>','<UNK>'], {}\n",
        "        self.phon_vocab = ['<PAD>', '<EOW>', '<SOW>', '<UNK>',\\\n",
        "                           'N', 'a', 'a:', 'e', 'e:', 'i', 'i:', 'i::', 'o', 'o:', 'o::', 'u', 'u:', \\\n",
        "                           'b', 'by', 'ch', 'd', 'dy', 'f', 'g', 'gy', 'h', 'hy', 'j', 'k', 'ky', \\\n",
        "                           'm', 'my', 'n', 'ny', 'p', 'py', 'q', 'r', 'ry', 's', 'sh', 't', 'ts', 'w', 'y', 'z']\n",
        "        self.mora_vocab = ['<PAD>', '<EOW>', '<SOW>', '<UNK>',\\\n",
        "                           'ァ', 'ア', 'ィ', 'イ', 'ゥ', 'ウ', 'ェ', 'エ', 'ォ', 'オ', \\\n",
        "                           'カ', 'ガ', 'キ', 'ギ', 'ク', 'グ', 'ケ', 'ゲ', 'コ', 'ゴ', \\\n",
        "                           'サ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ゼ', 'ソ', 'ゾ', \\\n",
        "                           'タ', 'ダ', 'チ', 'ヂ', 'ッ', 'ツ', 'ヅ', 'テ', 'デ', 'ト', 'ド', \\\n",
        "                           'ナ', 'ニ', 'ヌ', 'ネ', 'ノ', \\\n",
        "                           'ハ', 'バ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ヘ', 'ベ', 'ペ', 'ホ', 'ボ', 'ポ', \\\n",
        "                           'マ', 'ミ', 'ム', 'メ', 'モ', \\\n",
        "                           'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', \\\n",
        "                           'ラ', 'リ', 'ル', 'レ', 'ロ', 'ワ', 'ン', 'ー'] \n",
        "        \n",
        "        # 全モーラリストを `mora_vocab` として登録\n",
        "        self.mora_vocab=[\n",
        "            '<PAD>', '<EOW>', '<SOW>', '<UNK>',\n",
        "            'ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'うぃ', 'うぇ', 'うぉ', 'ぇ', 'え', 'お',\n",
        "            'か', 'が', 'き', 'きゃ', 'きゅ', 'きょ', 'ぎ', 'ぎゃ', 'ぎゅ', 'ぎょ', 'く', 'くぁ', 'くぉ', 'ぐ', 'ぐぁ', 'け', 'げ', 'こ', 'ご',\n",
        "            'さ', 'ざ', 'し', 'しぇ', 'しゃ', 'しゅ', 'しょ', 'じ', 'じぇ', 'じゃ', 'じゅ', 'じょ', 'す', 'ず', 'せ', 'ぜ', 'そ', 'ぞ',\n",
        "            'た', 'だ', 'ち', 'ちぇ', 'ちゃ', 'ちゅ', 'ちょ', 'ぢ', 'ぢゃ', 'ぢょ', 'っ', 'つ', 'つぁ', 'つぃ', 'つぇ', 'つぉ', 'づ', 'て',\n",
        "            'てぃ', 'で', 'でぃ', 'でゅ', 'と', 'ど',\n",
        "            'な', 'に', 'にぇ', 'にゃ', 'にゅ', 'にょ', 'ぬ', 'ね', 'の',\n",
        "            'は', 'ば', 'ぱ', 'ひ', 'ひゃ', 'ひゅ', 'ひょ', 'び', 'びゃ', 'びゅ', 'びょ', 'ぴ', 'ぴゃ', 'ぴゅ', 'ぴょ',\n",
        "            'ふ', 'ふぁ', 'ふぃ', 'ふぇ', 'ふぉ', 'ふゅ', 'ぶ', 'ぷ', 'へ', 'べ', 'ぺ', 'ほ', 'ぼ', 'ぽ',\n",
        "            'ま', 'み', 'みゃ', 'みゅ', 'みょ', 'む', 'め', 'も',\n",
        "            'や', 'ゆ', 'よ', 'ら', 'り', 'りゃ', 'りゅ', 'りょ', 'る', 'れ', 'ろ', 'ゎ', 'わ', 'ゐ', 'ゑ', 'を', 'ん', 'ー',\n",
        "            # 2022_1017 added\n",
        "            'ずぃ', 'ぶぇ', 'ぶぃ', 'ぶぁ', 'ゅ', 'ぶぉ', 'いぇ', 'ぉ', 'くぃ', 'ひぇ', 'くぇ', 'ぢゅ', 'りぇ',\n",
        "        ]\n",
        "        \n",
        "        # モーラに用いる音を表すリストを `mora_p_vocab` として登録\n",
        "        self.mora_p_vocab = ['<PAD>', '<EOW>', '<SOW>', '<UNK>',  \\\n",
        "        'N', 'a', 'b', 'by', 'ch', 'd', 'dy', 'e', 'f', 'g', 'gy', 'h', 'hy', 'i', 'j', 'k', 'ky', \\\n",
        "        'm', 'my', 'n', 'ny', 'o', 'p', 'py', 'q', 'r', 'ry', 's', 'sh', 't', 'ts', 'u', 'w', 'y', 'z']\n",
        "\n",
        "        # 母音を表す音から ひらがな への変換表を表す辞書を `vow2hira` として登録\n",
        "        self.vow2hira = {'a':'あ', 'i':'い', 'u':'う', 'e':'え', 'o':'お', 'N':'ん'}\n",
        "\n",
        "        self.mora_freq = {'<PAD>':0, '<EOW>':0, '<SOW>':0, '<UNK>':0}\n",
        "        self.mora_p = {}\n",
        "\n",
        "        # NTT 日本語語彙特性データから，`self.train_data` を作成\n",
        "        self.ntt_freq, self.ntt_orth2hira = self.make_ntt_freq_data(ps71_fname=ps71_fname)\n",
        "        self.ntt_freq_vocab = self.set_train_vocab()\n",
        "        self.train_data, self.excluded_data = {}, []\n",
        "        max_orth_length, max_phon_length, max_mora_length, max_mora_p_length = 0, 0, 0, 0\n",
        "        self.train_vocab = []\n",
        "        \n",
        "        num = '０１２３４５６７８９'\n",
        "        alpha = 'ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ'   # ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ'\n",
        "        hira = 'あいうえおかがきぎくぐけげこごさざしじすずせぜそぞただちぢつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもやゆよらりるれろわゐゑをん'\n",
        "        kata = 'アイウエオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモヤユヨラリルレロワヰヱヲン'\n",
        "        onechars = hira+alpha+num # +kata\n",
        "        for i, orth in enumerate(onechars):\n",
        "            \n",
        "            if not orth in self.train_vocab:\n",
        "                self.train_vocab.append(orth)\n",
        "            _yomi = yomi(orth).strip()\n",
        "            hira = jaconv.kata2hira(_yomi)\n",
        "            phon_juli = jaconv.hiragana2julius(hira).split(' ')\n",
        "                \n",
        "            # 書記素 ID リスト `orth_ids` に書記素を登録\n",
        "            for o in orth:\n",
        "                if not o in self.orth_vocab:\n",
        "                    self.orth_vocab.append(o)\n",
        "            orth_ids = [self.orth_vocab.index(o) for o in orth]\n",
        "            phon_ids = [self.phon_vocab.index(p) if p in self.phon_vocab else self.phon_vocab.index('<UNK>') for p in phon_juli]\n",
        "            \n",
        "            self.train_data[i] = {\n",
        "                'orig':orth,\n",
        "                'orth':orth,\n",
        "                'yomi':_yomi,\n",
        "                'phon':phon_juli,\n",
        "                'phon_ids': phon_ids,\n",
        "                'orth_ids': orth_ids\n",
        "            }\n",
        "\n",
        "\n",
        "        for orth in tqdm(self.ntt_freq_vocab):\n",
        "            if orth in stop_list:       # stop list に登録されていたらスキップ\n",
        "                continue\n",
        "                \n",
        "            if orth in self.train_vocab: # すでに登録されている単語であればスキップ\n",
        "                continue\n",
        "            else:\n",
        "                self.train_vocab.append(orth)\n",
        "                \n",
        "            n_i = len(self.train_data)\n",
        "\n",
        "            # 書記素 `orth` から 読みリスト，音韻表現リスト，音韻表現反転リスト，\n",
        "            # 書記表現リスト，書記表現反転リスト，モーラ表現リスト，モーラ表現反転リスト の 7 つのリストを得る\n",
        "            _yomi, _phon, _phon_r, _orth, _orth_r, _mora, _mora_r = self.get7lists_from_orth(orth_wrd=orth)\n",
        "\n",
        "            # 音韻語彙リスト `self.phon_vocab` に音韻が存在していれば True そうでなければ False というリストを作成し，\n",
        "            # そのリスト無いに False があれば，排除リスト `self.excluded_data` に登録する\n",
        "            #if False in [True if p in self.phon_vocab else False for p in _phon]:\n",
        "            #    self.excluded_data.append(orth)\n",
        "            #    continue\n",
        "\n",
        "            phon_ids, phon_ids_r, orth_ids, orth_ids_r, mora_ids, mora_ids_r = self.get6ids(_phon, _orth, _yomi)\n",
        "            _yomi, _mora1, _mora1_r, _mora, _mora_ids, _mora_p, _mora_p_r, _mora_p_ids, _mora_p_ids_r, _juls = self.yomi2mora_transform(_yomi)\n",
        "            self.train_data[n_i] = {'orig': orth, 'yomi': _yomi,\n",
        "                                    'orth':_orth, 'orth_ids': orth_ids, 'orth_r': _orth_r, 'orth_ids_r': orth_ids_r,\n",
        "                                    'phon':_phon, 'phon_ids': phon_ids, 'phon_r': _phon_r, 'phon_ids_r': phon_ids_r,\n",
        "                                    'mora': _mora1, 'mora_r': _mora1_r, 'mora_ids': _mora_ids, 'mora_p': _mora_p,\n",
        "                                    'mora_p_r': _mora_p_r, 'mora_p_ids': _mora_p_ids, 'mora_p_ids_r': _mora_p_ids_r,\n",
        "                                   }\n",
        "            len_orth, len_phon, len_mora, len_mora_p = len(_orth), len(_phon), len(_mora), len(_mora_p)\n",
        "            max_orth_length = len_orth if len_orth > max_orth_length else max_orth_length\n",
        "            max_phon_length = len_phon if len_phon > max_phon_length else max_phon_length\n",
        "            max_mora_length = len_mora if len_mora > max_mora_length else max_mora_length\n",
        "            max_mora_p_length = len_mora_p if len_mora_p > max_mora_p_length else max_mora_p_length\n",
        "            \n",
        "            if len(self.train_data) >= self.traindata_size: # 上限値に達したら終了する\n",
        "                #self.train_vocab = [self.train_data[x]['orig'] for x in self.train_data.keys()]\n",
        "                break\n",
        "\n",
        "        self.max_orth_length = max_orth_length\n",
        "        self.max_phon_length = max_phon_length\n",
        "        self.max_mora_length = max_mora_length\n",
        "        self.max_mora_p_length = max_mora_p_length\n",
        "        \n",
        "\n",
        "\n",
        "    def yomi2mora_transform(self, yomi):\n",
        "        \"\"\"ひらがな表記された引数 `yomi` から，日本語の 拍(モーラ)  関係のデータを作成する\n",
        "        引数:\n",
        "        yomi:str ひらがな表記された単語 UTF-8 で符号化されていることを仮定している\n",
        "\n",
        "        戻り値:\n",
        "        yomi:str 入力された引数\n",
        "        _mora1:list[str] `_mora` に含まれる長音 `ー` を直前の母音で置き換えた，モーラ単位の分かち書きされた文字列のリスト\n",
        "        _mora1_r:list[str] `_mora1` を反転させた文字列リスト\n",
        "        _mora:list[str] `self.moraWakatchi()` によってモーラ単位で分かち書きされた文字列のリスト\n",
        "        _mora_ids:list[int] `_mora` を対応するモーラ ID で置き換えた整数値からなるリスト\n",
        "        _mora_p:list[str] `_mora` を silius によって音に変換した文字列リスト\n",
        "        _mora_p_r:list[str] `_mora_p` の反転リスト\n",
        "        _mora_p_ids:list[int] `mora_p` の各要素を対応する 音 ID に変換した数値からなるリスト\n",
        "        _mora_p_ids_r:list[int] `mora_p_ids` の各音を反転させた数値からなるリスト\n",
        "        _juls:list[str]: `yomi` を julius 変換した音素からなるリスト\n",
        "        \"\"\"\n",
        "        _mora = self.moraWakachi(yomi) # 一旦モーラ単位の分かち書きを実行して `_mora` に格納\n",
        "\n",
        "        # 単語をモーラ反転した場合に長音「ー」の音が問題となるので，長音「ー」を母音で置き換えるためのプレースホルダとして. `_mora` を用いる\n",
        "        _mora1 = _mora.copy()\n",
        "\n",
        "        # その他のプレースホルダの初期化，モーラ，モーラ毎 ID, モーラ音素，モーラの音素の ID， モーラ音素の反転，モーラ音素の反転 ID リスト\n",
        "        mora_ids, mora_p, mora_p_ids, mora_p_r, _mora_p_ids_r = [], [], [], [], []\n",
        "        _m0 = 'ー' # 長音記号\n",
        "\n",
        "        for i, _m in enumerate(_mora): # 各モーラ単位の処理と登録\n",
        "\n",
        "            __m = _m0 if _m == 'ー' else _m               # 長音だったら，前音の母音を __m とし，それ以外は自分自身を __m に代入\n",
        "            _mora1[i] = __m                               # 長音を変換した結果を格納\n",
        "            mora_ids.append(self.mora_vocab.index(__m))  # モーラを ID 番号に変換\n",
        "            mora_p += jaconv.hiragana2julius(__m).split()\n",
        "            #_mora_p += self.mora2jul[__m]                 # モーラを音素に変換して `_mora_p` に格納\n",
        "\n",
        "            # 変換した音素を音素 ID に変換して，`_mora_p_ids` に格納\n",
        "            #for _p in jaconv.hiragana2julius(_m).split():\n",
        "            #    idx = self.phon_vocab.index(_p)\n",
        "            #    mora_p_ids.append(idx)\n",
        "            #mora_p_ids = [self.phon_vocab.index(_p) for _p in jaconv.hiragana2julius(__m).split()]\n",
        "            #_mora_p_ids += [self.mora_p_vocab.index(_p) for _p in self.mora2jul[__m]]\n",
        "\n",
        "            if not _m in self.mora_freq: # モーラの頻度表を集計\n",
        "                self.mora_freq[__m] = 1\n",
        "            else:\n",
        "                self.mora_freq[__m] +=1\n",
        "\n",
        "            if self.hira2julius(__m)[-1] in self.vow2hira:      # 直前のモーラの最終音素が母音であれば\n",
        "                _m0 = self.vow2hira[self.hira2julius(__m)[-1]]  # 直前の母音を代入しておく。この処理が 2022_0311 でのポイントであった\n",
        "                \n",
        "        mora_p_ids = [self.phon_vocab.index(_p) for _p in mora_p]\n",
        "\n",
        "        # モーラ分かち書きした単語 _mora1 の反転を作成し `_mora1_r` に格納\n",
        "        _mora1_r = [m for m in _mora1[::-1]]\n",
        "        mora_p_r = []\n",
        "        for _m in _mora1_r:                   # 反転した各モーラについて\n",
        "            # モーラ単位で julius 変換して音素とし `_mora_p_r` に格納\n",
        "            for _jul in jaconv.hiragana2julius(_m).split():\n",
        "                mora_p_r.append(_jul)\n",
        "            #_mora_p_r += self.mora2jul[_m]\n",
        "\n",
        "            # mora_p_r に格納した音素を音素 ID に変換し mora_p_ids に格納\n",
        "            #mora_p_ids += [self.mora_p_vocab.index(_p) for _p in self.mora2jul[_m]]\n",
        "            \n",
        "        mora_p_ids_r = [self.phon_vocab.index(_m) for _m in mora_p_r]\n",
        "        _juls = self.hira2julius(yomi)\n",
        "\n",
        "        return yomi, _mora1, _mora1_r, _mora, mora_ids, mora_p, mora_p_r, mora_p_ids, mora_p_ids_r, _juls\n",
        "\n",
        "    def orth2orth_ids(self, \n",
        "                      orth:str):\n",
        "        orth_ids = [self.orth_vocab.index(ch) if ch in self.orth_vocab else self.orth_vocab.index('<UNK>') for ch in orth]\n",
        "        return orth_ids\n",
        "\n",
        "    def phon2phon_ids(self, \n",
        "                      phon:list):\n",
        "        phon_ids = [self.phon_vocab.index(ph) if ph in self.phon_vocab else self.phon_vocab.index('<UNK>') for ph in phon]\n",
        "        return phon_ids\n",
        "    \n",
        "    def yomi2phon_ids(self,\n",
        "                      yomi:str):\n",
        "        phon_ids = []\n",
        "        for _jul in self.hira2julius(yomi):\n",
        "            if _jul in self.phon_vocab:\n",
        "                ph = self.phon_vocab.index(_jul)\n",
        "            else:\n",
        "                ph = self.phon_vocab.index('<UNK>')\n",
        "            phon_ids.append(ph)\n",
        "        return phon_ids\n",
        "    \n",
        "    \n",
        "    def orth_ids2tkn(self, ids:list):\n",
        "        return [self.orth_vocab[idx] for idx in ids]\n",
        "\n",
        "    def orth_tkn2ids(self, tkn:list):\n",
        "        return [self.orth_vocab.index(_tkn) if _tkn in self.orth_vocab else self.orth_vocab.index('<UNK>') for _tkn in tkn]\n",
        "\n",
        "    def mora_p_ids2tkn(self, ids:list):\n",
        "        return [self.mora_p_vocab[idx] for idx in ids]\n",
        "\n",
        "    def mora_p_tkn2ids(self, tkn:list):\n",
        "        return [self.mora_p_vocab.index(_tkn) if _tkn in self.mora_p_vocab else self.mora_p_vocab('<UNK>') for _tkn in tkn]\n",
        "\n",
        "    def mora_ids2tkn(self, ids:list):\n",
        "        return [self.mora_vocab[idx] for idx in ids]\n",
        "\n",
        "    def mora_tkn2ids(self, tkn:list):\n",
        "        return [self.mora_vocab.index(_tkn) if _tkn in self.mora_vocab else self.mora_vocab('<UNK>') for _tkn in tkn]\n",
        "\n",
        "    def phon_ids2tkn(self, ids:list):\n",
        "        return [self.phon_vocab[idx] for idx in ids]\n",
        "\n",
        "    def phon_tkn2ids(self, tkn:list):\n",
        "        return [self.phon_vocab.index(_tkn) if _tkn in self.phon_vocab else self.phon_vocab.index('<UNK>') for _tkn in tkn]\n",
        "\n",
        "    def get6ids(self, _phon, _orth, yomi):\n",
        "\n",
        "        # 音韻 ID リスト `phon_ids` に音素を登録する\n",
        "        phon_ids = [self.phon_vocab.index(p) if p in self.phon_vocab else self.phon_vocab.index('<UNK>') for p in _phon]\n",
        "\n",
        "        # 直上の音韻 ID リストの逆転を作成\n",
        "        phon_ids_r = [p_id for p_id in phon_ids[::-1]]\n",
        "\n",
        "        # 書記素 ID リスト `orth_ids` に書記素を登録\n",
        "        for o in _orth:\n",
        "            if not o in self.orth_vocab:\n",
        "                self.orth_vocab.append(o)\n",
        "        orth_ids = [self.orth_vocab.index(o) for o in _orth]\n",
        "\n",
        "        # 直上の書記素 ID リストの逆転を作成\n",
        "        orth_ids_r = [o_id for o_id in orth_ids[::-1]]\n",
        "        #orth_ids_r = [o_id for o_id in _orth[::-1]]\n",
        "\n",
        "        mora_ids = []\n",
        "        for _p in self.hira2julius(yomi):\n",
        "            mora_ids.append(self.phon_vocab.index(_p) if _p in self.phon_vocab else self.phon_vocab.index('<UNK>'))\n",
        "\n",
        "        mora_ids_r = [m_id for m_id in mora_ids]\n",
        "        return phon_ids, phon_ids_r, orth_ids, orth_ids_r, mora_ids, mora_ids_r\n",
        "\n",
        "\n",
        "    def moraWakachi(self, hira_text):\n",
        "        \"\"\" ひらがなをモーラ単位で分かち書きする\n",
        "        https://qiita.com/shimajiroxyz/items/a133d990df2bc3affc12\"\"\"\n",
        "\n",
        "        return self.re_mora.findall(hira_text)\n",
        "\n",
        "\n",
        "    def _kana_moraWakachi(kan_text):\n",
        "        self.cond = '('+self.c1+'|'+self.c2+'|'+self.c3+'|'+self.c4+')'\n",
        "        self.re_mora = re.compile(self.cond)\n",
        "\n",
        "        return re_mora.findall(kana_text)\n",
        "\n",
        "    \n",
        "    def get7lists_from_orth(self, orth_wrd):\n",
        "        \"\"\"書記素 `orth` から 読みリスト，音韻表現リスト，音韻表現反転リスト，\n",
        "        書記表現リスト，書記表現反転リスト，モーラ表現リスト，モーラ表現反転リスト の 7 つのリストを得る\"\"\"\n",
        "\n",
        "        # 単語の表層形を，読みに変換して `_yomi` に格納\n",
        "        # ntt_orth2hira という命名はおかしかったから修正 2022_0309\n",
        "        if orth_wrd in self.ntt_orth2hira:\n",
        "            _yomi = self.ntt_orth2hira[orth_wrd]\n",
        "        else:\n",
        "            _yomi = jaconv.kata2hira(self.yomi(orth_wrd).strip())\n",
        "\n",
        "        # `_yomi` を julius 表記に変換して `_phon` に代入\n",
        "        _phon = self.hira2julius(_yomi)# .split(' ')\n",
        "\n",
        "        # 直上の `_phon` の逆転を作成して `_phone_r` に代入\n",
        "        _phon_r = [_p_id for _p_id in _phon[::-1]]\n",
        "\n",
        "        # 書記素をリストに変換\n",
        "        _orth = [c for c in orth_wrd]\n",
        "\n",
        "        # 直上の `_orth` の逆転を作成して `_orth_r` に代入\n",
        "        _orth_r = [c for c in _orth[::-1]]\n",
        "\n",
        "        #_mora = self.moraWakachi(jaconv.hira2kata(_yomi))\n",
        "        _mora = self.moraWakachi(_yomi)\n",
        "        for _m in _mora:\n",
        "            if not _m in self.mora_vocab:\n",
        "                self.mora_vocab.append(_m)\n",
        "            for _j in self.hira2julius(_m):\n",
        "                if not _j in self.mora_p:\n",
        "                    self.mora_p[_j] = 1\n",
        "                else:\n",
        "                    self.mora_p[_j] += 1\n",
        "        _mora_r = [_m for _m in _mora[::-1]]\n",
        "        return _yomi, _phon, _phon_r, _orth, _orth_r, _mora, _mora_r\n",
        "\n",
        "\n",
        "    def hira2julius(self, text:str)->str:\n",
        "        \"\"\"`jaconv.hiragana2julius()` では未対応の表記を扱う\"\"\"\n",
        "        text = text.replace('ゔぁ', ' b a')\n",
        "        text = text.replace('ゔぃ', ' b i')\n",
        "        text = text.replace('ゔぇ', ' b e')\n",
        "        text = text.replace('ゔぉ', ' b o')\n",
        "        text = text.replace('ゔゅ', ' by u')\n",
        "\n",
        "        #text = text.replace('ぅ゛', ' b u')\n",
        "        text = jaconv.hiragana2julius(text).split()\n",
        "        return text\n",
        "\n",
        "\n",
        "    def __len__(self)->int:\n",
        "        return len(self.train_data)\n",
        "\n",
        "    \n",
        "    def __call__(self, x:int)->dict:\n",
        "        return self.train_data[x]\n",
        "\n",
        "    \n",
        "    def __getitem__(self, x:int)->dict:\n",
        "        return self.train_data[x]\n",
        "\n",
        "    \n",
        "    def set_train_vocab(self):\n",
        "    #def set_train_vocab_minus_test_vocab(self):\n",
        "        \"\"\"JISX2008-1990 コードから記号とみなしうるコードを集めて ja_symbols とする\n",
        "        記号だけから構成されている word2vec の項目は排除するため\n",
        "        \"\"\"\n",
        "        self.ja_symbols = '、。，．・：；？！゛゜´\\' #+ \\'｀¨＾‾＿ヽヾゝゞ〃仝々〆〇ー—‐／＼〜‖｜…‥‘’“”（）〔〕［］｛｝〈〉《》「」『』【】＋−±×÷＝≠＜＞≦≧∞∴♂♀°′″℃¥＄¢£％＃＆＊＠§☆★○●◎◇◆□■△▲▽▼※〒→←↑↓〓∈∋⊆⊇⊂⊃∪∩∧∨¬⇒⇔∀∃∠⊥⌒∂∇≡≒≪≫√∽∝∵∫∬Å‰♯♭♪†‡¶◯#ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ'\n",
        "        #self.ja_symbols_normalized = jaconv.normalize(self.ja_symbols)\n",
        "\n",
        "        print(f'# 訓練に用いる単語の選定 {self.traindata_size} 語')\n",
        "        vocab = []; i=0\n",
        "        while i<len(self.ntt_freq):\n",
        "            word = self.ntt_freq[i]\n",
        "            if word == '\\u3000': # NTT 日本語の語彙特性で，これだけ変なので特別扱い\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            # 良い回避策が見つからないので，以下の行の変換だけ特別扱いしている\n",
        "            word = jaconv.normalize(word).replace('・','').replace('ヴ','ブ')\n",
        "\n",
        "            if (not word in self.ja_symbols) and (not word.isascii()): # and (word in self.w2v):\n",
        "                \n",
        "                if not word in vocab:\n",
        "                    vocab.append(word)\n",
        "                    if len(vocab) >= self.traindata_size:\n",
        "                        return vocab\n",
        "            i += 1\n",
        "        return vocab\n",
        "\n",
        "\n",
        "    def make_ntt_freq_data(self,\n",
        "                           ps71_fname:str=None):\n",
        "\n",
        "        print('# NTT日本語語彙特性 (天野，近藤; 1999, 三省堂)より頻度情報を取得')\n",
        "\n",
        "        if ps71_fname == None:\n",
        "            #データファイルの保存してあるディレクトリの指定\n",
        "            ntt_dir = 'ccap'\n",
        "            psy71_fname = 'psylex71utf8.txt'  # ファイル名\n",
        "            psy71_fname = 'psylex71utf8.txt.gz'  # ファイル名\n",
        "            #with gzip.open(os.path.join(ntt_dir,psy71_fname), 'r') as f:\n",
        "            with gzip.open(os.path.join(ntt_dir,psy71_fname), 'rt', encoding='utf-8') as f:\n",
        "                ntt71raw = f.readlines()\n",
        "        else:\n",
        "            with open(ps71_fname, 'r') as f:\n",
        "                ntt71raw = f.readlines()\n",
        "\n",
        "        tmp = [line.split(' ')[:6] for line in ntt71raw]\n",
        "        tmp2 = [[int(line[0]),line[2],line[4],int(line[5]), line[3]] for line in tmp]\n",
        "        #単語ID(0), 単語，品詞，頻度 だけ取り出す\n",
        "\n",
        "        ntt_freq = {x[0]-1:{'単語':jaconv.normalize(x[1]),\n",
        "                            '品詞':x[2],\n",
        "                            '頻度':x[3],\n",
        "                            'よみ':jaconv.kata2hira(jaconv.normalize(x[4]))\n",
        "                            } for x in tmp2}\n",
        "        #ntt_freq = {x[0]-1:{'単語':x[1],'品詞':x[2],'頻度':x[3], 'よみ':x[4]} for x in tmp2}\n",
        "        ntt_orth2hira = {ntt_freq[x]['単語']:ntt_freq[x]['よみ'] for x in ntt_freq}\n",
        "        #print(f'#登録総単語数: {len(ntt_freq)}')\n",
        "\n",
        "        Freq = np.zeros((len(ntt_freq)), dtype=np.uint)  #ソートに使用する numpy 配列\n",
        "        for i, x in enumerate(ntt_freq):\n",
        "            Freq[i] = ntt_freq[i]['頻度']\n",
        "\n",
        "        Freq_sorted = np.argsort(Freq)[::-1]  #頻度降順に並べ替え\n",
        "\n",
        "        # self.ntt_freq には頻度順に単語が並んでいる\n",
        "        return [ntt_freq[x]['単語']for x in Freq_sorted], ntt_orth2hira\n",
        "\n",
        "_vocab = VOCAB(\n",
        "    traindata_size=params['traindata_size'],     \n",
        "    w2v=None, \n",
        "    yomi=yomi,\n",
        "    stop_list=fushimi1999_list) \n",
        "\n",
        "train_wordlist = [v['orig'] for k, v in _vocab.train_data.items()]\n",
        "\n",
        "top_n = 300\n",
        "print(f'語彙先頭の項目 {top_n} を印字')\n",
        "for i, wrd in enumerate(train_wordlist[:top_n]):\n",
        "    _end = \" \" if (i+1) % 10 != 0 else \"\\n\"\n",
        "    print((i+1, wrd), end=_end)"
      ],
      "metadata": {
        "id": "jo85wpbnjF-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Train_dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data:VOCAB=None,\n",
        "                 source_vocab:list=None,\n",
        "                 target_vocab:list=None,\n",
        "                 source_ids:str=None,\n",
        "                 target_ids:str=None,\n",
        "                ):\n",
        "\n",
        "        if data == None:\n",
        "            self.data = VOCAB()\n",
        "        else:\n",
        "            self.data = data\n",
        "        self.order = {i:self.data[x] for i, x in enumerate(self.data)}\n",
        "\n",
        "        self.source_ids = source_ids\n",
        "        self.target_ids = target_ids\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "\n",
        "    def __len__(self)->int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, x:int):\n",
        "        return self.order[x][self.source_ids] + [self.source_vocab.index('<EOW>')], self.order[x][self.target_ids] + [self.target_vocab.index('<EOW>')]\n",
        "\n",
        "    def convert_source_ids_to_tokens(self, ids:list):\n",
        "        return [self.source_vocab[idx] for idx in ids]\n",
        "\n",
        "    def convert_target_ids_to_tokens(self, ids:list):\n",
        "        return [self.target_vocab[idx] for idx in ids]\n",
        "\n",
        "\n",
        "class Val_dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"同じく検証データセットの定義\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                data:dict=None,\n",
        "                source_vocab:list=None,\n",
        "                target_vocab:list=None,\n",
        "                source_ids:str=None,\n",
        "                target_ids:str=None,\n",
        "                ):\n",
        "\n",
        "        if 'pdata' in str(data.keys()):\n",
        "            self.data = data['pdata']\n",
        "        else:\n",
        "            self.data = data\n",
        "\n",
        "        self.order = {i:self.data[x] for i, x in enumerate(self.data)}\n",
        "\n",
        "        self.target_ids = target_ids\n",
        "        self.source_ids = source_ids\n",
        "\n",
        "        self.source_vocab = source_vocab if source_vocab != None else VOCAB().mora_p_vocab\n",
        "        self.target_vocab = target_vocab if target_vocab != None else VOCAB().mora_p_vocab\n",
        "\n",
        "\n",
        "    def __len__(self)->int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, x:int):\n",
        "        return self.order[x][self.source_ids] + [self.source_vocab.index('<EOW>')], self.order[x][self.target_ids] + [self.target_vocab.index('<EOW>')]\n",
        "\n",
        "    def convert_source_ids_to_tokens(self, ids:list):\n",
        "        return [self.source_vocab[idx] for idx in ids]\n",
        "\n",
        "    def convert_target_ids_to_tokens(self, ids:list):\n",
        "        return [self.target_vocab[idx] for idx in ids]\n",
        "\n",
        "def make_X_vals(_dataset=None,\n",
        "                source_vocab=None,\n",
        "                target_vocab=None,\n",
        "                source_ids=None,\n",
        "                target_ids=None,\n",
        "                ):\n",
        "\n",
        "    if _dataset == None:\n",
        "        print('_dataset must be set')\n",
        "        sys.exit()\n",
        "\n",
        "    sala_r29val = Val_dataset(\n",
        "        data=_dataset['sala_r29'],\n",
        "        source_vocab=source_vocab,\n",
        "        target_vocab=target_vocab,\n",
        "        source_ids=source_ids,\n",
        "        target_ids=target_ids)\n",
        "\n",
        "    sala_r30val = Val_dataset(\n",
        "        data=_dataset['sala_r30'],\n",
        "        source_vocab=source_vocab,\n",
        "        target_vocab=target_vocab,\n",
        "        source_ids=source_ids,\n",
        "        target_ids=target_ids)\n",
        "\n",
        "    sala_r31val = Val_dataset(\n",
        "        data=_dataset['sala_r31'],\n",
        "        source_vocab=source_vocab,\n",
        "        target_vocab=target_vocab,\n",
        "        source_ids=source_ids,\n",
        "        target_ids=target_ids)\n",
        "\n",
        "    tlpa2val    = Val_dataset(\n",
        "        data=_dataset['tlpa2'],\n",
        "        source_vocab=source_vocab,\n",
        "        target_vocab=target_vocab,\n",
        "        source_ids=source_ids,\n",
        "        target_ids=target_ids)\n",
        "\n",
        "    tlpa3val    = Val_dataset(\n",
        "        data=_dataset['tlpa3'],\n",
        "        source_vocab=source_vocab,\n",
        "        target_vocab=target_vocab,\n",
        "        source_ids=source_ids,\n",
        "        target_ids=target_ids)\n",
        "\n",
        "    tlpa4val    = Val_dataset(\n",
        "        data=_dataset['tlpa4'],\n",
        "        source_vocab=source_vocab,\n",
        "        target_vocab=target_vocab,\n",
        "        source_ids=source_ids,\n",
        "        target_ids=target_ids)\n",
        "\n",
        "    X_vals = { \n",
        "        'sala_r29val': sala_r29val,\n",
        "        'sala_r30val': sala_r30val,\n",
        "        'sala_r31val': sala_r31val,\n",
        "        'tlpa2val': tlpa2val, \n",
        "        'tlpa3val': tlpa3val, \n",
        "        'tlpa4val': tlpa4val}\n",
        "\n",
        "    return X_vals\n",
        "\n",
        "\n",
        "def make_vocab_dataset(_dict:dict, vocab:VOCAB=None)->dict:\n",
        "    \"\"\"上記 VOCAB を用いた下請け関数\n",
        "    読み，音韻，モーラなどの情報を作成してデータセットといしての体裁を整える\"\"\"\n",
        "    \n",
        "    _data = {}\n",
        "    if vocab == None:\n",
        "        vocab = VOCAB()\n",
        "    x = [x[0] for x in _dict.values()]\n",
        "    for _x in x:\n",
        "        i = len(_data)  # 連番の番号を得る\n",
        "        orth = vocab.ntt_orth2hira[_x] if _x in vocab.ntt_orth2hira else _x\n",
        "        _yomi, _phon, _phon_r, _orth, _orth_r, _mora, _mora_r = vocab.get7lists_from_orth(orth)\n",
        "        phon_ids, phon_ids_r, orth_ids, orth_ids_r, mora_ids, mora_ids_r = vocab.get6ids(_phon, _orth, _yomi)\n",
        "        _yomi, _mora1, _mora1_r, _mora, _mora_ids, _mora_p, _mora_p_r, _mora_p_ids, _mora_p_ids_r, _juls = vocab.yomi2mora_transform(_yomi)\n",
        "        _data[i] = {'orig': orth, \n",
        "                    'yomi': _yomi, \n",
        "                    'orth':_orth, 'orth_ids': orth_ids, 'orth_r': _orth_r, 'orth_ids_r': orth_ids_r,\n",
        "                    'phon':_phon, 'phon_ids': phon_ids, 'phon_r': _phon_r, 'phon_ids_r': phon_ids_r,\n",
        "                    'mora': _mora1, 'mora_r': _mora1_r, 'mora_ids': _mora_ids, 'mora_p': _mora_p,\n",
        "                    'mora_p_r': _mora_p_r, 'mora_p_ids': _mora_p_ids, 'mora_p_ids_r': _mora_p_ids_r, }\n",
        "    return _data    "
      ],
      "metadata": {
        "id": "mDNxhzlUjStx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_soure_and_target_from_params(\n",
        "    params=None,\n",
        "    _vocab=None,\n",
        "    source=None,\n",
        "    target=None,\n",
        "    is_print:bool=True):\n",
        "\n",
        "    if source == 'orth':\n",
        "        source_vocab = _vocab.orth_vocab\n",
        "        source_ids = 'orth_ids'\n",
        "    elif source == 'phon':\n",
        "        source_vocab = _vocab.phon_vocab\n",
        "        source_ids = 'phon_ids'\n",
        "    elif source == 'mora':\n",
        "        source_vocab = _vocab.mora_vocab\n",
        "        source_ids = 'mora_ids'\n",
        "    elif source == 'mora_p':\n",
        "        source_vocab = _vocab.mora_p_vocab\n",
        "        source_ids = 'mora_p_ids'\n",
        "    elif source == 'mora_p_r':\n",
        "        source_vocab = _vocab.mora_p_vocab\n",
        "        source_ids = 'mora_p_ids_r'\n",
        "\n",
        "    if target == 'orth':\n",
        "        target_vocab = _vocab.orth_vocab\n",
        "        target_ids = 'orth_ids'\n",
        "    elif target == 'phon':\n",
        "        target_vocab = _vocab.phon_vocab\n",
        "        target_ids = 'phon_ids'\n",
        "    elif target == 'mora':\n",
        "        target_vocab = _vocab.mora_vocab\n",
        "        target_ids = 'mora_ids'\n",
        "    elif target == 'mora_p':\n",
        "        target_vocab = _vocab.mora_p_vocab\n",
        "        target_ids = 'mora_p_ids'\n",
        "    elif target == 'mora_p_r':\n",
        "        target_vocab = _vocab.mora_p_vocab\n",
        "        target_ids = 'mora_p_ids_r'\n",
        "\n",
        "    if is_print:\n",
        "        print(colored(f'source:{source}','blue', attrs=['bold']), f'{source_vocab}')\n",
        "        print(colored(f'target:{target}','cyan', attrs=['bold']), f'{target_vocab}')\n",
        "        print(colored(f'source_ids:{source_ids}','blue', attrs=['bold']), f'{source_ids}')\n",
        "        print(colored(f'target_ids:{target_ids}','cyan', attrs=['bold']), f'{target_ids}')\n",
        "\n",
        "    return source_vocab, source_ids, target_vocab, target_ids"
      ],
      "metadata": {
        "id": "DoYj8N2hj6iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _max_len はアテンション機構のデコーダで必要になるため，全条件で最長の長さを指定する必要がある\n",
        "_max_len = _vocab.max_orth_length\n",
        "_max_len = _max_len if _max_len > _vocab.max_phon_length else _vocab.max_phon_length\n",
        "_max_len = _max_len if _max_len > _vocab.max_mora_length else _vocab.max_mora_length\n",
        "_max_len = _max_len if _max_len > _vocab.max_mora_p_length else _vocab.max_mora_p_length\n",
        "_vocab.max_length = _max_len + 1\n",
        "print(colored(f'_vocab.max_length: {_vocab.max_length}', 'blue', attrs=['bold']))\n",
        "\n",
        "# ソース，すなわち encoder 側の，項目番号，項目 ID，decoder 側の項目，項目 ID を設定\n",
        "source_vocab, source_ids, target_vocab, target_ids = get_soure_and_target_from_params(\n",
        "    #params=None,\n",
        "    _vocab=_vocab,\n",
        "    source=source,\n",
        "    target=target,\n",
        "    is_print=False)\n",
        "    #is_print=True)\n",
        "\n",
        "print(colored(f'source:{source}','blue', attrs=['bold']), f'{sorted(source_vocab)}')\n",
        "print(colored(f'target:{target}','cyan', attrs=['bold']), f'{sorted(target_vocab)}')\n",
        "#print(colored(f'source_ids:{source_ids}','blue', attrs=['bold']), f'{sorted(source_ids)}')\n",
        "#print(colored(f'target_ids:{target_ids}','cyan', attrs=['bold']), f'{sorted(target_ids)}')\n",
        "\n",
        "\n",
        "# 検証データとして，TLPA と SALA のデータを用いる\n",
        "tlpa1, tlpa2, tlpa3, tlpa4, sala_r29, sala_r30, sala_r31 = lam.read_json_tlpa1234_sala_r29_30_31(\n",
        "    json_fname='lam/2022_0508SALA_TLPA.json')\n",
        "\n",
        "_dataset = {}\n",
        "_data_names = ['tlpa2', 'tlpa3', 'tlpa4', 'sala_r29', 'sala_r30', 'sala_r31']\n",
        "for data in _data_names:\n",
        "    _dataset[data] = {'rawdata':eval(data),\n",
        "                      'pdata': make_vocab_dataset(eval(data),vocab=_vocab)}\n",
        "\n",
        "# 以下は後から付け足したので，コードが汚くなっている。\n",
        "# 時間ができたらコードの整理をすること\n",
        "X_vals = make_X_vals(_dataset=_dataset,\n",
        "                         source_vocab=source_vocab,\n",
        "                         target_vocab=target_vocab,\n",
        "                         source_ids=source_ids,\n",
        "                         target_ids=target_ids)"
      ],
      "metadata": {
        "id": "wf7f0YoJjtLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.1 任意の単語 orthography を変換するための関数"
      ],
      "metadata": {
        "id": "FkEb7pK3kek_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "epghxj85jwsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.2 SALA and TLPA dataset"
      ],
      "metadata": {
        "id": "XEN-R3IrkiXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 検証データとして，TLPA と SALA のデータを用いる\n",
        "tlpa1, tlpa2, tlpa3, tlpa4, sala_r29, sala_r30, sala_r31 = lam.read_json_tlpa1234_sala_r29_30_31(\n",
        "    json_fname='lam/2022_0508SALA_TLPA.json')\n",
        "\n",
        "_dataset = {}\n",
        "_data_names = ['tlpa2', 'tlpa3', 'tlpa4', 'sala_r29', 'sala_r30', 'sala_r31']\n",
        "for data in _data_names:\n",
        "    _dataset[data] = {'rawdata':eval(data),\n",
        "                      'pdata': make_vocab_dataset(eval(data), vocab=_vocab)}\n",
        "\n",
        "# 以下は後から付け足したので，コードが汚くなっている。\n",
        "# 時間ができたらコードの整理をすること\n",
        "X_vals = make_X_vals(_dataset=_dataset,\n",
        "                         source_vocab=source_vocab,\n",
        "                         target_vocab=target_vocab,\n",
        "                         source_ids=source_ids,\n",
        "                         target_ids=target_ids)\n",
        "\n",
        "_data_names = ['tlpa2', 'tlpa3', 'tlpa4', 'sala_r29', 'sala_r30', 'sala_r31']    \n",
        "for data in _data_names:\n",
        "    print(colored(data, 'blue', attrs=['bold']), eval(data))"
      ],
      "metadata": {
        "id": "-nHE36y4kkn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "verbose = True\n",
        "_src, _tgt = source+'_ids', target+'_ids'\n",
        "\n",
        "if verbose:\n",
        "    for i, w in enumerate(fushimi1999_list):\n",
        "        ids = _vocab.orth_tkn2ids(w)\n",
        "        tkn = _vocab.orth_ids2tkn(ids)\n",
        "        print(i, w, tkn, ids)\n",
        "        #os.system(f\"say {w} --voice kyoko\")"
      ],
      "metadata": {
        "id": "bM36cq10kmUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.3 一文字データセットの定義"
      ],
      "metadata": {
        "id": "h2rxjl4tlbPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Onechar_dataset(torch.utils.data.Dataset): #_vocab.train_data):\n",
        "    def __init__(self,\n",
        "                 source:str='orth',\n",
        "                 target:str='mora_p',\n",
        "                 source_vocab=source_vocab,\n",
        "                 target_vocab=target_vocab,\n",
        "                ):\n",
        "        super().__init__()\n",
        "\n",
        "        _src = source\n",
        "        _tgt = target\n",
        "\n",
        "        _src = 'mora' if _src == 'mora_p' else _src\n",
        "        _tgt = 'mora' if _tgt == 'mora_p' else _tgt\n",
        "        _src, _tgt = _src+'_ids', _tgt+'_ids'\n",
        "        \n",
        "        digit_alpha = '０１２３４５６７８９ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ' # ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ'\n",
        "        hira = 'あいうえおかがきぎくぐけげこごさざしじすずせぜそぞただちぢつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもやゆよらりるれろわゐゑをん'\n",
        "        kata = 'アイウエオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモヤユヨラリルレロワヰヱヲン'\n",
        "        onechars = digit_alpha+hira #+kata\n",
        "        \n",
        "        data_dict = {}\n",
        "        for i, orth in enumerate(onechars):\n",
        "            _yomi = yomi(orth).strip()\n",
        "            hira = jaconv.kata2hira(_yomi)\n",
        "            phon_juli = jaconv.hiragana2julius(hira).split(' ')\n",
        "            phon_ids = _vocab.phon_tkn2ids(phon_juli)\n",
        "            orth_ids = _vocab.orth_tkn2ids(orth)\n",
        "            \n",
        "            _data = {'_yomi':_yomi,\n",
        "                     'phon':phon_juli,\n",
        "                     'phon_ids':phon_ids,\n",
        "                     'orth':orth,\n",
        "                     'orth_ids':orth_ids}\n",
        "            __src, __tgt = _data[_src], _data[_tgt]\n",
        "            data_dict[i] = {'yomi':_yomi,\n",
        "                            'orth':orth,\n",
        "                            'src':__src,\n",
        "                            'tgt':__tgt,\n",
        "                            '_phon':phon_juli,\n",
        "                            'phon_ids':phon_ids,\n",
        "                            'orth_ids':orth_ids}\n",
        "\n",
        "        self.source_vocab = source_vocab if source_vocab != None else VOCAB().mora_p_vocab\n",
        "        self.target_vocab = target_vocab if target_vocab != None else VOCAB().mora_p_vocab\n",
        "            \n",
        "            \n",
        "        self.data_dict = data_dict\n",
        "\n",
        "    def __len__(self)->int:\n",
        "        return len(self.data_dict)\n",
        "    \n",
        "    def __getitem__(self,\n",
        "                    x:int):\n",
        "        _data = self.data_dict[x]\n",
        "        return _data['src']+[self.source_vocab.index('<EOW>')], _data['tgt']+[self.target_vocab.index('<EOW>')]\n",
        "    \n",
        "#    def __getitem__(self, x:int):\n",
        "#        return self.order[x][self.source_ids] + [self.source_vocab.index('<EOW>')], self.order[x][self.target_ids] + [self.target_vocab.index('<EOW>')]\n",
        "    \n",
        "        \n",
        "onechar_dataset = Onechar_dataset(source=source, target=target)\n",
        "# print(onechar_dataset.__len__())\n",
        "# print(onechar_dataset.__getitem__(0))\n",
        "\n",
        "for i in range(onechar_dataset.__len__()):\n",
        "    inp, tch = onechar_dataset.__getitem__(i)\n",
        "    print(f'{i:3d}', \n",
        "          f'orth:{onechar_dataset.data_dict[i][\"orth\"]}', \n",
        "          f'inp:{inp}, tch:{tch}', \n",
        "          f\"{colored(onechar_dataset.data_dict[i]['yomi'],'blue',attrs=['bold'])}\")"
      ],
      "metadata": {
        "id": "6tpxlb1-kqNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 モデルの定義"
      ],
      "metadata": {
        "id": "sEhmIBbXlg2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 自作ライブラリ LAM の読み込み\n",
        "#import lam \n",
        "#from lam import EncoderRNN\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    \"\"\"RNNによる符号化器\"\"\"\n",
        "    def __init__(self,\n",
        "            n_inp:int=0,\n",
        "            n_hid:int=0):\n",
        "            #device=device):\n",
        "        super().__init__()\n",
        "        self.n_hid = n_hid if n_hid != 0 else 8\n",
        "        self.n_inp = n_inp if n_inp != 0 else 8\n",
        "\n",
        "        self.embedding = nn.Embedding(n_inp, n_hid)\n",
        "        self.gru = nn.GRU(n_hid, n_hid)\n",
        "\n",
        "    def forward(self,\n",
        "                inp:int=0,\n",
        "                hid:int=0,\n",
        "                device=device\n",
        "               ):\n",
        "        embedded = self.embedding(inp).view(1, 1, -1)\n",
        "        out = embedded\n",
        "        out, hid = self.gru(out, hid)\n",
        "        return out, hid\n",
        "\n",
        "    def initHidden(self)->torch.Tensor:\n",
        "        return torch.zeros(1, 1, self.n_hid, device=device)\n",
        "\n",
        "\n",
        "#from lam import AttnDecoderRNN\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    \"\"\"注意付き復号化器の定義\"\"\"\n",
        "    def __init__(self, \n",
        "                 n_hid:int=0, \n",
        "                 n_out:int=0, \n",
        "                 dropout_p:float=0.0, \n",
        "                 max_length:int=0):\n",
        "        super().__init__()\n",
        "        self.n_hid = n_hid\n",
        "        self.n_out = n_out\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_out, self.n_hid)\n",
        "        self.attn = nn.Linear(self.n_hid * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.n_hid * 2, self.n_hid)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.n_hid, self.n_hid)\n",
        "        self.out = nn.Linear(self.n_hid, self.n_out)\n",
        "\n",
        "    def forward(self, \n",
        "                inp:int=0, \n",
        "                hid:int=0, \n",
        "                encoder_outputs:torch.Tensor=None, \n",
        "                device=device):\n",
        "        embedded = self.embedding(inp).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hid[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        out = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        out = self.attn_combine(out).unsqueeze(0)\n",
        "\n",
        "        out = F.relu(out)\n",
        "        out, hid = self.gru(out, hid)\n",
        "\n",
        "        out = F.log_softmax(self.out(out[0]), dim=1)\n",
        "        return out, hid, attn_weights\n",
        "\n",
        "    def initHidden(self)->torch.Tensor:\n",
        "        return torch.zeros(1, 1, self.n_hid, device=device)\n",
        "\n",
        "    \n",
        "def convert_ids2tensor(\n",
        "    sentence_ids:list, \n",
        "    device:torch.device=torch.device(\"cuda:0\" if torch.cuda.is_available () else \"cpu\")):\n",
        "    \n",
        "    \"\"\"数値 ID リストをテンソルに変換\n",
        "    例えば，[0,1,2] -> tensor([[0],[1],[2]])\n",
        "    \"\"\"\n",
        "    return torch.tensor(sentence_ids, dtype=torch.long, device=device).view(-1, 1)\n",
        "    \n",
        "    \n",
        "def evaluate(encoder:nn.Module,\n",
        "             decoder:nn.Module,\n",
        "             input_ids,\n",
        "             max_length,\n",
        "             source_vocab,\n",
        "             target_vocab,\n",
        "             source_ids,\n",
        "             target_ids,\n",
        "            )->(list,torch.LongTensor):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = convert_ids2tensor(input_ids)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.n_hid, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[source_vocab.index('<SOW>')]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words, decoded_ids = [], []  # decoded_ids を追加\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs, device=device)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            decoded_ids.append(int(topi.squeeze().detach())) # decoded_ids に追加\n",
        "            if topi.item() == target_vocab.index('<EOW>'):\n",
        "                decoded_words.append('<EOW>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(target_vocab[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoded_ids, decoder_attentions[:di + 1]  # decoded_ids を返すように変更\n",
        "        #return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "    \n",
        "#from lam import calc_accuracy\n",
        "def calc_accuracy(\n",
        "    _dataset,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    max_length=None,\n",
        "    source_vocab=None,\n",
        "    target_vocab=None,\n",
        "    source_ids=None,\n",
        "    target_ids=None,\n",
        "    isPrint=False):\n",
        "\n",
        "    ok_count = 0\n",
        "    for i in range(_dataset.__len__()):\n",
        "        _input_ids, _target_ids = _dataset.__getitem__(i)\n",
        "        _output_words, _output_ids, _attentions = evaluate(\n",
        "            encoder=encoder,\n",
        "            decoder=decoder,\n",
        "            input_ids=_input_ids,\n",
        "            max_length=max_length,\n",
        "            source_vocab=source_vocab,\n",
        "            target_vocab=target_vocab,\n",
        "            source_ids=source_ids,\n",
        "            target_ids=target_ids,\n",
        "        )\n",
        "        ok_count += 1 if _target_ids == _output_ids else 0\n",
        "        if (_target_ids != _output_ids) and (isPrint):\n",
        "            print(i, _target_ids == _output_ids, _output_words, _input_ids, _target_ids)\n",
        "\n",
        "    return ok_count/_dataset.__len__()\n",
        "\n",
        "\n",
        "encoder = EncoderRNN(\n",
        "    len(source_vocab), \n",
        "    params['hidden_size']).to(device)\n",
        "\n",
        "decoder = AttnDecoderRNN(\n",
        "    n_hid=params['hidden_size'], \n",
        "    n_out=len(target_vocab), \n",
        "    dropout_p=params['dropout_p'],\n",
        "    max_length=_vocab.max_length).to(device)\n",
        "    \n",
        "if (params['pretrained']) and (params['path_saved'] != False) and os.path.exists(params['path_saved']):\n",
        "    \"\"\"セーブした学習済のモデルがあれば読み込む\"\"\"\n",
        "    \n",
        "    checkpoint = torch.load(params['path_saved'])\n",
        "    encoder.load_state_dict(checkpoint['encoder'])\n",
        "    decoder.load_state_dict(checkpoint['decoder'])\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    print(colored(f\"セーブした学習済のモデル {params['path_saved']} があるので読み込みました\",\n",
        "          color='blue', attrs=['bold']))\n",
        "\n",
        "    \n",
        "# モデルの概要を印字\n",
        "print(f'encoder:{encoder}')\n",
        "print(f'decoder:{decoder}')\n",
        "        \n",
        "for test_name, val_dataset in X_vals.items():\n",
        "    acc = calc_accuracy(_dataset=val_dataset,\n",
        "                        encoder=encoder,\n",
        "                        decoder=decoder,\n",
        "                        max_length=_vocab.max_length,\n",
        "                        source_vocab=source_vocab,\n",
        "                        target_vocab=target_vocab,\n",
        "                        source_ids=source_ids,\n",
        "                        target_ids=target_ids)\n",
        "    print(colored(f'{test_name} の精度:{acc:.3f}','blue', attrs=['bold']))\n",
        "\n",
        "\n",
        "# params の印刷\n",
        "print(colored(params,'blue',attrs=['bold']))    "
      ],
      "metadata": {
        "id": "_02HgFOJldj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 `_train`, `_fit` など下請け関数の定義"
      ],
      "metadata": {
        "id": "An8vrQp0lqc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def asMinutes(s:int)->str:\n",
        "    \"\"\"時間変数を見やすいように，分と秒に変換して返す\"\"\"\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return f'{int(m):2d}分 {int(s):2d}秒'\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since:time.time,\n",
        "            percent:time.time)->str:\n",
        "    \"\"\"開始時刻 since と，現在の処理が全処理中に示す割合 percent を与えて，経過時間と残り時間を計算して表示する\"\"\"\n",
        "    now = time.time()  #現在時刻を取得\n",
        "    s = now - since    # 開始時刻から現在までの経過時間を計算\n",
        "    #s = since - now\n",
        "    es = s / (percent) # 経過時間を現在までの処理割合で割って終了予想時間を計算\n",
        "    rs = es - s        # 終了予想時刻から経過した時間を引いて残り時間を計算\n",
        "\n",
        "    return f'経過時間:{asMinutes(s)} (残り時間 {asMinutes(rs)})'\n",
        "\n",
        "\n",
        "def check_vals_performance(encoder=None, \n",
        "                           decoder=None,\n",
        "                           _dataset=None,\n",
        "                           max_length=0,\n",
        "                           source_vocab=None, \n",
        "                           target_vocab=None,\n",
        "                           source_ids=None, \n",
        "                           target_ids=None):\n",
        "\n",
        "    if _dataset == None or encoder == None or decoder == None or max_length == 0 or source_vocab == None:\n",
        "        return\n",
        "    print('検証データ:',end=\"\")\n",
        "    for _x in _dataset:\n",
        "        ok_count = 0\n",
        "        for i in range(_dataset[_x].__len__()):\n",
        "            _input_ids, _target_ids = _dataset[_x].__getitem__(i)\n",
        "            _output_words, _output_ids, _attentions = evaluate(encoder, \n",
        "                                                               decoder, \n",
        "                                                               _input_ids,\n",
        "                                                               max_length,\n",
        "                                                               source_vocab=source_vocab, \n",
        "                                                               target_vocab=target_vocab,\n",
        "                                                               source_ids=source_ids, \n",
        "                                                               target_ids=target_ids)\n",
        "            ok_count += 1 if _target_ids == _output_ids else 0\n",
        "        print(f'{_x}:{ok_count/_dataset[_x].__len__():.3f},',end=\"\")\n",
        "    print()\n",
        "\n",
        "\n",
        "def _train(input_tensor:torch.Tensor=None, \n",
        "           target_tensor:torch.Tensor=None,\n",
        "           encoder:torch.nn.Module=None, \n",
        "           decoder:torch.nn.Module=None,\n",
        "           encoder_optimizer:torch.optim=None, \n",
        "           decoder_optimizer:torch.optim=None,\n",
        "           criterion:torch.nn.modules.loss=torch.nn.modules.loss.CrossEntropyLoss,\n",
        "           max_length:int=_vocab.max_length,\n",
        "           target_vocab:list=None,\n",
        "           teacher_forcing_ratio:float=0.,\n",
        "           device:torch.device=device)->float:\n",
        "    \n",
        "    \"\"\"inpute_tensor (torch.Tensor() に変換済の入力系列) を 1 つ受け取って，\n",
        "    encoder と decoder の訓練を行う\n",
        "    \"\"\"\n",
        "    \n",
        "    encoder_hidden = encoder.initHidden() # 符号化器の中間層を初期化\n",
        "    encoder_optimizer.zero_grad()         # 符号化器の最適化関数の初期化\n",
        "    decoder_optimizer.zero_grad()         # 復号化器の最適化関数の初期化\n",
        "\n",
        "    input_length = input_tensor.size(0)   # 0 次元目が系列であることを仮定\n",
        "    target_length = target_tensor.size(0)\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.n_hid, device=device)\n",
        "    \n",
        "    loss = 0.  # 損失関数値\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            inp=input_tensor[ei], \n",
        "            hid=encoder_hidden, \n",
        "            device=device)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[target_vocab.index('<SOW>')]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    ok_flag = True\n",
        "    # 教師強制をするか否かを確率的に決める\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    \n",
        "    if use_teacher_forcing: # 教師強制する場合 Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, \n",
        "                                                                        decoder_hidden, \n",
        "                                                                        encoder_outputs,\n",
        "                                                                        device=device)\n",
        "            decoder_input = target_tensor[di]      # 教師強制 する\n",
        "            \n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            ok_flag = (ok_flag) and (decoder_output.argmax() == target_tensor[di].detach().numpy()[0])\n",
        "            if decoder_input.item() == target_vocab.index('<EOW>'):\n",
        "                break\n",
        "\n",
        "    else: # 教師強制しない場合 Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, \n",
        "                                                                        decoder_hidden, \n",
        "                                                                        encoder_outputs,\n",
        "                                                                        device=device)\n",
        "            topv, topi = decoder_output.topk(1)     # 教師強制しない\n",
        "            decoder_input = topi.squeeze().detach() \n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            ok_flag = (ok_flag) and (decoder_output.argmax() == target_tensor[di].detach().numpy()[0])\n",
        "            if decoder_input.item() == target_vocab.index('<EOW>'):\n",
        "                break\n",
        "\n",
        "    loss.backward()           # 誤差逆伝播\n",
        "    encoder_optimizer.step()  # encoder の学習\n",
        "    decoder_optimizer.step()  # decoder の学習\n",
        "    return loss.item() / target_length, ok_flag\n",
        "\n",
        "\n",
        "def _fit(encoder:torch.nn.Module, \n",
        "         decoder:torch.nn.Module,\n",
        "         epochs:int=1,\n",
        "         lr:float=0.0001,\n",
        "         n_sample:int=3,\n",
        "         teacher_forcing_ratio=False,\n",
        "         train_dataset:torch.utils.data.Dataset=None,\n",
        "         val_dataset:dict=None,\n",
        "         source_vocab:list=None,\n",
        "         target_vocab:list=None,\n",
        "         source_ids:str=None,\n",
        "         target_ids:list=None,\n",
        "         params:dict=None,\n",
        "         max_length:int=1,\n",
        "         device=device,\n",
        "        )->list:\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    encoder_optimizer = params['optim_func'](encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = params['optim_func'](decoder.parameters(), lr=lr)\n",
        "    criterion = params['loss_func']\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        ok_count = 0\n",
        "        \n",
        "        #エポックごとに学習順をシャッフルする\n",
        "        learning_order = np.random.permutation(train_dataset.__len__())\n",
        "        \n",
        "        for i in range(train_dataset.__len__()):\n",
        "            x = learning_order[i]   # ランダムにデータを取り出す\n",
        "            input_ids, target_ids = train_dataset.__getitem__(x)\n",
        "            input_tensor = convert_ids2tensor(input_ids)\n",
        "            target_tensor = convert_ids2tensor(target_ids)\n",
        "\n",
        "            #訓練の実施\n",
        "            loss, ok_flag = _train(input_tensor=input_tensor, \n",
        "                                   target_tensor=target_tensor,\n",
        "                                   encoder=encoder, \n",
        "                                   decoder=decoder,\n",
        "                                   encoder_optimizer=encoder_optimizer, \n",
        "                                   decoder_optimizer=decoder_optimizer,\n",
        "                                   criterion=criterion,\n",
        "                                   max_length=max_length,\n",
        "                                   target_vocab=target_vocab,\n",
        "                                   teacher_forcing_ratio=teacher_forcing_ratio,\n",
        "                                   device=device)\n",
        "            epoch_loss += loss\n",
        "            ok_count += 1 if ok_flag else 0\n",
        "\n",
        "\n",
        "        losses.append(epoch_loss/train_dataset.__len__())\n",
        "        print(colored(f'エポック:{epoch:2d} 損失:{epoch_loss/train_dataset.__len__():.2f}', 'blue', attrs=['bold']),\n",
        "              colored(f'{timeSince(start_time, (epoch+1) * train_dataset.__len__()/(epochs * train_dataset.__len__()))}',\n",
        "                      'cyan', attrs=['bold']),\n",
        "              colored(f'訓練データの精度:{ok_count/train_dataset.__len__():.3f}', 'blue', attrs=['bold']))\n",
        "\n",
        "        check_vals_performance(_dataset=val_dataset,\n",
        "                               encoder=encoder,\n",
        "                               decoder=decoder,\n",
        "                               max_length=max_length,\n",
        "                               source_vocab=source_vocab,\n",
        "                               target_vocab=target_vocab,\n",
        "                               source_ids=source_ids,\n",
        "                               target_ids=target_ids)\n",
        "        if n_sample > 0:\n",
        "            evaluateRandomly(encoder, decoder, n=n_sample)\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "h4Sq5RFeljZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 一文字データの学習"
      ],
      "metadata": {
        "id": "sE9cGKmZmBZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "id": "7jeyjswXmGsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "losses += _fit(encoder=encoder, \n",
        "               decoder=decoder, \n",
        "               device=device,\n",
        "               epochs=10,\n",
        "               #epochs=params['epochs'], \n",
        "               max_length=_vocab.max_length,\n",
        "               n_sample=0,\n",
        "               params=params,\n",
        "               source_vocab=source_vocab,\n",
        "               target_vocab=target_vocab,\n",
        "               source_ids=source_ids,\n",
        "               target_ids=target_ids,\n",
        "               teacher_forcing_ratio=params['teacher_forcing_ratio'],\n",
        "               #train_dataset=train_dataset,\n",
        "               train_dataset=onechar_dataset,\n",
        "               #lr=params['lr'],\n",
        "               lr=0.001,\n",
        "               val_dataset=None,\n",
        "               #val_dataset=X_vals,\n",
        "              )"
      ],
      "metadata": {
        "id": "7pzrnqUWmEsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aYKY1iolnQpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "irg8HDOLmLv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lt *.pt"
      ],
      "metadata": {
        "id": "UaeuwLmHmSFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.load('2023_0123hoge.pt')\n",
        "enocder=X['encoder']\n",
        "decoder=X['decoder']"
      ],
      "metadata": {
        "id": "4oRDSPnIm_7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 1\n",
        "for k1 in fushimi1999.keys():\n",
        "    k1_old = \"\"\n",
        "    for w1 in fushimi1999[k1]:\n",
        "        ans=evaluate(encoder,\n",
        "                     decoder,\n",
        "                     [_vocab.orth_vocab.index(w1[0]),_vocab.orth_vocab.index(w1[1]),1],\n",
        "                     34,\n",
        "                     source_vocab,\n",
        "                     target_vocab,\n",
        "                     source_ids,\n",
        "                     target_ids)\n",
        "        if k1 != k1_old:\n",
        "            k1_old = k1\n",
        "            print(colored(f'{k1}:', 'cyan', attrs=['bold']), end=\"\\n\")\n",
        "        counter =  1 if (counter % 10) == 0 else (counter + 1)\n",
        "        _end = \"\\n\" if counter==1 else \", \"\n",
        "        print(f'{w1}:{\"\".join(p for p in ans[0][:-1])}', end=_end)"
      ],
      "metadata": {
        "id": "Gux1_SYNnEOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 訓練データセットの定義"
      ],
      "metadata": {
        "id": "QtO6X02Cnso8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練データセットと検証データセットを作成\n",
        "train_dataset = Train_dataset(data=_vocab.train_data,\n",
        "                              source_vocab=source_vocab, \n",
        "                              target_vocab=target_vocab,\n",
        "                              source_ids=source_ids,   # おそらくこの 2 行を入れないといけなかった\n",
        "                              target_ids=target_ids)  # そうでなければ，デフォルトの `mora_p_r` になってしまう\n",
        "\n",
        "P  = int(train_dataset.__len__() * 0.9)\n",
        "_P = train_dataset.__len__() - P\n",
        "_train_dataset, _val_dataset = torch.utils.data.random_split(dataset=train_dataset,\n",
        "                                                            lengths=(P, _P),\n",
        "                                                            generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "                \n",
        "# 訓練データセットと検証データセットを作成\n",
        "print(f'len(_train_dataset):{len(_train_dataset)}',\n",
        "      f'len(_val_dataset):{len(_val_dataset)}')      "
      ],
      "metadata": {
        "id": "fJ3_Vpxant5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 本来のデータセットの学習"
      ],
      "metadata": {
        "id": "sYUKgfpIoHX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "losses += _fit(encoder=encoder, \n",
        "               decoder=decoder, \n",
        "               device=device,\n",
        "               #epochs=1,\n",
        "               epochs=params['epochs'], \n",
        "               max_length=_vocab.max_length,\n",
        "               n_sample=0,\n",
        "               params=params,\n",
        "               source_vocab=source_vocab,\n",
        "               target_vocab=target_vocab,\n",
        "               source_ids=source_ids,\n",
        "               target_ids=target_ids,\n",
        "               teacher_forcing_ratio=params['teacher_forcing_ratio'],\n",
        "               train_dataset=_train_dataset,\n",
        "               #train_dataset=onechar_dataset,\n",
        "               lr=params['lr'],\n",
        "               #val_dataset=None,\n",
        "               val_dataset={'_val':_val_dataset}\n",
        "              )"
      ],
      "metadata": {
        "id": "R0BhfFVYnV6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ppk1z_EXoX3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 訓練済モデルの保存"
      ],
      "metadata": {
        "id": "8iN_HKw7oTYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(params)\n",
        "torch.save({'encoder': encoder,'decoder': decoder, 'params':params}, '2023_0123lam_o2p_hid64.pt')"
      ],
      "metadata": {
        "id": "T836uAoQngh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_fushimi1999_list(encoder=encoder,\n",
        "                       decoder=decoder):\n",
        "\n",
        "    counter = 1\n",
        "    for k1 in fushimi1999.keys():\n",
        "        k1_old = \"\"\n",
        "        for w1 in fushimi1999[k1]:\n",
        "            ans=evaluate(encoder,\n",
        "                         decoder,\n",
        "                         [_vocab.orth_vocab.index(w1[0]),_vocab.orth_vocab.index(w1[1]),1],\n",
        "                         _vocab.max_length,\n",
        "                         source_vocab,\n",
        "                         target_vocab,\n",
        "                         source_ids,\n",
        "                         target_ids)\n",
        "            if k1 != k1_old:\n",
        "                k1_old = k1\n",
        "                print(colored(f'{k1}:', 'cyan', attrs=['bold']), end=\"\\n\")\n",
        "            counter =  1 if (counter % 10) == 0 else (counter + 1)\n",
        "            _end = \"\\n\" if counter==1 else \", \"\n",
        "            print(f'{w1}',\n",
        "                  colored(f'/{\"\".join(p for p in ans[0][:-1])}/','grey', attrs=['bold']), end=_end)\n",
        "\n",
        "            \n",
        "check_fushimi1999_list()    "
      ],
      "metadata": {
        "id": "8yGIRPr5qtcM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}