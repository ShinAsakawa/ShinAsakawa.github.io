{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2023notebooks/2023_1124Knd_Ijn_Ask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0bf32fd7-3283-4e6e-abd2-70f5a046f3f4",
      "metadata": {
        "tags": [],
        "id": "0bf32fd7-3283-4e6e-abd2-70f5a046f3f4"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77b2be91-f5ca-4d9a-b4ee-599d114b1ae0",
      "metadata": {
        "tags": [],
        "id": "77b2be91-f5ca-4d9a-b4ee-599d114b1ae0"
      },
      "source": [
        "* date: 2023_1027\n",
        "* author: 浅川伸一\n",
        "* filename: 2023_1027Knd_Ijn_Ask.ipynb 名前変更，旧名は `2023_1027Knd_Ijn_Ask_s2p_p2s.ipynb` 理由は orthgraphy も追加したから。\n",
        "\n",
        "# 符号化器‐復号化器 (encoder-decoder a.k.a seq2seq) モデルによる，単語認識過程 beyond triangle\n",
        "\n",
        "`fit_seq2seq()`, `eval_seq2seq()` は，encoder 側が 系列データでも，埋め込みベクトルでも動作する。\n",
        "従って，o2o, o2p, p2o, p2p, s2o, s2p の 6 モデルはこれでよいようだ。\n",
        "残された，o2s, p2s, s2s を開発すれば良い。\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2006Kello_fig4.svg\" style=\"width:39%\">\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2006Kello_junction_fig5.svg\" style=\"width:39%\">\n",
        "<!-- <img src=\"2006Kello_fig4.svg\" width=\"39%\"><img src=\"2006Kello_junction_fig5.svg\" style=\"width:39%\"> -->\n",
        "<div style=\"background-color:lavender;text-align:left;width:66%\">\n",
        "左: RNN を用いた符号化器 (encoder) -復号化器 (decoder) モデル。\n",
        "右: Kello らの結節点 (junction) モデル。中央の語彙ノード (lexical nodes) 上の数字 45263 は，交差点モデルが扱うことが可能な語彙数。<br/>\n",
        "左: Kello+2006 Fig.4, 右: Kello+2006 Fig5\n",
        "</div>\n",
        "</center>\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2004Harm_Seidenberg_fig1_extended.svg\" style=\"width:39%\">\n",
        "<!-- <img src=\"2004Harm_Seidenberg_fig1_extended.svg\" width=\"39%\"> -->\n",
        "<div style=\"background-color:lavender;text-align:center;width:44%\">\n",
        "Harm\\&Seidenberg2004 Fig. 1 を改変。\n",
        "</div>\n",
        "</center>\n",
        "\n",
        "1. O(rthgraphy), P(honology), S(emnatics) のそれぞれに対して，ソースとターゲットと見立てた，9 つのデータセット，モデルを用意した。\n",
        "モデル名を下表に示す。\n",
        "表中の x2y は，ソースが x [o,p,s] でターゲットが y [o,p,s] であるモデルを意味する。\n",
        "カッコ内は，ソースとターゲットのそれぞれが，系列データであれば Seq であり，埋め込みベクトルデータであれば Vec である。\n",
        "\n",
        "|source\\target   | O   | P   |  S |\n",
        "|:--:|:--:|:---:|:--:|\n",
        "| O | o2o (Seq2seq)| o2p (Seq2Seq)| o2s (Seq2Vec)|\n",
        "| P | p2o (Seq2Seq)| p2p (Seq2Seq)| p2s (Seq2Vec)|\n",
        "| S | s2o (Vec2Seq)| s2p (Vec2Seq)| s2s (Vec2Vec)|\n",
        "\n",
        "ソースからターゲットへと系列データかベクトル埋め込みデータかによって，モデルは 4 種類に分類できる。\n",
        "\n",
        "1. 系列から系列へ: 4 (o2o, o2p, p2o, p2p)，\n",
        "2. 系列からベクトル埋め込みへ: 2 (o2s, p2s)\n",
        "3. ベクトル埋め込みから系列へ: 2 (s2o, s2p)\n",
        "4. ベクトル埋め込みからベクトル埋め込み 1\n",
        "\n",
        "## メモ\n",
        "\n",
        "1. 2023_1027 に近藤先生には，このコードのプロトタイプをお見せした。\n",
        "すなわち GitHub にアップロード済である。\n",
        "このファイルには，その後の改良が加えられている。\n",
        "ただし，最初のセルで MeCab をソースコードからダウンロードして，コンパイル & インストールに時間を要したため，実施まではお見せしていない。\n",
        "\n",
        "2. MeCab の使用は，未知語が入力として与えられた場合，仮のヨミを得るために使用している。\n",
        "上記の役割を除けば MeCab は不要だと判断し，MeCab の使用を中止した\n",
        "\n",
        "\n",
        "<center>\n",
        "<!-- <img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2004Harm_Seidenberg_fig4c.svg\"><br/>\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2004Harm_Seidenberg_fig4d.svg\"><br/> -->\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2004Harm_Seidenberg_fig4ab.svg\"><br/>\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2004Harm_Seidenberg_fig4c.svg\">\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2004Harm_Seidenberg_fig4d.svg\"><br/>\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/1999Levelt_blueprint.jpg\" width=49%\"><br/>\n",
        "<!-- <img src=\"2004Harm_Seidenberg_fig4ab.svg\"><br/>\n",
        "<img src=\"2004Harm_Seidenberg_fig4c.svg\">\n",
        "<img src=\"2004Harm_Seidenberg_fig4d.svg\"><br/> -->\n",
        "`Harm & Seidenberg (2004)`, Figure 4 c, and d,\n",
        "`Levelt 1999\n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/assets/2015Loung_fig1.svg\" width=\"24%\"><br/>\n",
        "ニューラル翻訳モデル。\n",
        "青色がソース言語モデル，赤がターゲット言語モデルである。\n",
        "ソース言語モデルの，最終時刻の中間層状態を，ターゲット言語モデルの開始時の中間層状態として用いる。\n",
        "Loung+2015 Fig.1 より。    \n",
        "</center>\n",
        "\n",
        "\n",
        "* 文献\n",
        "    * Harm & Seidenberg (2004) Computing the Meanings of Words in Reading: Cooperative Division of Labor Between Visual and Phonological Processes, Psychological Review, DOI:10.1037/0033-295X.111.3.662\n",
        "    * Seq2seq 翻訳モデル: Sutskever+ (2014) Sequence to Sequence Learning with Neural Networks, [arXiv:1409.3215](https://arxiv.org/abs/1409.3215)\n",
        "    * 注意つき符号化器‐復号化器モデル: Bahdanau, Cho, & Bengio (2015) NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE, [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)\n",
        "    * もう一つの注意つき符号化器‐復号化器モデル Luong+ (2015) Effective Approaches to Attention-based Neural Machine Translation, [arXiv:1508.04025](https://arxiv.org/abs/1508.04025)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b78c605b-fbc7-462c-b614-a5290fc29b3c",
      "metadata": {
        "id": "b78c605b-fbc7-462c-b614-a5290fc29b3c"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a0c3143-7e2a-46a8-9178-9d2b55bffe44",
      "metadata": {
        "id": "1a0c3143-7e2a-46a8-9178-9d2b55bffe44"
      },
      "source": [
        "## 共通のハイパーパラメータ宣言"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5257050f-e3c3-4f4b-83ae-27b1e17bf2d9",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5257050f-e3c3-4f4b-83ae-27b1e17bf2d9",
        "outputId": "eeaab6b3-5a2d-4cfc-ced5-c109ee9330ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 1024,\n",
              " 'adam_betas': (0.9, 0.98),\n",
              " 'adam_eps': 1e-09,\n",
              " 'adam_lr': 0.001,\n",
              " 'n_hid': 128,\n",
              " 'n_layers': 1,\n",
              " 'bidirectional': False}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Definition of hyper parameters\n",
        "config = {\n",
        "    'batch_size': 1024,\n",
        "    'adam_betas':(0.9, 0.98),\n",
        "    'adam_eps':1e-9,\n",
        "    'adam_lr':0.001,\n",
        "    'n_hid': 128,\n",
        "    'n_layers': 1,\n",
        "    'bidirectional': False,\n",
        "}\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d49ff24c-ae69-494c-8869-2d447118c27e",
      "metadata": {
        "id": "d49ff24c-ae69-494c-8869-2d447118c27e"
      },
      "source": [
        "## 必要なライブラリの輸入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a81e137d-f5b3-412d-985d-191972ae9b11",
      "metadata": {
        "tags": [],
        "id": "a81e137d-f5b3-412d-985d-191972ae9b11"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import jaconv\n",
        "except ImportError:\n",
        "    !pip install jaconv\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "if isColab:\n",
        "    !pip install --upgrade termcolor==1.1\n",
        "from termcolor import colored\n",
        "\n",
        "try:\n",
        "    import RAM\n",
        "except ImportError:\n",
        "    !git clone https://github.com/ShinAsakawa/RAM.git\n",
        "    import RAM\n",
        "\n",
        "# 近藤先生との議論から音韻情報の代替案として，ローマ字表記を採用することとした。\n",
        "# このとき，訓令式の表記にすることとした。ヘボン式，パスポート式ではないことに注意\n",
        "try:\n",
        "    from kunrei import kunrei\n",
        "except ImportError:\n",
        "    !wget https://shinasakawa.github.io/2023notebooks/kunrei.py -O kunrei.py\n",
        "    from kunrei import kunrei"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba9c3bd-6967-4ce7-9425-1552f88cca02",
      "metadata": {
        "id": "0ba9c3bd-6967-4ce7-9425-1552f88cca02"
      },
      "source": [
        "## 意味表現として word2vec による意味埋め込みベクトルを使う"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9cd0ceb5-5c94-4531-80de-e08d8916f067",
      "metadata": {
        "tags": [],
        "id": "9cd0ceb5-5c94-4531-80de-e08d8916f067"
      },
      "outputs": [],
      "source": [
        "# word2vec のため gensim を使う\n",
        "import requests\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "import os\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "w2v_2017 = {\n",
        "    'cbow200': 'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz',\n",
        "    'sgns200': 'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_sgns.bin.gz',\n",
        "    'cbow300': 'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid300_win20_neg20_sgns.bin.gz',\n",
        "    'sgns300': 'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz'\n",
        "}\n",
        "\n",
        "w2v_2021 = {\n",
        "    'cbow128': { 'id': '1B9HGhLZOja4Xku5c_d-kMhCXn1LBZgDb',\n",
        "                'outfile': '2021_05jawiki_hid128_win10_neg10_cbow.bin.gz'},\n",
        "    'sgns128': { 'id': '1OWmFOVRC6amCxsomcRwdA6ILAA5s4y4M',\n",
        "                'outfile': '2021_05jawiki_hid128_win10_neg10_sgns.bin.gz'},\n",
        "    'cbow200': { 'id': '1JTkU5SUBU2GkURCYeHkAWYs_Zlbqob0s',\n",
        "                'outfile': '2021_05jawiki_hid200_win20_neg20_sgns.bin.gz'}\n",
        "}\n",
        "\n",
        "is2017=True\n",
        "\n",
        "if isColab:\n",
        "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "    if is2017:\n",
        "        response = requests.get(w2v_2017['cbow200'])\n",
        "        fname = w2v_2017['cbow200'].split('/')[-1]\n",
        "        with open(fname, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    else:\n",
        "        #訓練済 word2vec ファイルの取得\n",
        "        (f_id, outfile) = w2v_2021['sgns128']['id'], w2v_2021['sgns128']['outfile']\n",
        "        gdd.download_file_from_google_drive(file_id=f_id,\n",
        "                                            dest_path=outfile,\n",
        "                                            unzip=False,\n",
        "                                            showsize=True)\n",
        "\n",
        "if is2017:\n",
        "    w2v_base = os.path.join(HOME, 'study/2016wikipedia/') if not isColab else '.'\n",
        "    w2v_file = '2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz'\n",
        "    w2v_file = os.path.join(w2v_base, w2v_file)\n",
        "else:\n",
        "    w2v_base = os.path.join(HOME, 'study/2019attardi_wikiextractor.git/wiki_texts/AA') if isMac else '.'\n",
        "    w2v_file = '2021_05jawiki_hid128_win10_neg10_sgns.bin'\n",
        "\n",
        "w2v = KeyedVectors.load_word2vec_format(\n",
        "    w2v_file,\n",
        "    encoding='utf-8',\n",
        "    unicode_errors='replace',\n",
        "    binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11fb582c-3043-4c72-be98-fab860a4c135",
      "metadata": {
        "id": "11fb582c-3043-4c72-be98-fab860a4c135"
      },
      "source": [
        "## データセット Psylex71_Dataset の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9057cfa3-bd54-497c-950a-3c34576b8b64",
      "metadata": {
        "tags": [],
        "id": "9057cfa3-bd54-497c-950a-3c34576b8b64"
      },
      "outputs": [],
      "source": [
        "# データセットとしての Psylex71_Dataset の読み込み\n",
        "from RAM import Psylex71_Dataset\n",
        "\n",
        "psylex71_ds = Psylex71_Dataset(max_words=30000)\n",
        "print(f'psylex71_ds の単語数:{psylex71_ds.__len__()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd4995d2-8a4f-4721-8a79-eca7330b961b",
      "metadata": {
        "id": "bd4995d2-8a4f-4721-8a79-eca7330b961b"
      },
      "source": [
        "## データセットのヒストグラム描画"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ff4af1-2184-4304-bef0-64bf2504e40e",
      "metadata": {
        "tags": [],
        "id": "45ff4af1-2184-4304-bef0-64bf2504e40e"
      },
      "outputs": [],
      "source": [
        "from RAM import draw_word_char_histgram\n",
        "draw_word_char_histgram(_dict=psylex71_ds.data_dict, key='phon', title='音韻', figsize2=(8,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ddbc27f-5ebb-4fbd-9d75-da7585048341",
      "metadata": {
        "id": "1ddbc27f-5ebb-4fbd-9d75-da7585048341"
      },
      "source": [
        "## psylex71_ds に存在する全単語を word2vec の埋め込みベクトル行列にする"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94cd4aab-6dbb-41f4-bb02-b74f2768043c",
      "metadata": {
        "tags": [],
        "id": "94cd4aab-6dbb-41f4-bb02-b74f2768043c"
      },
      "outputs": [],
      "source": [
        "# psylex71_ds データから word2vec の埋め込みベクトル行列を得る\n",
        "_words = [dct['orth'] for dct in psylex71_ds.data_dict.values()]\n",
        "\n",
        "# gensim() の `vectors_for_all()` 関数を持ちて，望む語彙で構成される word2vec 単語埋め込みモデルを作成\n",
        "w2v_psylex71 = w2v.vectors_for_all(_words)\n",
        "\n",
        "# NaN データが入っている可能性がるので変換\n",
        "w2v_psylex71.vectors = np.nan_to_num(w2v_psylex71.vectors)\n",
        "print(f'w2v_psylex71.vectors.shape:{w2v_psylex71.vectors.shape}')\n",
        "words = w2v_psylex71.index_to_key\n",
        "#len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4baeee79-6f53-459d-b1ed-688788eae64b",
      "metadata": {
        "id": "4baeee79-6f53-459d-b1ed-688788eae64b"
      },
      "source": [
        "## psylex71 データセット中の単語における w2v の表示テスト"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bf14fdcf-d6b5-461e-ac0a-f5f9fe55a9fd",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf14fdcf-d6b5-461e-ac0a-f5f9fe55a9fd",
        "outputId": "a334aa37-eeb7-43e0-8fc2-49997ad9bc11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "単語を入力してください:\n"
          ]
        }
      ],
      "source": [
        "Wrd = input('単語を入力してください:')\n",
        "color = 'blue'\n",
        "while (Wrd != \"\"):\n",
        "    if Wrd in w2v_psylex71:\n",
        "        Idx = w2v_psylex71.key_to_index[Wrd]\n",
        "        print(f'入力単語 Wrd:{colored(Wrd, color, attrs=[\"bold\"])},',\n",
        "              f'対応する単語番号 Idx:{colored(Idx, color, attrs=[\"bold\"])},',\n",
        "              f'w2v_psylex71.get_index({Wrd}):{colored(w2v_psylex71.get_index(Wrd), color, attrs=[\"bold\"])}')\n",
        "    else:\n",
        "        print(colored(f'{Wrd} という単語はありません。','red', attrs=['bold']))\n",
        "    Wrd = input('単語を入力してください (終了するには改行のみを入力):')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f386089a-8e73-4640-a13d-71fbcd59c404",
      "metadata": {
        "id": "f386089a-8e73-4640-a13d-71fbcd59c404"
      },
      "source": [
        "## 書記素データの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46064f0e-355d-47cf-8eb5-9fdd983edfd2",
      "metadata": {
        "tags": [],
        "id": "46064f0e-355d-47cf-8eb5-9fdd983edfd2"
      },
      "outputs": [],
      "source": [
        "import RAM\n",
        "\n",
        "def _grapheme(words=words):\n",
        "    \"\"\"必要と思われる書記素リストを返す\"\"\"\n",
        "\n",
        "    num_alpha='０１２３４５６７８９ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ'\n",
        "    hira = 'あいうえおかがきぎくぐけげこごさざしじすずせぜそぞただちぢつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもやゆよらりるれろわゐゑをんぁぃぅぇっゃゅょゎ'+'ゔ'\n",
        "    kata = 'アイウエオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモヤユヨラリルレロワヰヱヲン'+'ヴヷヸヹヺァィゥヵヶェォッャョュヮ'\n",
        "    symbols='、。，．・：；？！゛゜´｀¨＾‾＿ヽヾゝゞ〃仝々〆〇ー—‐／＼〜‖｜…‥‘’“”（）〔〕［］｛｝〈〉《》「」『』【】＋−±×÷＝≠＜＞≦≧∞∴♂♀°′″℃¥＄¢£％＃＆＊＠§☆★○●◎◇' + '◆□■△▲▽▼※〒→←↑↓〓∈∋⊆⊇⊂⊃∪∩∧∨¬⇒⇔∀∃∠⊥⌒∂∇≡≒≪≫√∽∝∵∫∬Å‰♯♭♪†‡¶◯'\n",
        "    #greek='ΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩαβγδεζηθικλμνξοπρστυφχψω'\n",
        "    #rosian='АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
        "    #digit_symbols='①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳⑴⑵⑶⑷⑸⑹⑺⑻⑼⑽⑾⑿⒀⒁⒂⒃⒄⒅⒆⒇❶❷❸❹❺❻❼❽❾⒈⒉⒊⒋⒌⒍⒎⒏⒐'\n",
        "    #alpha_symbols='ⅠⅡⅢⅣⅤⅥⅦⅧⅨⅩⅪⅫⅰⅱⅲⅳⅴⅵⅶⅷⅸⅹⅺⅻ⒜⒝⒞⒟⒠⒡⒢⒣⒤⒥⒦⒧⒨⒩⒪⒫⒬⒭⒮⒯⒰⒱⒲⒳⒴⒵'\n",
        "    #units='㎜㎟㎝㎠㎤㎡㎥㎞㎢㎎㎏㏄㎖㎗ℓ㎘㎳㎲㎱㎰℉㏔㏋㎐㎅㎆㎇№㏍℡'\n",
        "    #suits='♤♧♡♢♠♣♥♦〠☎〄☞☜☝☟⇆⇄⇅⇨⇦⇧⇩'\n",
        "    #etc='①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳ⅠⅡⅢⅣⅤⅥⅦⅧⅨⅩ㍉㌔㌢㍍㌘㌧㌃㌶㍑㍗㌍㌦㌣㌫㍊㌻㎜㎝㎞㎎㎏㏄㎡㍻〝〟№㏍℡㊤㊥㊦㊧㊨㈱㈲㈹㍾㍽㍼≒≡∫∮∑√⊥∠∟⊿∵∩∪㊙'\n",
        "    #etc2='㍉㌢㍍㌔㌖㌅㌳㍎㌃㌶㌘㌕㌧㍑㍊㌹㍗㌍㍂㌣㌦㌻㌫㌀㌞㌪㌱㍇㍾㍽㍼㍻㍿∮∟⊿〝'\n",
        "\n",
        "    # RAM で作成済の常用漢字リストを用いて単漢字リストを作成\n",
        "    # 平成 22 年の改定により常用漢字は 2136 文字ある\n",
        "    chars_list = [ch for ch in num_alpha+hira+kata+symbols]+ RAM.chars_joyo().char_list\n",
        "    not_chars_list = []\n",
        "    for wrd in tqdm(words):\n",
        "        for ch in wrd:\n",
        "            if (ch not in chars_list) and (ch not in not_chars_list):\n",
        "                not_chars_list.append(ch)\n",
        "    not_chars_list = sorted(not_chars_list)\n",
        "    grapheme = chars_list + not_chars_list\n",
        "    # 上記の処理により grapheme には 2768 文字である。\n",
        "    # これに特殊トークン 4 つ ['<PAD>', '<SOW>', '<EOW>', '<UNK>'] を加えたリストを返す\n",
        "\n",
        "    return ['<PAD>', '<SOW>', '<EOW>', '<UNK>'] + grapheme\n",
        "\n",
        "grapheme = _grapheme()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ab15140-dca3-4f07-8080-4d8b10503bcc",
      "metadata": {
        "tags": [],
        "id": "2ab15140-dca3-4f07-8080-4d8b10503bcc"
      },
      "source": [
        "## データセットの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e1c5962c-2596-4e67-b7e0-3e58bae4f8bf",
      "metadata": {
        "tags": [],
        "id": "e1c5962c-2596-4e67-b7e0-3e58bae4f8bf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import gensim\n",
        "\n",
        "def _collate_fn(batch):\n",
        "    inps, tgts = list(zip(*batch))\n",
        "    inps = list(inps)\n",
        "    tgts = list(tgts)\n",
        "    return inps, tgts\n",
        "\n",
        "\n",
        "class psylex71_w2v_Dataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 #direction='s2p',  # ['s2p', 'p2s']\n",
        "                 source='seme',    # エンコーダ用 入力データ, ['orth', seme', 'phon'] のいずれか一つ\n",
        "                 target='phon',    # デコーダ用 出力データ ,  ['orth', seme', 'phon'] のいずれか一つ\n",
        "                 w2v:gensim.models.keyedvectors.KeyedVectors=w2v_psylex71,\n",
        "                 old_ds:RAM.dataset.Psylex71_Dataset=psylex71_ds,\n",
        "                 #mecab_yomi=yomi,\n",
        "                 grapheme:list=grapheme,\n",
        "                ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.ds_name = 'psylex71_'+source+\"2\"+target\n",
        "        self.source, self.target = source, target\n",
        "\n",
        "        self.w2v = w2v\n",
        "        self.old_ds = old_ds\n",
        "        #self.mecab_yomi = yomi         # 未知の単語が入力された場合 MeCab を使って読みをえるため\n",
        "        self.grapheme = grapheme\n",
        "\n",
        "        self.words = w2v.index_to_key  # gensim の KeyedVectors を利用して単語リストとする\n",
        "        self.W = w2v.vectors\n",
        "\n",
        "        # 訓令式に従った日本語ローマ字表記 `kurei.py` 参照\n",
        "        self.phoneme = ['<PAD>', '<SOW>', '<EOW>', '<UNK>', # 特殊トークン，純に，埋め草，語頭，語末，未知\n",
        "                        'a', 'i', 'u', 'e', 'o',            # 母音\n",
        "                        'a:', 'i:', 'u:', 'e:', 'o:',       # 長母音\n",
        "                        'N', 'Q',                           # 撥音，拗音\n",
        "                        'b', 'by', 'ch', 'd', 'dy', 'f', 'g', 'gy', 'h', 'hy', # 子音\n",
        "                        'j', 'k', 'ky', 'm', 'my', 'n', 'ny',  'p', 'py', 'r', # 子音\n",
        "                        'ry', 's', 'sy', 't', 'ty', 'w', 'y', 'z', 'zy']       # 子音\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx:int):\n",
        "        wrd = self.words[idx]\n",
        "\n",
        "        if self.source == 'phon':\n",
        "            src = torch.LongTensor(self.wrd2phon_ids(wrd))\n",
        "        elif self.source == 'seme':\n",
        "            src = torch.tensor(self.w2v.get_vector(idx))\n",
        "        elif self.source == 'orth':\n",
        "            src = torch.LongTensor(self.wrd2orth_ids(wrd))\n",
        "        else:\n",
        "            src = None\n",
        "\n",
        "        if self.target == 'phon':\n",
        "            tgt = torch.LongTensor(self.wrd2phon_ids(wrd))\n",
        "        elif self.target == 'seme':\n",
        "            tgt = torch.tensor(self.w2v.get_vector(idx))\n",
        "        elif self.target == 'orth':\n",
        "            tgt = torch.LongTensor(self.wrd2orth_ids(wrd))\n",
        "        else:\n",
        "            tgt = None\n",
        "\n",
        "        return src, tgt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.w2v)\n",
        "\n",
        "    def getitem(self,\n",
        "                idx:int):\n",
        "        wrd = self.words[idx]\n",
        "        _yomi = self.wrd2yomi(wrd)\n",
        "        _yomi = kunrei(_yomi).split(' ')\n",
        "        phon_ids = [self.phoneme.index(idx) for idx in _yomi]\n",
        "        orth_ids = [self.grapheme.index(idx) for idx in wrd]\n",
        "        return wrd, _yomi, phon_ids, orth_ids\n",
        "\n",
        "    def source_ids2source(self, ids:list):\n",
        "\n",
        "        if self.source == 'phon':\n",
        "            return self.phon_ids2phn(ids)\n",
        "        elif self.source == 'orth':\n",
        "            return self.orth_ids2orth(ids)\n",
        "        elif self.source == 'seme':\n",
        "            wrd = self.getitem(ids)[0]\n",
        "            return w2v.similar_by_word(wrd)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "    def target_ids2target(self, ids:list):\n",
        "\n",
        "        if self.target == 'phon':\n",
        "            return self.phon_ids2phn(ids)\n",
        "        elif self.target == 'orth':\n",
        "            return self.orth_ids2orth(ids)\n",
        "        elif self.target == 'seme':\n",
        "            wrd = self.getitem(ids)[0]\n",
        "            return w2v.similar_by_word(wrd)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "    def wrd2orth_ids(self, wrd:str)->list:\n",
        "        ids = [self.grapheme.index(ch) for ch in wrd]\n",
        "        ids = [self.grapheme.index('<SOW>')] + ids + [self.grapheme.index('<EOW>')]\n",
        "        #ids = [[self.grapheme.index('<SOW>')] + ids + [self.grapheme.index('<EOW>')]]\n",
        "        return ids\n",
        "\n",
        "    def wrd2phon_ids(self, wrd:str)->list:\n",
        "        _yomi = self.wrd2yomi(wrd)\n",
        "        _yomi = kunrei(_yomi).split(' ')\n",
        "        ids = [self.phoneme.index(idx) for idx in _yomi]\n",
        "        ids = [self.phoneme.index('<SOW>')] + ids + [self.phoneme.index('<EOW>')]\n",
        "        return ids\n",
        "\n",
        "    def get_wrdidx_from_word(self, wrd:str):\n",
        "        if wrd in self.words:\n",
        "            wrd_idx = self.w2v.get_index(wrd)\n",
        "        else:\n",
        "            wrd_idx = -1\n",
        "        return wrd_idx\n",
        "\n",
        "    def wrd2emb(self, wrd:str)->np.ndarray:\n",
        "        if wrd in self.words:\n",
        "            return self.w2v.get_vector(wrd)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def wrd2wrd_ids(self, wrd:str)->int:\n",
        "        if wrd in self.words:\n",
        "            return self.words.index(wrd)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def orth_ids2orth(self,\n",
        "                      ids:np.ndarray)->str:\n",
        "    #def orth_ids2orth(self, ids:list)->str:\n",
        "        ret = [self.grapheme[idx] for idx in ids]\n",
        "        return ret\n",
        "\n",
        "    def wrd_idx2wrd(self, idx:int)->str:\n",
        "        if 0 <= idx and idx < len(self.words):\n",
        "            return self.words[idx]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def wrd2onehot(self, wrd:str)->np.ndarray:\n",
        "        ret = np.zeros((self.W.shape[0],), dtype=np.int32)\n",
        "        if wrd in self.words:\n",
        "            ret[self.w2v.get_index(wrd)] = 1\n",
        "            return ret\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def phon_ids2phn(self, ids:np.ndarray):\n",
        "        ret = \"\".join([self.phoneme[idx] for idx in ids])\n",
        "        return ret\n",
        "\n",
        "    def wrd2yomi(self, wrd:str)->list:\n",
        "        if wrd in self.words:\n",
        "            _yomi = self.old_ds.orth2info_dict[wrd]['ヨミ']\n",
        "        else:\n",
        "            _yomi = self.mecab_yomi(wrd).strip().split()[0]\n",
        "        return _yomi\n",
        "\n",
        "    def wrd2info(self, wrd:str)->dict:\n",
        "        if wrd in self.words:\n",
        "            return self.old_ds.orth2info_dict[wrd]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "# 全部で 9 通りのデータセットを定義\n",
        "psylex71_ds_o2o = psylex71_w2v_Dataset(source='orth', target='orth')\n",
        "psylex71_ds_o2p = psylex71_w2v_Dataset(source='orth', target='phon')\n",
        "psylex71_ds_o2s = psylex71_w2v_Dataset(source='orth', target='seme')\n",
        "\n",
        "psylex71_ds_p2o = psylex71_w2v_Dataset(source='phon', target='orth')\n",
        "psylex71_ds_p2p = psylex71_w2v_Dataset(source='phon', target='phon')\n",
        "psylex71_ds_p2s = psylex71_w2v_Dataset(source='phon', target='seme')\n",
        "\n",
        "psylex71_ds_s2o = psylex71_w2v_Dataset(source='seme', target='orth')\n",
        "psylex71_ds_s2p = psylex71_w2v_Dataset(source='seme', target='phon')\n",
        "psylex71_ds_s2s = psylex71_w2v_Dataset(source='seme', target='seme')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab9e4b9d-c78a-43f7-8333-8af7eba1b507",
      "metadata": {
        "tags": [],
        "id": "ab9e4b9d-c78a-43f7-8333-8af7eba1b507"
      },
      "source": [
        "### 訓練データセットとテストデータへの分割。現時点で未使用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "355225fa-0f41-47d0-a44c-643a925eadfd",
      "metadata": {
        "tags": [],
        "id": "355225fa-0f41-47d0-a44c-643a925eadfd"
      },
      "outputs": [],
      "source": [
        "# N_train = int(_psylex71_ds.__len__() * 0.9)\n",
        "# N_test  = _psylex71_ds.__len__() - N_train\n",
        "# train_dataset, val_dataset = torch.utils.data.random_split(dataset=_psylex71_ds, lengths=(N_train, N_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b13512-63c0-4ab6-b300-cc5a49e29f41",
      "metadata": {
        "id": "17b13512-63c0-4ab6-b300-cc5a49e29f41"
      },
      "source": [
        "# モデルの定義\n",
        "\n",
        "<div style=\"font-family:serif;font-size:14pt;color:purple;font-weight:900\">\n",
        "\n",
        "PyTorch RNN モデルの実装に対する注意メモ\n",
        "    \n",
        "* Encoder 側のデータと Decoder 側のデータそれぞれに対して Padding の処理を行う。\n",
        "* Encoder 側のデータには Padding 値として `0` で埋める。\n",
        "* Decoder 側のデータをモデルの forward で使う場合には、Padding 値は `0` を埋める。\n",
        "* ただし，Decoder 側のデータを教師データとして使う場合には，Padding 値には -1 を用いて，埋めることに注意。\n",
        "* `nn.Embedding()` のオプションに `padding_idx=O` を付け，`CrosEntropyLoss` のオプションに `ignore_index=-1` を付ける。\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72e4ebad-75bb-4dd7-9d8a-00d328583e4d",
      "metadata": {
        "id": "72e4ebad-75bb-4dd7-9d8a-00d328583e4d"
      },
      "source": [
        "## 各モデルの宣言"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f03551ca-bfe7-40d7-b668-c05106e01a3d",
      "metadata": {
        "tags": [],
        "id": "f03551ca-bfe7-40d7-b668-c05106e01a3d"
      },
      "outputs": [],
      "source": [
        "# 全モデル共通使用するライブラリの輸入\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from RAM import Seq2Seq_wAtt\n",
        "from RAM import Vec2Seq\n",
        "from RAM import Seq2Vec\n",
        "from RAM import Vec2Vec\n",
        "\n",
        "from RAM import fit_seq2seq\n",
        "from RAM import fit_seq2vec\n",
        "from RAM import fit_seq2vec as fit_veq2seq\n",
        "#from RAM import fit_vec2vec\n",
        "\n",
        "from RAM import eval_seq2seq\n",
        "from RAM import eval_seq2vec\n",
        "#from RAM import eval_vec2seq\n",
        "#from RAM import eval_vec2vec\n",
        "\n",
        "\n",
        "n_layers = config['n_layers']\n",
        "bidirectional = config['bidirectional']\n",
        "n_hid = config['n_hid']\n",
        "\n",
        "print(colored('# 1 写字モデル o2o', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_o2o\n",
        "o2o = Seq2Seq_wAtt(\n",
        "    enc_vocab_size=len(ds.grapheme),\n",
        "    dec_vocab_size=len(ds.grapheme),\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(o2o.eval())\n",
        "\n",
        "print(colored('# 2 o2p 音読モデル意味関与なし', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_o2p\n",
        "o2p = Seq2Seq_wAtt(\n",
        "    enc_vocab_size=len(ds.grapheme),\n",
        "    dec_vocab_size=len(ds.phoneme),\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(o2p.eval())\n",
        "\n",
        "print(colored('# 3 o2s 印字理解モデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_o2s\n",
        "o2s = Seq2Vec(\n",
        "    enc_vocab_size=len(ds.grapheme),\n",
        "    sem_dim=ds.w2v.vector_size,\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(o2s.eval())\n",
        "\n",
        "print(colored('# 4 p2o 聞き書き ディクテーションモデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_p2o\n",
        "p2o = Seq2Seq_wAtt(\n",
        "    enc_vocab_size=len(ds.phoneme),\n",
        "    dec_vocab_size=len(ds.grapheme),\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(p2o.eval())\n",
        "\n",
        "print(colored('# 5 p2p 復唱モデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_p2p\n",
        "p2p = Seq2Seq_wAtt(\n",
        "    enc_vocab_size=len(ds.phoneme),\n",
        "    dec_vocab_size=len(ds.phoneme),\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(p2p.eval())\n",
        "\n",
        "print(colored('# 6 p2s 聴理解モデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_p2s\n",
        "p2s = Seq2Vec(\n",
        "    enc_vocab_size=len(ds.phoneme),\n",
        "    sem_dim=ds.w2v.vector_size,\n",
        "    n_hid=n_hid, n_layers=n_layers, bidirectional=bidirectional).to(device)\n",
        "print(p2s.eval())\n",
        "\n",
        "print(colored('# 7 s2o 書字モデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_s2o\n",
        "s2o = Vec2Seq(\n",
        "    sem_dim=ds.w2v.vector_size,\n",
        "    dec_vocab_size=len(ds.grapheme),\n",
        "    n_hid=n_hid, n_layers=n_layers, bidirectional=bidirectional).to(device)\n",
        "print(s2o.eval())\n",
        "\n",
        "print(colored('# 8 s2p 発話モデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_s2p\n",
        "s2p = Vec2Seq(\n",
        "    sem_dim=ds.w2v.vector_size,\n",
        "    dec_vocab_size=len(ds.phoneme),\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(s2p.eval())\n",
        "\n",
        "print(colored('# 9 s2s 意味理解モデル', color='blue', attrs=[\"bold\"]))\n",
        "ds = psylex71_ds_s2s\n",
        "s2s = Vec2Vec(\n",
        "    sem_dim=ds.w2v.vector_size,\n",
        "    n_hid=n_hid).to(device)\n",
        "print(s2s.eval())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b04e5db6-b498-4dac-add0-2c7207d36490",
      "metadata": {
        "tags": [],
        "id": "b04e5db6-b498-4dac-add0-2c7207d36490"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# 読字モデル\n",
        "errors_o2p = fit_seq2seq(epochs=5, model=o2p, ds=psylex71_ds_o2p, device=device)\n",
        "result_o2p = eval_seq2seq(model=o2p, ds=psylex71_ds_o2p, device=device)\n",
        "\n",
        "# 復唱モデル\n",
        "errors_p2p = fit_seq2seq(epochs=5, model=p2p, ds=psylex71_ds_p2p, device=device)\n",
        "result_p2p = eval_seq2seq(model=p2p, ds=psylex71_ds_p2p, device=device)\n",
        "\n",
        "# 書き取りディクテーションモデル\n",
        "errors_p2o = fit_seq2seq(epochs=10, model=p2o, ds=psylex71_ds_p2o, device=device)\n",
        "result_p2o = eval_seq2seq(model=p2o, ds=psylex71_ds_p2o, device=device)\n",
        "\n",
        "# 写字モデル\n",
        "errors_o2o = fit_seq2seq(epochs=10, model=o2o, ds=psylex71_ds_o2o, device=device)\n",
        "result_o2o = eval_seq2seq(model=o2o, ds=psylex71_ds_o2o, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58c86846-795c-40bf-ad4c-4d9befb0e8b3",
      "metadata": {
        "tags": [],
        "id": "58c86846-795c-40bf-ad4c-4d9befb0e8b3"
      },
      "outputs": [],
      "source": [
        "results = ['result_o2p', 'result_o2o', 'result_p2p', 'result_p2o']\n",
        "#results = ['result_o2p', 'result_p2o']\n",
        "for result in results:\n",
        "    print(result, eval(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8d270de-d05e-474e-bbaf-d85e46b83e55",
      "metadata": {
        "id": "b8d270de-d05e-474e-bbaf-d85e46b83e55"
      },
      "source": [
        "## o2p 結果をファイルに保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74aeb5b-ec4b-4cb5-8bbf-fdb26016f6ff",
      "metadata": {
        "tags": [],
        "id": "c74aeb5b-ec4b-4cb5-8bbf-fdb26016f6ff"
      },
      "outputs": [],
      "source": [
        "o2p.eval()\n",
        "fname_saved = '2023_1117o2p_h128.pt'\n",
        "torch.save(o2p.state_dict(), fname_saved)\n",
        "state_dict = torch.load(fname_saved)\n",
        "\n",
        "ds = psylex71_ds_o2p\n",
        "_o2p = Seq2Seq_wAtt(\n",
        "    enc_vocab_size=len(ds.grapheme),\n",
        "    dec_vocab_size=len(ds.phoneme),\n",
        "    n_layers=n_layers, bidirectional=bidirectional, n_hid=n_hid).to(device)\n",
        "print(_o2p.eval())\n",
        "\n",
        "_o2p.load_state_dict(state_dict)\n",
        "_error_o2p = eval_seq2seq(model=_o2p, ds=psylex71_ds_o2p, device=device)\n",
        "#print(errors['正解率'])\n",
        "_error_o2p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b6f3067-c068-464d-ab45-fdfdb533abbc",
      "metadata": {
        "id": "8b6f3067-c068-464d-ab45-fdfdb533abbc"
      },
      "outputs": [],
      "source": [
        "# o2s 黙読モデル\n",
        "## モデルの訓練\n",
        "errors_o2s = fit_seq2vec(epochs=1, model=o2s, ds=psylex71_ds_o2s, device=device)\n",
        "\n",
        "## モデルの結果評価\n",
        "result_o2s = eval_seq2vec(model=o2s, ds=psylex71_ds_o2s, isPrint=True, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292c9a02-a909-4d6b-a967-a333731b69a0",
      "metadata": {
        "id": "292c9a02-a909-4d6b-a967-a333731b69a0"
      },
      "outputs": [],
      "source": [
        "# p2s 聞き取りモデル\n",
        "## モデルの訓練\n",
        "errors_p2s = fit_seq2vec(epochs=1, model=p2s, ds=psylex71_ds_p2s, device=device)\n",
        "\n",
        "## モデルの結果評価\n",
        "eval_seq2vec(model=p2s, ds=psylex71_ds_p2s, isPrint=True, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b0e2cad-a74b-4a2a-9d8e-7bcc4aaa3bf4",
      "metadata": {
        "id": "4b0e2cad-a74b-4a2a-9d8e-7bcc4aaa3bf4"
      },
      "outputs": [],
      "source": [
        "# s2o 書き出しモデル\n",
        "## モデルの訓練\n",
        "errors_s2o = fit_seq2seq(epochs=1, model=s2o, ds=psylex71_ds_s2o)\n",
        "#errors_s2o = fit_seq2seq(epochs=15, model=s2o, ds=psylex71_ds_s2o)\n",
        "\n",
        "## モデルの結果評価\n",
        "result_s2o = eval_seq2seq(model=s2o, ds=psylex71_ds_s2o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d340a6ce-ef05-45ce-bbe5-e4be2b3b9dd8",
      "metadata": {
        "tags": [],
        "id": "d340a6ce-ef05-45ce-bbe5-e4be2b3b9dd8"
      },
      "outputs": [],
      "source": [
        "# s2p 発話モデル\n",
        "## モデルの訓練\n",
        "errors_s2p = fit_seq2seq(epochs=15, model=s2p, ds=psylex71_ds_s2p)\n",
        "\n",
        "## モデルの結果評価\n",
        "result_s2p = eval_seq2seq(model=s2p, ds=psylex71_ds_s2p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1817467e-6c2f-41da-8f37-db6963295e2d",
      "metadata": {
        "tags": [],
        "id": "1817467e-6c2f-41da-8f37-db6963295e2d"
      },
      "outputs": [],
      "source": [
        "# s2s 自発的納得モデル\n",
        "start_time = time.time()   # 開始時刻の保存\n",
        "\n",
        "dataloader = dl_s2s\n",
        "\n",
        "model = s2s\n",
        "# 最適化手法の定義\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "# 訓練ループ\n",
        "model.train()\n",
        "interval = int(ds.__len__()/batch_size) >> 2\n",
        "losses = []\n",
        "\n",
        "dataloader = dl_s2s\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    i = 0\n",
        "    for _inp, _tch in dataloader:\n",
        "\n",
        "        enc_inp = pad_sequence(_inp, batch_first=True).to(device)\n",
        "        dec_inp = pad_sequence(_tch, batch_first=True).to(device)\n",
        "        tch = pad_sequence(_tch, batch_first=True, padding_value=-1.0).to(device)\n",
        "\n",
        "        out = model(enc_inp, dec_inp)\n",
        "        loss = criterion(out[0], tch[0])\n",
        "        for h in range(1,len(tch)):\n",
        "            loss += criterion(out[h], tch[h])\n",
        "        losses.append(loss.item()/batch_size)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        i += 1\n",
        "        if (i % interval) == 0:\n",
        "            print(f'epoch:{epoch:2d}, batch:{i:2d}, loss:{loss.item()/batch_size:.3f}')\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "print(f'Training time {total_time_str}')\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title(f'epochs:{epochs}, batch_size:{batch_size}, n_hid:{n_hid}, n_layers:{n_layers}, time collapsed:{total_time_str}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54575f98-cb76-44d2-9f77-d3cec05b42aa",
      "metadata": {
        "tags": [],
        "id": "54575f98-cb76-44d2-9f77-d3cec05b42aa"
      },
      "outputs": [],
      "source": [
        "errors_s2p = fit_seq2seq(epochs=15, model=s2p, ds=psylex71_ds_s2p)\n",
        "result_s2p = eval_seq2seq(model=s2p, ds=psylex71_ds_s2p)\n",
        "\n",
        "errors_s2o = fit_seq2seq(epochs=15, model=s2o, ds=psylex71_ds_s2o)\n",
        "result_s2o = eval_seq2seq(model=s2o, ds=psylex71_ds_s2o)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd11ab9-8d0f-4e55-9946-5a60b193a70e",
      "metadata": {
        "tags": [],
        "id": "5cd11ab9-8d0f-4e55-9946-5a60b193a70e"
      },
      "source": [
        "# 書記+意味$\\rightarrow$音韻 os2p モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e31ef51-ab30-46a4-a136-69f0a9f6e873",
      "metadata": {
        "id": "3e31ef51-ab30-46a4-a136-69f0a9f6e873"
      },
      "source": [
        "## データセットの定義"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d1cc2c-8971-4179-9675-a25115904c86",
      "metadata": {
        "id": "03d1cc2c-8971-4179-9675-a25115904c86"
      },
      "source": [
        "### **<font style=\"color:teal\">書記符号化器から書記埋め込みベクトルを取り出す</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "742e19cc-5042-4003-a325-2ff763cfd707",
      "metadata": {
        "tags": [],
        "id": "742e19cc-5042-4003-a325-2ff763cfd707",
        "outputId": "985d4a7c-1968-4549-8d51-6fea0d9065c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orth_emb.shape:(26877, 128)\n",
            "len(grapheme):2772\n"
          ]
        }
      ],
      "source": [
        "orth_emb_vec = []\n",
        "_o2p.eval()\n",
        "ds = psylex71_ds_o2p\n",
        "for N in range(ds.__len__()):\n",
        "    inp, tch = ds.__getitem__(N)\n",
        "    enc_emb = _o2p.encoder_emb(inp.to(device))\n",
        "    enc_out, (hnx, cnx) = o2p.encoder(enc_emb)\n",
        "    orth_emb_vec.append(hnx.detach().squeeze(0).detach().cpu().numpy())\n",
        "orth_emb = np.array(orth_emb_vec)\n",
        "print(f'orth_emb.shape:{orth_emb.shape}')\n",
        "print(f'len(grapheme):{len(grapheme)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OS2P 用のデータセットを定義"
      ],
      "metadata": {
        "id": "zVdjR14k5fgB"
      },
      "id": "zVdjR14k5fgB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a31f85-b59d-4d63-8a69-363d964ad357",
      "metadata": {
        "tags": [],
        "id": "a3a31f85-b59d-4d63-8a69-363d964ad357"
      },
      "outputs": [],
      "source": [
        "class OS2P_Dataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 #phoneme:list=phoneme,\n",
        "                 grapheme:list=grapheme,\n",
        "                 orth_vecs:np.ndarray=orth_emb,\n",
        "                 w2v:gensim.models.keyedvectors.KeyedVectors=w2v_psylex71,\n",
        "                 seme_vecs:np.array=w2v_psylex71.vectors,\n",
        "                 old_ds:RAM.dataset.Psylex71_Dataset=psylex71_ds,\n",
        "                )->None:\n",
        "        self.ds_name = 'orth_seme_2_phon_dataset'\n",
        "        self.w2v = w2v\n",
        "\n",
        "        self.phoneme = ['<PAD>', '<SOW>', '<EOW>', '<UNK>', # 特殊トークン，純に，埋め草，語頭，語末，未知\n",
        "                        'a', 'i', 'u', 'e', 'o',            # 母音\n",
        "                        'a:', 'i:', 'u:', 'e:', 'o:',       # 長母音\n",
        "                        'N', 'Q',                           # 撥音，拗音\n",
        "                        'b', 'by', 'ch', 'd', 'dy', 'f', 'g', 'gy', 'h', 'hy', # 子音\n",
        "                        'j', 'k', 'ky', 'm', 'my', 'n', 'ny',  'p', 'py', 'r', # 子音\n",
        "                        'ry', 's', 'sy', 't', 'ty', 'w', 'y', 'z', 'zy']       # 子音\n",
        "\n",
        "        self.orth_vecs = orth_vecs\n",
        "        self.seme_vecs = seme_vecs\n",
        "        self.words = w2v.index_to_key\n",
        "        self.old_ds = old_ds\n",
        "        wrd2phn = {}\n",
        "        for wrd in self.words:\n",
        "            yomi = self.wrd2yomi(wrd)\n",
        "            phon = kunrei(yomi).split(' ')\n",
        "            wrd2phn[wrd] = phon\n",
        "        self.wrd2phn = wrd2phn\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words) # 936\n",
        "\n",
        "    def __getitem__(self, idx:int)->list:\n",
        "        orth_vec = torch.tensor(self.orth_vecs[idx]).to(device)\n",
        "        seme_vec = torch.tensor(self.seme_vecs[idx]).to(device)\n",
        "        inp = torch.cat((orth_vec, seme_vec)).to(device)\n",
        "\n",
        "        wrd = self.words[idx]\n",
        "        phn_ids = self.wrd2phon_ids(wrd)\n",
        "        tch = phn_ids\n",
        "        # phon_ids = self.wrd2phn[wrd]\n",
        "        # tch = [self.phoneme.index('<SOW>')]+phon_ids+[self.phoneme.index('<EOW>')]\n",
        "        # print(f'wrd:{wrd}, tch:{tch}, type(tch):{type(tch)}')\n",
        "        # sys.exit()\n",
        "        tch = torch.LongTensor(tch).to(device)\n",
        "        return inp, tch\n",
        "\n",
        "    def getitem(self, idx:int):\n",
        "        wrd = self.words[idx]\n",
        "        phon = self.wrd2phn[wrd]\n",
        "        phon_ids = [self.phoneme.index(p) for p in phon]\n",
        "        phon_ids = [self.phoneme.index('<SOW>')]+phon_ids+[self.phoneme.index('<EOW>')]\n",
        "        return wrd, phon, phon_ids\n",
        "\n",
        "    def phon_ids2phon(self, ids:list)->list:\n",
        "        return [self.phoneme[idx] for idx in ids]\n",
        "\n",
        "    def target_ids2target(self, ids:list):\n",
        "        return self.phon_ids2phon(ids)\n",
        "\n",
        "    def phon_ids2phon(self, ids:list)->list:\n",
        "        return [self.phoneme[idx] for idx in ids]\n",
        "\n",
        "    def wrd2yomi(self, wrd:str)->list:\n",
        "        if wrd in self.words:\n",
        "            _yomi = self.old_ds.orth2info_dict[wrd]['ヨミ']\n",
        "        # else:\n",
        "        #     _yomi = self.mecab_yomi(wrd).strip().split()[0]\n",
        "        return _yomi\n",
        "\n",
        "    def wrd2phon_ids(self, wrd:str)->list:\n",
        "        _yomi = self.wrd2yomi(wrd)\n",
        "        _yomi = kunrei(_yomi).split(' ')\n",
        "        ids = [self.phoneme.index(idx) for idx in _yomi]\n",
        "        ids = [self.phoneme.index('<SOW>')] + ids + [self.phoneme.index('<EOW>')]\n",
        "        return ids\n",
        "\n",
        "    def wrd2info(self, wrd:str)->dict:\n",
        "        if wrd in self.words:\n",
        "            return self.old_ds.orth2info_dict[wrd]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "os2p_ds = OS2P_Dataset()\n",
        "#print(os2p_ds.__len__())\n",
        "Ns = np.random.permutation(os2p_ds.__len__())\n",
        "for N in Ns[:3]:\n",
        "    inp, tch = os2p_ds.__getitem__(N)\n",
        "    print(f'inp.size():{inp.size()}')\n",
        "    print(f'n_hid:{n_hid}')\n",
        "    print(f'tch:{tch}')\n",
        "    print(f'os2p_ds.phon_ids2phon(tch.detach().cpu().numpy()):{os2p_ds.phon_ids2phon(tch.detach().cpu().numpy())}')\n",
        "    print(f'os2p_ds.words[0]:{os2p_ds.words[N]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの定義"
      ],
      "metadata": {
        "id": "y8CrLFNt5m20"
      },
      "id": "y8CrLFNt5m20"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc56343-50f5-4cf7-858d-98ff6b96649e",
      "metadata": {
        "tags": [],
        "id": "7dc56343-50f5-4cf7-858d-98ff6b96649e"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "class Vec2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 inp_dim:int,\n",
        "                 dec_vocab_size:int,\n",
        "                 n_hid:int,\n",
        "                 n_layers:int=2,\n",
        "                 decoder:nn.Module=o2p.decoder,\n",
        "                 bidirectional:bool=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder = copy.deepcopy(decoder)\n",
        "\n",
        "        # 単語の意味ベクトル a.k.a 埋め込み表現 を decoder の中間層に接続するための変換層\n",
        "        # 別解としては，入力層に接続する方法があるが，それはまた別実装にする\n",
        "        self.enc_transform_layer = nn.Linear(\n",
        "            in_features=inp_dim,\n",
        "            out_features=n_hid)\n",
        "        self.decoder_emb = nn.Embedding(\n",
        "            num_embeddings=dec_vocab_size,\n",
        "            embedding_dim=n_hid,\n",
        "            padding_idx=0)\n",
        "\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=n_hid,\n",
        "            hidden_size=n_hid,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional)\n",
        "\n",
        "        # 最終出力層\n",
        "        self.bi_fact = 2 if bidirectional else 1\n",
        "        self.out_layer = nn.Linear(self.bi_fact * n_hid, dec_vocab_size)\n",
        "\n",
        "    def forward(self, enc_inp, dec_inp):\n",
        "        enc_emb = self.enc_transform_layer(enc_inp.to(device))\n",
        "        hnx, cnx = enc_emb.clone(), enc_emb.clone()\n",
        "        hnx = hnx.unsqueeze(0)\n",
        "        cnx = cnx.unsqueeze(0)\n",
        "\n",
        "        if self.bi_fact == 2:\n",
        "            hnx = hnx.repeat(2)\n",
        "            cnx = cnx.repeat(2)\n",
        "\n",
        "        dec_emb = self.decoder_emb(dec_inp.to(device))\n",
        "\n",
        "        batch_size = enc_inp.size(0)\n",
        "        exp_hid_size = self.decoder.get_expected_hidden_size(enc_inp, batch_sizes=[batch_size])\n",
        "        dec_out, (hny, cny) = self.decoder(dec_emb,(hnx, cnx))\n",
        "\n",
        "        return self.out_layer(dec_out)\n",
        "\n",
        "# 以下確認作業\n",
        "ds = os2p_ds\n",
        "os2p = Vec2Seq(\n",
        "    inp_dim=ds.w2v.vector_size+n_hid,\n",
        "    dec_vocab_size=len(ds.phoneme),\n",
        "    n_hid=n_hid,\n",
        "    n_layers=n_layers,\n",
        "    bidirectional=bidirectional).to(device)\n",
        "print(os2p.eval())\n",
        "\n",
        "res = fit_seq2seq(epochs=1, model=os2p, ds=os2p_ds, interval=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b59c99e4-7f26-4135-8d12-c3be2b0c7865",
      "metadata": {
        "id": "b59c99e4-7f26-4135-8d12-c3be2b0c7865"
      },
      "source": [
        "## モデルの訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec27afd-71df-4297-96f8-112ce9a92382",
      "metadata": {
        "tags": [],
        "id": "1ec27afd-71df-4297-96f8-112ce9a92382"
      },
      "outputs": [],
      "source": [
        "res = fit_seq2seq(epochs=10, model=os2p, ds=os2p_ds, interval=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83c84efb-4b6d-40cd-a237-fba44dc17367",
      "metadata": {
        "tags": [],
        "id": "83c84efb-4b6d-40cd-a237-fba44dc17367"
      },
      "source": [
        "## 訓練結果の評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b983241c-f826-4d34-8adb-d53382426923",
      "metadata": {
        "tags": [],
        "id": "b983241c-f826-4d34-8adb-d53382426923"
      },
      "outputs": [],
      "source": [
        "os2p_errors = eval_seq2seq(model=os2p, ds=os2p_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "539eeb62-0bd9-4b7b-88d2-1a70ce9cdb63",
      "metadata": {
        "tags": [],
        "id": "539eeb62-0bd9-4b7b-88d2-1a70ce9cdb63"
      },
      "outputs": [],
      "source": [
        "os2p_errors.keys()\n",
        "for err in os2p_errors['エラー']:\n",
        "    print(err)\n",
        "print(f\"正解率: {os2p_errors['正解率']:.3f} %\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e282e402-56e2-4cde-b9ed-ed7665433f05",
      "metadata": {
        "tags": [],
        "id": "e282e402-56e2-4cde-b9ed-ed7665433f05"
      },
      "outputs": [],
      "source": [
        "print(os2p.eval())\n",
        "state_dict = os2p.state_dict()\n",
        "fname = '2023_1117os2ps_hid128.pt'\n",
        "torch.save(state_dict, fname)\n",
        "\n",
        "os2p_saved = Vec2Seq(\n",
        "    inp_dim=ds.w2v.vector_size+n_hid,\n",
        "    dec_vocab_size=len(ds.phoneme),\n",
        "    n_hid=n_hid,\n",
        "    n_layers=n_layers,\n",
        "    bidirectional=bidirectional).to(device)\n",
        "os2p_saved.load_state_dict(state_dict)\n",
        "os2p_saved_errors = eval_seq2seq(model=os2p_saved, ds=os2p_ds)\n",
        "#os2p_saved_errors.keys()\n",
        "for err in os2p_saved_errors['エラー']:\n",
        "    print(err)\n",
        "print(f\"正解率: {os2p_saved_errors['正解率']:.3f} %\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}