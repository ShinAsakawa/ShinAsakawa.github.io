{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNX+znxBrESMtKJseoBAoLb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2024notebooks/2024_0322ijn_knd_compare_2023former_latter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqMwKc_bcQgD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "\n",
        "#import pandas as pd\n",
        "#from RAM import\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import jaconv\n",
        "except ImportError:\n",
        "    !pip install jaconv\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "\n",
        "if isColab:\n",
        "    !pip install --upgrade termcolor==1.1\n",
        "from termcolor import colored\n",
        "\n",
        "try:\n",
        "    import RAM\n",
        "except ImportError:\n",
        "    !git clone https://github.com/ShinAsakawa/RAM.git\n",
        "    import RAM\n",
        "\n",
        "# 近藤先生との議論から音韻情報の代替案として，ローマ字表記を採用することとした。\n",
        "# このとき，訓令式の表記にすることとした。ヘボン式，パスポート式ではないことに注意\n",
        "try:\n",
        "    from kunrei import kunrei\n",
        "except ImportError:\n",
        "    !wget https://shinasakawa.github.io/2023notebooks/kunrei.py -O kunrei.py\n",
        "    from kunrei import kunrei"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2023Former\n",
        "## データセット Psylex71_Dataset の読み込み"
      ],
      "metadata": {
        "id": "Gs9y3puycZH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットとしての Psylex71_Dataset の読み込み\n",
        "from RAM import Psylex71_Dataset\n",
        "\n",
        "psylex71_ds1 = Psylex71_Dataset(max_words=30000)\n",
        "print(f'psylex71_ds1 の単語数:{psylex71_ds1.__len__()}')"
      ],
      "metadata": {
        "id": "hgp0g9GAcSD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "O = {}\n",
        "for i in range(psylex71_ds1.__len__()):\n",
        "    #print(psylex71_ds1.__getitem__(i))\n",
        "    o_ids, p_ids = psylex71_ds1.__getitem__(i)\n",
        "    src = psylex71_ds1.orth_ids2tkn(o_ids)\n",
        "    tgt = psylex71_ds1.phon_ids2tkn(p_ids)\n",
        "    #print(src, tgt, o_ids, p_ids)\n",
        "    O[i] = (src, tgt, o_ids, p_ids)"
      ],
      "metadata": {
        "id": "pVHqY4hYch76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# for idx in list(O.keys())[-3:]:\n",
        "#     print(idx, O[idx])"
      ],
      "metadata": {
        "id": "GRBwJTG3cmg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# help(pd.DataFrame.from_dict)\n",
        "O2 = pd.DataFrame.from_dict(O,orient='index', columns=['src','tgt','src_ids', 'tgt_ids'])\n",
        "\n",
        "fname = '2024_3022cnps_former.xlsx'\n",
        "O2.to_excel(fname)\n",
        "\n",
        "O3 = pd.read_excel(fname, index_col=0)\n",
        "#O3.head()\n",
        "O3.tail()"
      ],
      "metadata": {
        "id": "z98V2E1Oc6dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "O2.tail()"
      ],
      "metadata": {
        "id": "NwG1B7pCc9fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2023Latter\n",
        "## 意味表現として word2vec による意味埋め込みベクトルを使う"
      ],
      "metadata": {
        "id": "4w2X27tCdE-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# word2vec のため gensim を使う\n",
        "import requests\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "import os\n",
        "HOME = os.environ['HOME']\n",
        "\n",
        "w2v_2017 = {\n",
        "    'cbow200': 'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz',\n",
        "    'sgns200': 'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_sgns.bin.gz',\n",
        "    'cbow300': 'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid300_win20_neg20_sgns.bin.gz',\n",
        "    'sgns300': 'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz'\n",
        "}\n",
        "\n",
        "w2v_2021 = {\n",
        "    'cbow128': { 'id': '1B9HGhLZOja4Xku5c_d-kMhCXn1LBZgDb',\n",
        "                'outfile': '2021_05jawiki_hid128_win10_neg10_cbow.bin.gz'},\n",
        "    'sgns128': { 'id': '1OWmFOVRC6amCxsomcRwdA6ILAA5s4y4M',\n",
        "                'outfile': '2021_05jawiki_hid128_win10_neg10_sgns.bin.gz'},\n",
        "    'cbow200': { 'id': '1JTkU5SUBU2GkURCYeHkAWYs_Zlbqob0s',\n",
        "                'outfile': '2021_05jawiki_hid200_win20_neg20_sgns.bin.gz'}\n",
        "}\n",
        "\n",
        "is2017=True\n",
        "\n",
        "if isColab:\n",
        "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "    if is2017:\n",
        "        response = requests.get(w2v_2017['cbow200'])\n",
        "        fname = w2v_2017['cbow200'].split('/')[-1]\n",
        "        with open(fname, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    else:\n",
        "        #訓練済 word2vec ファイルの取得\n",
        "        (f_id, outfile) = w2v_2021['sgns128']['id'], w2v_2021['sgns128']['outfile']\n",
        "        gdd.download_file_from_google_drive(file_id=f_id,\n",
        "                                            dest_path=outfile,\n",
        "                                            unzip=False,\n",
        "                                            showsize=True)\n",
        "\n",
        "if is2017:\n",
        "    w2v_base = os.path.join(HOME, 'study/2016wikipedia/') if not isColab else '.'\n",
        "    w2v_file = '2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz'\n",
        "    w2v_file = os.path.join(w2v_base, w2v_file)\n",
        "else:\n",
        "    w2v_base = os.path.join(HOME, 'study/2019attardi_wikiextractor.git/wiki_texts/AA') if isMac else '.'\n",
        "    w2v_file = '2021_05jawiki_hid128_win10_neg10_sgns.bin'\n",
        "\n",
        "w2v = KeyedVectors.load_word2vec_format(\n",
        "    w2v_file,\n",
        "    encoding='utf-8',\n",
        "    unicode_errors='replace',\n",
        "    binary=True)"
      ],
      "metadata": {
        "id": "yvJnTtG2c_zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## psylex71_ds に存在する全単語を word2vec の埋め込みベクトル行列にする"
      ],
      "metadata": {
        "id": "CW_pg5QfelPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# psylex71_ds データから word2vec の埋め込みベクトル行列を得る\n",
        "_words = [dct['orth'] for dct in psylex71_ds1.data_dict.values()]\n",
        "\n",
        "# gensim() の `vectors_for_all()` 関数を持ちて，望む語彙で構成される word2vec 単語埋め込みモデルを作成\n",
        "w2v_psylex71 = w2v.vectors_for_all(_words)\n",
        "\n",
        "# NaN データが入っている可能性がるので変換\n",
        "w2v_psylex71.vectors = np.nan_to_num(w2v_psylex71.vectors)\n",
        "print(f'w2v_psylex71.vectors.shape:{w2v_psylex71.vectors.shape}')\n",
        "words = w2v_psylex71.index_to_key\n",
        "#len(words)## psylex71_ds に存在する全単語を word2vec の埋め込みベクトル行列にする"
      ],
      "metadata": {
        "id": "0VrbWijEdHtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 書記素データの定義"
      ],
      "metadata": {
        "id": "I3hm1Fwlep2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import RAM\n",
        "\n",
        "def _grapheme(words=words):\n",
        "    \"\"\"必要と思われる書記素リストを返す\"\"\"\n",
        "\n",
        "    num_alpha='０１２３４５６７８９ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ'\n",
        "    hira = 'あいうえおかがきぎくぐけげこごさざしじすずせぜそぞただちぢつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもやゆよらりるれろわゐゑをんぁぃぅぇっゃゅょゎ'+'ゔ'\n",
        "    kata = 'アイウエオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモヤユヨラリルレロワヰヱヲン'+'ヴヷヸヹヺァィゥヵヶェォッャョュヮ'\n",
        "    symbols='、。，．・：；？！゛゜´｀¨＾‾＿ヽヾゝゞ〃仝々〆〇ー—‐／＼〜‖｜…‥‘’“”（）〔〕［］｛｝〈〉《》「」『』【】＋−±×÷＝≠＜＞≦≧∞∴♂♀°′″℃¥＄¢£％＃＆＊＠§☆★○●◎◇' + '◆□■△▲▽▼※〒→←↑↓〓∈∋⊆⊇⊂⊃∪∩∧∨¬⇒⇔∀∃∠⊥⌒∂∇≡≒≪≫√∽∝∵∫∬Å‰♯♭♪†‡¶◯'\n",
        "    #greek='ΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩαβγδεζηθικλμνξοπρστυφχψω'\n",
        "    #rosian='АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
        "    #digit_symbols='①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳⑴⑵⑶⑷⑸⑹⑺⑻⑼⑽⑾⑿⒀⒁⒂⒃⒄⒅⒆⒇❶❷❸❹❺❻❼❽❾⒈⒉⒊⒋⒌⒍⒎⒏⒐'\n",
        "    #alpha_symbols='ⅠⅡⅢⅣⅤⅥⅦⅧⅨⅩⅪⅫⅰⅱⅲⅳⅴⅵⅶⅷⅸⅹⅺⅻ⒜⒝⒞⒟⒠⒡⒢⒣⒤⒥⒦⒧⒨⒩⒪⒫⒬⒭⒮⒯⒰⒱⒲⒳⒴⒵'\n",
        "    #units='㎜㎟㎝㎠㎤㎡㎥㎞㎢㎎㎏㏄㎖㎗ℓ㎘㎳㎲㎱㎰℉㏔㏋㎐㎅㎆㎇№㏍℡'\n",
        "    #suits='♤♧♡♢♠♣♥♦〠☎〄☞☜☝☟⇆⇄⇅⇨⇦⇧⇩'\n",
        "    #etc='①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳ⅠⅡⅢⅣⅤⅥⅦⅧⅨⅩ㍉㌔㌢㍍㌘㌧㌃㌶㍑㍗㌍㌦㌣㌫㍊㌻㎜㎝㎞㎎㎏㏄㎡㍻〝〟№㏍℡㊤㊥㊦㊧㊨㈱㈲㈹㍾㍽㍼≒≡∫∮∑√⊥∠∟⊿∵∩∪㊙'\n",
        "    #etc2='㍉㌢㍍㌔㌖㌅㌳㍎㌃㌶㌘㌕㌧㍑㍊㌹㍗㌍㍂㌣㌦㌻㌫㌀㌞㌪㌱㍇㍾㍽㍼㍻㍿∮∟⊿〝'\n",
        "\n",
        "    # RAM で作成済の常用漢字リストを用いて単漢字リストを作成\n",
        "    # 平成 22 年の改定により常用漢字は 2136 文字ある\n",
        "    chars_list = [ch for ch in num_alpha+hira+kata+symbols]+ RAM.chars_joyo().char_list\n",
        "    not_chars_list = []\n",
        "    for wrd in tqdm(words):\n",
        "        for ch in wrd:\n",
        "            if (ch not in chars_list) and (ch not in not_chars_list):\n",
        "                not_chars_list.append(ch)\n",
        "    not_chars_list = sorted(not_chars_list)\n",
        "    grapheme = chars_list + not_chars_list\n",
        "    # 上記の処理により grapheme には 2768 文字である。\n",
        "    # これに特殊トークン 4 つ ['<PAD>', '<SOW>', '<EOW>', '<UNK>'] を加えたリストを返す\n",
        "\n",
        "    return ['<PAD>', '<SOW>', '<EOW>', '<UNK>'] + grapheme\n",
        "\n",
        "grapheme = _grapheme()"
      ],
      "metadata": {
        "id": "1qWbXByJenWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import gensim\n",
        "\n",
        "def _collate_fn(batch):\n",
        "    inps, tgts = list(zip(*batch))\n",
        "    inps = list(inps)\n",
        "    tgts = list(tgts)\n",
        "    return inps, tgts\n",
        "\n",
        "\n",
        "class psylex71_w2v_Dataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 #direction='s2p',  # ['s2p', 'p2s']\n",
        "                 source='seme',    # エンコーダ用 入力データ, ['orth', seme', 'phon'] のいずれか一つ\n",
        "                 target='phon',    # デコーダ用 出力データ ,  ['orth', seme', 'phon'] のいずれか一つ\n",
        "                 w2v:gensim.models.keyedvectors.KeyedVectors=w2v_psylex71,\n",
        "                 old_ds:RAM.dataset.Psylex71_Dataset=psylex71_ds1,\n",
        "                 #mecab_yomi=yomi,\n",
        "                 grapheme:list=grapheme,\n",
        "                ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.ds_name = 'psylex71_'+source+\"2\"+target\n",
        "        self.source, self.target = source, target\n",
        "\n",
        "        self.w2v = w2v\n",
        "        self.old_ds = old_ds\n",
        "        #self.mecab_yomi = yomi         # 未知の単語が入力された場合 MeCab を使って読みをえるため\n",
        "        self.grapheme = grapheme\n",
        "\n",
        "        self.words = w2v.index_to_key  # gensim の KeyedVectors を利用して単語リストとする\n",
        "        self.W = w2v.vectors\n",
        "\n",
        "        # 訓令式に従った日本語ローマ字表記 `kurei.py` 参照\n",
        "        self.phoneme = ['<PAD>', '<SOW>', '<EOW>', '<UNK>', # 特殊トークン，純に，埋め草，語頭，語末，未知\n",
        "                        'a', 'i', 'u', 'e', 'o',            # 母音\n",
        "                        'a:', 'i:', 'u:', 'e:', 'o:',       # 長母音\n",
        "                        'N', 'Q',                           # 撥音，拗音\n",
        "                        'b', 'by', 'ch', 'd', 'dy', 'f', 'g', 'gy', 'h', 'hy', # 子音\n",
        "                        'j', 'k', 'ky', 'm', 'my', 'n', 'ny',  'p', 'py', 'r', # 子音\n",
        "                        'ry', 's', 'sy', 't', 'ty', 'w', 'y', 'z', 'zy']       # 子音\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx:int):\n",
        "        wrd = self.words[idx]\n",
        "\n",
        "        if self.source == 'phon':\n",
        "            src = torch.LongTensor(self.wrd2phon_ids(wrd))\n",
        "        elif self.source == 'seme':\n",
        "            src = torch.tensor(self.w2v.get_vector(idx))\n",
        "        elif self.source == 'orth':\n",
        "            src = torch.LongTensor(self.wrd2orth_ids(wrd))\n",
        "        else:\n",
        "            src = None\n",
        "\n",
        "        if self.target == 'phon':\n",
        "            tgt = torch.LongTensor(self.wrd2phon_ids(wrd))\n",
        "        elif self.target == 'seme':\n",
        "            tgt = torch.tensor(self.w2v.get_vector(idx))\n",
        "        elif self.target == 'orth':\n",
        "            tgt = torch.LongTensor(self.wrd2orth_ids(wrd))\n",
        "        else:\n",
        "            tgt = None\n",
        "\n",
        "        return src, tgt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.w2v)\n",
        "\n",
        "    def getitem(self,\n",
        "                idx:int):\n",
        "        wrd = self.words[idx]\n",
        "        _yomi = self.wrd2yomi(wrd)\n",
        "        _yomi = kunrei(_yomi).split(' ')\n",
        "        phon_ids = [self.phoneme.index(idx) for idx in _yomi]\n",
        "        orth_ids = [self.grapheme.index(idx) for idx in wrd]\n",
        "        return wrd, _yomi, phon_ids, orth_ids\n",
        "\n",
        "    def source_ids2source(self, ids:list):\n",
        "\n",
        "        if self.source == 'phon':\n",
        "            return self.phon_ids2phn(ids)\n",
        "        elif self.source == 'orth':\n",
        "            return self.orth_ids2orth(ids)\n",
        "        elif self.source == 'seme':\n",
        "            wrd = self.getitem(ids)[0]\n",
        "            return w2v.similar_by_word(wrd)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "    def target_ids2target(self, ids:list):\n",
        "\n",
        "        if self.target == 'phon':\n",
        "            return self.phon_ids2phn(ids)\n",
        "        elif self.target == 'orth':\n",
        "            return self.orth_ids2orth(ids)\n",
        "        elif self.target == 'seme':\n",
        "            wrd = self.getitem(ids)[0]\n",
        "            return w2v.similar_by_word(wrd)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "    def wrd2orth_ids(self, wrd:str)->list:\n",
        "        ids = [self.grapheme.index(ch) for ch in wrd]\n",
        "        ids = [self.grapheme.index('<SOW>')] + ids + [self.grapheme.index('<EOW>')]\n",
        "        #ids = [[self.grapheme.index('<SOW>')] + ids + [self.grapheme.index('<EOW>')]]\n",
        "        return ids\n",
        "\n",
        "    def wrd2phon_ids(self, wrd:str)->list:\n",
        "        _yomi = self.wrd2yomi(wrd)\n",
        "        _yomi = kunrei(_yomi).split(' ')\n",
        "        ids = [self.phoneme.index(idx) for idx in _yomi]\n",
        "        ids = [self.phoneme.index('<SOW>')] + ids + [self.phoneme.index('<EOW>')]\n",
        "        return ids\n",
        "\n",
        "    def get_wrdidx_from_word(self, wrd:str):\n",
        "        if wrd in self.words:\n",
        "            wrd_idx = self.w2v.get_index(wrd)\n",
        "        else:\n",
        "            wrd_idx = -1\n",
        "        return wrd_idx\n",
        "\n",
        "    def wrd2emb(self, wrd:str)->np.ndarray:\n",
        "        if wrd in self.words:\n",
        "            return self.w2v.get_vector(wrd)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def wrd2wrd_ids(self, wrd:str)->int:\n",
        "        if wrd in self.words:\n",
        "            return self.words.index(wrd)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def orth_ids2orth(self,\n",
        "                      ids:np.ndarray)->str:\n",
        "    #def orth_ids2orth(self, ids:list)->str:\n",
        "        ret = [self.grapheme[idx] for idx in ids]\n",
        "        return ret\n",
        "\n",
        "    def wrd_idx2wrd(self, idx:int)->str:\n",
        "        if 0 <= idx and idx < len(self.words):\n",
        "            return self.words[idx]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def wrd2onehot(self, wrd:str)->np.ndarray:\n",
        "        ret = np.zeros((self.W.shape[0],), dtype=np.int32)\n",
        "        if wrd in self.words:\n",
        "            ret[self.w2v.get_index(wrd)] = 1\n",
        "            return ret\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def phon_ids2phn(self, ids:np.ndarray):\n",
        "        ret = \"\".join([self.phoneme[idx] for idx in ids])\n",
        "        return ret\n",
        "\n",
        "    def wrd2yomi(self, wrd:str)->list:\n",
        "        if wrd in self.words:\n",
        "            _yomi = self.old_ds.orth2info_dict[wrd]['ヨミ']\n",
        "        else:\n",
        "            _yomi = self.mecab_yomi(wrd).strip().split()[0]\n",
        "        return _yomi\n",
        "\n",
        "    def wrd2info(self, wrd:str)->dict:\n",
        "        if wrd in self.words:\n",
        "            return self.old_ds.orth2info_dict[wrd]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "# 全部で 9 通りのデータセットを定義\n",
        "# psylex71_ds_o2o = psylex71_w2v_Dataset(source='orth', target='orth')\n",
        "psylex71_ds_o2p = psylex71_w2v_Dataset(source='orth', target='phon')\n",
        "# psylex71_ds_o2s = psylex71_w2v_Dataset(source='orth', target='seme')\n",
        "\n",
        "# psylex71_ds_p2o = psylex71_w2v_Dataset(source='phon', target='orth')\n",
        "# psylex71_ds_p2p = psylex71_w2v_Dataset(source='phon', target='phon')\n",
        "# psylex71_ds_p2s = psylex71_w2v_Dataset(source='phon', target='seme')\n",
        "\n",
        "# psylex71_ds_s2o = psylex71_w2v_Dataset(source='seme', target='orth')\n",
        "# psylex71_ds_s2p = psylex71_w2v_Dataset(source='seme', target='phon')\n",
        "# psylex71_ds_s2s = psylex71_w2v_Dataset(source='seme', target='seme')"
      ],
      "metadata": {
        "id": "lo7zN-vXesGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = psylex71_ds_o2p\n",
        "print(list(set([x if str(x)[0] != '_' else None for x in dir(ds)])))"
      ],
      "metadata": {
        "id": "eANrAZo1eujd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = {}\n",
        "for i, n in enumerate(ds.words[:]):\n",
        "    wrd = ds.words[i]\n",
        "    src = wrd\n",
        "    src, tgt, t_ids, s_ids = ds.getitem(i)\n",
        "    D[i] = (src, s_ids, tgt, t_ids)\n",
        "\n",
        "\n",
        "# help(pd.DataFrame.from_dict)\n",
        "D2 = pd.DataFrame.from_dict(D, orient='index', columns=['src','tgt','src_ids', 'tgt_ids'])\n",
        "\n",
        "fname = '2024_3022cnps_latter.xlsx'\n",
        "D2.to_excel(fname)\n",
        "\n",
        "D3 = pd.read_excel(fname, index_col=0)\n",
        "#O3.head()\n",
        "D3.tail()"
      ],
      "metadata": {
        "id": "Yd1FI_imexa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBFjVGj0ezOD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}