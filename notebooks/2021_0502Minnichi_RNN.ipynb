{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "2021_0502Minnichi_RNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021_0502Minnichi_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCd-RXCh96wZ"
      },
      "source": [
        "# みんなの日本語 言語モデル の作成\n",
        "- date: 2021_0502\n",
        "- author: 浅川伸一\n",
        "- 概要:\n",
        "\n",
        "岩下先生からいただいた みんなの日本語 テキストデータ から言語モデルを作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jOrIyda96wi"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 表示精度桁数の設定\n",
        "np.set_printoptions(suppress=False, formatter={'float': '{:6.3f}'.format})\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRrSWDTe-Ga1"
      },
      "source": [
        "# minnchi.txt を upload してださい。\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxqyGdJe96wj"
      },
      "source": [
        "print('# データの読み込み all に格納')\n",
        "all = open('minnichi.txt', 'r').read().strip().split('\\n')\n",
        "\n",
        "print('#all 内の \"。\" の位置を探して，文を分割。結果を data に格納')\n",
        "text_data = []\n",
        "for line in all:\n",
        "    positions = []\n",
        "    for i, word in enumerate(line):\n",
        "        if word == '。':\n",
        "            positions.append(i)\n",
        "    c = []\n",
        "    p0 = 0\n",
        "    for p in positions:\n",
        "        text_data.append(line[p0:p+1])\n",
        "        p0 = p+1\n",
        "    if len(positions) == 0:\n",
        "        text_data.append(line)\n",
        "\n",
        "print(f'#総文数: {len(text_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilqo0uZf96wl"
      },
      "source": [
        "print('#data 内の単語頻度を計測し word_freq に格納')\n",
        "word_freq = {}\n",
        "for line in text_data:\n",
        "    for word in line.split():\n",
        "        if not word in word_freq:\n",
        "            word_freq[word] = 1\n",
        "        else:\n",
        "            word_freq[word] += 1\n",
        "\n",
        "print('#後の処理のため 単語リスト word_list を作成')\n",
        "word_list = sorted(list(word_freq))\n",
        "n_vocab = len(word_list)\n",
        "print(f'#総語彙数: {n_vocab}')\n",
        "\n",
        "print('#ワンホットベクトルを作成するため wrd2idx, idx2wrd を作成')\n",
        "wrd2idx = {w:i for i, w in enumerate(word_list)}\n",
        "idx2wrd = {i:w for i, w in enumerate(word_list)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwAwmS1e96wl"
      },
      "source": [
        "print('#text_data を単語 ID へ変換した X を作成')\n",
        "X = []\n",
        "for line in text_data:\n",
        "    words = line.split()\n",
        "    X.append([wrd2idx[word] for word in words])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRaAssvE_v1L"
      },
      "source": [
        "print(len(X))\n",
        "print('#データチェック')\n",
        "No = int(input('チェックのため数字を入力してください'))\n",
        "print(f'X[{No}]:\\n単語 ID 系列:  {X[No]} \\nIDを単語に変換:{[idx2wrd[id] for id in X[No]]}\\n元データ:      {text_data[No]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RqOtkFz96wm"
      },
      "source": [
        "print('#単語ID からなるデータ X を作成')\n",
        "X = []\n",
        "for line in text_data:\n",
        "    words = line.split()\n",
        "    X.append([wrd2idx[word] for word in words])\n",
        "\n",
        "print('#最大文長となるデータを探して表示する')\n",
        "max_len, line_no = 0, 0\n",
        "for i, line in enumerate(X):\n",
        "    if len(line) > max_len:\n",
        "        max_len = len(line)\n",
        "        line_no = i\n",
        "        #print(line_no, line)\n",
        "        print(line_no, \"\".join(idx2wrd[id] for id in line))\n",
        "\n",
        "print(f'最大文長: {max_len} 単語，データ番号: {line_no}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q39Mbwg696wn"
      },
      "source": [
        "print('#ソフトマックス関数の定義')\n",
        "def softmax(x, beta=1):\n",
        "    xt = np.exp(beta * x - np.mean(beta * x))\n",
        "    return xt / np.sum(xt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VThnOR0p96wn"
      },
      "source": [
        "print('#ハイパーパラメータ の設定: 学習率 lr, 中間層ニューロン数 n_hid, 最大系列長 seq_len')\n",
        "lr = 1e-1          #学習率\n",
        "n_hid = 20         #中間層のニューロン数\n",
        "seq_len = max_len  #RNN の時間ステップ数\n",
        "\n",
        "n_data = len(X)\n",
        "n_vocab = len(word_list)\n",
        "\n",
        "print('#推定すべきパラメータ: 結合係数行列とバイアス項の初期化')\n",
        "Wxh = np.random.randn(n_hid, n_vocab) * 0.01   # 入力層 -> 中間層\n",
        "Whh = np.random.randn(n_hid, n_hid)   * 0.01   # 中間層 -> 中間層 リカレント結合\n",
        "Why = np.random.randn(n_vocab, n_hid) * 0.01   # 中間層 -> 出力層\n",
        "bh = np.zeros((n_hid, 1))                      # 中間層バイアス項\n",
        "by = np.zeros((n_vocab, 1))                    # 出力層バイアス項\n",
        "print(f'# Wxh:{Wxh.shape}, Whh:{Whh.shape}, Why:{Why.shape}, bh:{bh.shape}, by:{by.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWQABP2696wn"
      },
      "source": [
        "print('#前向きパス(次単語予測) と 後向きパス(誤差逆伝播) 関数の定義')\n",
        "def rnn_forward(inputs, targets, h_prev, Wxh=Wxh, Whh=Whh, Why=Why, bh=bh, by=by):\n",
        "    \"\"\"RNN 前向きパス \n",
        "\n",
        "    引数:\n",
        "    - inputs: int のリスト \n",
        "    - targets: int のリスト\n",
        "    - h_prev: 隠れ層の初期状態 Hx1 (H 行 x 1 列)\n",
        "    \n",
        "    戻り値:\n",
        "    - loss: np.array\n",
        "        損失値，出力層のニューロンごとの損失値\n",
        "    - prob: np.array\n",
        "        各時刻ごとの各項目(単語)の予測確率 足し合わせると 1 になる\n",
        "    - H: dict\n",
        "        各時刻ごとの中間層の状態\n",
        "    - X: dict\n",
        "        各時刻ごとの入力層の状態\n",
        "\n",
        "    \n",
        "    y0       y1         y2\n",
        "    |         |         | Why\n",
        "    h --Whh-- h --Whh-- h       h と y にはバイアス項 bh, hy 付く\n",
        "    |         |         | Wxh\n",
        "    x0       x1         x2\n",
        "    \"\"\"\n",
        "    X, H, Y, prob = {}, {}, {}, {}\n",
        "    H[-1] = np.copy(h_prev)\n",
        "    loss = 0\n",
        "    \n",
        "    for t in range(len(inputs)):\n",
        "        \n",
        "        X[t] = np.zeros((n_vocab,1))           # ワンホット表現\n",
        "        X[t][inputs[t]] = 1\n",
        "        \n",
        "        #隠れ層の状態\n",
        "        H[t] = np.tanh(np.dot(Wxh, X[t]) + np.dot(Whh, H[t-1]) + bh)\n",
        "        \n",
        "        Y[t] = np.dot(Why, H[t]) + by           #次項目(モーラ) の対数確率\n",
        "        prob[t] = softmax(Y[t])                 #次項目の確率\n",
        "        loss += -np.log(prob[t][targets[t],0])  #交差エントロピー損失\n",
        "        \n",
        "    return loss, prob, H, X\n",
        "\n",
        "\n",
        "def rnn_backword(inputs, targets, prob_, h_, x_, Wxh=Wxh, Whh=Whh, Why=Why, bh=bh, by=by):\n",
        "    \"\"\"RNN の後向きパス: 勾配計算\n",
        "    戻り値: 6 つ\n",
        "    - dWxh, dWhh, dWhy, dbh, dby: 勾配 \n",
        "    - h_[len(inputs)-1]: 隠れ層の状態\n",
        "        \n",
        "    y0       y1         y2\n",
        "    |         |         | Why\n",
        "    h --Whh-- h --Whh-- h       h と y にはバイアス項 bh, hy 付く\n",
        "    |         |         | Wxh\n",
        "    x0       x1         x2\n",
        "    \"\"\"\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "    dh_next = np.zeros_like(h_[0])\n",
        "    \n",
        "    for t in reversed(range(len(inputs))):\n",
        "        #y への逆伝播\n",
        "        #参考 http://cs231n.github.io/neural-networks-case-study/#grad\n",
        "        dy = np.copy(prob_[t])\n",
        "        dy[targets[t]] -= 1               \n",
        "        dWhy += np.dot(dy, h_[t].T)\n",
        "        dby += dy\n",
        "        \n",
        "        dh = np.dot(Why.T, dy) + dh_next  #中間層へ誤差逆伝播\n",
        "        delta = (1 - h_[t] * h_[t]) * dh  #tanh での誤差逆伝播\n",
        "\n",
        "        dbh  += delta\n",
        "        dWxh += np.dot(delta, x_[t].T)\n",
        "        dWhh += np.dot(delta, h_[t-1].T)\n",
        "        dh_next = np.dot(Whh.T, delta)\n",
        "\n",
        "    #勾配爆発緩和のためのクリッピング\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam)       \n",
        "\n",
        "    return dWxh, dWhh, dWhy, dbh, dby, h_[len(inputs)-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGwYS4gW96wo"
      },
      "source": [
        "def sample_seq(h_prev, seed, n):\n",
        "    \"\"\"モデルを用いて項目(文字)番号の系列をランダムサンプリングを繰り返し，\n",
        "    n 個からなる項目番号の系列を返す\n",
        "    \n",
        "    引数:\n",
        "    - h_prev: np.array\n",
        "        前時刻の中間層の状態 中間層の素子数 x 1の行列\n",
        "    - seed: int\n",
        "        文字id 時刻 0 で与える文字ID 番号\n",
        "    \n",
        "    戻り値:\n",
        "    - idxes: list()\n",
        "        項目番号列からなる予測系列のリスト\n",
        "    \"\"\"\n",
        "    x = np.zeros((n_vocab, 1))\n",
        "    hid = h_prev\n",
        "    x[seed] = 1\n",
        "    idxes = []\n",
        "    for t in range(n):\n",
        "        hid = np.tanh(np.dot(Wxh, x) + np.dot(Whh, hid) + bh)\n",
        "        out = np.dot(Why, hid) + by\n",
        "        prob = softmax(out)\n",
        "        idx = np.random.choice(range(n_vocab), p=prob.ravel())\n",
        "        #if idx2wrd[idx] == '。':\n",
        "        #    return idxes\n",
        "        x = np.zeros((n_vocab, 1))\n",
        "        x[idx] = 1\n",
        "    return idxes\n",
        "\n",
        "\n",
        "def gen_seq(h_prev, seed, n):\n",
        "    \"\"\"モデルを用いて項目(文字)番号の系列を生成して，\n",
        "    n 個からなる項目番号の系列を返す\n",
        "    sample_seq との相違は，\n",
        "    \n",
        "    引数:\n",
        "    - h_prev: np.array\n",
        "        前時刻の中間層の状態 中間層の素子数 x 1の行列\n",
        "    - seed: int\n",
        "        文字id 時刻 0 で与える文字番号\n",
        "    \n",
        "    戻り値:\n",
        "    - idxes: list()\n",
        "        項目番号列からなる予測系列のリスト\n",
        "    \"\"\"\n",
        "    x = np.zeros((n_vocab, 1))\n",
        "    x[seed] = 1\n",
        "    hid = h_prev\n",
        "    idxes = []\n",
        "    for t in range(n):\n",
        "        hid = np.tanh(np.dot(Wxh, x) + np.dot(Whh, hid) + bh)\n",
        "        out = np.dot(Why, hid) + by\n",
        "        prob = softmax(out)\n",
        "        idx = np.argmax(prob)\n",
        "        idxes.append(idx)\n",
        "        x = np.zeros((n_vocab, 1))\n",
        "        x[idx] = 1\n",
        "    return idxes\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EZLZ2u196wp"
      },
      "source": [
        "print('#動作確認 N 個の文をランダムサンプリングして前向きパスに通してエラーが発生しないかチェックする')\n",
        "N = int(input('数字を入力してください:'))\n",
        "if N == 0 or N > len(X): N=3\n",
        "for line_id in np.random.permutation(len(X))[:N]:\n",
        "    x = X[line_id][:-1]           #入力系列データ 各時刻における入力単語系列\n",
        "    y = X[line_id][1:]            #出力系列データ 次時刻における教師単語系列\n",
        "    h_prev = np.zeros((n_hid,1))  #中間層状態を 0 でクリア\n",
        "    loss, prob, _, _ = rnn_forward(x, y, h_prev)  #  前向き処理の実行\n",
        "    print(f'line no.:{line_id} 損失値={loss:.3f}, 正解系列(y):{y} ->「{\"\".join(idx2wrd[idx] for idx in y)}」text_data:「{text_data[line_id]}」')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr1hUGOM96wq",
        "outputId": "25da53d2-d332-46e6-8d8e-2a649dcdbd5e"
      },
      "source": [
        "# m で始まる変数は，Adagrad で用いるメモリ変数。それぞれ，\n",
        "# mWxh: 入力から中間層への結合係数行列\n",
        "# mWhh: 中間層へのリカレント結合係数行列\n",
        "# mWhy: 中間層から出力層への結合係数行列\n",
        "# mbh: 中間層のバイアス項\n",
        "# mby: 出力層のバイアス項\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) \n",
        "\n",
        "smooth_loss = -np.log(1.0/n_vocab) * max_len      #損失値の理論的上限値\n",
        "\n",
        "max_iter = 10 ** 2\n",
        "interval = max_iter >> 2\n",
        "losses = []\n",
        "\n",
        "for iter in range(max_iter):\n",
        "    \n",
        "    idxs = np.random.permutation(len(X))  # 全データをシャッフル\n",
        "    idxs = range(len(X))\n",
        "    for idx in idxs:\n",
        "        \n",
        "        if len(X[idx]) == 1:\n",
        "            continue\n",
        "        inputs = X[idx][:-1]\n",
        "        targets = X[idx][1:]\n",
        "\n",
        "        #前向き処理\n",
        "        h_prev = np.zeros((n_hid,1)) # reset RNN memory\n",
        "        loss, prob, h_, x_ = rnn_forward(inputs, targets, h_prev)\n",
        "        \n",
        "        #損失値の処理\n",
        "        losses.append(loss)\n",
        "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "        #後向き処理\n",
        "        dWxh, dWhh, dWhy, dbh, dby, h_ = rnn_backword(inputs, targets, prob, h_, x_)\n",
        "        \n",
        "        #Adagrad によるパラメータ更新\n",
        "        for param, _delta, _Hessian in zip([Wxh, Whh, Why, bh, by], \n",
        "                                           [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                           [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            _Hessian += _delta * _delta\n",
        "        param += -lr * _delta / np.sqrt(_Hessian + 1e-8) # 更新\n",
        "\n",
        "    #途中結果の印字\n",
        "    if iter % interval == 0:\n",
        "        pred_idxes = gen_seq(h_prev, inputs, 50)\n",
        "        pred_text = ''.join(idx2wrd[idx] for idx in pred_idxes)\n",
        "        print(f'--- 訓練回数:{iter:<5d} ---\\n{pred_text}\\n---')\n",
        "        print(f'損失値:{smooth_loss:.3f}') # print progress\n",
        "\n",
        "\n",
        "h_prev = np.zeros((n_hid,1)) # reset RNN memory\n",
        "pred_idxes = gen_seq(h_prev, X[0][1:], 50)\n",
        "pred_text = ''.join(idx2wrd[idx] for idx in pred_idxes)\n",
        "print(f'----\\n{pred_text}\\n----')\n",
        "plt.plot(losses)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- 訓練回数:0     ---\n",
            "。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
            "---\n",
            "損失値:205.545\n",
            "--- 訓練回数:25    ---\n",
            "。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
            "---\n",
            "損失値:57.620\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}