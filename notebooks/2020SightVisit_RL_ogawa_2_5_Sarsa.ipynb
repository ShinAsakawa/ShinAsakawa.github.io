{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020SightVisit_RL_ogawa_2_5_Sarsa.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020SightVisit_RL_ogawa_2_5_Sarsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6mkAyab-UNU"
      },
      "source": [
        "# SARSA のデモ\n",
        "\n",
        " <a href=\"mailto:asakawa@ieee.org\">浅川伸一</a>\n",
        "\n",
        "本ファイルは小川雄太郎の「[つくりながら学ぶ！深層強化学習](https://www.amazon.co.jp/dp/4839965625/)」(マイナビ出版 2018/6/28) \n",
        "の 2.2. 迷路とエージェントを実装，にでてくる[SARSAによる迷路探索攻略](https://github.com/YutaroOgawa/Deep-Reinforcement-Learning-Book/blob/master/program/2_5_Policygradient.ipynb) です。\n",
        "\n",
        "すぐれた教科書を書かれた小川さんに感謝いたします"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_Yi2ToS-UNV"
      },
      "source": [
        "# 使用するパッケージの宣言\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj20lxeg-UNY"
      },
      "source": [
        "# 初期位置での迷路の様子\n",
        "\n",
        "# 図を描く大きさと、図の変数名を宣言\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = plt.gca()\n",
        "\n",
        "# 赤い壁を描く\n",
        "plt.plot([1, 1], [0, 1], color='red', linewidth=2)\n",
        "plt.plot([1, 2], [2, 2], color='red', linewidth=2)\n",
        "plt.plot([2, 2], [2, 1], color='red', linewidth=2)\n",
        "plt.plot([2, 3], [1, 1], color='red', linewidth=2)\n",
        "\n",
        "# 状態を示す文字S0～S8を描く\n",
        "plt.text(0.5, 2.5, 'S0', size=14, ha='center')\n",
        "plt.text(1.5, 2.5, 'S1', size=14, ha='center')\n",
        "plt.text(2.5, 2.5, 'S2', size=14, ha='center')\n",
        "plt.text(0.5, 1.5, 'S3', size=14, ha='center')\n",
        "plt.text(1.5, 1.5, 'S4', size=14, ha='center')\n",
        "plt.text(2.5, 1.5, 'S5', size=14, ha='center')\n",
        "plt.text(0.5, 0.5, 'S6', size=14, ha='center')\n",
        "plt.text(1.5, 0.5, 'S7', size=14, ha='center')\n",
        "plt.text(2.5, 0.5, 'S8', size=14, ha='center')\n",
        "plt.text(0.5, 2.3, 'START', ha='center')\n",
        "plt.text(2.5, 0.3, 'GOAL', ha='center')\n",
        "\n",
        "# 描画範囲の設定と目盛りを消す設定\n",
        "ax.set_xlim(0, 3)\n",
        "ax.set_ylim(0, 3)\n",
        "plt.tick_params(axis='both', which='both', bottom='off', top='off',\n",
        "                labelbottom='off', right='off', left='off', labelleft='off')\n",
        "\n",
        "# 現在地S0に緑丸を描画する\n",
        "line, = ax.plot([0.5], [2.5], marker=\"o\", color='g', markersize=60)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWZel4CC-UNd"
      },
      "source": [
        "# 初期の方策を決定するパラメータtheta_0を設定\n",
        "\n",
        "# 行は状態0～7、列は移動方向で↑、→、↓、←を表す\n",
        "theta_0 = np.array([[np.nan, 1, 1, np.nan],  # s0\n",
        "                    [np.nan, 1, np.nan, 1],  # s1\n",
        "                    [np.nan, np.nan, 1, 1],  # s2\n",
        "                    [1, 1, 1, np.nan],  # s3\n",
        "                    [np.nan, np.nan, 1, 1],  # s4\n",
        "                    [1, np.nan, np.nan, np.nan],  # s5\n",
        "                    [1, np.nan, np.nan, np.nan],  # s6\n",
        "                    [1, 1, np.nan, np.nan],  # s7、※s8はゴールなので、方策はなし\n",
        "                    ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hdlY89j-UNf"
      },
      "source": [
        "# 方策パラメータtheta_0をランダム方策piに変換する関数の定義\n",
        "\n",
        "\n",
        "def simple_convert_into_pi_from_theta(theta):\n",
        "    '''単純に割合を計算する'''\n",
        "\n",
        "    [m, n] = theta.shape  # thetaの行列サイズを取得\n",
        "    pi = np.zeros((m, n))\n",
        "    for i in range(0, m):\n",
        "        pi[i, :] = theta[i, :] / np.nansum(theta[i, :])  # 割合の計算\n",
        "\n",
        "    pi = np.nan_to_num(pi)  # nanを0に変換\n",
        "\n",
        "    return pi\n",
        "\n",
        "# ランダム行動方策pi_0を求める\n",
        "pi_0 = simple_convert_into_pi_from_theta(theta_0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgsQ24qU-UNh"
      },
      "source": [
        "# 初期の行動価値関数Qを設定\n",
        "\n",
        "[a, b] = theta_0.shape  # 行と列の数をa, bに格納\n",
        "Q = np.random.rand(a, b) * theta_0\n",
        "# * theta0をすることで要素ごとに掛け算をし、Qの壁方向の値がnanになる\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5-cNQ76-UNk"
      },
      "source": [
        "# ε-greedy法を実装\n",
        "\n",
        "\n",
        "def get_action(s, Q, epsilon, pi_0):\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "\n",
        "    # 行動を決める\n",
        "    if np.random.rand() < epsilon:\n",
        "        # εの確率でランダムに動く\n",
        "        next_direction = np.random.choice(direction, p=pi_0[s, :])\n",
        "    else:\n",
        "        # Qの最大値の行動を採用する\n",
        "        next_direction = direction[np.nanargmax(Q[s, :])]\n",
        "\n",
        "    # 行動をindexに\n",
        "    if next_direction == \"up\":\n",
        "        action = 0\n",
        "    elif next_direction == \"right\":\n",
        "        action = 1\n",
        "    elif next_direction == \"down\":\n",
        "        action = 2\n",
        "    elif next_direction == \"left\":\n",
        "        action = 3\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "def get_s_next(s, a, Q, epsilon, pi_0):\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "    next_direction = direction[a]  # 行動aの方向\n",
        "\n",
        "    # 行動から次の状態を決める\n",
        "    if next_direction == \"up\":\n",
        "        s_next = s - 3  # 上に移動するときは状態の数字が3小さくなる\n",
        "    elif next_direction == \"right\":\n",
        "        s_next = s + 1  # 右に移動するときは状態の数字が1大きくなる\n",
        "    elif next_direction == \"down\":\n",
        "        s_next = s + 3  # 下に移動するときは状態の数字が3大きくなる\n",
        "    elif next_direction == \"left\":\n",
        "        s_next = s - 1  # 左に移動するときは状態の数字が1小さくなる\n",
        "\n",
        "    return s_next\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCrLS5nF-UNm"
      },
      "source": [
        "# Sarsaによる行動価値関数Qの更新\n",
        "\n",
        "\n",
        "def Sarsa(s, a, r, s_next, a_next, Q, eta, gamma):\n",
        "\n",
        "    if s_next == 8:  # ゴールした場合\n",
        "        Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n",
        "\n",
        "    else:\n",
        "        Q[s, a] = Q[s, a] + eta * (r + gamma * Q[s_next, a_next] - Q[s, a])\n",
        "\n",
        "    return Q\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ2d2FYO-UNo"
      },
      "source": [
        "# Sarsaで迷路を解く関数の定義、状態と行動の履歴および更新したQを出力\n",
        "\n",
        "\n",
        "def goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi):\n",
        "    s = 0  # スタート地点\n",
        "    a = a_next = get_action(s, Q, epsilon, pi)  # 初期の行動\n",
        "    s_a_history = [[0, np.nan]]  # エージェントの移動を記録するリスト\n",
        "\n",
        "    while (1):  # ゴールするまでループ\n",
        "        a = a_next  # 行動更新\n",
        "\n",
        "        s_a_history[-1][1] = a\n",
        "        # 現在の状態（つまり一番最後なのでindex=-1）に行動を代入\n",
        "\n",
        "        s_next = get_s_next(s, a, Q, epsilon, pi)\n",
        "        # 次の状態を格納\n",
        "\n",
        "        s_a_history.append([s_next, np.nan])\n",
        "        # 次の状態を代入。行動はまだ分からないのでnanにしておく\n",
        "\n",
        "        # 報酬を与え,　次の行動を求めます\n",
        "        if s_next == 8:\n",
        "            r = 1  # ゴールにたどり着いたなら報酬を与える\n",
        "            a_next = np.nan\n",
        "        else:\n",
        "            r = 0\n",
        "            a_next = get_action(s_next, Q, epsilon, pi)\n",
        "            # 次の行動a_nextを求めます。\n",
        "\n",
        "        # 価値関数を更新\n",
        "        Q = Sarsa(s, a, r, s_next, a_next, Q, eta, gamma)\n",
        "\n",
        "        # 終了判定\n",
        "        if s_next == 8:  # ゴール地点なら終了\n",
        "            break\n",
        "        else:\n",
        "            s = s_next\n",
        "\n",
        "    return [s_a_history, Q]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iX6q_SI-UNr"
      },
      "source": [
        "# Sarsaで迷路を解く\n",
        "\n",
        "eta = 0.1  # 学習率\n",
        "gamma = 0.9  # 時間割引率\n",
        "epsilon = 0.5  # ε-greedy法の初期値\n",
        "v = np.nanmax(Q, axis=1)  # 状態ごとに価値の最大値を求める\n",
        "is_continue = True\n",
        "episode = 1\n",
        "\n",
        "while is_continue:  # is_continueがFalseになるまで繰り返す\n",
        "    print(\"エピソード:\" + str(episode))\n",
        "\n",
        "    # ε-greedyの値を少しずつ小さくする\n",
        "    epsilon = epsilon / 2\n",
        "\n",
        "    # Sarsaで迷路を解き、移動した履歴と更新したQを求める\n",
        "    [s_a_history, Q] = goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0)\n",
        "\n",
        "    # 状態価値の変化\n",
        "    new_v = np.nanmax(Q, axis=1)  # 状態ごとに価値の最大値を求める\n",
        "    print(np.sum(np.abs(new_v - v)))  # 状態価値の変化を出力\n",
        "    v = new_v\n",
        "\n",
        "    print(\"迷路を解くのにかかったステップ数は\" + str(len(s_a_history) - 1) + \"です\")\n",
        "\n",
        "    # 100エピソード繰り返す\n",
        "    episode = episode + 1\n",
        "    if episode > 100:\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}