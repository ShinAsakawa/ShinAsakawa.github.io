{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_0420Onomatopea_project.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNgO1WxCt1Nk72J6cZs11ZR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021_0420Onomatopea_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS0Rqwfq6-VR"
      },
      "source": [
        "# Colab では以下の 2 行の行頭の # を削除してから実行してください\n",
        "!pip install mecab-python3\n",
        "!pip install unidic-lite\n",
        "# Colab では以下の 2 行の行頭の # を削除してから実行してください\n",
        "!pip install jaconv\n",
        "!pip install japanize_matplotlib\n",
        "#!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGxmAL3K89ir"
      },
      "source": [
        "import requests\n",
        "\n",
        "# word2vec の訓練済モデルを入手\n",
        "urls =['http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz',\n",
        "       'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_sgns.bin.gz',\n",
        "       'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid300_win20_neg20_sgns.bin.gz',\n",
        "       'http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz']\n",
        "\n",
        "url = urls[0]\n",
        "w2v_fname = url.split('/')[-1]\n",
        "r = requests.get(url)\n",
        "with open(w2v_fname, 'wb') as f:\n",
        "    total_length = int(r.headers.get('content-length'))\n",
        "    print('Downloading {0} - {1} bytes'.format(w2v_fname, (total_length)))\n",
        "    f.write(r.content)\n",
        "\n",
        "w2v_base = '.'\n",
        "w2v_file = os.path.join(w2v_base, w2v_fname)\n",
        "w2v = KeyedVectors.load_word2vec_format(w2v_fname, \n",
        "                                        encoding='utf-8', \n",
        "                                        unicode_errors='replace',\n",
        "                                        binary=True) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhucI2Xp7Ndh"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "# 2021/Jan 近藤先生からいただいたオノマトペ辞典のデータ\n",
        "#ひとつ下の '日本語オノマトペ辞典4500より.xls' は著作権の問題があり，公にできません。\n",
        "# そのため Google Colab での解法，ローカルファイルよりアップロードする\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # ここで `日本語オノマトペ辞典4500より.xls` を指定してアップロードする\n",
        "\n",
        "ccap_base = '.'\n",
        "#onomatopea_excel = '日本語オノマトペ辞典4500より.xlsx'\n",
        "onomatopea_excel = '2021-0325日本語オノマトペ辞典4500より.xls'\n",
        "onmtp2761 = pd.read_excel(os.path.join(ccap_base, onomatopea_excel), sheet_name='2761語')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww3SKiEU8Ppl"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=2)  # numpy の表示桁数設定\n",
        "np.set_printoptions(suppress=False, formatter={'float': '{:6.3f}'.format})\n",
        "#torch.set_printoptions(precision=3)\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# word2vec データ処理のため gensim を使う\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "from scipy import stats\n",
        "\n",
        "#import tqdm\n",
        "import termcolor\n",
        "import jaconv  # ひらがなカタカナ変換用 `pip install jaconv` してください\n",
        "import japanize_matplotlib  # matplotlib の日本語表示"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26E50UhE8i1q"
      },
      "source": [
        "onomatopea = list(set(sorted(onmtp2761['オノマトペ'])))\n",
        "print('# オノマトペのうち，word2vec に登録があるかどうかを調査')\n",
        "kana_onmtp, kata_onmtp = [], []\n",
        "count = 0\n",
        "for word in onomatopea:\n",
        "    count += 1\n",
        "    if word in w2v.vocab:\n",
        "        kana_onmtp.append(word)\n",
        "\n",
        "    kata_w = jaconv.hira2kata(word)\n",
        "    if kata_w in w2v.vocab:\n",
        "        kata_onmtp.append(kata_w)\n",
        "        \n",
        "Vono = kana_onmtp + kata_onmtp\n",
        "\n",
        "print(f'Word2vec (wikipedida_ja を使って訓練) に存在するオノマトペ数: {len(Vono)}，全オノマトペ項目(小野オノマトペ辞典4500語) {len(onomatopea)} 語')\n",
        "print('総数がオノマトペデータより多いのは，平仮名表記とカタカナ表記と両者で wikipedia に登録があった場合に重複してカウントしているからです。')\n",
        "print(f'カタカナ オノマトペ総数: {len(kata_onmtp)}')\n",
        "print(f'ひらがな オノマトペ総数: {len(kana_onmtp)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddup5zEf-D3F"
      },
      "source": [
        "def P_orth(X):\n",
        "    \"\"\"Return Orthogonal Projection and its complement matrices of X\n",
        "    直交射影行列とその補空間への射影行列を返す。\n",
        "    X を (n行 m列)としたとき， (m,m) 型の逆行列を算出。(n,n)型ではないことに注意\n",
        "    \n",
        "    引数\n",
        "    x: np.array (n,m)\n",
        "        入力行列\n",
        "    戻り値\n",
        "    P: np.array (m,m)\n",
        "        射影行列\n",
        "    Q: np.arrary(m,m)\n",
        "        直交補空間への射影行列\n",
        "    \"\"\"\n",
        "    XT = X.T\n",
        "    X_XT = np.dot(X, XT)\n",
        "    iXXT = np.linalg.inv(X_XT)\n",
        "    XT_iXXT = np.dot(XT,iXXT) # np.dot(XT, iXXT)\n",
        "    XT_iXXT_X = np.dot(XT_iXXT, X)\n",
        "    P = XT_iXXT_X\n",
        "    I = np.eye((P.shape[0]))\n",
        "    Q = I - P\n",
        "    return P, Q\n",
        "\n",
        "def w2vMat(w2v=w2v, wordlist=['イヌ','ネコ', 'トラ', 'ライオン']):\n",
        "    \"\"\"len(wordlist)行，word2vec 次元数 列を持つ行列 を返す\"\"\"\n",
        "\n",
        "    if w2v == None:\n",
        "        assert('Set a `gensim.models.keyedvectors.Word2VecKeyedVectors` as an w2v argument')\n",
        "        \n",
        "    # 行列の確保\n",
        "    X = np.zeros((len(set(wordlist)), w2v.vector_size), dtype=np.float)\n",
        "        \n",
        "    # 各行に word2vec ベクトルをコピー\n",
        "    for i, w in enumerate(wordlist):\n",
        "        X[i] = np.copy(w2v[w])\n",
        "            \n",
        "    return X\n",
        "\n",
        "\n",
        "Mono = w2vMat(wordlist=Vono)\n",
        "print(f'#全オノマトペ項目を用いた単語埋め込みベクトル行からなる行列のサイズ:{Mono.shape}')\n",
        "P_ono, Q_ono = P_orth(Mono)\n",
        "print(P_ono.shape, Q_ono.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BoTjESU-GeT"
      },
      "source": [
        "#カタカナ全文字を `kata_chars` へ保存\n",
        "kata_chars = [c for c in 'ァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモャヤュユョヨラリルレロヮワヰヱヲンヴヵヶ']\n",
        "\n",
        "vocabs = set(sorted(list(w2v.vocab)[1:]))  # word2vec に登録されている全単語を保存\n",
        "kata_words = []  # word2vec in wikipedia.ja に登録されている全カタカナリストを作成\n",
        "for w in Vono:\n",
        "    # 全単語リスト `vocabs` からカタカナだけで構成されている単語を kata_words に登録\n",
        "    kata_flag = True\n",
        "    for c in w:\n",
        "        if c not in kata_chars:\n",
        "            kata_flag = False\n",
        "            break\n",
        "    if kata_flag == True:\n",
        "        kata_words.append(w)\n",
        "\n",
        "#結果の確認\n",
        "print('カタカナ単語総数:{0}, 総語彙数:{1}, 比率(%):{2:.3f}'.format(len(kata_words), \n",
        "                                                     len(vocabs), \n",
        "                                                     len(kata_words)/len(vocabs) * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rZdUTBo-Zso"
      },
      "source": [
        "n = 0\n",
        "onmtp_w2v_kata = []  #カタカナ表記のオノマトペリスト\n",
        "onmtp_w2v_hira = []  #ひらがな表記のオノマトペリスト\n",
        "\n",
        "#全カタカナ語についてオノマトペデータベースに登録のある単語であればリストに追加\n",
        "for w in kata_words:\n",
        "    if w in Vono:\n",
        "        # カタカナ単語がオノマトペ辞典に載っている単語であればリストに追加\n",
        "        onmtp_w2v_kata.append(w)\n",
        "\n",
        "    w = jaconv.kata2hira(w)  # ひらがなに変換\n",
        "    if w in Vono:\n",
        "        # ひらがなに変換した単語がオノマトペ辞典に載っている単語であればリストに追加\n",
        "        onmtp_w2v_hira.append(w)\n",
        "\n",
        "print(f'# wikipeida.ja に登録されているカタカナ単語 {len(kata_words)} 語のうち')\n",
        "print(f'# オノマトペ辞典に載っている単語数:{len(onmtp_w2v_kata)}, ひらがな変換するとオノマトペ辞典に載っている単語数 {len(onmtp_w2v_hira)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM6-Ccmd-q04"
      },
      "source": [
        "print('#上で調べたword2vecにエントリの存在するオノマトペ全単語の品詞を MeCab を使って調べる')\n",
        "import MeCab\n",
        "\n",
        "#m = MeCab.Tagger() #形態素解析用オブジェクトの宣言\n",
        "\n",
        "kata_pos_wiki = {}\n",
        "print('#日本語ウィキペディア全カタカナ単語の場合:')\n",
        "for w in kata_words:\n",
        "    pos = str(m.parse(w).strip().split(',')[1:3])\n",
        "    if pos in kata_pos_wiki:\n",
        "        kata_pos_wiki[pos] += 1\n",
        "    else:\n",
        "        kata_pos_wiki[pos] = 1\n",
        "\n",
        "print(json.dumps(kata_pos_wiki, ensure_ascii=False, indent=4))\n",
        "\n",
        "print('#そのうちオノマトペ辞典に登録されている単語の場合:')\n",
        "onmtp_set = set(onmtp_w2v_kata + onmtp_w2v_hira)\n",
        "onmtp_pos = {}\n",
        "for w in onmtp_set:\n",
        "    #pos = str(m.parse(w).strip().split(',')[1:3])\n",
        "    w_ = MeCab.Tagger().parse(w).strip().split(',')\n",
        "    pos = w_[1] + w_[2]\n",
        "    if pos not in onoma_pos:\n",
        "        onmtp_pos[pos] = list()\n",
        "    onmtp_pos[pos].append(w)\n",
        "\n",
        "#print(json.dumps(onoma_pos, ensure_ascii=False, indent=4))\n",
        "print(list(onmtp_pos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77-LlTgfBmXd"
      },
      "source": [
        "MeCab.Tagger().parse('りんご').strip().split('\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_0zYeEn-tq_"
      },
      "source": [
        "#視覚化のためのライブラリを読み込む\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "np.set_printoptions(suppress=False, formatter={'float': '{:7.5f}'.format})\n",
        "\n",
        "onmtp_list =list(onmtp_set)\n",
        "Onmtp_w2v = w2vMat(wordlist=onmtp_list)\n",
        "\n",
        "Onmtp_w2v_norm = np.array([x/np.linalg.norm(x) for x in Onmtp_w2v])\n",
        "R_onmtp = Onmtp_w2v_norm.dot(Onmtp_w2v_norm.T)\n",
        "print(R_onmtp.shape)\n",
        "\n",
        "R_onmtp_df = pd.DataFrame(data=R_onmtp, index=onmtp_list)\n",
        "fig, ax = plt.subplots(figsize=(12,10))         # Sample figsize in inches\n",
        "sns.heatmap(R_onmtp_df, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DDbA002-v8h"
      },
      "source": [
        "def ax_scatter_gram(ax, p1, p2, wordlist, title=None, fontsize=10, color='cyan', x_label=\"第1軸\", y_label=\"第2軸\"):\n",
        "    ax.scatter(p1, p2, s=60, color=color)\n",
        "    for i, label in enumerate(wordlist):\n",
        "        ax.annotate(label, (p1[i], p2[i]),fontsize=fontsize)\n",
        "    ax.set_xlabel(x_label)\n",
        "    ax.set_ylabel(y_label)\n",
        "    ax.set_title(title,fontsize=fontsize*1.2)\n",
        "\n",
        "def plot_pca(ax, R, wordlist, title=\"\"):\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(R)\n",
        "    pca1, pca2 = pca_result[:,0], pca_result[:,1] \n",
        "    print('\\tExplained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "    ax_scatter_gram(ax, pca1, pca2, wordlist, title=title, x_label=\"第一主成分\", y_label=\"第二主成分\")\n",
        "\n",
        "def plot_tsne(ax, R, wordlist, title=\"\"):\n",
        "    #tsne = TSNE()\n",
        "    tsne_result = TSNE(n_components=2).fit_transform(R)\n",
        "    print(tsne_result.shape)\n",
        "    tsne1, tsne2 = tsne_result[:,0], tsne_result[:,1]\n",
        "    ax_scatter_gram(ax, tsne1, tsne2, wordlist, title=title, x_label=\"tSNE 1\", y_label=\"tSNE 2\")\n",
        "\n",
        "\n",
        "###plot_pca(ax, Onmtp_w2v, onmtp_list, title='オノマトペ附置 (PCA)')\n",
        "    \n",
        "fig, ax = plt.subplots(figsize=(12,13))         # Sample figsize in inches\n",
        "plot_pca(ax, R_onmtp, onmtp_list, title='オノマトペ附置 (PCA)')\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,13))         # Sample figsize in inches\n",
        "plot_tsne(ax, R_onmtp, onmtp_list, title='オノマトペ附置(tSNE)')\n",
        "plt.show()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcDgs0fB-31t"
      },
      "source": [
        "#東大の松下研究室の辞書を利用\n",
        "\n",
        "import requests\n",
        "\n",
        "url='http://www17408ui.sakura.ne.jp/tatsum/database/VDLJ_Ver1_0_General-Learners_Basic-2500.xlsx'\n",
        "excel_fname = url.split('/')[-1]\n",
        "r = requests.get(url)\n",
        "with open(excel_fname, 'wb') as f:\n",
        "    total_length = int(r.headers.get('content-length'))\n",
        "    print('Downloading {0} - {1} bytes'.format(excel_fname, (total_length)))\n",
        "    f.write(r.content)\n",
        "\n",
        "x = pd.read_excel(excel_fname, sheet_name='基本語2500　Basic 2500 Words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxIS1lEC-7Ah"
      },
      "source": [
        "x = pd.read_excel(excel_fname, sheet_name='基本語2500　Basic 2500 Words')\n",
        "xx = x[['ふつうの（新聞の）書きかた\\nStandard (Newspaper) Orthography','ふつうの読みかた（カタカナ）\\nStandard Reading (Katakana)','品詞\\nPart of Speech']]\n",
        "x_tiny = xx.rename(columns = {'ふつうの（新聞の）書きかた\\nStandard (Newspaper) Orthography':'word',\n",
        "                              'ふつうの読みかた（カタカナ）\\nStandard Reading (Katakana)': 'yomi',\n",
        "                              '品詞\\nPart of Speech':'POS'}, inplace = False)\n",
        "\n",
        "basic_list = [w for w in list(sorted(set(x_tiny['word']))) if w in w2v]\n",
        "Mat_basic = w2vMat(wordlist=basic_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-v3UabL--9q"
      },
      "source": [
        "Mat_basic_norm = np.array([x/np.linalg.norm(x) for x in Mat_basic])\n",
        "R_basic = Mat_basic_norm.dot(Mat_basic_norm.T)\n",
        "print(R_basic.shape)\n",
        "\n",
        "RR_df = pd.DataFrame(data=R_basic, index=basic_list)\n",
        "fig, ax = plt.subplots(figsize=(12,10))         # Sample figsize in inches\n",
        "sns.heatmap(RR_df, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60WxG465_B5V"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12,13))         # Sample figsize in inches\n",
        "plot_pca(ax, R_basic, basic_list, title='松下基本単語 (PCA)')\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,13))         # Sample figsize in inches\n",
        "plot_tsne(ax, R_basic, basic_list, title='松下基本単語(tSNE)')\n",
        "plt.show()    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}