{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-0316onematopea.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMvIoxFtA0fLXu6+XEUWbgu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021_0316onematopea.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wYkhDLKHG7J"
      },
      "source": [
        "# Prof. Kondo onomatopea\n",
        "- date: 2021-0316\n",
        "- file: 2021-0316onematopea.ipynb\n",
        "\n",
        "近藤先生から頂いた課題の回答\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHK8tCZLHDIg"
      },
      "source": [
        "# word2vec データの読み込み\n",
        "# ファイルの所在に応じて変更してください\n",
        "w2v_base = './'\n",
        "w2v_file = '2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWOdKjAkHTPi"
      },
      "source": [
        "#ひとつ下の '日本語オノマトペ辞典4500より.xls' は著作権の問題があり，公にできません。\n",
        "# そのため Google Colab での解法，ローカルファイルよりアップロードする\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # ここで `日本語オノマトペ辞典4500より.xls` を指定してアップロードする\n",
        "\n",
        "# word2vec データの読み込み\n",
        "# ローカルディスクから読み込むようになっています。colab でお使いの場合には適宜変更してください\n",
        "# word2vec の訓練済モデルを入手\n",
        "!wget http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz\n",
        "#!wget http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_sgns.bin.gz\n",
        "#!wget http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid300_win20_neg20_sgns.bin.gz\n",
        "#!wget http://www.cis.twcu.ac.jp/~asakawa/2017jpa/2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO2MLNtFHZPx"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=2)  # numpy の表示桁数設定\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# word2vec でーた処理のため gensim を使う\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "from scipy import stats\n",
        "\n",
        "import termcolor\n",
        "import tqdm\n",
        "\n",
        "# Colab では以下の 2 行の行頭の # を削除してから実行してください\n",
        "!pip install jaconv\n",
        "!pip install japanize_matplotlib\n",
        "import jaconv  # ひらがなカタカナ変換用 `pip install jaconv` してください\n",
        "import japanize_matplotlib  # matplotlib の日本語表示\n",
        "\n",
        "# 2021/Jan 近藤先生からいただいたオノマトペ辞典のデータ\n",
        "ccap_base = './'\n",
        "#onomatopea_excel = '日本語オノマトペ辞典4500より.xlsx'\n",
        "onomatopea_excel = '日本語オノマトペ辞典4500より.xls'\n",
        "onmtp2761 = pd.read_excel(os.path.join(ccap_base, onomatopea_excel), sheet_name='2761語')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1VEQrdwHr5K"
      },
      "source": [
        "# 訓練済 word2vec，訓練データは wikipedia 全文  読み込みに時間がかかります\n",
        "#w2v_base = '/Users/asakawa/study/2016wikipedia/'\n",
        "#w2v_file = '2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_cbow.bin.gz'\n",
        "#w2v_file = '2017Jul_jawiki-wakati_neologd_hid200_win20_neg10_cbow.bin.gz'\n",
        "#w2v_file = '2017Jul_jawiki-wakati_neologd_hid200_win20_neg20_sgns.bin.gz'\n",
        "#w2v_file = '2017Jul_jawiki-wakati_neologd_hid200_win20_neg10_sgns.bin.gz'\n",
        "asakawa_w2v_file = os.path.join(w2v_base, w2v_file)\n",
        "asakawa_w2v = KeyedVectors.load_word2vec_format(asakawa_w2v_file, \n",
        "                                                encoding='utf-8', \n",
        "                                                unicode_errors='replace',\n",
        "                                                binary=True) \n",
        "\n",
        "w2v = asakawa_w2v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAJ7U3RdHud4"
      },
      "source": [
        "onomatopea = list(set(sorted(onmtp2761['オノマトペ'])))\n",
        "\n",
        "print('# オノマトペのうち，word2vec に登録があるかどうかを調査')\n",
        "kana_entries, kata_entries = [], []\n",
        "count = 0\n",
        "for word in onomatopea:\n",
        "    count += 1\n",
        "    if word in w2v.vocab:\n",
        "        kana_entries.append(word)\n",
        "\n",
        "    kata_w = jaconv.hira2kata(word)\n",
        "    if kata_w in w2v.vocab:\n",
        "        kata_entries.append(kata_w)\n",
        "        \n",
        "entries = kana_entries + kata_entries\n",
        "\n",
        "#print(len(kana_entries), len(kata_entries), len(onomatopea), count)\n",
        "print('There are ', len(entries), ' in ', len(onomatopea), ' onomatopea words in word2vec from jawikipedia')\n",
        "print('総数がオノマトペデータより多いのは，平仮名表記とカタカナ表記と両者で wikipedia に登録があった場合に重複してカウントしているからです。')\n",
        "print('カタカナ オノマトペ総数:', len(kata_entries))\n",
        "print('ひらがな オノマトペ総数:', len(kana_entries))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E6yJF8bHyLB"
      },
      "source": [
        "test_n = 10  # 何語ランダムサンプリングするかを決める数\n",
        "topn = 5    # 上位 何語を表示するかを決める数\n",
        "print('確認用 {} 個のオノマトペをランダムサンプリングして表示。実行毎に結果が異なります:'.format(test_n))\n",
        "for _ in range(test_n):\n",
        "    word = np.random.choice(entries)\n",
        "    print('単語:{0}, ID:{1}'.format(termcolor.colored(word,'green'), entries.index(word)), end=\" \")\n",
        "    print('word2vec の単純な最近隣語:',  termcolor.colored([x[0] for x in w2v.similar_by_word(word,topn=topn)],'blue'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oALY8EhvI7bB"
      },
      "source": [
        "print('ここからが本体')\n",
        "class projection():\n",
        "    \"\"\"射影行列の作成\n",
        "    オノマトペ空間への射影行列の作成\n",
        "    \n",
        "    引数:\n",
        "    w2v: gensim.KeyedVectors()\n",
        "        訓練済 word2vec データ\n",
        "    wordlist: list\n",
        "        射影空間を作成する語彙リスト\n",
        "    \"\"\"\n",
        "    def __init__(self, w2v=w2v, wordlist=entries,topn=5):\n",
        "        self.topn=10\n",
        "        self.entries = wordlist\n",
        "        self.w2v = w2v\n",
        "        \n",
        "        # 行列 P の初期化 entries 行，word2vec 次元 行の行列 \n",
        "        self.X = np.zeros((len(set(self.entries)), w2v.vector_size), dtype=np.float)\n",
        "        \n",
        "        # 各行に word2vec ベクトルをコピー\n",
        "        for i, x in enumerate(set(self.entries)):\n",
        "            self.X[i] = np.copy(self.w2v[x])\n",
        "        \n",
        "        invX = np.linalg.inv(np.dot(self.X, self.X.T))   #  (X X^T)^{-1}\n",
        "        self.P = np.dot(self.X.T, np.dot(invX, self.X))  # X^T (X X^T)^{-1} X 射影行列\n",
        "        #self.P_ = np.dot(self.X.T, self.X)               # 規格化していない射影行列\n",
        "\n",
        "        self.I = np.eye(len(self.P))              # 単位行列\n",
        "        self.C = self.I - self.P                  # 直交補空間\n",
        "        XXT = np.dot(self.X, self.X.T)       # shpae(dim, dim)\n",
        "        iXXT = np.linalg.inv(XXT)            # shape(dim, dim)\n",
        "        XTiXTX = np.dot(self.X.T, iXXT)      # shape(w2v_dim, dim)\n",
        "        self.P = np.dot(XTiXTX,self.X)       # shape(w2v_dim, w2v_dim)\n",
        "        self.CovP = np.dot(self.X.T,self.X)  # shape(w2v_dim, w2v_dim)\n",
        "        self.CorP = np.corrcoef(self.X.T)    # shape(w2v_dim, w2v_dim)\n",
        "\n",
        "\n",
        "    def prointProj(self, targets=['電車','ネコ','東京'], topn=5):\n",
        "        \"\"\"targets で指定された単語を射影したベクトルから得られる最近接語の印字\"\"\"\n",
        "        for target in targets:\n",
        "            if not target in self.w2v:\n",
        "                continue\n",
        "            x = np.array(self.w2v[target])\n",
        "            Px = np.dot(self.P, x)\n",
        "            print(target, ':--> ', end=\"\")\n",
        "            for word in self.w2v.similar_by_vector(Px,topn=topn):\n",
        "                print(word[0], end=\" \")\n",
        "            print()\n",
        "            Cx = np.dot(self.C, x)\n",
        "            print('\\t 直交補空間への射影:--> ', end='')\n",
        "            for word in w2v.similar_by_vector(Cx,topn=topn):\n",
        "                print(word[0], end=\" \")\n",
        "            print()\n",
        "            \n",
        "            \n",
        "    def getProjWord(self, targets=['電車','ネコ','東京'], topn=5, Proj=None):\n",
        "        \"\"\"targets で指定された単語を射影したベクトルから得られる最近接語リストを返す\"\"\"\n",
        "        if Proj == 'Prj':\n",
        "            P = self.P\n",
        "        if Proj == 'Cov':\n",
        "            P = self.CovP\n",
        "        elif Proj == 'Cor':\n",
        "            P = self.CorP\n",
        "        elif Proj == 'w2v':\n",
        "            P = self.I\n",
        "        else:\n",
        "            P = self.P\n",
        "        \n",
        "        if topn == None:\n",
        "            topn = self.topn\n",
        "\n",
        "        ret = {}\n",
        "        for target in targets:\n",
        "            if not target in self.w2v:\n",
        "                continue\n",
        "            x = np.array(self.w2v[target])\n",
        "            Px = np.dot(P, x)\n",
        "            retdic = self.w2v.similar_by_vector(Px,topn=topn)\n",
        "            ret[target] = [x[0] for x in retdic]\n",
        "        return ret\n",
        "\n",
        "\n",
        "    def __call__(self, targets=['魚', '肴', 'さかな', 'サカナ'], Proj=None,topn=5):\n",
        "        if Proj == None:\n",
        "            Proj = 'Prj'\n",
        "        if topn == None:\n",
        "            topn = self.topn\n",
        "        return self.getProjWord(targets, Proj=Proj, topn=topn)\n",
        "\n",
        "\n",
        "    \n",
        "topn = 5\n",
        "P = projection(topn=topn)\n",
        "\n",
        "# 以下はテスト用\n",
        "print(P.getProjWord())\n",
        "word = 'ペチャクチャ'\n",
        "print(termcolor.colored(word, 'green'), P.getProjWord([word, 'イヌ'],topn=topn))\n",
        "\n",
        "#v = np.dot(P.CorP, w2v[word])\n",
        "#print([x[0] for x in w2v.similar_by_vector(v)])\n",
        "\n",
        "#v = np.dot(P.CovP, w2v[word])\n",
        "#print([x[0] for x in w2v.similar_by_vector(v)])\n",
        "for Proj in ['Prj', 'Cov', 'Cor', 'w2v']:\n",
        "    print(Proj, P([word], Proj=Proj, topn=topn))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcGiRBIjUWNr"
      },
      "source": [
        "print('# 近藤先生との相談で，各オノマトペの射影行列がどのくらいオノマトペを含むかを調べる')\n",
        "\n",
        "topn = 1024  # 上位何語分を検討するかを決める数\n",
        "#topn = 3\n",
        "print('# 上位 {0} 最隣接語を対象とする。\\nオノマトペ総数:{1}'.format(topn,len(entries)))\n",
        "\n",
        "def hit_words(word_list):\n",
        "    hit_list = []\n",
        "    for w in word_list:\n",
        "        if w in entries:\n",
        "            hit_list.append(w)\n",
        "    return hit_list\n",
        "\n",
        "\n",
        "count_dic = {}\n",
        "for i, word in tqdm.tqdm(enumerate(set(entries[:]))):\n",
        "    #print('{:07d} words processed    \\r'.format(i), end=\"\")\n",
        "    count_dic[word] = {}\n",
        "    for Proj in ['Prj', 'Cov', 'Cor', 'w2v']:\n",
        "        _list = P([word], Proj=Proj, topn=topn)\n",
        "        hit_list = hit_words(_list[word])\n",
        "        count_dic[word][Proj] = len(hit_list)\n",
        "\n",
        "#print(json.dumps(count_dic, ensure_ascii=False, indent=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWnjxiOoVMRY"
      },
      "source": [
        "#  結果を保存しローカルファイルにダウンロード\n",
        "json_file = 'onematopea_count.json'\n",
        "excel_file = 'onomatopea_count.xlsx'\n",
        "with open(json_file, 'w') as f:\n",
        "    json.dump(count_dic, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "df_json = pd.read_json(json_file).transpose()\n",
        "\n",
        "df_json.to_excel(excel_file)\n",
        "\n",
        "# ダウンロードする場合には，次行の行頭の # を削除してください。\n",
        "#files.download(excel_file)\n",
        "print(df_json.head())\n",
        "df_json.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnEmeBeLULzg"
      },
      "source": [
        "test_n = 5\n",
        "all_words = list(w2v.vocab.keys())[1:]  # 最初は '</S>' なので除外\n",
        "print('確認用 全 wikipedia.ja 登録単語数 {0} から {1} 語をランダムサンプリング'.format(len(all_words), test_n), end=\"\")\n",
        "print('実行毎に結果が異なります:\\n')\n",
        "for _ in range(test_n):\n",
        "    word = np.random.choice(all_words)\n",
        "    print('ターゲット単語:{0}, 射影ベクトルの最近隣語:{1}'.format(\n",
        "        termcolor.colored(word,'green'), \n",
        "        termcolor.colored(P([word], topn=3),'blue')))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqi8bmetcMdk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}