{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "2021_0429onomatopia_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2021_0429onomatopia_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nJtJ-UJ6p9t"
      },
      "source": [
        "# オノマトペの音韻表現を得る"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "AwJJycyt6p90",
        "outputId": "34d88df1-eb2a-4161-b5da-b5b90f225ffa"
      },
      "source": [
        "# オノマトペ4500本から wikipedia_ja にエントリがあった単語を抜き出したデータファイルが\n",
        "# onomatopa_list.txt である\n",
        "# データの読み込み\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "with open('onomatopea_list.txt', 'r') as f:\n",
        "    a = f.readlines()\n",
        "onmtp_list = [w.strip() for w in a]  # 行末の改行記号の切り取り\n",
        "\n",
        "!wget https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/2020ccap/ja_util.py\n",
        "# ja_util.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b7bd6ffa-c26f-464d-aa61-6ae50faaaba6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b7bd6ffa-c26f-464d-aa61-6ae50faaaba6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--2021-04-29 04:55:44--  https://raw.githubusercontent.com/ShinAsakawa/ShinAsakawa.github.io/master/2020ccap/ja_util.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25198 (25K) [text/plain]\n",
            "Saving to: ‘ja_util.py.2’\n",
            "\n",
            "\rja_util.py.2          0%[                    ]       0  --.-KB/s               \rja_util.py.2        100%[===================>]  24.61K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-04-29 04:55:44 (24.0 MB/s) - ‘ja_util.py.2’ saved [25198/25198]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zukgHHol9Jj2",
        "outputId": "b2e14367-b37f-40d9-ca7c-84653aabd8b1"
      },
      "source": [
        "#!pip install mecab-python3==0.996.3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mecab-python3==0.996.3 in /usr/local/lib/python3.7/dist-packages (0.996.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ylY8v2L6p91",
        "outputId": "33c44594-4b26-4386-d9d4-5faeb2eb0d26"
      },
      "source": [
        "# ja_util が  mora_wakati() が開発中のため，再読み込みが必要なので autoreload 2\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "import ja_util\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "!pip install jaconv\n",
        "import jaconv"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jaconv in /usr/local/lib/python3.7/dist-packages (0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIyJHXlX6p91"
      },
      "source": [
        "# データの作成\n",
        "onmtp_dict = {}\n",
        "mora_list = []\n",
        "max_word_len = 0\n",
        "for word in onmtp_list:\n",
        "    word_len = len(word)\n",
        "    if max_word_len < word_len:\n",
        "        max_word_len = word_len\n",
        "    morae = ja_util.mora_wakati().parse(word)\n",
        "    for mora in morae:\n",
        "        if not mora in mora_list:\n",
        "            mora_list.append(mora)\n",
        "    onmtp_dict[word] = morae\n",
        "\n",
        "mora_list = sorted(mora_list)\n",
        "mora_list.insert(0,'<eow>')\n",
        "mora_list.append('<sow>')\n",
        "#print(mora_list)\n",
        "#print(onmtp_dict)\n",
        "\n",
        "mora2idx = {m:i for i, m in enumerate(mora_list)}\n",
        "idx2mora = {i:m for i, m in enumerate(mora_list)}\n",
        "#print(mora2idx)\n",
        "#print(idx2mora)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5kAWUXT6p92"
      },
      "source": [
        "X = np.zeros((len(onmtp_list), max_word_len+2), dtype=np.int)\n",
        "for i, word in enumerate(onmtp_list):\n",
        "    X[i,0] = mora2idx['<sow>']\n",
        "    for j, mora in enumerate(onmtp_dict[word]):\n",
        "        X[i,j+1] = mora2idx[mora]\n",
        "    X[i,j+2] = mora2idx['<eow>']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4PeA9N76p92"
      },
      "source": [
        "data = X\n",
        "#chars = list(set(data))\n",
        "chars = mora_list\n",
        "#n_data, n_vocab = len(data), len(chars)\n",
        "n_data = X.shape[0]\n",
        "n_vocab = len(mora_list)\n",
        "\n",
        "#print(f'#データの総文字数:{n_data}\\t文字種:{n_vocab}')\n",
        "#chr2idx = { ch:i for i,ch in enumerate(chars) }\n",
        "#idx2chr = { i:ch for i,ch in enumerate(chars) }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIY3qyIU6p93"
      },
      "source": [
        "# ハイパーパラメータ\n",
        "n_hid = 20           # size of hidden layer of neurons\n",
        "seq_len = X.shape[1] # number of steps to unroll the RNN for\n",
        "lr = 1e-1\n",
        "\n",
        "# 結合係数行列とバイアス項の初期化\n",
        "Wxh = np.random.randn(n_hid, n_vocab) * 0.01 # input to hidden\n",
        "Whh = np.random.randn(n_hid, n_hid)   * 0.01 # hidden to hidden\n",
        "Why = np.random.randn(n_vocab, n_hid) * 0.01 # hidden to output\n",
        "bh = np.zeros((n_hid, 1)) # hidden bias\n",
        "by = np.zeros((n_vocab, 1)) # output bias"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE-swGj56p93"
      },
      "source": [
        "def loss_f(inputs, targets, hprev):\n",
        "    \"\"\"\n",
        "    inputs,targets are both list of integers.\n",
        "    hprev is Hx1 array of initial hidden state\n",
        "    returns the loss, gradients on model parameters, and last hidden state\n",
        "    \"\"\"\n",
        "    x_state, h_state, y_state, prob_state = {}, {}, {}, {}\n",
        "    h_state[-1] = np.copy(hprev)\n",
        "    loss = 0\n",
        "    \n",
        "    # forward pass\n",
        "    for t in range(len(inputs)):\n",
        "        x_state[t] = np.zeros((n_vocab,1)) # encode in 1-of-k representation\n",
        "        x_state[t][inputs[t]] = 1\n",
        "        h_state[t] = np.tanh(np.dot(Wxh, x_state[t]) + np.dot(Whh, h_state[t-1]) + bh) # hidden state\n",
        "        y_state[t] = np.dot(Why, h_state[t]) + by     # unnormalized log probabilities for next chars\n",
        "        prob_state[t] = np.exp(y_state[t]) / np.sum(np.exp(y_state[t])) # probabilities for next chars\n",
        "        loss += -np.log(prob_state[t][targets[t],0]) # softmax (cross-entropy loss)  \n",
        "        \n",
        "    # backward pass: compute gradients going backwards\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "    dhnext = np.zeros_like(h_state[0])\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(prob_state[t])\n",
        "        # backprop into y. \n",
        "        #see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
        "        dy[targets[t]] -= 1      \n",
        "        dWhy += np.dot(dy, h_state[t].T)\n",
        "        dby += dy\n",
        "        \n",
        "        # backprop into h\n",
        "        dh = np.dot(Why.T, dy) + dhnext \n",
        "        \n",
        "        #backprop through tanh nonlinearity        \n",
        "        delta = (1 - h_state[t] * h_state[t]) * dh\n",
        "        dbh  += delta\n",
        "        dWxh += np.dot(delta, x_state[t].T)\n",
        "        dWhh += np.dot(delta, h_state[t-1].T)\n",
        "        dhnext = np.dot(Whh.T, delta)\n",
        "            \n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
        "    return loss, dWxh, dWhh, dWhy, dbh, dby, h_state[len(inputs)-1]\n",
        "\n",
        "def sample(hprev, seed, n):\n",
        "    \"\"\" \n",
        "    sample a sequence of integers from the model \n",
        "    h is memory state, seed_ix is seed letter for first time step\n",
        "    \"\"\"\n",
        "    x = np.zeros((n_vocab, 1))\n",
        "    hid = hprev\n",
        "    x[seed] = 1\n",
        "    idxes = []\n",
        "    for t in range(n):\n",
        "        hid = np.tanh(np.dot(Wxh, x) + np.dot(Whh, hid) + bh)\n",
        "        out = np.dot(Why, hid) + by\n",
        "        prob = np.exp(out) / np.sum(np.exp(out))\n",
        "        idx = np.random.choice(range(n_vocab), p=prob.ravel())\n",
        "        x = np.zeros((n_vocab, 1))\n",
        "        x[idx] = 1\n",
        "        idxes.append(idx)\n",
        "    return idxes"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJSekxqg6p94"
      },
      "source": [
        "pos = 0\n",
        "\n",
        "# m で始まる変数は，Adagrad で用いるメモリ変数。それぞれ，\n",
        "# mWxh: 入力から中間層への結合係数行列\n",
        "# mWhh: 中間層へのリカレント結合係数行列\n",
        "# mWhy: 中間層から出力層への結合係数行列\n",
        "# mbh: 中間層のバイアス項\n",
        "# mby: 出力層のバイアス項\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) \n",
        "\n",
        "smooth_loss = -np.log(1.0/n_vocab) * seq_len # 時刻 0 での損失値\n",
        "\n",
        "max_iter = 10 ** 5\n",
        "interval = max_iter >> 2\n",
        "losses = []\n",
        "for itr in range(max_iter):\n",
        "    \n",
        "    # prepare inputs (we're sweeping from left to right in steps seq_len long)\n",
        "    if pos + seq_len + 1 >= len(data) or itr == 0: \n",
        "        hprev = np.zeros((n_hid,1)) # reset RNN memory\n",
        "        pos = 0 # go from start of data\n",
        "    inputs = [chr2idx[ch] for ch in data[pos:pos+seq_len]]\n",
        "    targets = [chr2idx[ch] for ch in data[pos+1:pos+seq_len+1]]\n",
        "\n",
        "    # sample from the model now and then\n",
        "    if itr % interval == 0:\n",
        "        sample_ix = sample(hprev, inputs[0], 200)\n",
        "        txt = ''.join(idx2chr[ix] for ix in sample_ix)\n",
        "        print(f'--- 反復訓練数={itr} ---\\n{txt}\\n---')\n",
        "\n",
        "    # forward seq_len characters through the net and fetch gradient\n",
        "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = loss_f(inputs, targets, hprev)\n",
        "    losses.append(loss)\n",
        "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "    \n",
        "    if itr % interval == 0:\n",
        "        print(f'反復学習回数:{itr:05d} 損失値:{smooth_loss:.3f}') # print progress\n",
        "\n",
        "    # perform parameter update with Adagrad\n",
        "    for param, _delta, _Hessian in zip([Wxh, Whh, Why, bh, by], \n",
        "                                       [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                       [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "        _Hessian += _delta * _delta\n",
        "        param += -lr * _delta / np.sqrt(_Hessian + 1e-8) # adagrad update\n",
        "\n",
        "    pos += seq_len # move data pointer\n",
        "\n",
        "    \n",
        "hprev = np.zeros((n_hid,1)) # reset RNN memory\n",
        "sample_ix = sample(hprev, inputs[0], 200)\n",
        "txt = ''.join(idx2chr[ix] for ix in sample_ix)\n",
        "print(f'----\\n{txt}\\n----')\n",
        "plt.plot(losses)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtPYU02f6p95"
      },
      "source": [
        "# 確認のため再読み込み\n",
        "with open('onomatopea_morae.json', 'r') as f:\n",
        "    onmtp_data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaaPJBBO6p95"
      },
      "source": [
        "list(onmtp_data)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYX_8zI_6p96"
      },
      "source": [
        "print(sorted(mora_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_A9Ch3y6p96"
      },
      "source": [
        "mora2idx = {}\n",
        "mora2idx['<eow>'] = 0\n",
        "a = {}\n",
        "b = []\n",
        "for mora in mora_dict:\n",
        "    for m in mora_dict[mora]:\n",
        "        b.append(m)\n",
        "        if m in a:\n",
        "            a[m] += 1\n",
        "        else:\n",
        "            a[m] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uvVp1tJ6p96"
      },
      "source": [
        "mora2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVkB7liz6p96"
      },
      "source": [
        "m_dict = {}\n",
        "for i, m in enumerate(sorted(set(b))):\n",
        "    m_dict[i] = {'no':i,\n",
        "                 'idx': mora2idx[m],\n",
        "                 'frq': a[m]\n",
        "                }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9HN_trT6p97"
      },
      "source": [
        "mora2idx = ja_util.mora_wakati().mora2idx\n",
        "idx2mora = ja_util.mora_wakati().idx2mora\n",
        "\n",
        "start_mora = len(ja_util.mora_wakati().mora2idx) # + 1\n",
        "mora2idx['<sow>'] = start_mora\n",
        "idx2mora[start_mora] = '<sow>'\n",
        "\n",
        "end_mora = 0 \n",
        "\n",
        "max_mora_len = 9\n",
        "Z = np.zeros((len(onmtp_data), max_mora_len+1), dtype=np.int)\n",
        "for i, word in enumerate(onmtp_data):\n",
        "    max_mora_len = len(onmtp_data[word]) if max_mora_len < len(onmtp_data[word]) else max_mora_len\n",
        "    #print(word, onmtp_data[word], end=\"\\t\")\n",
        "    Z[i,0] = start_mora\n",
        "    for j, mora in enumerate(onmtp_data[word]):\n",
        "        Z[i,j+1] = int(mora2idx[mora])\n",
        "    Z[i,j+2] = end_mora"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJHl7hbo6p97"
      },
      "source": [
        "print(a)\n",
        "#print(mora_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFDA8-Gw6p97"
      },
      "source": [
        "#max_mora_len = 9\n",
        "#print(Z[0])\n",
        "#print(idx2mora)\n",
        "#print(mora2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l_vp8B46p97"
      },
      "source": [
        "#print(Z[0])\n",
        "#print(idx2kana)\n",
        "for i, w in enumerate(Z[:10]):\n",
        "    print(list(onmtp_data)[i], end=\" \")\n",
        "    for x in w:\n",
        "        if x > 0:\n",
        "            print(x, idx2mora[x], end=' ')\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdT_suGe6p98"
      },
      "source": [
        "import re\n",
        "#re_mora = re.compile(ja_util.mora_wakati().cond)\n",
        "\n",
        "c1 = '[ウクスツヌフムユルグズヅブプヴ][ァィェォ]' #ウ段＋「ァ/ィ/ェ/ォ」\n",
        "c2 = '[イキシシニヒミリギジヂビピ][ャュェョ]' #イ段（「イ」を除く）＋「ャ/ュ/ェ/ョ」\n",
        "c2 = '[イキシチニヒミリギジヂビピ][ャュェョ]' #イ段（「イ」を除く）＋「ャ/ュ/ェ/ョ」\n",
        "c3 = '[テデ][ィュ]' #「テ/デ」＋「ャ/ィ/ュ/ョ」\n",
        "c4 = '[ァィゥェォー]' #カタカナ１文字（長音含む）\n",
        "c5 = '[，、.。「」]'\n",
        "#c6 = '[ィ]' #カタカナ１文字（長音含む）\n",
        "cond = '('+c1+'|'+c2+'|'+c3+'|'+c4+'|'+c5+')'\n",
        "#cond = '('+self.c1+'|'+self.c2+'|'+self.c3+'|'+self.c4+'|'+self.c5+'|'+self.c6+')'\n",
        "re_mora = re.compile(cond)\n",
        "\n",
        "print(re_mora)\n",
        "re_mora.findall('キェピャーエィー')\n",
        "\n",
        "print(re_mora.findall('ホゲ'))\n",
        "print(re_mora.findall('キェピャーエィー'))\n",
        "print(re_mora.findall('ガッキュウホウカイ'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQgrteLM6p98"
      },
      "source": [
        "import numpy as np\n",
        "from gensim import corpora\n",
        "from collections import defaultdict\n",
        "from pprint import pprint\n",
        "\n",
        "documents = [\"Human machine interface for lab abc computer applications\",\n",
        "             \"A survey of user opinion of computer system response time\",\n",
        "             \"The EPS user interface management system\",\n",
        "             \"System and human system engineering testing of EPS\",\n",
        "             \"Relation of user perceived response time to error measurement\",\n",
        "             \"The generation of random binary unordered trees\",\n",
        "             \"The intersection graph of paths in trees\",\n",
        "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "             \"Graph minors A survey\"]\n",
        "stoplist = set('for a of the and to in'.split())\n",
        "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
        "         for document in documents]\n",
        "#texts_save = texts\n",
        "\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts1 = [[token for token in text if frequency[token] > 1]\n",
        "         for text in texts]\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "copspMky6p98"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkTFFPA06p99"
      },
      "source": [
        "#data I/O\n",
        "data_input = open('input.txt', 'r').read()\n",
        "#print(data_input)\n",
        "\n",
        "texts = [[ch for ch in line] for line in data_input.split()]\n",
        "#print(texts)\n",
        "\n",
        "freq = defaultdict(int)\n",
        "for text in texts:\n",
        "    for ch in text:\n",
        "        freq[ch] += 1\n",
        "print(freq)\n",
        "dic = corpora.Dictionary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z4edXPJ6p99"
      },
      "source": [
        "import unicodedata\n",
        "import os\n",
        "import sys\n",
        "import MeCab\n",
        "import glob\n",
        "\n",
        "# 岩下先生から頂いた「みんなの日本語」データの読み込み\n",
        "jlpt_base = '/Users/asakawa/study/2021jlpt'\n",
        "minnichi_files = sorted(glob.glob(os.path.join(jlpt_base, 'MINNICHI_*.txt')))\n",
        "\n",
        "# みんなの日本語テキストを読み込み\n",
        "minnichi_text = {}\n",
        "for file in minnichi_files:\n",
        "    fname = os.path.split(file)[-1].split('.')[0]\n",
        "\n",
        "    if not fname in minnichi_text:\n",
        "        minnichi_text[fname] = []\n",
        "    txt = []\n",
        "    with open(file,'r') as f:\n",
        "        texts = f.readlines()\n",
        "        \n",
        "        for txt in texts:\n",
        "            txt = txt.strip()\n",
        "            if len(txt) == 0: continue                 # 空行をスキップする\n",
        "            #txt = unicodedata.normalize(\"NFKC\", txt)  # 全角記号をざっくり半角へ置換（でも不完全）\n",
        "            txt = unicodedata.normalize(\"NFC\", txt)    # 全角記号をざっくり半角へ置換（でも不完全）\n",
        "            minnichi_text[fname].append(txt)\n",
        "\n",
        "print(minnichi_text['MINNICHI_D_005_03'])\n",
        "#print(minnichi_text['MINNICHI_D_005_03'][5].replace(\"　\",\"<spc>\"))\n",
        "#minnichi_text['MINNICHI_D_004_02']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nStuplvO6p99"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# ja_util は 2021 Apr に作った日本語処理関係\n",
        "import ja_util"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiIKpbWi6p99"
      },
      "source": [
        "#help(ja_util)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC8MtDis6p99"
      },
      "source": [
        "for fname in list(minnichi_text)[:3]:\n",
        "    for line in minnichi_text[fname]:\n",
        "        kata = MeCab.Tagger('-Oyomi').parse(line).strip()\n",
        "        morae = ja_util.mora_wakati().parse(kata)\n",
        "        print(f'{kata} ', end=\": \")\n",
        "        for mora in morae:\n",
        "            print(ja_util.mora_wakati().kana2mora[mora], end=\" \")\n",
        "        print()\n",
        "        #print(ja_util.mora_wakati().parse2romaji(kata))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1DXCskT6p9-"
      },
      "source": [
        "with open('minnichi_all.txt', 'w') as f:\n",
        "    for doc in minnichi_text:\n",
        "        for line in minnichi_text[doc]:\n",
        "            f.writelines(line+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGsSZ18d6p9-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "minnichi = open('minnichi_all.txt', 'r').read()\n",
        "data = minnichi\n",
        "chars = list(set(data))\n",
        "n_data, n_vocab = len(data), len(chars)\n",
        "print(f'#データの総文字数:{n_data}\\t文字種:{n_vocab}')\n",
        "chr2idx = { ch:i for i,ch in enumerate(chars) }\n",
        "idx2chr = { i:ch for i,ch in enumerate(chars) }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATirCiUI6p9-"
      },
      "source": [
        "params = {\n",
        "    'n_data': n_data, \n",
        "    'n_vocab': n_vocab, \n",
        "    'chr2idx': chr2idx, \n",
        "    'idx2chr':idx2chr\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MfHIjqi6p9-"
      },
      "source": [
        "# ハイパーパラメータ\n",
        "n_hid = 100  # size of hidden layer of neurons\n",
        "seq_len = 25 # number of steps to unroll the RNN for\n",
        "lr = 1e-1\n",
        "\n",
        "params['n_hid'] = n_hid\n",
        "params['seq_len'] = seq_len\n",
        "params['lr'] = lr\n",
        "\n",
        "# 結合係数行列とバイアス項の初期化\n",
        "Wxh = np.random.randn(n_hid, n_vocab)*0.01 # input to hidden\n",
        "Whh = np.random.randn(n_hid, n_hid)*0.01 # hidden to hidden\n",
        "Why = np.random.randn(n_vocab, n_hid)*0.01 # hidden to output\n",
        "bh = np.zeros((n_hid, 1)) # hidden bias\n",
        "by = np.zeros((n_vocab, 1)) # output bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmpV7AZE6p9_"
      },
      "source": [
        "print(list(params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K42HK3cm6p9_"
      },
      "source": [
        "def loss_f(inputs, targets, hprev):\n",
        "    \"\"\"\n",
        "    引数:\n",
        "    - inputs, targets: 共に int のリスト\n",
        "    - hprev: 隠れ層の初期状態 Hx1 (H 行 x 1 列)\n",
        "    \n",
        "    戻り値:\n",
        "    - loss: 損失値\n",
        "    - dWxh, dWhh, dWhy, dbh, dby: 勾配 \n",
        "    - h_state[len(inputs)-1]: 隠れ層の状態\n",
        "    \"\"\"\n",
        "    x_state, h_state, y_state, prob_state = {}, {}, {}, {}\n",
        "    h_state[-1] = np.copy(hprev)\n",
        "    loss = 0\n",
        "    \n",
        "    # 前向きパス forward pass\n",
        "    for t in range(len(inputs)):\n",
        "        x_state[t] = np.zeros((n_vocab,1)) # ワンホット表現 encode in 1-of-k representation\n",
        "        x_state[t][inputs[t]] = 1\n",
        "        h_state[t] = np.tanh(np.dot(Wxh, x_state[t]) + np.dot(Whh, h_state[t-1]) + bh) # 隠れ層の状態\n",
        "        y_state[t] = np.dot(Why, h_state[t]) + by     # unnormalized log probabilities for next chars\n",
        "        prob_state[t] = np.exp(y_state[t]) / np.sum(np.exp(y_state[t])) # probabilities for next chars\n",
        "        loss += -np.log(prob_state[t][targets[t],0]) # softmax (cross-entropy loss)  \n",
        "        \n",
        "    # backward pass: compute gradients going backwards\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "    dhnext = np.zeros_like(h_state[0])\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(prob_state[t])\n",
        "        # backprop into y. \n",
        "        #see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
        "        dy[targets[t]] -= 1      \n",
        "        dWhy += np.dot(dy, h_state[t].T)\n",
        "        dby += dy\n",
        "        \n",
        "        # backprop into h\n",
        "        dh = np.dot(Why.T, dy) + dhnext \n",
        "        \n",
        "        #backprop through tanh nonlinearity        \n",
        "        delta = (1 - h_state[t] * h_state[t]) * dh\n",
        "        dbh  += delta\n",
        "        dWxh += np.dot(delta, x_state[t].T)\n",
        "        dWhh += np.dot(delta, h_state[t-1].T)\n",
        "        dhnext = np.dot(Whh.T, delta)\n",
        "            \n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
        "    return loss, dWxh, dWhh, dWhy, dbh, dby, h_state[len(inputs)-1]\n",
        "\n",
        "def sample(hprev, seed, n):\n",
        "    \"\"\" \n",
        "    sample a sequence of integers from the model \n",
        "    h is memory state, seed_ix is seed letter for first time step\n",
        "    \"\"\"\n",
        "    x = np.zeros((n_vocab, 1))\n",
        "    hid = hprev\n",
        "    x[seed] = 1\n",
        "    idxes = []\n",
        "    for t in range(n):\n",
        "        hid = np.tanh(np.dot(Wxh, x) + np.dot(Whh, hid) + bh)\n",
        "        out = np.dot(Why, hid) + by\n",
        "        prob = np.exp(out) / np.sum(np.exp(out))\n",
        "        idx = np.random.choice(range(n_vocab), p=prob.ravel())\n",
        "        x = np.zeros((n_vocab, 1))\n",
        "        x[idx] = 1\n",
        "        idxes.append(idx)\n",
        "    return idxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoIqlG_N6p9_"
      },
      "source": [
        "pos = 0\n",
        "\n",
        "# m で始まる変数は，Adagrad で用いるメモリ変数。それぞれ，\n",
        "# mWxh: 入力から中間層への結合係数行列\n",
        "# mWhh: 中間層へのリカレント結合係数行列\n",
        "# mWhy: 中間層から出力層への結合係数行列\n",
        "# mbh: 中間層のバイアス項\n",
        "# mby: 出力層のバイアス項\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) \n",
        "\n",
        "smooth_loss = -np.log(1.0/n_vocab) * seq_len # 時刻 0 での損失値"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHUBL2fR6p-A"
      },
      "source": [
        "max_iter = 10 ** 4\n",
        "interval = max_iter >> 2\n",
        "losses = []\n",
        "for itr in range(max_iter):\n",
        "    \n",
        "    # prepare inputs (we're sweeping from left to right in steps seq_len long)\n",
        "    if pos + seq_len + 1 >= len(data) or itr == 0: \n",
        "        hprev = np.zeros((n_hid,1)) # reset RNN memory\n",
        "        pos = 0 # go from start of data\n",
        "    inputs = [chr2idx[ch] for ch in data[pos:pos+seq_len]]\n",
        "    targets = [chr2idx[ch] for ch in data[pos+1:pos+seq_len+1]]\n",
        "\n",
        "    # sample from the model now and then\n",
        "    if itr % interval == 0:\n",
        "        sample_ix = sample(hprev, inputs[0], 200)\n",
        "        txt = ''.join(idx2chr[ix] for ix in sample_ix)\n",
        "        print(f'--- 反復訓練数={itr} ---\\n{txt}\\n---')\n",
        "\n",
        "    # forward seq_len characters through the net and fetch gradient\n",
        "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = loss_f(inputs, targets, hprev)\n",
        "    losses.append(loss)\n",
        "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "    \n",
        "    if itr % interval == 0:\n",
        "        print(f'反復学習回数:{itr:05d} 損失値:{smooth_loss:.3f}') # print progress\n",
        "\n",
        "    # perform parameter update with Adagrad\n",
        "    for param, _delta, _Hessian in zip([Wxh, Whh, Why, bh, by], \n",
        "                                       [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                       [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "        _Hessian += _delta * _delta\n",
        "        param += -lr * _delta / np.sqrt(_Hessian + 1e-8) # adagrad update\n",
        "\n",
        "    pos += seq_len # move data pointer\n",
        "\n",
        "    \n",
        "hprev = np.zeros((n_hid,1)) # reset RNN memory\n",
        "sample_ix = sample(hprev, inputs[0], 200)\n",
        "txt = ''.join(idx2chr[ix] for ix in sample_ix)\n",
        "print(f'----\\n{txt}\\n----')\n",
        "plt.plot(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uXf5-hE6p-A"
      },
      "source": [
        "\n",
        "Wxh = np.load('2021_0426Wxh.npy')\n",
        "Whh = np.load('2021_0426Whh.npy')\n",
        "Why = np.load('2021_0426Why.npy')\n",
        "bh = np.load('2021_0426bh.npy')\n",
        "by = np.load('2021_0426by.npy')\n",
        "\n",
        "hprev = np.zeros((n_hid,1)) # reset RNN memory\n",
        "sample_ix = sample(hprev, inputs[0], 200)\n",
        "txt = ''.join(idx2chr[ix] for ix in sample_ix)\n",
        "print(f'----\\n{txt}\\n----')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwsceBTR6p-A"
      },
      "source": [
        "idx2chr[inputs[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf5cWxlf6p-B"
      },
      "source": [
        "- source: <https://levelup.gitconnected.com/8-built-in-functions-every-python-programmer-should-know-3552eb768894>\n",
        "\n",
        "1. hash()\n",
        "The `hash()` method is used to return the hash value of an object if it has one. \n",
        "Hash values are integer numbers that are used to compare dictionary keys during a dictionary lookup.\n",
        "\n",
        "\n",
        "2. map()\n",
        "The `map()` function allows you to execute a specified function for each item in an iterable that it takes as input(both function and iterable).\n",
        "\n",
        "6. ord()\n",
        "This function is used to return the Unicode code point of a given character. \n",
        "The ord() function takes a character as input and then returns an integer number representing the given input character’s Unicode code point.\n",
        "\n",
        "7. dir()\n",
        "`dir()` is a powerful python built-in function, that returns a valid list of all the attributes of the specified object. \n",
        "It returns all the properties, even built-in properties that are the default for all objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14CETPuL6p-D"
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# 表示精度桁数の設定\n",
        "#np.set_printoptions(suppress=False, formatter={'float': '{:7.4f}'.format})\n",
        "np.set_printoptions(suppress=False, formatter={'float': '{:6.3f}'.format})\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import SVG, display\n",
        "#display(SVG(filename='../figures/2018Roelofs_fig1.svg'))\n",
        "#display(SVG(filename='../figures/2018Roelofs_fig3.svg'))\n",
        "\n",
        "#print('概念モデル')\n",
        "#display(SVG(url='https://raw.githubusercontent.com/project-ccap/project-ccap.github.io/master/figures/2018Roelofs_fig3.svg'))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}