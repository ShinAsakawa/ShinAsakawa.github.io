{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020SightVisit_kmnist_prototype.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBqAEWFAGrn4/vn8JMFVA1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020SightVisit_kmnist_prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEy880CvA3to"
      },
      "source": [
        "# 資格スクエア G 検定対策ビデオ教材 の kmninst プロトタイプ\n",
        "\n",
        "- filename: `2020SightVisit_kmnist_prototype.ipynb`\n",
        "- author: 浅川伸一\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrsTNwHoAsUs"
      },
      "source": [
        "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz\n",
        "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz\n",
        "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz\n",
        "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omBDQrA6Do7n"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def load(f):\n",
        "    return np.load(f)['arr_0']\n",
        "\n",
        "# Load the data\n",
        "Xkm_train = load('kmnist-train-imgs.npz')\n",
        "Xkm_test = load('kmnist-test-imgs.npz')\n",
        "ykm_train = load('kmnist-train-labels.npz')\n",
        "ykm_test = load('kmnist-test-labels.npz')\n",
        "\n",
        "# Flatten images\n",
        "n_samples = 2000\n",
        "#x_train = x_train.reshape(-1, 784)[:n_samples]\n",
        "#y_train = y_train[:n_samples]\n",
        "#x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "ind2c =[c for c in 'おきすつなまはやれを']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFcgB-ttE3kb"
      },
      "source": [
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSpncSHhAuVW"
      },
      "source": [
        "# ライブラリの輸入\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision \n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UNg7HqzCg4R"
      },
      "source": [
        "# PyTorch image augmentation module\n",
        "class PyTorchImageDataset(Dataset):\n",
        "    def __init__(self, image_list, transforms=None):\n",
        "        self.image_list = image_list\n",
        "        self.transforms = transforms\n",
        "         \n",
        "    def __len__(self):\n",
        "        return (len(self.image_list))\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        image = self.image_list[i]\n",
        "        image = Image.fromarray(image).convert('RGB')        \n",
        "        image = np.asarray(image).astype(np.uint8)\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return torch.tensor(image, dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3p8gHsEG5v1"
      },
      "source": [
        "pytorch_dataset = PyTorchImageDataset(image_list=Xkm_train, transforms=None)\n",
        "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=16, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO9EZrJXGwPo"
      },
      "source": [
        "def show_img(img):\n",
        "    #plt.figure(figsize=(18,15))\n",
        "    # unnormalize\n",
        "    #img = img / 2 + 0.5  \n",
        "    npimg = img.numpy()\n",
        "    npimg = np.clip(npimg, 0., 1.)\n",
        "    plt.imshow(np.transpose(npimg, (0, 1, 2)))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRKkX22qHN2X"
      },
      "source": [
        "data = iter(pytorch_dataloader)\n",
        "images = data.next()\n",
        "\n",
        "# show images\n",
        "#plt.imshow(np.asarray(images[0].numpy().astype(np.uint8)))\n",
        "show_img(images[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3B72uodZF_K"
      },
      "source": [
        "images[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCz2y21ABccp"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)  # 引数の意味は 入力チャンネル（特徴）数，出力チャンネル数, カーネルサイズ，ストライド の 4 つを指定します\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1) # 従って直上の nn.Conv2d() の第2引数と，この行の第一引数が等しい必要があります。\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128) # 9216 の計算だけ面倒です。\n",
        "        # 9216 の心は 入力画像が 28 x 28 で 3 x 3 の畳み込み(ストライド1)を 2 回かけるので，画像は 28x28 -> 26x26 -> 24x24 に小さくなっています。\n",
        "        # そこで 64 チャンネルあるので 24 x 24 x 64 = 36864\n",
        "        # これに 2 x 2 のマックスプーリングをかけるので 1/4 に減ります。\n",
        "        # すなわち 24 x 24 x 64 / 4 = 9216 です\n",
        "        self.fc2 = nn.Linear(128, 10) # 下位層からの入力次元が 128 で出力次元が 10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_tvKbimYg1s"
      },
      "source": [
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQbn0LcXYadL"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwYlpEuRYrXW"
      },
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(pytorch_dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        #inputs, labels = data\n",
        "        inputs = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq9UZrajNCKM"
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "for i, data in enumerate(pytorch_dataloader):\n",
        "    images = data\n",
        "    outputs = net(images)\n",
        "end = time.time()\n",
        "time_spent = (end-start)/60\n",
        "print(f\"{time_spent:.3} minutes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfZMBF6dNGb6"
      },
      "source": [
        "import math\n",
        "\n",
        "weights = torch.randn(784, 10) / math.sqrt(784)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, requires_grad=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV10XVT8dCSc"
      },
      "source": [
        "def log_softmax(x):\n",
        "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "\n",
        "def model(xb):\n",
        "    return log_softmax(xb @ weights + bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "545xDNvvdF70"
      },
      "source": [
        "bs = 64  # batch size\n",
        "\n",
        "xb = Xkm_train[0:bs]  # a mini-batch from x\n",
        "preds = model(xb)  # predictions\n",
        "preds[0], preds.shape\n",
        "print(preds[0], preds.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYkglrlbdKYK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}