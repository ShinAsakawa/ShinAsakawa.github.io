{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020SightVisit_kmnist_prototype.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMv0ng8WdniR2+jDrzrXzUq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/notebooks/2020SightVisit_kmnist_prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEy880CvA3to"
      },
      "source": [
        "# 資格スクエア G 検定対策ビデオ教材 の kmninst プロトタイプ\n",
        "\n",
        "- filename: `2020SightVisit_kmnist_prototype.ipynb`\n",
        "- author: 浅川伸一\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrsTNwHoAsUs"
      },
      "source": [
        "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz\n",
        "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz\n",
        "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz\n",
        "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omBDQrA6Do7n"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def load(f):\n",
        "    return np.load(f)['arr_0']\n",
        "\n",
        "# Load the data\n",
        "Xkm_train = load('kmnist-train-imgs.npz')\n",
        "Xkm_test = load('kmnist-test-imgs.npz')\n",
        "ykm_train = load('kmnist-train-labels.npz')\n",
        "ykm_test = load('kmnist-test-labels.npz')\n",
        "\n",
        "# Flatten images\n",
        "n_samples = 2000\n",
        "#x_train = x_train.reshape(-1, 784)[:n_samples]\n",
        "#y_train = y_train[:n_samples]\n",
        "#x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "ind2c =[c for c in 'おきすつなまはやれを']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFcgB-ttE3kb"
      },
      "source": [
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSpncSHhAuVW"
      },
      "source": [
        "# ライブラリの輸入\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "#from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# imports\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "#import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#import torchvision\n",
        "#import time\n",
        "#import albumentations as A\n",
        "#import tarfile\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysB0wAa-BGtV"
      },
      "source": [
        "# define pytorch transforms\n",
        "# source: https://debuggercafe.com/image-augmentation-using-pytorch-and-albumentations/\n",
        "# and https://albumentations.ai/docs/getting_started/image_augmentation/\n",
        "transform = transforms.Compose([\n",
        "     transforms.ToPILImage(),\n",
        "     transforms.Resize((300, 300)),\n",
        "     transforms.CenterCrop((100, 100)),\n",
        "     transforms.RandomCrop((80, 80)),\n",
        "     transforms.RandomHorizontalFlip(p=0.5),\n",
        "     transforms.RandomRotation(degrees=(-90, 90)),\n",
        "     transforms.RandomVerticalFlip(p=0.5),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "     ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UNg7HqzCg4R"
      },
      "source": [
        "# PyTorch image augmentation module\n",
        "class PyTorchImageDataset(Dataset):\n",
        "    def __init__(self, image_list, transforms=None):\n",
        "        self.image_list = image_list\n",
        "        self.transforms = transforms\n",
        "         \n",
        "    def __len__(self):\n",
        "        return (len(self.image_list))\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        image = self.image_list[i]\n",
        "        #image = plt.imread(self.image_list[i])\n",
        "        image = Image.fromarray(image).convert('RGB')        \n",
        "        image = np.asarray(image).astype(np.uint8)\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return torch.tensor(image, dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3p8gHsEG5v1"
      },
      "source": [
        "pytorch_dataset = PyTorchImageDataset(image_list=Xkm_train, transforms=None)\n",
        "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=16, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO9EZrJXGwPo"
      },
      "source": [
        "def show_img(img):\n",
        "    #plt.figure(figsize=(18,15))\n",
        "    # unnormalize\n",
        "    img = img / 2 + 0.5  \n",
        "    npimg = img.numpy()\n",
        "    npimg = np.clip(npimg, 0., 1.)\n",
        "    #plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (0, 1, 2)))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRKkX22qHN2X"
      },
      "source": [
        "data = iter(pytorch_dataloader)\n",
        "images = data.next()\n",
        "\n",
        "# show images\n",
        "#plt.imshow(np.asarray(images[0].numpy().astype(np.uint8)))\n",
        "show_img(images[0:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn-NvPrdF_XY"
      },
      "source": [
        "#Xkm_train.shape\n",
        "plt.imshow(Xkm_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCz2y21ABccp"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)  # input channels, output channels, kernel size, stride\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq9UZrajNCKM"
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "for i, data in enumerate(pytorch_dataloader):\n",
        "    images = data\n",
        "end = time.time()\n",
        "time_spent = (end-start)/60\n",
        "print(f\"{time_spent:.3} minutes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfZMBF6dNGb6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}